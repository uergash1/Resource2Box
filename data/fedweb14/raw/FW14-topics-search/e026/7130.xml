<?xml version='1.0' encoding='UTF-8'?>
<search_results><query id="7130">l1 versus l2 regularization</query><engine status="OK" timestamp="2014-04-26 01:35:18" name="WordPress" id="FW14-e026"/><snippets><snippet id="FW14-e026-7130-01"><link cache="FW14-topics-docs/e026/7130_01.html" timestamp="2014-04-26 01:35:19">http://lingpipe-blog.com/2009/07/30/zou-and-hastie-2005-regularization-and-variable-selection-via-the-elastic-net-prior/</link><title>Zou and Hastie (2005) Regularization and Variable Selection via the Elastic Net [Prior]</title><description>Zou and Hastie (2005) Regularization and Variable Selection via the Elastic Net [Prior]

30 Jul 2009 by breckbaldwin on LingPipe Blog lingpipe-blog.com/2009/07/30/zou-and-hastie-2005-regularization-and-variable-selection-via-the-elastic-net-prior/

…  an "interpolation" between the Laplace (L1, lasso) and Gaussian (L2, ridge) priors: where β is the vector …  later paper with prior variance decreasing on the X-axis versus the fitted coefficient values on the Y-axis, with a line for each …

Topics: LingPipe News, Carp\'s Blog, Reviews</description></snippet><snippet id="FW14-e026-7130-02"><link cache="FW14-topics-docs/e026/7130_02.html" timestamp="2014-04-26 01:35:54">http://aresearch.wordpress.com/2012/02/27/regularization-and-feature-selection-in-lspi-kolter-ng-icml-2009/</link><title>Regularization and Feature Selection in LSPI. Kolter, Ng. ICML 2009</title><description>Regularization and Feature Selection in LSPI. Kolter, Ng. ICML 2009

27 Feb 2012 by Ari Weinstein on Ari Weinstein's Research aresearch.wordpress.com/2012/02/27/regularization-and-feature-selection-in-lspi-kolter-ng-icml-2009/

…  is the paper that utilized L1 regularization for feature selection in LSPI "Our framework differs from …  a regularization term to the regular LSTD equation L2 regularization is easier because it allows for closed-form solutions to the …

Topics: dimension reduction</description></snippet><snippet id="FW14-e026-7130-03"><link cache="FW14-topics-docs/e026/7130_03.html" timestamp="2014-04-26 01:36:11">http://cxwangyi.wordpress.com/2010/09/28/regularizations-for-general-linear-regression/</link><title>Regularizations for General Linear Regression</title><description>Regularizations for General Linear Regression

28 Sep 2010 by cxwangyi on Yi Wang's Tech Notes cxwangyi.wordpress.com/2010/09/28/regularizations-for-general-linear-regression/

…  lasso [Tibshirani, 1996] refers to L1-regularization of linear models: $latex \Omega_\lambda(\vec{w}) = \lambda …  ||\vec{w}_g||_2$, where $latex ||\cdot||_2$ denotes the L2-norm: $latex ||\vec{x}||_2 = \sqrt{\sum x_i^2}$.  So, when all $latex …

Topics: Machine learning</description></snippet><snippet id="FW14-e026-7130-04"><link cache="FW14-topics-docs/e026/7130_04.html" timestamp="2014-04-26 01:36:44">http://ift6266h13.wordpress.com/2013/01/31/l1-and-l2-regularization/</link><title>L1 and L2 regularization</title><description>L1 and L2 regularization

31 Jan 2013 by Xavier Bouthillier on Representation Learning ift6266h13.wordpress.com/2013/01/31/l1-and-l2-regularization/

Q1 L1 and L2 regularization are usually scaled by two different coefficients. In the case of …

Topics: quiz, jan31, regularization, Initialization, #feb4</description></snippet><snippet id="FW14-e026-7130-05"><link cache="FW14-topics-docs/e026/7130_05.html" timestamp="2014-04-26 01:37:21">http://ift6266h13.wordpress.com/2013/02/04/regularization-of-weights-towards-zero/</link><title>Regularization of weights towards zero</title><description>Regularization of weights towards zero

4 Feb 2013 by archambaultv on Representation Learning ift6266h13.wordpress.com/2013/02/04/regularization-of-weights-towards-zero/

Use of L1 or L2 regularization term pushes the weight values toward zero. This is equivalent to saying we have prior knowledge on the density of those weights (Laplace density or Gaussian density). Why should our weight be small and near zero ?

Topics: #feb4, regularization</description></snippet><snippet id="FW14-e026-7130-06"><link cache="FW14-topics-docs/e026/7130_06.html" timestamp="2014-04-26 01:37:57">http://lingpipe-blog.com/2008/03/20/how-fat-is-your-priors-tail/</link><title>How Fat is Your (Prior's) Tail?</title><description>How Fat is Your (Prior's) Tail?

20 Mar 2008 by breckbaldwin on LingPipe Blog lingpipe-blog.com/2008/03/20/how-fat-is-your-priors-tail/

…  The common choices are the Laplace prior (aka L1 regularization, aka double exponential prior, aka the lasso), the Gaussian …  had a chat about regularization and about discriminitive versus generative models. He wasn't happy with Laplace priors being …

Topics: LingPipe News, Carp\'s Blog</description></snippet><snippet id="FW14-e026-7130-07"><link cache="FW14-topics-docs/e026/7130_07.html" timestamp="2014-04-26 01:38:21">http://wjrider.wordpress.com/2013/11/08/12-interesting-things-about-the-1-norm/</link><title>12 interesting things about the 1-norm</title><description>12 interesting things about the 1-norm

8 Nov 2013 by Bill Rider on The Regularized Singularity wjrider.wordpress.com/2013/11/08/12-interesting-things-about-the-1-norm/

…  perhaps far from compressed sensing at first blush. L1 is the sharpest and least convex norm. "When a traveler reaches a …  engineering largely because it is so easy to compute.  L2 is related to energy, which adds to the appeal.  People apply L2 almost …</description></snippet><snippet id="FW14-e026-7130-08"><link cache="FW14-topics-docs/e026/7130_08.html" timestamp="2014-04-26 01:38:43">http://yjlego.wordpress.com/2011/04/13/homotopy-continuation-for-sparse-signal-representation/</link><title>Homotopy Continuation for Sparse Signal Representation</title><description>Homotopy Continuation for Sparse Signal Representation

13 Apr 2011 by Lego on Lego's Blog yjlego.wordpress.com/2011/04/13/homotopy-continuation-for-sparse-signal-representation/

…  $latex \ell_1$-$latex \ell_2$ regularization problem, also known as basis pursuit (BP) is at the core of many …  Compressive Sensing, and Signal Denoising Based on lp-l2 Optimization; Homotopy Continuation Algorithms for l1-l2 …

Topics: $latex \ell_p$-$latex \ell_2$ Optimization</description></snippet><snippet id="FW14-e026-7130-09"><link cache="FW14-topics-docs/e026/7130_09.html" timestamp="2014-04-26 01:39:04">http://ahmadnursaeful13.wordpress.com/2012/10/15/contranstive-analysis-hypotesis/</link><title>CONTRASTIVE ANALYSIS HYPOTESIS</title><description>CONTRASTIVE ANALYSIS HYPOTESIS

15 Oct 2012 by ahmadnursaeful13 on ahmadnursaeful13 ahmadnursaeful13.wordpress.com/2012/10/15/contranstive-analysis-hypotesis/

…  negatife transfer. Dampak dari kebiasaan bahasa pertama (L1) akan menghasilkan positif dan negatif transfer. Pedagogical Assumption of CAH: L1 dan L2 diajarkan dengan proses pembentukkan kebiasaan yang sama. Dua komponen …</description></snippet><snippet id="FW14-e026-7130-10"><link cache="FW14-topics-docs/e026/7130_10.html" timestamp="2014-04-26 01:39:41">http://callierlibrary.wordpress.com/2008/08/08/consonant-identification-in-noise-by-native-and-non-native-listeners-effects-of-local-context/</link><title>Consonant identification in noise by native and non-native listeners: Effects of local context</title><description>Consonant identification in noise by native and non-native listeners: Effects of local context

8 Aug 2008 by Callier Library on COMD News callierlibrary.wordpress.com/2008/08/08/consonant-identification-in-noise-by-native-and-non-native-listeners-effects-of-local-context/

…  in noise is harder in second (L2) than first languages (L1). This could be because noise disrupts speech processing more in L2 than L1, …  Equivalent noise effects for L1 and L2 (Dutch) listeners, versus larger effects for L2 (Spanish) than L1. To explain this, the latter …

Topics: Bilingualism, noise, Speech Recognition</description></snippet></snippets></search_results>