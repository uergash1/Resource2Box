<?xml version='1.0' encoding='UTF-8'?>
<search_results><query id="7106">active learning survey</query><engine status="OK" timestamp="2014-04-19 05:53:49" name="CiteULike" id="FW14-e005"/><snippets><snippet id="FW14-e005-7106-01"><link cache="FW14-topics-docs/e005/7106_01.html" timestamp="2014-04-19 05:53:56">http://www.citeulike.org/user/paice/article/2223489</link><title>Student perceptions of active learning in a large cross-disciplinary classroom</title><description>This investigation reports on a study that assesses how students value active, cooperative, and traditional learning activities within a single large cross-disciplinary class. The study surveyed students' perceived value of a range of teaching techniques (from traditional to cooperative) utilized within a general education class. Students rated the various techniques on an ordinal scale and the values were statistically compared using a mean difference (paired sample) test. The study was replicated four times over four semesters, where class size ranged from ...</description></snippet><snippet id="FW14-e005-7106-02"><link cache="FW14-topics-docs/e005/7106_02.html" timestamp="2014-04-19 05:55:00">http://www.citeulike.org/user/ssouth22/article/6070448</link><title>Integrating web-delivered problem-based learning scenarios to the curriculum</title><description>This article reports on a small-scale research project (n=56) that investigated student educational gain. For the purposes of this study, gain is defined as an increase in the score that students obtain for pre/post intervention tests. Students received authentic exposure to the process via a web-delivered problem-based scenario. The students were randomly allocated to case and control groups. No statistically significant differences in educational gain were recorded between the two groups. However, the research highlights the requirement to fully integrate problem-based ...</description></snippet><snippet id="FW14-e005-7106-03"><link cache="FW14-topics-docs/e005/7106_03.html" timestamp="2014-04-19 05:56:08">http://www.citeulike.org/user/vlachmore/article/6358639</link><title>Active dual supervision: reducing the cost of annotating examples and features</title><description>When faced with the task of building machine learning or NLP models, it is often worthwhile to turn to active learning to obtain human annotations at minimal costs. Traditional active learning schemes query a human for labels of intelligently chosen examples. However, human effort can also be expended in collecting alternative forms of annotations. For example, one may attempt to learn a text classifier by labeling class-indicating words, instead of, or in addition to, documents. Learning from two different kinds of ...</description></snippet><snippet id="FW14-e005-7106-04"><link cache="FW14-topics-docs/e005/7106_04.html" timestamp="2014-04-19 05:57:17">http://www.citeulike.org/user/jdu/article/2189891</link><title>Coarse Sample Complexity Bounds for Active Learning</title><description>We characterize the sample complexity of active learning problems interms of a parameter which takes into account the distribution over theinput space, the specific target hypothesis, and the desired accuracy. ...</description></snippet><snippet id="FW14-e005-7106-05"><link cache="FW14-topics-docs/e005/7106_05.html" timestamp="2014-04-19 05:58:28">http://www.citeulike.org/user/evangelosmilios/article/5734657</link><title>Combining active learning and relevance vector machines for text classification</title><description>Relevance vector machines (RVM) have proven successful in many learning tasks. However, in large applications, they scale poorly. In many settings there is a large amount of unlabeled data which could be actively chosen by a learner and integrated in the learning procedure. The idea is to improve performance meanwhile reducing costs from data categorization. In this paper we propose an active learning RVM method based on the kernel trick. The underpinning idea is to define a working space between the ...</description></snippet><snippet id="FW14-e005-7106-06"><link cache="FW14-topics-docs/e005/7106_06.html" timestamp="2014-04-19 05:59:30">http://www.citeulike.org/group/3961/article/844964</link><title>Agnostic active learning</title><description>We state and analyze the first active learning algorithm which works in the presence of arbitrary forms of noise. The algorithm, A 2 (for Agnostic Active), relies only upon the assumption that the samples are drawn i.i.d . from a fixed distribution. We show that A 2 achieves an exponential improvement (i.e., requires only O (ln 1/ε) samples to find an ε-optimal classifier) over the usual sample complexity of supervised learning, for several settings ...</description></snippet><snippet id="FW14-e005-7106-07"><link cache="FW14-topics-docs/e005/7106_07.html" timestamp="2014-04-19 06:00:32">http://www.citeulike.org/user/wkiri/article/1557458</link><title>Online choice of active learning algorithms</title><description>This paper is concerned with the question of how to online combine an ensemble of active learners so as to expedite the learning progress during a pool-based active learning session. We develop a powerful active learning master algorithm, based a known competitive algorithm for the multi-armed bandit problem and a novel semi-supervised performance evaluation statistic. Taking an ensemble containing two of the best known active learning algorithms and a new algorithm, the resulting new active... ...</description></snippet><snippet id="FW14-e005-7106-08"><link cache="FW14-topics-docs/e005/7106_08.html" timestamp="2014-04-19 06:01:32">http://www.citeulike.org/user/jdu/article/3561663</link><title>Dual Strategy Active Learning</title><description>Active Learning methods rely on static strategies for sampling unlabeled point(s). These strategies range from uncertainty sampling and density estimation to multi-factor methods with learn-once-use-always model parameters. This paper proposes a dynamic approach, called DUAL, where the strategy selection parameters are adaptively updated based on estimated future residual error reduction after each actively sampled point. The objective of dual is to outperform static strategies over a large operating range: from very few to very many labeled points. Empirical results over six ...</description></snippet><snippet id="FW14-e005-7106-09"><link cache="FW14-topics-docs/e005/7106_09.html" timestamp="2014-04-19 06:02:38">http://www.citeulike.org/user/weiweiguo/article/3353324</link><title>Support vector machine active learning with applications to text classification</title><description>Support vector machines have met with signicant success in numerous real-world learning tasks. However, like most machine learning algorithms, they are generally applied using a randomly selected training set classied in advance. In many settings, we also have the option of using pool-based active learning. Instead of using a randomly selected training set, the learner has access to a pool of unlabeled instances and can request the labels for some number of them. We introduce a new algorithm for performing active ...</description></snippet><snippet id="FW14-e005-7106-10"><link cache="FW14-topics-docs/e005/7106_10.html" timestamp="2014-04-19 06:03:50">http://www.citeulike.org/user/weiweiguo/article/3979127</link><title>Support Vector Machine Active Learning with Applications to Text Classification</title><description>. Support vector machines have met with significant success in numerous real-world learning tasks. However, like most machine learning algorithms, they are generally applied using a randomly selected training set classified in advance. In many settings, we also have the option of using pool-based active learning. Instead of using a randomly selected training set, the learner has access to a pool of unlabeled instances and can request the labels for some number of them. We introduce a new algorithm for performing ...</description></snippet></snippets></search_results>