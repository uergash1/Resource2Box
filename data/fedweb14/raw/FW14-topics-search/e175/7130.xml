<?xml version='1.0' encoding='UTF-8'?>
<search_results><query id="7130">l1 versus l2 regularization</query><engine status="OK" timestamp="2014-04-25 08:22:41" name="AOL Video" id="FW14-e175"/><snippets><snippet id="FW14-e175-7130-01"><link cache="FW14-topics-docs/e175/7130_01.pdf" timestamp="2014-04-25 08:23:32">http://cs.nyu.edu/%7Erostami/presentations/L1_vs_L2.pdf</link><title>L1 vs. L2 Regularization and feature selection.</title><description>Main Topics. Covering Numbers. Definition. Convergence Bounds. L1 regularized logistic regression. L1 Regression Convergence Upper Bound. Rotational ...</description></snippet><snippet id="FW14-e175-7130-02"><link cache="FW14-topics-docs/e175/7130_02.pdf" timestamp="2014-04-25 08:24:05">http://ai.stanford.edu/%7Eang/papers/icml04-l1l2.pdf</link><title>Feature selection, L1 vs. L2 regularization, and ... \- ...</title><description></description></snippet><snippet id="FW14-e175-7130-03"><link cache="FW14-topics-docs/e175/7130_03.html" timestamp="2014-04-25 08:24:38">http://www.quora.com/Machine-Learning/What-is-the-difference-between-L1-and-L2-regularization</link><title>What is the difference between L1 and L2 regularization? - Quora</title><description>Answer 1 of 11: Practically, I think the biggest reasons for regularization are 1) to avoid overfitting by not generating high coefficients for predictors th...</description></snippet><snippet id="FW14-e175-7130-04"><link cache="FW14-topics-docs/e175/7130_04.html" timestamp="2014-04-25 08:34:36">http://metaoptimize.com/qa/questions/5205/when-to-use-l1-regularization-and-when-l2</link><title>When to use L1 regularization and when L2? - MetaOptimize Q+A</title><description>As per Andrew Ng's Feature selection, l1 vs l2 regularization, and rotational invariance paper, expect l1 regularization be better than l2 regularization if you have ...</description></snippet><snippet id="FW14-e175-7130-05"><link cache="FW14-topics-docs/e175/7130_05.html" timestamp="2014-04-25 08:35:18">http://en.wikipedia.org/wiki/Regularization_%28mathematics%29</link><title>Regularization (mathematics) - Wikipedia, the free encyclopedia</title><description>Regularization, in mathematics and statistics and particularly in the fields of .... " Stochastic gradient descent training for l1-regularized log-linear models with ...</description></snippet><snippet id="FW14-e175-7130-06"><link cache="FW14-topics-docs/e175/7130_06.html" timestamp="2014-04-25 08:36:00">http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/</link><title>Differences between L1 and L2 as Loss Function and ...</title><description>Dec 18, 2013 ... While practicing machine learning, you may have come upon a choice of the mysterious L1 vs L2. Usually the two decisions are : 1) L1-norm vs ...</description></snippet><snippet id="FW14-e175-7130-07"><link cache="FW14-topics-docs/e175/7130_07.pdf" timestamp="2014-04-25 08:36:43">http://cseweb.ucsd.edu/%7Eelkan/254spring05/Hammon.pdf</link><title>Ng feature selection talk</title><description>1\. Feature selection, L1 vs. L2 regularization, and rotational invariance. Andrew Ng. ICML 2004. Presented by Paul Hammon. April 14, 2005. 2. Outline. 1.</description></snippet><snippet id="FW14-e175-7130-08"><link cache="FW14-topics-docs/e175/7130_08.pdf" timestamp="2014-04-25 08:37:20">http://cseweb.ucsd.edu/%7Esaul/teaching/cse291s07/L1norm.pdf</link><title>L1-norm Regularization Overview</title><description>L2-Regularization. L1-Regularization. Proposed Algorithm basics. Summary ... the vector of unknowns, and v ∈ Rm is the noise and A ∈ Rmxn , we wish to find  ...</description></snippet><snippet id="FW14-e175-7130-09"><link cache="FW14-topics-docs/e175/7130_09.pdf" timestamp="2014-04-25 08:37:48">http://www.cs.berkeley.edu/%7Erussell/classes/cs194/f11/lectures/CS194%20Fall%202011%20Lecture%2004.pdf</link><title>Machine learning methodology: Overfitting, regular...</title><description>L2 regularization: complexity = sum of squares of weights. Combine with ... L2 Regularization Solution .... L1 regularization leads to many zero weights (sparsity ).</description></snippet><snippet id="FW14-e175-7130-10"><link cache="FW14-topics-docs/e175/7130_10.html" timestamp="2014-04-25 08:38:24">http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.81.145</link><title>Feature selection, l1 vs. l2 regularization, and rotational ...</title><description>We also give a lowerbound showing that any rotationally invariant algorithm— including logistic regression with L2 regularization, SVMs, and neural networks ...</description></snippet></snippets></search_results>