<?xml version='1.0' encoding='UTF-8'?>
<search_results><query id="7130">l1 versus l2 regularization</query><engine status="OK" timestamp="2014-04-21 19:42:04" name="CMU ClueWeb" id="FW14-e055"/><snippets><snippet id="FW14-e055-7130-01"><link cache="FW14-topics-docs/e055/7130_01.html" timestamp="2014-04-21 19:42:41">http://scikit-learn.sourceforge.net/dev/modules/sgd.html</link><title>3.3. Stochastic Gradient Descent — scikit-learn v0.11-git documentation</title><description>* 3.3. Stochastic Gradient Descent — scikit-learn v0.11-git documentation (clueweb12-1210wb-88-01332) http://scikit-learn.sourceforge.net/dev/modules/sgd.html ...concrete penalty can be set via the penalty parameter. SGD supports the following penalties: penalty=”l2”: L2 norm penalty on coef_. penalty=”l1”: L1 norm penalty on coef_. penalty=”elasticnet”: Convex combination of L2 and L1; rho * L2 + (1 - rho) * L1. The default setting is penalty=”l2”. The L1...misclassification error (Zero-one loss) as shown in the Figure below. Popular choices for the regularization term include: L2 norm: , L1 norm: , which leads to sparse solutions. Elastic Net: , a convex combination of L2 and L1. The Figure below shows the contours of the different regularization terms in the parameter space when . 3.3.6.1. SGD¶ Stochastic gradient descent is an optimization... [cached version]</description></snippet><snippet id="FW14-e055-7130-02"><link cache="FW14-topics-docs/e055/7130_02.html" timestamp="2014-04-21 19:45:09">http://scikit-learn.sourceforge.net/stable/modules/sgd.html</link><title>3.3. Stochastic Gradient Descent — scikit-learn 0.10 documentation</title><description>* 3.3. Stochastic Gradient Descent — scikit-learn 0.10 documentation (clueweb12-1214wb-18-21062) http://scikit-learn.sourceforge.net/stable/modules/sgd.html ...concrete penalty can be set via the penalty parameter. SGD supports the following penalties: penalty=”l2”: L2 norm penalty on coef_. penalty=”l1”: L1 norm penalty on coef_. penalty=”elasticnet”: Convex combination of L2 and L1; rho * L2 + (1 - rho) * L1. The default setting is penalty=”l2”. The L1...misclassification error (Zero-one loss) as shown in the Figure below. Popular choices for the regularization term include: L2 norm: , L1 norm: , which leads to sparse solutions. Elastic Net: , a convex combination of L2 and L1. The Figure below shows the contours of the different regularization terms in the parameter space when . 3.3.6.1. SGD¶ Stochastic gradient descent is an optimization... [cached version]</description></snippet><snippet id="FW14-e055-7130-03"><link cache="FW14-topics-docs/e055/7130_03.html" timestamp="2014-04-21 19:46:08">http://lingpipe-blog.com/2009/07/30/zou-and-hastie-2005-regularization-and-variable-selection-via-the-elastic-net-prior/</link><title>Zou and Hastie (2005) Regularization and Variable Selection via the Elastic Net [Prior] « LingPipe Blog</title><description>* Zou and Hastie (2005) Regularization and Variable Selection via the Elastic Net [Prior] « LingPipe Blog (clueweb12-1202wb-28-20627) http://lingpipe-blog.com/2009/07/30/zou-and-hastie-2005-regularization-and-variable-selection-via-the-elastic-net-prior/ ...the documentation to their glmnet R package: Friedman, Jerome, Trevor Hastie, and Rob Tibshirani. 2009. Regularized Paths for Generalized Linear Models via Coordinate Descent. CRAN documentation for glmnet. The idea’s basically an “interpolation” between the Laplace (L1, lasso) and Gaussian (L2, ridge) priors: where β is the vector coefficients, λ a factor for prior variance, and α the blending ratio between L2 and L1 priors, themselves represented by the squared L2 norm and (unsquared) L1 norm terms. Note that... [cached version]</description></snippet><snippet id="FW14-e055-7130-04"><link cache="FW14-topics-docs/e055/7130_04.html" timestamp="2014-04-21 19:48:25">http://mdp-toolkit.sourceforge.net/api/mdp.nodes.SGDClassifierScikitsLearnNode-class.html</link><title>mdp.nodes.SGDClassifierScikitsLearnNode</title><description>* mdp.nodes.SGDClassifierScikitsLearnNode (clueweb12-1211wb-43-07935) http://mdp-toolkit.sourceforge.net/api/mdp.nodes.SGDClassifierScikitsLearnNode-class.html ...Class SGDClassifierScikitsLearnNode regularized empirical loss with SGD. This node has been automatically generated by wrapping the ``sklearn.linear...model is updated along the way with a decreasing strength schedule (aka learning rate). The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve... [cached version]</description></snippet><snippet id="FW14-e055-7130-05"><link cache="FW14-topics-docs/e055/7130_05.html" timestamp="2014-04-21 19:49:08">http://www.cns.atr.jp/~oyamashi/SLR_WEB.html</link><title>SLR toolbox Web Page</title><description>* SLR toolbox Web Page (clueweb12-1601wb-60-17305) http://www.cns.atr.jp/~oyamashi/SLR_WEB.html ...classifiers (seven algorithms) SLR (sparse, linear boundary) L2-regularized LR (non-sparse, linear boundary) Relevance vector machine (non-sparse in feature space, linear or non-linear boundary) L1-regularized LR (sparse, linear boundary)  Four types of multi-class classifiers (six algorithms) Sparse multinomial LR (sparse, linear boundary) L2-regularized LR (non-sparse, linear boundary) SLR one-versus-the-rest (sparse, linear boundary) SLR one... [cached version]</description></snippet><snippet id="FW14-e055-7130-06"><link cache="FW14-topics-docs/e055/7130_06.html" timestamp="2014-04-21 19:52:53">http://www.cns.atr.jp/~oyamashi/SLR_WEB.html</link><title>SLR toolbox Web Page</title><description>* SLR toolbox Web Page (clueweb12-0500wb-13-21672) http://www.cns.atr.jp/~oyamashi/SLR_WEB.html ...classifiers (seven algorithms) SLR (sparse, linear boundary) L2-regularized LR (non-sparse, linear boundary) Relevance vector machine (non-sparse in feature space, linear or non-linear boundary) L1-regularized LR (sparse, linear boundary)  Four types of multi-class classifiers (six algorithms) Sparse multinomial LR (sparse, linear boundary) L2-regularized LR (non-sparse, linear boundary) SLR one-versus-the-rest (sparse, linear boundary) SLR one... [cached version]</description></snippet><snippet id="FW14-e055-7130-07"><link cache="FW14-topics-docs/e055/7130_07.html" timestamp="2014-04-21 19:55:17">http://metaoptimize.com/qa/questions/5205/when-to-use-l1-regularization-and-when-l2</link><title>When to use L1 regularization and when L2? - MetaOptimize Q+A</title><description>* When to use L1 regularization and when L2? - MetaOptimize Q+A (clueweb12-1412wb-96-04190) http://metaoptimize.com/qa/questions/5205/when-to-use-l1-regularization-and-when-l2 When to use L1 regularization and when L2? - MetaOptimize Q+A L1 regularization and when L2? 9 2 I know what L1 and L2 mean, it is different representation of regularization term in many learning schemes including SVM/Logistic Regression/CRF etc. How can I predict... [cached version]</description></snippet><snippet id="FW14-e055-7130-08"><link cache="FW14-topics-docs/e055/7130_08.html" timestamp="2014-04-21 19:56:20">http://lingpipe-blog.com/2008/03/20/how-fat-is-your-priors-tail/</link><title>How Fat is Your (Prior’s) Tail? « LingPipe Blog</title><description>* How Fat is Your (Prior’s) Tail? « LingPipe Blog (clueweb12-1200wb-46-21506) http://lingpipe-blog.com/2008/03/20/how-fat-is-your-priors-tail/ ...The common choices are the Laplace prior (aka L1 regularization, aka double exponential prior, aka the lasso), the Gaussian prior (aka L2 regularization, aka normal, aka ridge regression), and more recently...logistic regression for text categorization, which covers both L1 and L2 regularization, which also comes with an open-source implementation for the multinomial case. This paper does a good job of comparing regularized logistic regression to SVM baselines. Dave Lewis has... [cached version]</description></snippet><snippet id="FW14-e055-7130-09"><link cache="FW14-topics-docs/e055/7130_09.html" timestamp="2014-04-21 19:59:02">http://metaoptimize.com/qa/tags/regularization/</link><title>Questions Tagged With regularization - MetaOptimize Q+A</title><description>* Questions Tagged With regularization - MetaOptimize Q+A (clueweb12-1411wb-77-26272) http://metaoptimize.com/qa/tags/regularization/ ...30 at 17:11 Alexandre Passos ♦18357 (asked by downer488 ) linearclassifiers loss-function regularization 0 votes 1 answers 339 views Compare the sparsity performance of L1 regularization and the Bayesian model with Dirichlet distribution Feb 26 at 08:24 lizhonghua6 regularization dirichlet distribution l1 0 votes 1 answers 401 views Why the L2 regularization uses different regularization weights and regularization biases? Feb 19 at 08:45 Alexandre Passos ♦18357 (asked by lizhonghua6 ) regularization... [cached version]</description></snippet><snippet id="FW14-e055-7130-10"><link cache="FW14-topics-docs/e055/7130_10.html" timestamp="2014-04-21 20:01:48">http://metaoptimize.com/qa/questions/6110/are-there-any-requirements-for-l1-regularization</link><title>Are there any requirements for L1 regularization? - MetaOptimize Q+A</title><description>* Are there any requirements for L1 regularization? - MetaOptimize Q+A (clueweb12-1412wb-75-15415) http://metaoptimize.com/qa/questions/6110/are-there-any-requirements-for-l1-regularization Are there any requirements for L1 regularization? - MetaOptimize Q+A ...31 '11 at 04:49) Leon Palafox It should be no problem to add the L1 loss to the fitness function. Whether this will be better than L2 loss depends on the problem itself. See: this post (May 31 '11 at 05:13) Philemon Brakel 0 In order to make a good choice for regularization you need to have a reasonably decent idea of what's causing problems. L1 and L2... [cached version]</description></snippet></snippets></search_results>