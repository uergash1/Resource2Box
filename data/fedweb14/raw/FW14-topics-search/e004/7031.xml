<?xml version='1.0' encoding='UTF-8'?>
<search_results><query id="7031">Thomas test</query><engine status="OK" timestamp="2014-04-18 18:27:19" name="CiteSeerX" id="FW14-e004"/><snippets><snippet id="FW14-e004-7031-01"><link cache="FW14-topics-docs/e004/7031_01.html" timestamp="2014-04-18 18:27:27">http://citeseerx.ist.psu.edu/viewdoc/summary;jsessionid=5C49EFB90FE37298B94AA71E773E1E9B?doi=10.1.1.129.2536&amp;rank=1</link><title>Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithms</title><description>Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithms

by Thomas G. Dietterich , 1998

"... and enough data for a sep- 1898 Thomas G. Dietterich arate test set, determine which classifier will be more ..."

Abstract \- Cited by 531 (8 self) \- Add to MetaCart

This article reviews five approximate statistical &lt;em&gt;tests&lt;/em&gt; for determining whether one learning</description></snippet><snippet id="FW14-e004-7031-02"><link cache="FW14-topics-docs/e004/7031_02.html" timestamp="2014-04-18 18:28:32">http://citeseerx.ist.psu.edu/viewdoc/summary;jsessionid=5C49EFB90FE37298B94AA71E773E1E9B?doi=10.1.1.20.4565&amp;rank=2</link><title>Applications of Latent Semantic Analysis</title><description>Applications of Latent Semantic Analysis

by Thomas Landauer Landauer

"... statistical language modeling methods increases its practical scope. A variety of tests and applications ..."

Abstract \- Cited by 655 (13 self) \- Add to MetaCart

). New &lt;em&gt;Tests&lt;/em&gt;, Advances and Applications LSA now scales to ca. 100 million word corpora by larger</description></snippet><snippet id="FW14-e004-7031-03"><link cache="FW14-topics-docs/e004/7031_03.html" timestamp="2014-04-18 18:29:39">http://citeseerx.ist.psu.edu/viewdoc/summary;jsessionid=5C49EFB90FE37298B94AA71E773E1E9B?doi=10.1.1.117.338&amp;rank=3</link><title>Speaker verification using Adapted Gaussian mixture models</title><description>Speaker verification using Adapted Gaussian mixture models

by Douglas A. Reynolds, Thomas F. Quatieri, Robert B. Dunn \- Digital Signal Processing , 2000

"... ://www.idealibrary.com on Speaker Verification Using Adapted Gaussian Mixture Models 1 Douglas A. Reynolds, Thomas F. Quatieri ..."

Abstract \- Cited by 616 (38 self) \- Add to MetaCart

Evaluations (SREs). The system is built around the likelihood ratio &lt;em&gt;test&lt;/em&gt; for verification, using simple</description></snippet><snippet id="FW14-e004-7031-04"><link cache="FW14-topics-docs/e004/7031_04.html" timestamp="2014-04-18 18:30:50">http://citeseerx.ist.psu.edu/viewdoc/summary;jsessionid=5C49EFB90FE37298B94AA71E773E1E9B?doi=10.1.1.14.1729&amp;rank=4</link><title>Distributional Clustering Of English Words</title><description>Distributional Clustering Of English Words

by Fernando Pereira, Naftali Tishby, Lillian Lee \- In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics , 1993

"... coocurrence, and the models evaluated with respect to held-out test data. INTRODUCTION Methods ..."

Abstract \- Cited by 549 (28 self) \- Add to MetaCart

of word coocurrence, and the models evaluated with respect to held-out &lt;em&gt;test&lt;/em&gt; data.</description></snippet><snippet id="FW14-e004-7031-05"><link cache="FW14-topics-docs/e004/7031_05.html" timestamp="2014-04-18 18:31:58">http://citeseerx.ist.psu.edu/viewdoc/summary;jsessionid=5C49EFB90FE37298B94AA71E773E1E9B?doi=10.1.1.39.912&amp;rank=5</link><title>Estimating the Support of a High-Dimensional Distribution</title><description>Estimating the Support of a High-Dimensional Distribution

by Bernhard Schölkopf, John C. Platt, John Shawe-taylor, Alex J. Smola, Robert C. Williamson , 1999

"... such that the probability that a test point drawn from P lies outside of S is bounded by some a priori specified between 0 ..."

Abstract \- Cited by 501 (32 self) \- Add to MetaCart

want to estimate a "simple" subset S of input space such that the probability that a &lt;em&gt;test&lt;/em&gt; point drawn</description></snippet><snippet id="FW14-e004-7031-06"><link cache="FW14-topics-docs/e004/7031_06.html" timestamp="2014-04-18 18:33:08">http://citeseerx.ist.psu.edu/viewdoc/summary;jsessionid=5C49EFB90FE37298B94AA71E773E1E9B?doi=10.1.1.108.8490&amp;rank=6</link><title>Indexing by latent semantic analysis</title><description>Indexing by latent semantic analysis

by Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, Richard Harshman \- JOURNAL OF THE AMERICAN SOCIETY FOR INFORMATION SCIENCE , 1990

"... , University of Chicago, Chicago, IL 60637 Susan T. Dumais*, George W. Furnas, and Thomas K. Landauer Bell ..."

Abstract \- Cited by 2703 (32 self) \- Add to MetaCart

-threshold cosine values are re-turned. initial &lt;em&gt;tests&lt;/em&gt; find this completely automatic method for retrieval</description></snippet><snippet id="FW14-e004-7031-07"><link cache="FW14-topics-docs/e004/7031_07.html" timestamp="2014-04-18 18:34:18">http://citeseerx.ist.psu.edu/viewdoc/summary;jsessionid=5C49EFB90FE37298B94AA71E773E1E9B?doi=10.1.1.1.4458&amp;rank=7</link><title>Probabilistic Latent Semantic Indexing</title><description>Probabilistic Latent Semantic Indexing

by Thomas Hofmann , 1999

"... proper generative data model. Retrieval experiments on a number of test collections indicate substantial ..."

Abstract \- Cited by 784 (8 self) \- Add to MetaCart

number of &lt;em&gt;test&lt;/em&gt; collections indicate substantial performance gains over direct term matching methodsaswell</description></snippet><snippet id="FW14-e004-7031-08"><link cache="FW14-topics-docs/e004/7031_08.html" timestamp="2014-04-18 18:35:28">http://citeseerx.ist.psu.edu/viewdoc/summary;jsessionid=5C49EFB90FE37298B94AA71E773E1E9B?doi=10.1.1.184.4759&amp;rank=8</link><title>A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge</title><description>A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge

by Thomas K Landauer, Susan T. Dutnais \- Psychological review , 1997

"... of Acquisition, Induction, and Representation of Knowledge Thomas K Landauer University of Colorado at Boulder ..."

Abstract \- Cited by 1093 (9 self) \- Add to MetaCart

How do people know as much as they do with as little information as they get? The problem takes many forms; learning vocabulary from text is an especially dramatic and convenient case for research. A new general theory of acquired similarity and knowledge representation, latent semantic analysis (LSA), is presented and used to successfully simulate such learning and several other psycholinguistic phenomena. By inducing global knowledge indirectly from local co-occurrence data in a large body of representative text, LSA acquired knowledge about the full vocabulary of English at a comparable rate to schoolchildren. LSA uses no prior linguistic or perceptual similarity knowledge; it is based solely on a general mathematical learning method that achieves powerful inductive effects by extracting the right number of dimensions (e.g., 300) to represent objects and contexts. Relations to other theories, phenomena, and problems are sketched. Prologue "How much do we know at any time? Much more, or so I believe, than we know we know!" —Agatha Christie, The Moving Finger A typical American seventh grader knows the meaning of</description></snippet><snippet id="FW14-e004-7031-09"><link cache="FW14-topics-docs/e004/7031_09.html" timestamp="2014-04-18 18:36:38">http://citeseerx.ist.psu.edu/viewdoc/summary;jsessionid=5C49EFB90FE37298B94AA71E773E1E9B?doi=10.1.1.111.5804&amp;rank=9</link><title>Introduction to Algorithms, second edition</title><description>Introduction to Algorithms, second edition

by Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, Clifford Stein \- BOOK , 2001

"... ﻿Introduction to Algorithms Second Edition Thomas H. Cormen Charles E. Leiserson Ronald L. Rivest ..."

Abstract \- Cited by 707 (3 self) \- Add to MetaCart

This part will get you started in thinking about designing and analyzing algorithms. It is intended to be a gentle introduction to how we specify algorithms, some of the design strategies we will use throughout this book, and many of the fundamental ideas used in algorithm analysis. Later parts of this book will build upon this base. Chapter 1 is an overview of algorithms and their place in modern computing systems. This chapter defines what an algorithm is and lists some examples. It also makes a case that algorithms are a technology, just as are fast hardware, graphical user interfaces, object-oriented systems, and networks. In Chapter 2, we see our first algorithms, which solve the problem of sorting a sequence of n numbers. They are written in a pseudocode which, although not directly translatable to any conventional programming language, conveys the structure of the algorithm clearly enough that a competent programmer can implement it in the language of his choice. The sorting algorithms we examine are insertion sort, which uses an incremental approach, and merge sort, which uses a recursive technique known as “divide and conquer.” Although the time each requires increases with the value of n, the rate of increase differs between the two algorithms. We determine these running times in Chapter 2, and we develop a useful notation to express them. Chapter 3 precisely defines this notation, which we call asymptotic notation. It starts by defining several asymptotic notations, which we use for bounding algorithm running times from above and/or below. The rest of Chapter 3 is primarily a presentation of mathematical notation. Its purpose is more to ensure that your use of notation matches that in this book than to teach you new mathematical concepts.</description></snippet><snippet id="FW14-e004-7031-10"><link cache="FW14-topics-docs/e004/7031_10.html" timestamp="2014-04-18 18:37:34">http://citeseerx.ist.psu.edu/viewdoc/summary;jsessionid=5C49EFB90FE37298B94AA71E773E1E9B?doi=10.1.1.46.1529&amp;rank=10</link><title>A comparison of event models for Naive Bayes text classification</title><description>A comparison of event models for Naive Bayes text classification

by Andrew McCallum, Kamal Nigam , 1998

"... , equipped with these estimates, it classifies new test documents using Bayes' rule to turn the generative ..."

Abstract \- Cited by 753 (26 self) \- Add to MetaCart

Recent work in text classification has used two different first-order probabilistic models for classification, both of which make the naive Bayes assumption. Some use a multi-variate Bernoulli model, that is, a Bayesian Network with no dependencies between words and binary word features (e.g. Larkey and Croft 1996; Koller and Sahami 1997). Others use a multinomial model, that is, a uni-gram language model with integer word counts (e.g. Lewis and Gale 1994; Mitchell 1997). This paper aims to clarify the confusion by describing the differences and details of these two models, and by empirically comparing their classification performance on five text corpora. We find that the multi-variate Bernoulli performs well with small vocabulary sizes, but that the multinomial performs usually performs even better at larger vocabulary sizes---providing on average a 27% reduction in error over the multi-variate Bernoulli model at any vocabulary size.</description></snippet></snippets></search_results>