<?xml version='1.0' encoding='UTF-8'?>
<search_results><query id="7211">punctuation guide</query><engine status="OK" timestamp="2014-05-21 18:40:09" name="CiteSeerX" id="FW14-e004"/><snippets><snippet id="FW14-e004-7211-01"><link cache="FW14-topics-docs/e004/7211_01.html" timestamp="2014-05-21 18:40:11">http://citeseerx.ist.psu.edu/viewdoc/summary;jsessionid=AFD5FC4B8EB7B278CC7D7220DDF8CAC0?doi=10.1.1.416.3677&amp;rank=1</link><title>Head-Driven Statistical Models for Natural Language Parsing</title><description>Head-Driven Statistical Models for Natural Language Parsing

by Michael Collins, Michael Collins , 1999

"... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 1.6.1 Reader's Guide . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 2 Statistical ..."

Abstract \- Cited by 955 (16 self) \- Add to MetaCart

Mitch Marcus was a wonderful advisor. He gave consistently good advice, and allowed an ideal level of intellectual freedom in pursuing ideas and research topics. I would like to thank the members of my thesis committee |Aravind Joshi, Mark Liberman, Fernando Pereira and Mark Steedman | for the remarkable breadth and depth of their feedback. I had countless impromptu but in uential discussions with Jason Eisner, Dan Melamed and Adwait Ratnaparkhi in the LINC lab. They also provided feedback on many drafts of papers and thesis chapters. Paola Merlo pushed me to think about many new angles of the research. Dimitrios Samaras gave invaluable feedback on many portions of the work. Thanks to James Brooks for his contribution to the work that comprises chapter 5 of this thesis. The community of faculty, students and visitors involved with the Institute for Research in Cognitive Science at Penn provided an intensely varied and stimulating environment. I would like to thank them collectively. Some deserve special mention for discussions that contributed quite directly to this research: Breck Baldwin, Srinivas Bangalore, Dan</description></snippet><snippet id="FW14-e004-7211-02"><link cache="FW14-topics-docs/e004/7211_02.html" timestamp="2014-05-21 18:40:27">http://citeseerx.ist.psu.edu/viewdoc/summary;jsessionid=AFD5FC4B8EB7B278CC7D7220DDF8CAC0?doi=10.1.1.16.3103&amp;rank=2</link><title>A Sequential Algorithm for Training Text Classifiers</title><description>A Sequential Algorithm for Training Text Classifiers

by David D. Lewis, William A. Gale , 1994

"... and a test set of 51,991 titles. Titles were processed by lower casing text and removing punctuation ..."

Abstract \- Cited by 470 (9 self) \- Add to MetaCart

The ability to cheaply train text classifiers is critical to their use in information retrieval, content analysis, natural language processing, and other tasks involving data which is partly or fully textual. An algorithm for sequential sampling during machine learning of statistical classifiers was developed and tested on a newswire text categorization task. This method, which we call uncertainty sampling, reduced by as much as 500-fold the amount of training data that would have to be manually classified to achieve a given level of effectiveness. 1 Introduction Text classification is the automated grouping of textual or partially textual entities. Document retrieval, categorization, routing, filtering, and clustering, as well as natural language processing tasks such as tagging, word sense disambiguation, and some aspects of understanding can be formulated as text classification. As the amount of online text increases, the demand for text classification to aid the analysis and mana...</description></snippet><snippet id="FW14-e004-7211-03"><link cache="FW14-topics-docs/e004/7211_03.html" timestamp="2014-05-21 18:40:44">http://citeseerx.ist.psu.edu/viewdoc/summary;jsessionid=AFD5FC4B8EB7B278CC7D7220DDF8CAC0?doi=10.1.1.17.4650&amp;rank=3</link><title>Generic Schema Matching with Cupid</title><description>Generic Schema Matching with Cupid

by Jayant Madhavan, Philip Bernstein, Erhard Rahm \- In The VLDB Journal , 2001

"... best mapping ill-defined). This subjectivity makes it valuable to have user input to guide the match ..."

Abstract \- Cited by 458 (17 self) \- Add to MetaCart

Schema matching is a critical step in many applications, such as XML message mapping, data warehouse loading, and schema integration. In this paper, we investigate algorithms for generic schema matching, outside of any particular data model or application. We first present a taxonomy for past solutions, showing that a rich range of techniques is available. We then propose a new algorithm, Cupid, that discovers mappings between schema elements based on their names, data types, constraints, and schema structure, using a broader set of techniques than past approaches. Some of our innovations are the integrated use of linguistic and structural matching, context-dependent matching of shared types, and a bias toward leaf structure where much of the schema content resides. After describing our algorithm, we present experimental results that compare Cupid to two other schema matching systems.</description></snippet><snippet id="FW14-e004-7211-04"><link cache="FW14-topics-docs/e004/7211_04.html" timestamp="2014-05-21 18:41:04">http://citeseerx.ist.psu.edu/viewdoc/summary;jsessionid=AFD5FC4B8EB7B278CC7D7220DDF8CAC0?doi=10.1.1.41.1161&amp;rank=4</link><title>A Trainable Document Summarizer</title><description>A Trainable Document Summarizer

by Julian Kupiec, Jan Pedersen, Francine Chen , 1995

"... that the optimal extract can be far from unique. Numerous heuristics have been proposed to guide the selection ..."

Abstract \- Cited by 403 (2 self) \- Add to MetaCart

To summarize is to reduce in complexity, and hence in length, while retaining some of the essential qualities of the original. This paper focusses on document extracts, a particular kind of computed document summary.</description></snippet><snippet id="FW14-e004-7211-05"><link cache="FW14-topics-docs/e004/7211_05.html" timestamp="2014-05-21 18:41:21">http://citeseerx.ist.psu.edu/viewdoc/summary;jsessionid=AFD5FC4B8EB7B278CC7D7220DDF8CAC0?doi=10.1.1.61.3530&amp;rank=5</link><title>Coarse-to-fine n-best parsing and MaxEnt discriminative reranking</title><description>Coarse-to-fine n-best parsing and MaxEnt discriminative reranking

by Eugene Charniak, Mark Johnson \- In ACL , 2005

"... ’s guide to Bikel’s re-implementation of Collins’ parser, http://www.cis.upenn.edu/ dbikel ..."

Abstract \- Cited by 385 (14 self) \- Add to MetaCart

Discriminative reranking is one method for constructing high-performance statistical parsers (Collins, 2000). A discriminative reranker requires a source of candidate parses for each sentence. This paper describes a simple yet novel method for constructing sets of 50-best parses based on a coarse-to-fine generative parser (Charniak, 2000). This method generates 50-best lists that are of substantially higher quality than previously obtainable.</description></snippet><snippet id="FW14-e004-7211-06"><link cache="FW14-topics-docs/e004/7211_06.html" timestamp="2014-05-21 18:41:41">http://citeseerx.ist.psu.edu/viewdoc/summary;jsessionid=AFD5FC4B8EB7B278CC7D7220DDF8CAC0?doi=10.1.1.23.555&amp;rank=6</link><title>OTTER3.0 Reference Manual and Guide</title><description>OTTER3.0 Reference Manual and Guide

by William W. McCune , 1995

"... -4801 Distribution Category: Mathematics and Computer Science (UC-405) OTTER 3.0 Reference Manual and Guide ..."

Abstract \- Cited by 294 (34 self) \- Add to MetaCart

Abstract not found</description></snippet><snippet id="FW14-e004-7211-07"><link cache="FW14-topics-docs/e004/7211_07.html" timestamp="2014-05-21 18:41:58">http://citeseerx.ist.psu.edu/viewdoc/summary;jsessionid=AFD5FC4B8EB7B278CC7D7220DDF8CAC0?doi=10.1.1.229.775&amp;rank=7</link><title>Generating typed dependency parses from phrase structure parses</title><description>Generating typed dependency parses from phrase structure parses

by Marie-Catherine de Marneffe, Bill MacCartney, Christopher D. Manning \- IN PROC. INT’L CONF. ON LANGUAGE RESOURCES AND EVALUATION (LREC , 2006

"... with a flat structure in the Penn Treebank: (NP the new phone book and tour guide) Using the Collins ..."

Abstract \- Cited by 350 (19 self) \- Add to MetaCart

This paper describes a system for extracting typed dependency parses of English sentences from phrase structure parses. In order to capture inherent relations occurring in corpus texts that can be critical in real-world applications, many NP relations are included in the set of grammatical relations used. We provide a comparison of our system with Minipar and the Link parser. The typed dependency extraction facility described here is integrated in the Stanford Parser, available for download.</description></snippet><snippet id="FW14-e004-7211-08"><link cache="FW14-topics-docs/e004/7211_08.html" timestamp="2014-05-21 18:42:17">http://citeseerx.ist.psu.edu/viewdoc/summary;jsessionid=AFD5FC4B8EB7B278CC7D7220DDF8CAC0?doi=10.1.1.187.8482&amp;rank=8</link><title>Population-Based Incremental Learning: A Method for Integrating Genetic Search Based Function Optimization and Competitive Learning</title><description>Population-Based Incremental Learning: A Method for Integrating Genetic Search Based Function Optimization and Competitive Learning

by Shumeet Baluja , 1994

"... to recognize trends toward optimal solutions, and exploit such information by guiding the search toward them. A ..."

Abstract \- Cited by 298 (11 self) \- Add to MetaCart

Genetic algorithms (GAs) are biologically motivated adaptive systems which have been used, with varying degrees of success, for function optimization. In this study, an abstraction of the basic genetic algorithm, the Equilibrium Genetic Algorithm (EGA), and the GA in turn, are reconsidered within the framework of competitive learning. This new perspective reveals a number of different possibilities for performance improvements. This paper explores population-based incremental learning (PBIL), a method of combining the mechanisms of a generational genetic algorithm with simple competitive learning. The combination of these two methods reveals a tool which is far simpler than a GA, and which out-performs a GA on large set of optimization problems in terms of both speed and accuracy. This paper presents an empirical analysis of where the proposed technique will outperform genetic algorithms, and describes a class of problems in which a genetic algorithm may be able to perform better. Extensions to this algorithm are discussed and analyzed. PBIL and extensions are compared with a standard GA on twelve problems, including standard numerical optimization functions, traditional GA test suite problems, and NP-Complete problems.</description></snippet><snippet id="FW14-e004-7211-09"><link cache="FW14-topics-docs/e004/7211_09.html" timestamp="2014-05-21 18:42:35">http://citeseerx.ist.psu.edu/viewdoc/summary;jsessionid=AFD5FC4B8EB7B278CC7D7220DDF8CAC0?doi=10.1.1.46.4098&amp;rank=9</link><title>Can Punctuation Help Parsing?</title><description>Can Punctuation Help Parsing?

by Bernard Jones \- In 15 th International Conference on Computational Linguistics , 1994

"... of the topic." So while publishers' style guides are by far the most numerous type of account of punctuation ..."

Abstract \- Cited by 10 (0 self) \- Add to MetaCart

that &lt;em&gt;punctuation&lt;/em&gt; fulfils a linguistic role of its own, it is by no means clear how this role is defined. Since</description></snippet><snippet id="FW14-e004-7211-10"><link cache="FW14-topics-docs/e004/7211_10.html" timestamp="2014-05-21 18:42:54">http://citeseerx.ist.psu.edu/viewdoc/summary;jsessionid=AFD5FC4B8EB7B278CC7D7220DDF8CAC0?doi=10.1.1.39.595&amp;rank=10</link><title>Commas and Spaces: The Point of Punctuation</title><description>Commas and Spaces: The Point of Punctuation

by Robin L. Hill, Wayne S. Murray \- 11 th Annual CUNY Conference on Human Sentence Processing , 1998

"... ﻿Commas and Spaces: The Point of Punctuation Robin L. Hill &amp; Wayne S. Murray University of Dundee ..."

Abstract \- Cited by 4 (0 self) \- Add to MetaCart

While it has been widely assumed that &lt;em&gt;punctuation&lt;/em&gt; may play a critical role in parsing, there has</description></snippet></snippets></search_results>