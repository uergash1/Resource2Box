<?xml version='1.0' encoding='UTF-8'?>
<search_results><query id="7446">cycling on the mont ventoux</query><engine status="OK" timestamp="2014-04-20 05:23:53" name="CiteSeerX" id="FW14-e004"/><snippets><snippet id="FW14-e004-7446-01"><link cache="FW14-topics-docs/e004/7446_01.html" timestamp="2014-04-20 05:23:56">http://citeseerx.ist.psu.edu/viewdoc/summary;jsessionid=A074019E33967F3F20AC57C20F910874?doi=10.1.1.110.5995&amp;rank=1</link><title>Markov chains for exploring posterior distributions</title><description>Markov chains for exploring posterior distributions

by Luke Tierney \- Annals of Statistics , 1994

"... are now available. These include asymptotic approximations, numerical integra- tion and sampling or Monte ..."

Abstract \- Cited by 751 (6 self) \- Add to MetaCart

Your use of the JSTOR archive indicates your acceptance of JSTOR's Terms and Conditions of Use, available at</description></snippet><snippet id="FW14-e004-7446-02"><link cache="FW14-topics-docs/e004/7446_02.html" timestamp="2014-04-20 05:28:14">http://citeseerx.ist.psu.edu/viewdoc/summary;jsessionid=A074019E33967F3F20AC57C20F910874?doi=10.1.1.61.4999&amp;rank=2</link><title>An introduction to variational methods for graphical models</title><description>An introduction to variational methods for graphical models

by Michael I. Jordan \- Machine Learning , 1999

"... making use of Monte Carlo methods. A variety of Monte Carlo algorithms have been developed (see Neal ..."

Abstract \- Cited by 830 (62 self) \- Add to MetaCart

This paper presents a tutorial introduction to the use of variational methods for inference and learning in graphical models (Bayesian networks and Markov random fields). We present a number of examples of graphical models, including the QMR-DT database, the sigmoid belief network, the Boltzmann machine, and several variants of hidden Markov models, in which it is infeasible to run exact inference algorithms. We then introduce variational methods, which exploit laws of large numbers to transform the original graphical model into a simplified graphical model in which inference is efficient. Inference in the simpified model provides bounds on probabilities of interest in the original model. We describe a general framework for generating variational transformations based on convex duality. Finally we return to the examples and demonstrate how variational algorithms can be formulated in each case.</description></snippet><snippet id="FW14-e004-7446-03"><link cache="FW14-topics-docs/e004/7446_03.html" timestamp="2014-04-20 05:32:08">http://citeseerx.ist.psu.edu/viewdoc/summary;jsessionid=A074019E33967F3F20AC57C20F910874?doi=10.1.1.399.7501&amp;rank=3</link><title>On the statistical analysis of dirty pictures</title><description>On the statistical analysis of dirty pictures

by Julian Besag, Julian Besag \- Journal of the Royal Statistical Society B , 1986

"... injinite single-colour patches. Note that Monte Carlo simulations of Markov random fields are sometimes ..."

Abstract \- Cited by 928 (4 self) \- Add to MetaCart

Your use of the JSTOR archive indicates your acceptance of JSTOR's Terms and Conditions of Use, available at</description></snippet><snippet id="FW14-e004-7446-04"><link cache="FW14-topics-docs/e004/7446_04.html" timestamp="2014-04-20 05:34:47">http://citeseerx.ist.psu.edu/viewdoc/summary;jsessionid=A074019E33967F3F20AC57C20F910874?doi=10.1.1.15.2141&amp;rank=4</link><title>Functional Data Analysis</title><description>Functional Data Analysis

by J. Ramsay, B. Silverman, Liu A K, Belliveau J W, Dale A M Spatiotemporal Imaging , 1997

"... brain activity using functional MRI constrained magnetoencephalography data: Monte Carlo simulations ..."

Abstract \- Cited by 469 (13 self) \- Add to MetaCart

this article, but Ramsay and Silverman (1997) may be consulted for many more examples and methods, for consideration of the underlying philosophy, and for further references to the literature</description></snippet><snippet id="FW14-e004-7446-05"><link cache="FW14-topics-docs/e004/7446_05.html" timestamp="2014-04-20 05:38:14">http://citeseerx.ist.psu.edu/viewdoc/summary;jsessionid=A074019E33967F3F20AC57C20F910874?doi=10.1.1.192.2462&amp;rank=5</link><title>Graphical models, exponential families, and variational inference. Foundations Trends</title><description>Graphical models, exponential families, and variational inference. Foundations Trends

by Martin J. Wainwright, Michael I. Jordan \- Ihler (ihler@ics.uci.edu), University of California, Irvine. Michael

"... representations. The variational approach provides a complementary alternative to Markov chain Monte Carlo as a ..."

Abstract \- Cited by 428 (27 self) \- Add to MetaCart

representations. The variational approach provides a complementary alternative to Markov chain &lt;em&gt;Monte&lt;/em&gt; Carlo as a</description></snippet><snippet id="FW14-e004-7446-06"><link cache="FW14-topics-docs/e004/7446_06.html" timestamp="2014-04-20 05:39:15">http://citeseerx.ist.psu.edu/viewdoc/summary;jsessionid=A074019E33967F3F20AC57C20F910874?doi=10.1.1.129.159&amp;rank=6</link><title>Lottery Scheduling: Flexible Proportional-Share Resource Management</title><description>Lottery Scheduling: Flexible Proportional-Share Resource Management

by Carl A. Waldspurger, William E. Weihl , 1994

"... with proportional sharing, and permits I/O-bound tasks that use few processor cycles to start quickly. For example ..."

Abstract \- Cited by 419 (5 self) \- Add to MetaCart

This paper presents lottery scheduling, a novel randomized resource allocation mechanism. Lottery scheduling provides efficient, responsive control over the relative execution rates of computations. Such control is beyond the capabilities of conventional schedulers, and is desirable in systems that service requests of varying importance, such as databases, media-based applications, and networks. Lottery scheduling also supports modular resource management by enabling concurrent modules to insulate their resource allocation policies from one another. A currency abstraction is introduced to flexibly name, share, and protect resource rights. We also show that lottery scheduling can be generalized to manage many diverse resources, such as I/O bandwidth, memory, and access to locks. We have implemented a prototype lottery scheduler for the Mach 3.0 microkernel, and found that it provides flexible and responsive control over the relative execution rates of a wide range of applications. The overhead imposed by our unoptimized prototype is comparable to that of the standard Mach timesharing policy.</description></snippet><snippet id="FW14-e004-7446-07"><link cache="FW14-topics-docs/e004/7446_07.html" timestamp="2014-04-20 05:43:26">http://citeseerx.ist.psu.edu/viewdoc/summary;jsessionid=A074019E33967F3F20AC57C20F910874?doi=10.1.1.27.1022&amp;rank=7</link><title>Exact Sampling with Coupled Markov Chains and Applications to Statistical Mechanics</title><description>Exact Sampling with Coupled Markov Chains and Applications to Statistical Mechanics

by James Gary Propp, David Bruce Wilson , 1996

"... tilings [35] [45] and in the vast literature on Monte Carlo simulations in physics (Sokal gives a survey ..."

Abstract \- Cited by 406 (13 self) \- Add to MetaCart

For many applications it is useful to sample from a finite set of objects in accordance with some particular distribution. One approach is to run an ergodic (i.e., irreducible aperiodic) Markov chain whose stationary distribution is the desired distribution on this set; after the Markov chain has run for M steps, with M sufficiently large, the distribution governing the state of the chain approximates the desired distribution. Unfortunately it can be difficult to determine how large M needs to be. We describe a simple variant of this method that determines on its own when to stop, and that outputs samples in exact accordance with the desired distribution. The method uses couplings, which have also played a role in other sampling schemes; however, rather than running the coupled chains from the present into the future, one runs from a distant point in the past up until the present, where the distance into the past that one needs to go is determined during the running of the al...</description></snippet><snippet id="FW14-e004-7446-08"><link cache="FW14-topics-docs/e004/7446_08.html" timestamp="2014-04-20 05:47:08">http://citeseerx.ist.psu.edu/viewdoc/summary;jsessionid=A074019E33967F3F20AC57C20F910874?doi=10.1.1.31.8372&amp;rank=8</link><title>Minimizing Conflicts: A Heuristic Repair Method for Constraint-Satisfaction and Scheduling Problems</title><description>Minimizing Conflicts: A Heuristic Repair Method for Constraint-Satisfaction and Scheduling Problems

by Steven Minton, Andy Philips, Mark D. Johnston, Philip Laird \- J. ARTIFICIAL INTELLIGENCE RESEARCH , 1993

"... is updated on each cycle by randomly picking a set of neurons that represents a variable, and flipping ..."

Abstract \- Cited by 398 (6 self) \- Add to MetaCart

This paper describes a simple heuristic approach to solving large-scale constraint satisfaction and scheduling problems. In this approach one starts with an inconsistent assignment for a set of variables and searches through the space of possible repairs. The search can be guided by a value-ordering heuristic, the min-conflicts heuristic, that attempts to minimize the number of constraint violations after each step. The heuristic can be used with a variety of different search strategies. We demonstrate empirically that on the n-queens problem, a technique based on this approach performs orders of magnitude better than traditional backtracking techniques. We also describe a scheduling application where the approach has been used successfully. A theoretical analysis is presented both to explain why this method works well on certain types of problems and to predict when it is likely to be most effective.</description></snippet><snippet id="FW14-e004-7446-09"><link cache="FW14-topics-docs/e004/7446_09.html" timestamp="2014-04-20 05:49:35">http://citeseerx.ist.psu.edu/viewdoc/summary;jsessionid=A074019E33967F3F20AC57C20F910874?doi=10.1.1.380.9933&amp;rank=9</link><title>Understanding the Metropolis-Hastings Algorithm, The American Statistician 49</title><description>Understanding the Metropolis-Hastings Algorithm, The American Statistician 49

by Siddhartha Chib, Edward Greenberg , 1995

"... are illustrated with examples. KEY WORDS: Gibbs sampling; Markov chain Monte Carlo; Multivariate density ..."

Abstract \- Cited by 381 (20 self) \- Add to MetaCart

Your use of the JSTOR archive indicates your acceptance of JSTOR's Terms and Conditions of Use, available at</description></snippet><snippet id="FW14-e004-7446-10"><link cache="FW14-topics-docs/e004/7446_10.html" timestamp="2014-04-20 05:53:14">http://citeseerx.ist.psu.edu/viewdoc/summary;jsessionid=A074019E33967F3F20AC57C20F910874?doi=10.1.1.11.7111&amp;rank=10</link><title>Bayesian Analysis of Stochastic Volatility Models</title><description>Bayesian Analysis of Stochastic Volatility Models

by Eric Jacquier, Nicholas G. Polson, Peter E. Rossi , 1994

"... -chain Monte Carlo; Method of moments; Nonlinear filtering; Quasi-maximum likelihood; Stochastic volatility ..."

Abstract \- Cited by 371 (21 self) \- Add to MetaCart

this article is to develop new methods for inference and prediction in a simple class of stochastic volatility models in which logarithm of conditional volatility follows an autoregressive (AR) times series model. Unlike the autoregressive conditional heteroscedasticity (ARCH) and gener- alized ARCH (GARCH) models [see Bollerslev, Chou, and Kroner (1992) for a survey of ARCH modeling], both the mean and log-volatility equations have separate error terms. The ease of evaluating the ARCH likelihood function and the ability of the ARCH specification to accommodate the timevarying volatility found in many economic time series has fostered an explosion in the use of ARCH models. On the other hand, the likelihood function for stochastic volatility models is difficult to evaluate, and hence these models have had limited empirical application</description></snippet></snippets></search_results>