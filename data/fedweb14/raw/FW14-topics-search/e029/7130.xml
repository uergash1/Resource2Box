<?xml version='1.0' encoding='UTF-8'?>
<search_results><query id="7130">l1 versus l2 regularization</query><engine status="OK" timestamp="2014-04-22 00:12:48" name="Google Books" id="FW14-e029"/><snippets><snippet id="FW14-e029-7130-01"><link cache="FW14-topics-docs/e029/7130_01.html" timestamp="2014-04-22 00:12:49">http://books.google.nl/books?id=4z2hVy7y7M4C&amp;pg=PA84&amp;dq=l1+versus+l2+regularization&amp;hl=en&amp;sa=X&amp;ei=4ZdVU4U5o4jQBaPCgJAB&amp;redir_esc=y</link><title>Constituent Parsing by Classification - Page 84</title><description>6.4.5. l1-regularization. vs. l2. regularization. We induced l2-regularized decision trees. As noted on page 52, l2-regularized decision trees are identical to unregularized decision trees, except the confidences at the leaves are shrunk. The l1 ...</description></snippet><snippet id="FW14-e029-7130-02"><link cache="FW14-topics-docs/e029/7130_02.html" timestamp="2014-04-22 00:15:30">http://books.google.nl/books?id=NZP6AQAAQBAJ&amp;pg=PA430&amp;dq=l1+versus+l2+regularization&amp;hl=en&amp;sa=X&amp;ei=4ZdVU4U5o4jQBaPCgJAB&amp;redir_esc=y</link><title>Machine Learning: A Probabilistic Perspective - Page 430</title><description>13.3.1 Figure 13.3 Illustration of l1 (left) vs l2 (right) regularization of a least squares problem. Based on Figure 3.12 of (Hastie et al. 2001). that it has a spike near μ = 0. More precisely, consider a prior of the form p(w|λ) = D∏ j=1 Lap(wj|0,1/λ) ...</description></snippet><snippet id="FW14-e029-7130-03"><link cache="FW14-topics-docs/e029/7130_03.html" timestamp="2014-04-22 00:16:39">http://books.google.nl/books?id=1oM3atL3bBQC&amp;pg=PA24&amp;dq=l1+versus+l2+regularization&amp;hl=en&amp;sa=X&amp;ei=4ZdVU4U5o4jQBaPCgJAB&amp;redir_esc=y</link><title>Advances in Intelligent Data Analysis VII: 7th ... - Page 24</title><description>Kivinen, J., Warmuth, M.: Exponentiated gradient versus gradient descent for linear predictors. Information and Computation 132(1), ... Ng, A.Y.: Feature selection, l1 vs. l2 regularization, and rotational invariance. In: Proceedings of the Twenty ...</description></snippet><snippet id="FW14-e029-7130-04"><link cache="FW14-topics-docs/e029/7130_04.html" timestamp="2014-04-22 00:17:31">http://books.google.nl/books?id=ALD79j-ivVIC&amp;pg=PA297&amp;dq=l1+versus+l2+regularization&amp;hl=en&amp;sa=X&amp;ei=4ZdVU4U5o4jQBaPCgJAB&amp;redir_esc=y</link><title>Machine Learning: ECML 2007: 18th European Conference on ...</title><description>Ng, A.: Feature selection, L1 vs. L2 regularization, and rotational invariance. In: ICML, pp. 78–85. ACM Press, New York (2004) 14. Nocedal, J., Wright, S.J.: Numerical Optimization. Springer, New York (1999) 15. Perkins, S., Lacker, K., Theiler, ...</description></snippet><snippet id="FW14-e029-7130-05"><link cache="FW14-topics-docs/e029/7130_05.html" timestamp="2014-04-22 00:20:04">http://books.google.nl/books?id=Tbn1l9P1220C&amp;pg=PA1472&amp;dq=l1+versus+l2+regularization&amp;hl=en&amp;sa=X&amp;ei=4ZdVU4U5o4jQBaPCgJAB&amp;redir_esc=y</link><title>Advances in Neural Information Processing Systems 19: ... - Page 1472</title><description>[6] A. Y. Ng. Feature selection, l1 vs. l2 regularization, and rotational invariance. In International Conference on Machine Learning, 2004. [7] D. Pollard. Convergence of stochastic processes. Springer-Verlag, New York, 1984. [8] P. Spirtes, C.</description></snippet><snippet id="FW14-e029-7130-06"><link cache="FW14-topics-docs/e029/7130_06.html" timestamp="2014-04-22 00:23:02">http://books.google.nl/books?id=LlkYV9PcsQEC&amp;pg=PA136&amp;dq=l1+versus+l2+regularization&amp;hl=en&amp;sa=X&amp;ei=4ZdVU4U5o4jQBaPCgJAB&amp;redir_esc=y</link><title>Pattern Recognition in Bioinformatics: 6th IAPR ... - Page 136</title><description>Such a penalization, also known as regularization, gives rise to the l2-regularized LR problem: minw,vlavg(w,v) + λ w 2 2 ... For small n, large p problems the l1-regularized LR is thus usually considered instead by replacing the l2-norm · 2 2 in ...</description></snippet><snippet id="FW14-e029-7130-07"><link cache="FW14-topics-docs/e029/7130_07.html" timestamp="2014-04-22 00:26:16">http://books.google.nl/books?id=kP7rMM3PVlUC&amp;pg=PA183&amp;dq=l1+versus+l2+regularization&amp;hl=en&amp;sa=X&amp;ei=4ZdVU4U5o4jQBaPCgJAB&amp;redir_esc=y</link><title>Medical Image Computing and Computer-Assisted Intervention ...</title><description>We propose two alternative solutions based on either l1 or l2 regularization, justifying that non-negative l2 provides sparse representations analogous to l1. Finally, the results in section 4 illustrate how positive, unit-mass, probabilistic ODFs ...</description></snippet><snippet id="FW14-e029-7130-08"><link cache="FW14-topics-docs/e029/7130_08.html" timestamp="2014-04-22 00:28:01">http://books.google.nl/books?id=ri0QCCtm8vwC&amp;pg=PA453&amp;dq=l1+versus+l2+regularization&amp;hl=en&amp;sa=X&amp;ei=4ZdVU4U5o4jQBaPCgJAB&amp;redir_esc=y</link><title>Scale Space and Variational Methods in Computer Vision: ... - Page 453</title><description>have been made that guarantee existence of a minimizer in L1(Ω), while in this work we consider minimizers, which are Radon measures. ... with regularization terms RS(u):= ∑ ωi|〈u,φi〉| , where φi is a set of appropriate functions, typically forming a basis or frame. ... However, convex analysis in the Hilbert spaces L2 is not applicable for ·L1 Regularization, since on domains with finite measure, L2(Ω) ...</description></snippet><snippet id="FW14-e029-7130-09"><link cache="FW14-topics-docs/e029/7130_09.html" timestamp="2014-04-22 00:29:14">http://books.google.nl/books?id=qVDA6VKL80oC&amp;pg=PA178&amp;dq=l1+versus+l2+regularization&amp;hl=en&amp;sa=X&amp;ei=4ZdVU4U5o4jQBaPCgJAB&amp;redir_esc=y</link><title>Semismooth Newton Methods for Variational Inequalities and ...</title><description>We then obtain the regularization term % f9 max2{0, y (y — b)} dx, which is the Moreau—Yosida regularization [103, 104]. ... Lipschitz continuous, Proposition A.11 shows that v I—&gt; (12(1)) is continuously differentiable from L2(o) to L1(o) and ...</description></snippet><snippet id="FW14-e029-7130-10"><link cache="FW14-topics-docs/e029/7130_10.html" timestamp="2014-04-22 00:30:17">http://books.google.nl/books?id=9KYW522l1P0C&amp;pg=PA338&amp;dq=l1+versus+l2+regularization&amp;hl=en&amp;sa=X&amp;ei=4ZdVU4U5o4jQBaPCgJAB&amp;redir_esc=y</link><title>Dynamics of Visual Motion Processing: Neuronal, ... - Page 338</title><description>... Canadian Information Processing Society, Toronto, Ontario, Canada Andrew YN (2004) Feature selection, L1 vs. L2 regularization, and rotational invariance. In: Proceedings of the twenty-first international conference on machine learning, ...</description></snippet></snippets></search_results>