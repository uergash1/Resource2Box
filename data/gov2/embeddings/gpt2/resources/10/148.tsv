id	content
GX169-26-14225454	arXiv: chao-dyn/9806001  v1   1 Jun 1998     QUASICRYSTALLINE PATTERN FORMATION IN FLUID SUBSTRATES AND PHYLLOTAXIS   A. Mary Selvam       1. INTRODUCTION       The botanical elements which constitute plants are branches, leaves, petals, stamens, sepals, florets, etc. These plant elements begin their existence as primordia in the neighborhood of the undifferentiated shoot apex (extremity). Extensive observations in botany show that in more than 90% of plants studied worldwide (Jean, 1984, 1989, 1990, 1992a,b, 1994; Douady and Couder, 1991, 1992, 1993, 1995; Ryan et al, 1991; Bursill et al, 1992; Needham et al, 1993; Stewart, 1995) primordia emerge as protuberances at locations such that the angle subtended at the apical center by two successive primordia is equal to the golden angle  j   =2 p  / t   2  corresponding to approximately 137.5 °  where  t  is the golden mean  (1+ Ö  5)/2 = 1.618 . The golden mean is the most irrational number and is associated with the Fibonacci mathematical sequence 1, 1, 2, 3, 5, 8, ...... where each term is the sum of the two previous terms and the ratio of each term to the previous term approaches the golden mean  t  .  t  is the most irrational number in the sense that rational approximations converge very slowly to  t  as compared to other irrational numbers. Irrational numbers are numbers such as  which has an infinite number of non-periodic decimals. Rational approximations such as  p/q  where  p  and  q  are integers are used to represent   irrational numbers (Kappraff, 1992). The surprisingly precise geometrical placement of plant primordia results in the observed 'phyllotactic patterns', namely, the familiar spiral patterns found in the arrangement of leaves on a stem, in florets of composite flowers, the pattern of scales on pineapple and pine cone, etc. The word 'phyllotaxis' is of Greek origin ('phyllon', leaf and 'taxis', arrangement) and literally means, the study of the disposition of leaves on the stem. Phyllotaxis, in a broader sense now includes the study of arrangement of all plant elements which originate as primordia. Botanists, physicists and mathematicians alike have been fascinated by such universal rhythms governing plant growth and the field of phyllotaxis has a long history of more than 150 years. A comprehensive review of phyllotaxis has been given by Jean (1984,1994).     The emergence of primordia on the shoot is basically a branching (bifurcating) process. Such repeated (selfsimilar) branching on all scales is exhibited in the branching structures of roots, shoots and veins on leaf in the plant kingdom (Arber, 1950). Selfsimilar spatial patterns where the internal small scale structure resembles the large scale is ubiquitous to nature (Stevens, 1974; Freeman, 1987, 1989; Jean, 1994) and belongs to the category of ‘fractals’ (Mandelbrot, 1977; Basu, 1990) in the new science of  nonlinear dynamics and chaos  (Gleick, 1987). Objects in nature are selfsimilar fractals (West and Schlesinger, 1989; Schroeder, 1990; West, 1990; Goldberger et al, 1990) in the sense that ‘fractals’ denotes fractional Euclidean shape and ‘selfsimilarity’ denotes repetition on all length scales of the fundamental irregular shapes, namely, wrinkles or folds and bifurcations or branchings in spatial pattern. Tessier et al(1993) have documented and discussed the fractal nature of space-time fluctuations of atmospheric flows. The fractal dimension  D  is given by the relation            (1)   where  M  is the mass contained within a distance  R  from a point in the fractal object. The fractal dimension  D  is therefore obtained as the slope of the straight line portion of the log-log plot of  M  versus  R.  A constant value for the fractal dimension  D  indicates uniform stretching on a logarithmic scale. Eq. (1) may also be written in the form           (2)   where  ‘A’  is a constant. This type of scaling was used by D’Arcy Thompson (Thompson, 1963) in scaling anatomical structures. It appears quite often in the form of allometric growth laws in botany as well as in biology (Deering and West, 1992; Jean, 1994). This particular kind of scaling has been successfully used in biology for over a century. However, the significance of the existence of such allometric growth laws have not always been properly appreciated. This scaling often refers to a process that has ‘infinite’ levels of substructure that repeat at an ever decreasing cascade to even smaller scales over which one averages to obtain allometric growth law.    Objects in nature in general exhibit multifractal structure, i.e. the fractal dimension  D  is different for different length scale range  R . The dimension of a naturally occurring fractal is a quantitative measure of a qualitative property of a structure that is selfsimilar over some region of space or interval of time. For example, a tree is composed of the fundamental branching structure on all length scales.   Selfsimilarity indicates long-range correlations, i.e. the amplitude of short- and long-term fluctuations are related to each other by a non-dimensional scale factor alone. Voss (1993) has identified selfsimilar fractal structure of DNA-base sequence of plants indicating existence of long-range correlations at the molecular level. Long-range spatiotemporal correlations are generic to dynamical systems in nature and are now identified as signatures of self-organized criticality (Bak et al, 1988; Bak and Chen, 1991). Dynamical systems are systems which change with time.   Biological auto-organization and pattern formation have been studied over the past 40 years as non-equilibrium thermodynamic phenomena (Turing, 1952).Biological systems exhibit high degree of co-operation in the form of long-range communication. The concept of co-operative existence of fluctuations in the organization of coherent structures have been identified as self-organized co-operative phenomena (Nicolis and Prigogine, 1977; Prigogine, 1980; Prigogine and Stengers, 1988; Insinnia, 1992) and synergetics (Haken, 1980).    Mary Selvam (1990) has developed a cell dynamical system model for atmospheric flows which shows that the observed long-range spatiotemporal correlations namely, self-organized criticality are intrinsic to quantumlike mechanics governing turbulent flow dynamics. The model concepts are independent of the exact details, such as the chemical, physical, physiological, etc. properties of the dynamical systems(Rosen, 1993) and therefore is a general systems theory (Peacocke, 1989; Klir, 1993; Jean, 1994) applicable for all dynamical systems in nature.    Selfsimilar structures (space-time) incorporate the Fibonacci numbers and therefore the golden mean manifested as fivefold symmetry and spiral (Fibonacci) symmetry ( Stoddart, 1988; Hargittai, 1992; Hargittai and Pickover, 1992; Jean, 1994). Nature abounds in symmetrical structures from the macro- to the microscopic scales (Tarasov, 1986). Phyllotactic patterns in the plant kingdom are the clearest examples of selfsimilar structures or self-organized criticality. Therefore 'phyllotacticlike' patterns, namely patterns with fivefold and spiral symmetry underlie self-organized criticality in dynamical systems in nature.     In this paper the cell dynamical system model concepts developed for atmospheric flows are applied for explaining the observed phyllotactic patterns in the plant kingdom. It is shown that 'phyllotaxis like' patterns are signatures of self-organized criticality in dynamical systems in nature.The cell dynamical system model is based on the concept that spatial integration of fluctuations give rise to quasiperiodic structures incorporating Fibonacci sequence. The Fibonacci sequence is generated by cumulative summation process (Stewart ,1992). The other less frequent series (Jean ,1994) observed in phyllotaxis can be generated by similar cumulative summation process (see section 4.2) and therefore consistent with cell dynamical system model concepts.    The contents of this chapter are organized as follows. Section 2. discusses the identification of quasicrystalline structure of the quasiperiodic Penrose tiling pattern in phyllotactic patterns. Nonlinear dynamics and chaos in iterative processes with reference to phyllotaxis is discussed in Section 3. Section 4 deals with the traditional concepts of quantum mechanics, its limitations, and applications of quantumlike mechanics in phyllotaxis. Section 5 closes with discussions and conclusions.           2. QUASICRYSTALLINE STRUCTURE FOR PHYLLOTAXIS : QUASIPERIODIC PENROSE TILING PATTERN       The regular arrangement of plant parts resemble the newly identified (since 1984) quasicrystalline order in condensed matter physics (Nelson, 1986). An understanding of the physical mechanism underlying the spontaneous organization of mathematically precise patterns in macroscale plant development will benefit crystallographers investigating quasicrystalline order in the arrangement of atoms at the molecular level (Bursill et al, 1992). Traditional (last 100 years) crystallography has defined a crystalline structure as an arrangement of atoms that is periodic in three dimensions. Crystals have lattice structure with identical arrangement of atoms (Von Baeyer, 1990; Lord, 1991) with space filling cubes or hexagonal prisms. Five-fold symmetry was prohibited in classical crystallography. In 1984, an alloy of alluminium and magnesium was discovered which exhibited the symmetry of an icosahedron with five fold axis. At the same time Paul Steinhardt of the University of Pennsylvania and his student Dov Levine (Von Baeyer, 1990) had quite independently identified similar geometrical structure, now called quasicrystals. These developments were based on the important work on the mathematics of tilings done by Roger Penrose and others beginning in the 1970s (Penrose , 1974, 1979; Steinhardt and Ostlund, 1987). Penrose discovered a nonperiodic tiling of the plane, using two types of tiles, which is a quasiperiodic crystal with pentagonal symmetry (DiVincenzo, 1989). It is generally accepted that a quasicrystal can be understood as a systematic (but not periodic) filling of space by unit cells of more than one kind. Such extended structures in space can be orderly and systematic without being periodic. Penrose tiling pattern are two dimensional quasicrystals.    The geometric pattern is selfsimilar and exhibits long-range correlations and is quasiperiodic. Mary Selvam(1990) has shown that small scale fluctuations (turbulence) in fluid flows self-organize to form the quasiperiodic Penrose tiling pattern (Fig.1) with fractal selfsimilar geometry to spatial pattern and long-range temporal correlations for temporal fluctuations. Self-organized criticality is exhibited as the Penrose tiling pattern for spatial geometry which then incorporates temporal correlations for dynamical processes. The generating spiral for Penrose tiling pattern (Fig.1) is traced by progressively increasing radii whose length follow the Fibonacci mathematical series , the angular turning between successive radii being equal to  2 p  / t   2  . The generating spiral is the same for phyllotaxis and Penrose tiling and therefore plant part placement follows the precise geometry of the quasiperiodic Penrose tiling pattern in the spatial domain.     The universality in the botanical arrangement of leaves, florets etc. comes from identical growth processes which can be translated into simple assumptions for an iterative growth process (Douady and Couder, 1993). Selfsimilarity underlies all growth processes in nature. Jean (1994) has emphasized the selfsimilar geometry of botanical elements. Selfsimilar structures are generated by iteration (repetition) of simple rules for growth processes on all scales of space and time. Such iterative processes are simulated mathematically by numerical computations such as    Xn+1 = F(Xn)          (3)   where  Xn+1  , the value of the variable at ( n+1 )th computational step is a function  F  of its earlier value  Xn  . Mathematical models of real world dynamical systems are basically such iterative computational schemes implemented on finite precision digital computers. Computer precision imposes a limit (finite precision) on the accuracy (number of decimals) for numerical representation of  X . Since  X  is a real number (infinite number of decimals) finite precision introduces round-off error in iterative computations from the first stage of computation. The model iterative dynamical system generates computational structures which represent the cumulative sum of round-off error at successive iterations. Computed growth patterns exhibit selfsimilar fractal structure which incorporates the golden mean (Stewart, 1992). Fibonacci series underlying the golden mean characterizes cumulative summation of bifurcations, i.e., iterative (repeated) branching process. Phyllotaxis is basically a branching process with rhythmic branching out of primordia from the shoot. Phyllotaxis may therefore be studied under the broader perspective of the new science of  nonlinear dynamics and chaos  which seeks to understand the physics of such selfsimilar patterns generated by iterative processes in computed and real world systems.       3. NONLINEAR DYNAMICS AND CHAOS IN ITERATIVE PROCESSES        Mathematical models of real world dynamical systems such as atmospheric flows are based on Newtonian continuum dynamics and consist of nonlinear equations which do not have analytical solutions. Numerical solutions of such nonlinear model equations are obtained using numerical integration schemes which are basically iterative computations such as Eq. (3) (Section 2) which amplify exponentially with time the round-off and other initialization errors and give unrealistic solutions. Deterministic equations such as Eq. (3) which are precisely defined and mathematically formulated give chaotic solutions because of sensitive dependence on initial conditions. Finite precision computer realization of mathematical models of dynamical systems therefore exhibit deterministic chaos(Gleick, 1987). Historically, though deterministic chaos was identified nearly a century ago by Poincare(1892), it became an intensive field of research in recent years (since 1980’s) following Lorenz’s (Lorenz, 1963) study of sensitive dependence on initial conditions of a simple mathematical model of atmospheric flows.   Mary Selvam (1993) has compared round-off error to turbulent (small-scale) fluctuations in fluid flows and has shown that round-off error approximately doubles on an average for each iteration. Therefore round-off error will propagate to the mainstream computation within 50 iterations in single precision (7th decimal place accuracy) computations and thereafter the computed domain represents cumulative sum of round-off error growth. The computed domain, when resolved as a function of computational accuracy has overall logarithmic spiral geometry with the quasiperiodic Penrose tiling pattern for internal structure (Fig.1), namely quasicrystalline structure incorporating the golden mean. There is a very close similarity between the geometrical patterns generated during iterative computations and those found in nature (Jurgen et al., 1990; Stewart, 1992). Iterative computations generate patterns strongly reminiscent of plant forms and clearly these curious configurations show that the rules responsible for the construction of elaborate living tissue structures could be absurdly simple (Dewdney, 1986).     In summary, cumulative integration (summation) of fluctuations or bifurcations result in selfsimilar structures which can be resolved into the quasicrystalline geometry of Penrose tiling pattern.    Phyllotaxis is basically a branching (bifurcating) process at the level of primordia initiation in the neighborhood of the shoot apex. A hierarchy of branching structures representing cumulative summation of bifurcations incorporate the golden mean in the spatial geometry . The spatial geometry of quasicrystalline structures such as that of phyllotaxis, fluid flows and round-off error growth result from cumulative summation of persistent primary perturbations appropriate to the dynamical process. Quasicrystalline order implies long-range spatial and temporal correlations or self-organized criticality (section 2.0). Recent studies by Voss (1993) show that long-range correlations indicating selfsimilar spatial geometry are present at the level of DNA in plants. Plant phyllotaxis may therefore result from quasicrystalline ordering of morphogenetic chemical fields at the subcellular level in fluid substrates.        4.   QUANTUM MECHANICS : CONCEPTS AND LIMITATIONS FOR REAL WORLD SYSTEMS         Quantum mechanical laws govern the subatomic dynamics of quantum systems such as the photon or electron. Since its invention some sixty years ago, quantum theory has been developed to describe successfully the behavior of subatomic particles, the properties of atomic nucleus and the structure and properties of molecules and solids (Rae, 1988). Quantum physics, applicable to the microscopic building blocks of matter, however, fails to describe the behavior of bulk matter, i.e., macroscale real world dynamical systems such as atmospheric flows, plant growth, functions of physiological systems, etc. Classical mechanical laws based on Newton's laws of motion are traditionally (more than 200 years) used to describe macroscale dynamical behavior. Because quantum mechanics is the fundamental theory of nature, it should also encompass classical physics. That is, applied to macroscopic phenomena, quantum mechanics should reach a limit at which it becomes equivalent to classical mechanics. Yet until recently, the exact nature of this transition had not been fully elucidated (Nauenberg et al, 1994).    One of the most challenging problems in physics is how to obtain macroscopic behavior and macroscopic variables out of (quantum) microscopic dynamics. The problem of emergence of macroscopic variables out of microscopic dynamics is of crucial relevance in biology, even more so than in Physics (Vitiello, 1992).    Quantum mechanics describes the behavior of a subatomic particle in terms of a group of waves, i.e. a wavetrain that can be built up of a large number of sine waves of slightly differing frequency. Where the waves together produce an amplitude  y  , this region advances with a group velocity that can represent the velocity of a particle whose position is represented by the region of amplitude  y  . Variable  y   2 , the square of the wave amplitude, is proportional to the probability of finding a particle at the coordinates where  y  is evaluated (Kerwin, 1963).The wave function  y  , named Schrodinger's wave function, describes a subatomic particle in terms of probabilities of occurrence at different locations on the wave train, thereby creating wave-particle duality aspect for the particle. Though quantum mechanical laws are successful in describing subatomic phenomena, the following inconsistencies are yet to be resolved (Maddox, 1988).   (1) The interpretation of Schrodinger's wave function  y  as quantities whose squared amplitudes give the probability density that a particle will be at a particular place (if the arguments of the wave function are in space coordinates). Such a declaration that algebraically additive amplitudes must be squared to obtain probability densities is unsatisfactory in the absence of physically consistent and mathematically rigorous proof.   (2) The unresolved issue of nonlocality in quantum mechanics whereby the spatially separated parts of a quantum system (photon, electron, etc.) respond as a unified whole to local perturbation.   (3) Energy propagation and interchange in quantum systems occur in discrete quanta or packets of energy content  h n   where  h  is a universal constant of nature (Planck's constant) and  n  is the frequency in cycles per second of the electromagnetic radiation. The exact physical mechanism responsible for the manifestation of subatomic phenomena as discrete packets of energy propagating as waves, i.e., the wave-particle duality is not yet identified.   (5) Finally, quantum mechanical laws, which govern the ultimate structure of matter, cannot be interpreted in terms of macroscale real world phenomena.       4.1. Quantumlike mechanics in fluid flows : a physically consistent theory        In the following it is shown that atmospheric flow structure follows laws similar to quantum mechanical laws for subatomic dynamics. The apparent inconsistencies of quantum mechanical laws described above are explained in terms of the physically consistent characteristics inherent to eddy circulation patterns in atmospheric flow.   (1)  Atmospheric flows consist of a continuum of eddy fluctuations, which give rise to alternating regions of updrafts and downdrafts (Fig. 2). Under favorable conditions of moisture supply in the environment, convection and cloud formation occur in the updraft regions while subsidence and cloud dissipation occur in adjacent downdraft regions, thereby forming discrete regions of weather activity. The square of the eddy amplitude  y   2 , represents the eddy energy (kinetic) and is proportional to the intensity of the weather system at the location where  y  is measured. In Fig. 2, a fixed location O in the path of the wavetrain experiences progressive increase in cloud formation (height and thickness of clouds) and weather activity, reaching a peak corresponding to maximum amplitude of the wavetrain at point P, followed by gradual dissipation of clouds. The signature of the passage of the wavetrain will be recorded in the meteorological parameters, such as rainfall, temperature, etc., at location O.     Variable  y   2  represents the kinetic energy of eddies. Since the large eddy is but the sum total of the enclosed smaller scales, the large eddy energy content is equal to the sum of all its individual component eddy energies, and therefore, by the Central Limit Theorem (Mood and Graybill, 1963) in  Statistics  , the kinetic energy distribution is normal, i.e. the eddy energy probability density distribution is represented by the square of the eddy amplitude (i.e., the variance).   (2) Nonlocality is intrinsic to the instantaneously adjusting bi-directional energy flow structure of atmospheric eddies, e.g., the updrafts and downdrafts of the complete eddy circulation (Fig. 2) are in steady state momentum balance.   (3) The atmospheric eddy energy is made up of the sum of discrete quanta or packets of energy in individual small eddy circulations . This concept is somewhat analogous to quanta of electromagnetic radiation.   (4) The apparent wave-particle duality is physically consistent in the context of atmospheric flows since the bi-directional energy flow structure of a complete atmospheric eddy results in the formation of clouds in updraft regions and dissipation of clouds in downdraft regions (Fig. 2) thereby giving rise to the observed discrete cellular geometry to cloud structure. The wave-particle duality of quantum mechanical phenomena may therefore be associated with bimodal (i.e. formation and dissipation, respectively) of phenomenological form for the energy manifestation in the corresponding bi-directional energy flow structure.    The continuously evolving atmospheric eddy continuum traces out the quasi-periodic Penrose tiling pattern (Fig.1) where, as a natural consequence the eddy growth is associated with an increase in phase angle and associated long range correlations. Therefore, long-range space-time correlations are inherent to quantumlike mechanics in atmospheric flows. As mentioned earlier ( Section 1) long-range space-time correlations are ubiquitous to macroscale real world dynamical system in nature and such non-local connections are identified as signatures of self-organized criticality.    Self-organized criticality manifested as selfsimilar geometrical structure to space-time fluctuation pattern is therefore a signature of quantumlike mechanics in real world macroscale dynamical system.    The cell dynamical system model for atmospheric flows is applicable to all dynamical systems and may provide a unifying theory for subatomic scale to macroscale dynamics.    The above described analogy of quantumlike mechanics for atmospheric flows is similar to the concept of a subquantum level of fluctuations whose space-time organization gives rise to the observed manifestation of subatomic phenomena, i.e., quantum system as order out of chaos phenomena (Grossing, 1989).    The macroscale atmospheric flow structure may therefore provide physically consistent interpretation for the apparent inconsistencies of quantum mechanical laws thereby unifying the laws of natural phenomena.       4.2. Quantumlike mechanics and phyllotaxis        Phyllotaxis is basically a rhythmic process for placement of primordia. Rhythmic and periodic phenomena are intrinsic to plant and animal development (Barlow, 1994). Phyllotaxis, particularly is the precisely controlled periodic appearance of primordia on the shoot apex.     Jean (1990) mentions the following Fibonacci-type sequences in phyllotaxis :      (4)   where  J  ³  1  represents the number of genetic spirals in the system,  t  ³  2  are integers. The corresponding divergence angles  q  are :             (5)    Common values for the divergence, in degrees are 137.51, 99.5 and 68.7. The above Fibonacci-type sequences in phyllotaxis (Eqs. 4 & 5) are generated by a cumulative summation process. The cell dynamical system model for fluid flows (Mary Selvam, 1990) shows that cumulative summation (integration) of fluctuations give rise to quasicrystalline structures incorporating the golden mean. The observed phyllotactic patterns are therefore consistent with model concepts.    From elementary considerations of the observed precise quasicrystalline geometry of phyllotaxis, the following macroscale manifestation of quantumlike mechanical laws may be derived.    Any two successive primordia subtend an angle equal to  2 p   q   (   radians) at the shoot apex. The angular domain of each primordium is therefore equal to  2 p   q   . The fractional angular domain of each primordium for one complete cycle of rotation is then  q  . The Schrodinger wavefunction  y  , which represents amplitude of perturbation corresponds to this fractional angular domain  q  of each primordium. The variable  y   2 , equal to  q   2  represents the variance of the fractional angular domain of each primordium.    Since the angular domain  2 p   q   corresponds to one primordium, the probability  p 1  of occurrence of a primordium for one complete cycle of rotation in any one direction is equal to  q  . Therefore, considering either clockwise or anti-clockwise direction of rotation, the probability  p  of occurrence of a primordium is equal to  p 1 2  =  q   2  .    Therefore,  y   2 ,  square of amplitude  y  of primodium domain also represents probability of occurrence of primodium similar to quantum mechanical laws which govern sub-atomic dynamics (Section 4.0).   Also, long-range spatiotemporal correlations, namely non-local connections are intrinsic to the quasicrystalline structure of phyllotaxis. Therefore quantumlike mechanical laws may be applicable to phyllotaxis.    The appearance of primordia at regular intervals may be compared to the formation of clouds regularly spaced in updraft regions of eddy circulations in atmospheric flows (Fig. 2). Self-organization of small scale fluctuation in the atmosphere outside coupled to the water medium inside the plant stem may generate Schrodinger wavefunction  y  -like concentration of morphogenetic chemical fields which trigger the birth of primordia. Such a concept is analogous to the field theory of phyllotaxis (Schoute, 1913; Richards, 1948; Wardlaw 1949; Steeves and Sussex, 1989) and also to the general morphogenetic field theory of Sheldrake (1985). The observed universality of selfsimilar space-time structures in nature may be attributed to selforganization of fluctuations in the basic fluid form of the substrate, the self-organization process being independent of the exact nature of the substrate.           5. DISCUSSIONS AND CONCLUSIONS          In a majority of plants studied worldwide (Jean, 1994), the angle subtended at the shoot apex by two successive primordia, called the divergence angle is equal to the golden angle  2 p  / t   2  corresponding to approximately 137.5 °  .    Universal occurrence of divergence angle equal to the golden angle in plant kingdom indicates dynamical growth processes which are independent of exact details (chemical, physical, physiological, genetic etc.) of the plant. Phyllotaxis like patterns occur commonly in other living and nonliving systems (Jean, 1994). The bifurcating(branching) tree-like fractal (selfsimilar) structure underlie phyllotactic patterns.    Fractal (selfsimilar) geometry to the spatial pattern is ubiquitous to dynamical systems in nature. Selfsimilarity implies long-range correlation in space and time and is now (since 1988) identified as signature of self-organized criticality. Such non-local connection are intrinsic to subatomic dynamics of quantum systems which follow quantum mechanical laws . Therefore, macroscale dynamical system in nature exhibit signatures of quantumlike mechanics.    Phyllotactic patterns possess the quasicrystalline structure of the quasiperiodic Penrose tiling pattern. Mary Selvam (1990,1993) , Mary Selvam et al(1992), Mary Selvam and Joshi (1995) have shown that quasicrystalline structure of the quasiperiodic Penrose tiling pattern underlie iterative growth processes such as turbulent(small scale) fluctuations in fluid flows and round-off error growth in numerical integration schemes. Selfsimilarity in spatial structure extends down to the level of DNA molecules in plants (Voss, 1993). It is possible that plant growth processes may be coupled to the turbulent fluid environments of air and water respectively in the exterior and interior of the plant body down to sub-cellular levels resulting in the spontaneous generation of the quasicrystalline structure in plant growth pattern. Recent studies show that Phyllotaxislike symmetrical patterns are formed spontaneously by chemotactic bacteria growing in certain fluid substrates (Budrene and Berg, 1995). Quasicrystalline structure intrinsic to fluids may help formation of such symmetrical structures. The spontaneous formation of quasiperiodic structures in fluids has been recently demonstrated (Mukerjee, 1995).    In conclusion, self-organization of space-time fluctuations of all scales in fluid substrates contribute to manifested forms and functions of diverse phenomena from micro- to macroscales. The concept of unified rhythms in manifestation of diverse phenomenological forms in the universe has been expressed by poet Rabindranath Tagore (1967) as follows.     The birth and death of the leaves are the rapid whirls of the eddy whose wider circles move slowly among stars.             8. ACKNOWLEDGEMENTS       The author is grateful to Dr. A.S.R. Murty and Mrs.A.A.Shiralkar for their keen interest and encouragement during the course of this study.Thanks are due to Mr. M.I.R. Tinmaker for typing the manuscript.       9. REFERENCES        Arber A. (1950) :  The Natural Philosophy of Plant Form , Cambridge University Press, London.   Bak P.C., Tang C. and Wiesenfeld, K. (1988) : Self-organized criticality,  Phys. Rev. Ser. A38 , 364-374.   Bak P.C. and Chen, K. (1991) : Self-organized criticality,  Sci. Amer. 264 , 26-33.   Barlow P.W. (1994): Rhythm, periodicity and polarity as bases for morphogenesis in plants,  Biol. Rev. 69,  475-525.   Basu A. (1990) : A new geometry of nature,  Science Reporter May , 9-18.   Budrene E.O. and Berg H. C. (1995) : Dynamics of formation of symmetrical patterns by chemotactic bacteria,  Nature 376 , 49-53.   Bursill L. A., Rouse J.L. and Needham A. (1992) : Sunflower quasicrystallography. In  Spiral Symmetry , Hargittai I. and Pickover C.A. (eds), World Scientific, Singapore, pp. 295-322.   Deering W. and West B.J. (1992) : Fractal physiology,  IEEE Engineering in Medicine and Biology June , 40-46.   Dewdney, A. K. (1986) : Computer recreations,  Sci. Am. 255 , 14-23.   DiVincenzo, D. P. (1989) : Perfect quasicrystals ?,  Nature 340 , 504-505.   Douady, S. and Couder, Y. (1991): Phyllotaxis as a self-organized growth process. In Proc. NATO ARW  Growth Patterns in Physical Science and Biology , Granada, Spain, 7-11 Oct. 1991.   Douady, S. and Couder, Y.(1992): Phyllotaxis as a physical self-organized growth process,  Phys. Rev. Lett. 68(13),  2098-2101.   Douady S. and Couder, Y. (1993) : Phyllotaxis as a self-organized growth process, In  Growth Patterns in Physical Sciences and Biology , J.M. Garcia-Ruiz  et al.  (eds), Plenum Press, New York.   Douady S. and Couder, Y. (1995) : Phyllotaxis as a dynamical self-organizing process,  J. Theor. Biol.  (in press).   Freeman G.R. (1987): Introduction. In  Kinetics of Nonhomogenous Processes ,   Freeman, G.R. (ed), John Wiley and Sons, Inc., NY, pp. 1-18.   Freeman G.R. (1989) :Introduction. In  KNP89 : Kinetics of Nonhomogeneous Processes (KNP) and Nonlinear Dynamics,   Can. J. Phys. 68 , 655-659.   Goldberger A. L., Rigney D. R. and West B.J. (1990) : Chaos and fractals in human physiology,  Vigyan, Sci. Am. (Indian Edition) Feb ., 41-47.   Gleick J. (1987) :  Chaos : Making a New Science , Viking, New York.   Grossing G. (1989) : Quantum systems as order out of chaos phenomena,  Il Nuovo Cimento 103B , 497-510.   Haken H. (1980) :  Synergetics : An Introduction , Springer, Berlin.   Hargittai I. and Pickover C.A. (eds.) (1992) :  Spiral Symmetry , World Scientific, Singapore.   Hargittai I. (ed.) (1992) :  Fivefold Symmetry , World Scientific, Singapore.   Insinnia E.M. (1992) : Synchronicity and coherent excitations in microtubules,  Nanobiology 1 (2), 191-208.   Jean R. V. (1984) :  Mathematical Approach to Patterns and Form in Plant Growth , Wiley Interscience, New York.   Jean R. V. (1989) : Phyllotaxis : a reappraisal,  Can. J. Bot. 67 , 3103-3107.   Jean R. V. (1990): A synergic approach to plant pattern generation,  Math. Biosci. 98 , 13-47.   Jean R. V. (1992a): Nomothetical modelling of spiral symmetry in biology. In  Fivefold Symmetry , Hargittai I. (ed.), World Scientific, Singapore, pp. 505-528.   Jean R. V. (1992b): On the origins of spiral symmetry in plants. In  Spiral Symmetry,  Hargittai I. (ed.), World Scientific, Singapore, pp. 323 - 351.   Jean R. V. (1994) :  Phyllotaxis : A systemic Study in Plant Morphogenesis , Cambridge University Press, NY, USA.   Jurgen H., Peitgen H-O. and Saupe D. (1990) : The language of fractals.  Sci. Amer. 263,  40-49.   Kappraff, J. (1992): The relationship between mathematics and mysticism of the golden mean through history. In  Fivefold Symmetry , Hargittai I.(ed.), World Scientific, Singapore, pp. 33-65.   Kerwin L. (1963) :  Atomic Physics : An Introduction,  Holt, Rinehart and Winston, New York.   Klir G.J. (1993) : Systems science : a guided tour,  J. Biol. Systems 1,  27-58.   Lord E.A. (1991) : Quasicrystals and penrose patterns,  Current Science 61(5)  , 313-319.   Lorenz E.N. (1963) : Deterministic non-periodic flow,  J. Atmos. Sci. 20,  130-141.   Maddox J. (1988) : Licence to slang Copenhagen ?,  Nature 332,  581.   Mandelbrot B.B. (1977) :  Fractals : Form, Chance and Dimension , W.H.Freeman, San Francisco.   Mary Selvam A. (1990) : Deterministic chaos, fractals and quantumlike mechanics in atmospheric flows,  Can. J. Phys. 68,  831-841.   Mary Selvam A., Pethkar J.S. and Kulkarni M.K. (1992) : Signatures of a universal spectrum for atmospheric interannual variability in rainfall time series over the indian region,  Int'l. J. Climatol. 12,  137-152.   Mary Selvam A. (1993) : Universal quantification for deterministic chaos in dynamical systems,  Appl. Math. Modelling 17,  642-649.   Mary Selvam A. and Joshi R.R. (1995) : Universal spectrum for interannual variability in  COADS  global air and sea-surface temperatures,  Int'l. J. Climatol. 15,  613-623.   Mood A.M. and Graybill F.A. (1963) :  Introduction to the Theory of Statistics,  McGraw-Hill, New York.   Mukerjee M. (1995) : Quasimodal,  Sci. Amer. 273 (2) ,  18.   Nauenberg M., Stroud C. and Yeazell J. (1994) : The classical limit of an atom,  Sci. Amer. 270,  24-29.   Needham A. R., Rouse J.L. and Bursill L. A. (1993) : Chirality and phyllotaxis of  Helianthus Tuberosus  :   A Multihead Sunflower,  Current Topics in Bot. Research 1,  79-81.   Nelson D. R. (1986) : Quasicrystals,  Sci. Amer. 255,  42-51.   Nicolis G. and Prigogine I. (1977) :  Self-Organization in Non-Equilibrium Systems,  Wiley, New York.   Peacocke A. R. (1989) :  The Physical Chemistry of Biological Organization , Clarendon Press, Oxford, U.K.    Penrose R. (1974) : The role of asthetics in pure and applied mathematical research,  Bull. Inst. Math. Appl. 10 , 266- 271, reprinted in  The Physics of Quasicrystals , P.J. Steinhardt and S. Ostlund (eds.), World Scientific,Singapore.   Penrose R. (1979) : Pentaplexity,  Math. Intell. 2 (1) ,  32-37.   Poincare H. (1892) :  Les Methodes Nouvelles de la Mecanique Celeste , Gautheir-Villars, Paris.   Prigogine I. (1980) :  From Being to Becoming,  Freeman, San Francisco, CA, USA.   Prigogine I. and Stengers I. (1988) :  Order Out of Chaos,  3rd ed. Fontana Paperbacks, London.   Rae, A. (1988) :  Quantum Physics : Illusion or Reality? , Cambridge University Press, New York.   Richards F.J. (1948) : The geometry of phyllotaxis and its origin. In  Symp. Soc. Exper. Biol. 2 . Cambridge University Press, Cambridge, pp. 217-245.   Rosen R. (1993) : Some random thoughts about chaos and some chaotic thoughts about randomness,  J. Biol. Systems 1 (1), 19-26.    Ryan G.W., Rouse J.L. and Bursill L.A. (1991) : Quantitative analysis of sunflower seed packing,  J. Theor. Biol. 147 , 303-328.   Schoute J.C. (1913) : Beitrage zur Blattstellunglehre,  Rec. Trav. Bot. Neerl 10,  153-235.   Sheldrake R. (1985) :  A New Science of Life : The Hypothesis of Formative Causation,  Anthony Bland, London.   Schroeder M. (1990) :  Fractals, Chaos and Power Laws,  W.H. Freeman and Co., N.Y.   Steeves T. A. and Sussex I. M. (1989) :  Patterns in Plant Development , 2nd ed. Cambridge University Press, New York.   Steinhardt P.J. and Ostlund S. (eds.) (1987) :  The Physics of Quasicrystals,  World Scientific, Singapore.   Stevens P.S. (1974) :  Patterns in Nature,  Little, Brown and Co. Inc., Boston, USA.   Stewart I. (1992) : Where do nature's patterns come from ?,  Nature 135, 14.    Stewart I. (1995) : Daisy, daisy, give your answer do,  Sci. Amer. 272,  76-79.   Stoddart F. (1988) : Unnatural product synthesis,  Nature 334,  10-11.   Tagore R. (1967) :  Stray Birds , Macmillan and Company Limited, U.K.   Tarasov L. (1986) :  This Amazingly Symmetrical World,  Mir Publishers, Moscow.   Tessier Y., Lovejoy S. and Schertzer D. (1993) : Universal multifractals : theory and observations for rain and clouds,  J. Appl. Meteor. 32,  223-250.   Thompson D.W. (1963) :  On Growth and Form,  2nd ed. Cambridge University Press, Cambridge.   Turing A.M. (1952) : The chemical basis of morphogenesis,  Phil. Trans. Roy Soc.   (London) B237 , 37-52.   Vitiello G. (1992) : Coherence and electromagnetic fields in living matter,  Nanobiology 1,  221-228.   Von Baeyer H. (1990) : Impossible crystals,  Discover February,  69-78.   Voss R.F. (1993) :  1/f  Noise and fractals in DNA-base sequences. In  Applications of Fractals and Chaos,  Crilly A.J. Earnshaw R.A. and Jones H. (eds.), Springer-Verlag, New York, 7-20.   Wardlaw C.W. (1949) : Experiments on organogenesis in ferns,  Growth (Suppl.) 13,  93-131.   West B.J. and Shlesinger M.F. (1989) : On the ubiquity of  1/f  noise,  Int'l. J. Mod. Phys. B3 (6), 795-819.   West B.J. (1990) : Fractal forms in physiology,  Int’l. J. Mod. Phys.   B4 (10), 1629-1669.                    LEGEND       Fig. 1  : The quasiperiodic Penrose tiling pattern.   Fig. 2  : Wave trains and cloud formation in atmospheric flows.                             Fig. 1  : The quasiperiodic Penrose tiling pattern.                                         Fig. 2  : Wave trains and cloud formation in atmospheric flows.
GX238-04-3201358	"Mathematics by Exp eriment: Plausible Reasoning in the 21st Century and Exp eriments in Mathematics: Computational Paths to Discovery Jonathan M. Borwein Centre for Experimental and Constructive Mathematics Department of Mathematics Simon Fraser University David H. Bailey Lawrence Berkeley National Laboratory Roland Girgensohn Zentrum Mathematik, Technische Universit Munchen at  Copyright c 2003 September 30, 2003   i  Preface This document is an adapted selection of excerpts from two newly published books, Mathematics by Experiment: Plausible Reasoning in the 21st Century, and Experimentation in Mathematics: Computational Paths to Discovery, published by AK Peters, Natick, Massachussetts. We have gleaned from these two volumes material that explains what experimental mathematics is all about, as well as some of the more engaging examples of experimental mathematics in action. The experimental methodology that we describe in these books provides a compelling way to generate understanding and insight; to generate and confirm or confront conjectures; and generally to make mathematics more tangible, lively and fun for both the professional researcher and the novice. We have concentrated primarily on examples from analysis and number theory, but there are numerous excursions into other areas of mathematics as well. Much of this material is gleaned from existing sources, but there is a significant amount of material that, as far as we are aware, has not yet appeared in the literature. Each of the two volumes is targeted to a fairly broad cross-section of mathematically trained readers. Most of the first volume should be readable by anyone with solid undergraduate coursework in mathematics. Most of the second volume should be readable by persons with upper-division undergraduate or graduate-level coursework. Some programming experience is useful, but not required. Borwein's work is supported by the Canada Research Chair Program and the Natural Sciences and Engineering Council of Canada. Bailey's work is supported by the Director, Office of Computational and Technology Research, Division of Mathematical, Information, and Computational Sciences of the U.S. Department of Energy, under contract number DE-AC03-76SF00098. Jonathan M. Borwein David H. Bailey Roland Girgensohn jborwein@cecm.sfu.ca dhbailey@lbl.gov girgen@cecm.sfu.ca   ii  Chapters of the Two Volumes Volume Chapter Title No. Pages 1 1 What is Experimental Mathematics? 52 2 Experimental Mathematics in Action 66 3 Pi and Its Friends 48 4 Normality of Numbers 34 5 The Power of Constructive Proofs I 44 6 Numerical Techniques I 32 7 Making Sense of Experimental Math 26 Bibliography and Index 25 Total 327 2 1 Sequences, Series, Products and Integrals 76 2 Fourier Series and Integrals 66 3 Zeta Functions and Multizeta Valaues 58 4 Partitions and Powers 46 5 Primes and Polynomials 40 6 The Power of Constructive Proofs II 40 7 Numerical Techniques II 40 Bibliography and Index 26 Total 392  Exp erimental Mathematics Web Site The authors have established a web site containing an updated collection of links to many of the URLs mentioned in the two volumes, plus errata, software, tools, and other web useful information on experimental mathematics. This can be found at the following URL: http://www.expmath.info   Contents 1 What is Exp erimental Mathematics? 1.1 Background . . . . . . . . . . . . . . . 1.2 Proof versus Truth . . . . . . . . . . . 1.3 Paradigm Shifts . . . . . . . . . . . . . 1.4 Commentary and Additional Examples 2 Exp 2.1 2.2 2.3 2.4 2.5 2.6 2.7 1 1 3 4 6 7 7 9 12 15 17 19 20  . . . .  . . . .  . . . . . . . . . . .  . . . . . . . . . . .  . . . . . . . . . . .  . . . . . . . . . . .  . . . . . . . . . . .  . . . . . . . . . . .  . . . . . . . . . . .  . . . . . . . . . . .  . . . . . . . . . . .  . . . . . . . . . . .  . . . . . . . . . . .  . . . . . . . . . . .  . . . . . . . . . . .  erimental Mathematics in Action A Curious Anomaly in the Gregory Series Bifurcation Points in the Logistic Iteration Experimental Mathematics and Sculpture Recognition of Euler Sums . . . . . . . . . Quantum Field Theory . . . . . . . . . . . Definite Integrals and Infinite Series . . . . Commentary and Additional Examples . .  3 Pi and Its Friends 23 3.1 Computing Individual Digits of Pi . . . . . . . . . . . . . . . . . . 23 3.2 Commentary and Additional Examples . . . . . . . . . . . . . . . 30 4 Sequences, Series, Pro ducts and Integrals 4.1 Pi Is Not 22/7 . . . . . . . . . . . . . . . . 4.2 High Precision Fraud . . . . . . . . . . . . 4.3 Knuth's Series Problem . . . . . . . . . . . 4.4 Commentary and Additional Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 32 35 38 40  5 Partitions and Powers 43 5.1 Partition Functions . . . . . . . . . . . . . . . . . . . . . . . . . . 43 iii   iv 5.1.1 The ""Exact"" Formula Singular Values . . . . . . . Some Fibonacci Sums . . . . Commentary and Additional for the Partition .......... .......... Examples . . . . Function ..... ..... ..... . . . .  CONTENTS . . . . . . . . . . . . . . . . . . . . 45 46 47 50 53 53 54 58 61 64  5.2 5.3 5.4  6 Numerical Techniques I I 6.1 Numerical Quadrature . . . . . . . . . . . . . . . . . . . . . . . . 6.1.1 Error Function Quadrature . . . . . . . . . . . . . . . . . . 6.2 Commentary and Additional Examples . . . . . . . . . . . . . . . Bibliography Index   Chapter 1 What is Exp erimental Mathematics? The computer has in turn changed the very nature of mathematical experience, suggesting for the first time that mathematics, like physics, may yet become an empirical discipline, a place where things are discovered because they are seen. David Berlinski, ""Ground Zero: A Review of The Pleasures of Counting, by T. W. Koerner,"" 1997 If mathematics describes an ob jective world just like physics, there is no reason why inductive methods should not be applied in mathematics just the same as in physics. Kurt G odel, Some Basic Theorems on the Foundations, 1951  1.1  Background  [From Volume 1, Section 1.1] One of the greatest ironies of the information technology revolution is that while the computer was conceived and born in the field of pure mathematics, through the genius of giants such as John von Neumann and Alan Turing, until recently this marvelous technology had only a minor impact within the field that gave it birth. 1   2  CHAPTER 1. WHAT IS EXPERIMENTAL MATHEMATICS?  This has not been the case in applied mathematics, as well as in most other scientific and engineering disciplines, which have aggressively integrated computer technology into their methodology. For instance, physicists routinely utilize numerical simulations to study exotic phenomena ranging from supernova explosions to big bang cosmology--phenomena that in many cases are beyond the reach of conventional laboratory experimentation. Chemists, molecular biologists, and material scientists make use of sophisticated quantum-mechanical computations to unveil the world of atomic-scale phenomena. Aeronautical engineers employ large-scale fluid dynamics calculations to design wings and engines for jet aircraft. Geologists and environmental scientists utilize sophisticated signal processing computations to probe the earth's natural resources. Biologists harness large computer systems to manage and analyze the exploding volume of genome data. And social scientists--economists, psychologists, and sociologists--make regular use of computers to spot trends and inferences in empirical data. Perhaps the most important advancement in bringing mathematical research into the computer age is the development of broad spectrum mathematical software products, such as Mathematica and Maple. These days, many mathematicians are highly skilled with these tools and use them as part of their day-to-day research work. As a result, we are starting to see a wave of new mathematical results discovered partly or entirely with the aid of computer-based tools. Further developments in hardware (the gift of Moore's Law of semiconductor technology), software tools, and the increasing availability of valuable Internetbased facilities, are all ensuring that mathematicians will have their day in the computational sun. This new approach to mathematics--the utilization of advanced computing technology in mathematical research--is often called experimental mathematics. The computer provides the mathematician with a ""laboratory"" in which he or she can perform experiments: analyzing examples, testing out new ideas, or searching for patterns. Our books are about this new, and in some cases not so new, way of doing mathematics. To be precise, by experimental mathematics, we mean the methodology of doing mathematics that includes the use of computations for: (1) gaining insight and intuition; (2) discovering new patterns and relationships; (3) using graphical displays to suggest underlying mathematical principles; (4) testing and especially falsifying conjectures; (5) exploring a possible result to see if it is worth formal proof; (6) suggesting approaches for formal   1.2. PROOF VERSUS TRUTH  3  proof; (7) replacing lengthy hand derivations with computer-based derivations; (8) confirming analytically derived results. Note that the above activities are, for the most part, quite similar to the role of laboratory experimentation in the physical and biological sciences. In particular, they are very much in the spirit of what is often termed ""computational experimentation"" in physical science and engineering, which is why we feel the qualifier ""experimental"" is particularly appropriate in the term experimental mathematics.  1.2  Pro of versus Truth  [From Volume 1, Sections 1.3] In any discussion of an experimental approach to mathematical research, the questions of reliability and standards of proof justifiably come to center stage. We certainly do not claim that computations utilized in an experimental approach to mathematics by themselves constitute rigorous proof of the claimed results. Rather, we see the computer primarily as an exploratory tool to discover mathematical truths, and to suggest avenues for formal proof. Nonetheless, we feel that in many cases computations constitute very strong evidence, evidence that is at least as compelling as some of the more complex formal proofs in the literature. Prominent examples include: (1) the determina24 tion that the Fermat number F24 = 22 + 1 is composite, by Crandall, Mayer, and Papadopoulos [24]; (2) the recent computation of  to more than one trillion decimal digits by Yasumasa Kanada and his team; and (3) the Internet-based computation of binary digits of  beginning at position one quadrillion organized by Colin Percival. These are among the largest computations ever done, mathematical or otherwise (the  computations are described in greater detail in Volume 1, Chapter 3). Given the numerous possible sources of error, including programming bugs, hardware bugs, software bugs, and even momentary cosmic-ray induced glitches (all of which are magnified by the sheer scale of these computations), one can very reasonably question the validity of these results. But for exactly such reasons, computations such as these typically employ very strong validity checks. In the case of computations of digits of  , it has been customary for many years to verify a result either by repeating the computation using a different algorithm, or by repeating with a slightly different   4  CHAPTER 1. WHAT IS EXPERIMENTAL MATHEMATICS?  index position. For example, if one computes hexadecimal digits of  beginning at position one trillion (we shall see how this can be done in Chapter 3), then this can be checked by repeating the computation at hexadecimal position one trillion minus one. It is easy to verify (see Algorithm 3 in Section 3.1) that these two calculations take almost completely different tra jectories, and thus can be considered ""independent."" If both computations generate 25 hexadecimal digits beginning at the respective positions, then 24 digits should perfectly overlap. If these 24 hexadecimal digits do agree, then we can argue that the probability that these digits are in error, in a very strong (albeit heuristic) sense, is roughly one part in 1624  7.9  1028 , a figure much larger even than Avogadro's number (6.022  1022 ). Percival's actual computation of the quadrillionth binary digit (i.e., the 250 trillionth hexadecimal digit) of  was verified by a similar scheme, which for brevity we have simplified here. Independent checks and extremely high numerical confidence levels still do not constitute formal proofs of correctness. What's more, we shall see in Section 1.4 of the second volume (and in Section 4.2 of this document) some examples of ""high-precision frauds,"" namely ""identities"" that hold to high precision, yet are not precisely true. Even so, one can argue that many computational results are as reliable, if not more so, than a highly complicated piece of human mathematics. For example, perhaps only 50 or 100 people alive can, given enough time, digest al l of Andrew Wiles' extraordinarily sophisticated proof of Fermat's Last Theorem. If there is even a one percent chance that each has overlooked the same subtle error (and they may be psychologically predisposed to do so, given the numerous earlier results that Wiles' result relies on), then we must conclude that computational results are in many cases actually more secure than the proof of Fermat's Last Theorem.  1.3  Paradigm Shifts  [From Volume 1, Section 1.4] We acknowledge that the experimental approach to mathematics that we propose will be difficult for some in the field to swallow. Many may still insist that mathematics is all about formal proof, and from their viewpoint, computations have no place in mathematics. But in our view, mathematics is not ultimately about formal proof; it is instead about secure mathematical knowledge. We   1.3. PARADIGM SHIFTS  5  are hardly alone in this regard--many prominent mathematicians throughout history have either exemplified or explicitly espoused such a view. Jacques Hadamard (18651963) was perhaps the greatest mathematician to think deeply and seriously about cognition in mathematics. He nicely declared: The ob ject of mathematical rigor is to sanction and legitimize the conquests of intuition, and there was never any other ob ject for it. (J. Hadamard, from E. Borel, ""Lecons sur la theorie des fonctions,"" 1928, quoted in [40]) G. H. Hardy was another of the 20th century's towering figures in mathematics. In addition to his own mathematical achievements in number theory, he is well known as the mentor of Ramanujan. In his Rouse Ball lecture in 1928, Hardy emphasized the intuitive and constructive components of mathematical discovery: I have myself always thought of a mathematician as in the first instance an observer, a man who gazes at a distant range of mountains and notes down his observations. . . . The analogy is a rough one, but I am sure that it is not altogether misleading. If we were to push it to its extreme we should be led to a rather paradoxical conclusion; that we can, in the last analysis, do nothing but point; that proofs are what Littlewood and I call gas, rhetorical flourishes designed to affect psychology, pictures on the board in the lecture, devices to stimulate the imagination of pupils. This is plainly not the whole truth, but there is a good deal in it. The image gives us a genuine approximation to the processes of mathematical pedagogy on the one hand and of mathematical discovery on the other; it is only the very unsophisticated outsider who imagines that mathematicians make discoveries by turning the handle of some miraculous machine. Finally the image gives us at any rate a crude picture of Hilbert's metamathematical proof, the sort of proof which is a ground for its conclusion and whose ob ject is to convince. [17, Preface] As one final example, in the modern age of computers, we quote John Milnor, a contemporary Fields medalist:   6  CHAPTER 1. WHAT IS EXPERIMENTAL MATHEMATICS? If I can give an abstract proof of something, I'm reasonably happy. But if I can get a concrete, computational proof and actually produce numbers I'm much happier. I'm rather an addict of doing things on computer, because that gives you an explicit criterion of what's going on. I have a visual way of thinking, and I'm happy if I can see a picture of what I'm working with. [41, page 78]  1.4  Commentary and Additional Examples  [From Volume 1, Chapter 1 Commentary] 1. Hales' computer-assisted pro of of Kepler's conjecture. In 1611, Kepler described the stacking of equal-sized spheres into the familiar arrangement we see for oranges in the grocery store. He asserted that this packing is the tightest possible. This assertion is now known as the Kepler conjecture, and has persisted for centuries without rigorous proof. Hilbert included the Kepler conjecture in his famous list of unsolved problems in 1900. In 1994, Thomas Hales, now at the University of Pittsburgh, proposed a five-step program that would result in a proof: (a) treat maps that only have triangular faces; (b) show that the face-centered cubic and hexagonal-close packings are local maxima in the strong sense that they have a higher score than any Delaunay star with the same graph; (c) treat maps that contain only triangular and quadrilateral faces (except the pentagonal prism); (d) treat maps that contain something other than a triangle or quadrilateral face; (e) treat pentagonal prisms. In 1998, Hales announced that the program was now complete, with Samuel Ferguson (son of Helaman Ferguson) completing the crucial fifth step. This pro ject involved extensive computation, using an interval arithmetic package, a graph generator, and Mathematica. As this book was going to press, the Annals of Mathematics has decided to publish Hales' paper, but with a cautionary note, because although a team of referees is ""99% certain"" that the computer-assisted proof is sound, they have not been able to verify every detail [42]. One wonders if every other article in this journal has implicitly been certified to be correct with more than 99% certainty.   Chapter 2 Exp erimental Mathematics in Action The purpose of computing is insight, not numbers. Richard Hamming, Numerical Methods for Scientists and Engineers, 1962 In this chapter, we will present a few particularly engaging examples of modern experimental mathematics in action. We invite those readers with access to some of the computational tools we mention below to personally try some of these examples.  2.1  A Curious Anomaly in the Gregory Series  [From Volume 1, Section 2.2] In 1988, Joseph Roy North of Colorado Springs observed that Gregory's series for  ,   =4 k=1  (-1)k+1 = 4(1 - 1/3 + 1/5 - 1/7 +    ), 2k - 1  (2.1.1)  when truncated to 5,000,000 terms, gives a value that differs strangely from the true value of  . Here is the truncated Gregory value and the true value of  : 7   8  CHAPTER 2. EXPERIMENTAL MATHEMATICS IN ACTION  3.14159245358979323846464338327950278419716939938730582097494182230781640... 3.14159265358979323846264338327950288419716939937510582097494459230781640... 2 -2 10 -122 2770  The series value differs, as one might expect from a series truncated to 5,000,000 terms, in the seventh decimal place--a ""4"" where there should be a ""6."" But the next 13 digits are correct! Then, following another erroneous digit, the sequence is once again correct for an additional 12 digits. In fact, of the first 46 digits, only four differ from the corresponding decimal digits of  . Further, the ""error"" digits appear to occur in positions that have a period of 14, as shown above. Such anomalous behavior begs explanation. Once observed, it is natural (and easy given a modern computer algebra system) to ask if something similar happens with the logarithm. Indeed it does, as the following value obtained by truncating the series log 2 = 1 - 1/2 + 1/3 - 1/4 +    shows: 0.69314708055995530941723212125817656807551613436025525140068000949418722... 0.69314718055994530941723212145817656807550013436025525412068000949339362... 1 -1 2 -16 272 -7936  Here again, the ""erroneous"" digits appear in locations with a period of 14. In the first case, the differences from the ""correct"" values are (2, -2, 10, -122, 2770), while in the second case the differences are (1, -1, 2, -16, 272, -7936). We note that each integer in the first set is even; dividing by two, we obtain (1, -1, 5, -122, 1385). How can we find out exactly what is going on here? A great place to start is by enlisting the help of an excellent resource for the computational mathematician: Neil Sloane and Simon Plouffe's Internet-based integer sequence recognition tool, available at http://www.research.att.com/~njas/sequences. This tool has no difficulty recognizing the first sequence as ""Euler numbers"" and the second as ""tangent numbers."" Euler numbers and tangent numbers are defined in terms of the Taylor's series for sec x and tan x, respectively:   sec x = k=0   (-1)k E2k x (2k )!  2k  tan x = k=0  (-1)k+1 T2k+1 x (2k + 1)!  2k+1  .  (2.1.2)   2.2. BIFURCATION POINTS IN THE LOGISTIC ITERATION  9  Indeed, this discovery, made originally through the print version of the sequence recognition tool available more than a decade ago, led to a formal proof that these sequences are indeed the source of the ""errors"" in these sequences. The precise result is that the following asymptotic expansions hold:  -2 2 log 2 - k=1 N /2  k=1 N /2  (-1)k+1  2k - 1 (-1)k k +1    m=0  E2m N 2m+1   (2.1.3) (2.1.4)  1  + N  m=1  T2m-1 . N 2m  Now the genesis of the anomaly mentioned above is clear: North, in computing  by Gregory's series, had by chance truncated the series at 5,000,000 terms, which is exactly one-half of a fairly large power of ten. Indeed, setting N = 10, 000, 000 in Equation (2.1.3) shows that the first hundred or so digits of the truncated series value are small perturbations of the correct decimal expansion for  . And the asymptotic expansions show up on the computer screen, as we observed above. Similar phenomena occur for other constants. (See [13] for proofs of (2.1.3) and (2.1.4), together with some additional details.)  2.2  Bifurcation Points in the Logistic Iteration  [From Volume 1, Section 2.3] One of the classic examples of a chaotic iteration is known as the logistic iteration: Fix a real number r > 0, select x0 in the unit interval (0, 1), and then iterate x k+1  = rxk (1 - xk ).  (2.2.5)  This is termed the ""logistic"" iteration because of its roots in computational ecology: It mimics the behavior of a biological population, which, if it becomes too numerous, exhausts its available food supply and then falls back to a smaller population, possibly oscillating in an irregular manner over many generations. For values of r < 1, the iterates (xk ) quickly converge to zero. For 1 < r < 3, the iterates converge to a single nonzero limit point. At r = 3, a bifurcation occurs: For 3 < r < 3.449489 . . . = 1 + 6, the iterates oscillate between two   10  CHAPTER 2. EXPERIMENTAL MATHEMATICS IN ACTION  Figure 2.1: Bifurcation in the logistic iteration.  distinct  limit points. A second bifurcation occurs at r = 1 + 6. In particular, for 1 + 6 < r < 3.544090359 . . ., the iterates oscillate in a periodic fashion between four distinct limit points. This pattern of limit point bifurcation and period doubling occurs at successively shorter intervals, until r > 3.5699457 . . ., when iterates behave in a completely chaotic manner. This behavior is shown in Figure 2.1. Until recently, the identity of the third bifurcation point, namely the constant b3 = 3.544090359 . . ., was not known. It is fairly straightforward, by means of recursive substitutions of Equation (2.2.5), to demonstrate that this constant must be algebraic, but the bound on the degree of the integer polynomial that b3 satisfies is quite large and thus not very useful. A tool that can be used in such situations is an integer relation algorithm. This is an algorithm which, when given n real numbers (x1 , x2 ,    , xn ), returns integers (a1 , a2 ,    , an ), not all zero, such that a1 x1 +a2 x2 +  +an xn = 0 (if such a solution exists). Such computations must be done using very high precision arithmetic, or else the results are not numerically significant. At present the best algorithm for integer relation detection appears to be the ""PSLQ"" algorithm   2.2. BIFURCATION POINTS IN THE LOGISTIC ITERATION  11  of mathematician-sculptor Helaman Ferguson [30, 6, 8], although the ""LLL"" algorithm is also often used. We discuss integer relation detection in greater depth in Volume 1, Chapter 6. For the time being we mention the Internetbased integer relation tool at http://www.cecm.sfu.ca/pro jects/IntegerRelations and the Experimental Mathematician's Toolkit at http://www.expmath.info. One straightforward application of an integer relation tool is to recover the polynomial satisfied by an algebraic number. If you suspect that a constant , whose numerical value can be calculated to high precision, is algebraic of degree n, then you can test this conjecture by computing the (n + 1)-long vector (1, , 2 ,    , n ), and then using this vector as input to an integer relation calculation. If it finds a solution vector (a0 , a1 , a2 ,    , an ) with a sufficiently high degree of numerical accuracy, then you can be fairly confident that these integers are precisely the coefficients of the polynomial satisfied by . In the present example, where  = b3 , a predecessor algorithm to PSLQ recovered the polynomial 0 = 4913 + 2108t2 - 604t3 - 977t4 + 8t5 + 44t6 + 392t7 - 193t8 - 40t9 +48t10 - 12t11 + t12 . (2.2.6) You might like to try to rediscover this polynomial by using the Internet-based tool mentioned above. To do this requires a high-precision value of b3 . Its value correct to 120 decimal digits is: 3.5440903595 5192285361 5965986604 8045405830 9984544457 3675457812 2530305842 9428588630 1225625856 6424891799 9626089927 7589974545 If you do not wish to type this number in, you may find it by using Mathematica: FindRoot[4913 + 2108*t^2 - 604*t^3 - 977*t^4 + 8*t^5 + 44*t^6 + 392*t^7 - 193*t^8 - 40*t^9 + 48*t^10 - 12*t^11 + t^12 == 0, {t, 3.544}, WorkingPrecision -> 125]  or by using a similar command with the Experimental Mathematician's Toolkit. Recently, the fourth bifurcation point b4 = 3.564407266095 . . . was identified by a similar, but much more challenging, integer relation calculation. In particular, it was found that  = -b4 (b4 - 2) satisfies a certain integer polynomial of degree 120. The recovered coefficients descend monotonically from   12  CHAPTER 2. EXPERIMENTAL MATHEMATICS IN ACTION  25730  1.986  1072 down to 1. This calculation required 10,000 decimal digit precision arithmetic, and more than one hour on 48 processors of a parallel computer system. Full details can be found in [8]. The relation produced was recently verified by Konstantinos Karamanos, using the Magma computer algebra system [36].  2.3  Exp erimental Mathematics and Sculpture  [From Volume 1, Section 2.4] In the previous section, we mentioned the PSLQ algorithm, which was discovered in 1993 by Helaman Ferguson. This is certainly a signal accomplishment-- for example, the PSLQ algorithm (with associated lattice reduction algorithms) was recently named one of ten ""algorithms of the century"" by Computing in Science and Engineering [6]. Nonetheless Ferguson is even more well-known for his numerous mathematics-inspired sculptures, which grace numerous research institutes in the United States. Photos and highly readable explanations of these sculptures can be seen in a lovely book written by his wife, Claire [29]. Together, the Fergusons recently won the 2002 Communications Award, bestowed by the Joint Policy Board of Mathematics. The citation for this award declares that the Fergusons ""have dazzled the mathematical community and a far wider public with exquisite sculptures embodying mathematical ideas, along with artful and accessible essays and lectures elucidating the mathematical concepts."" There is a remarkable and unanticipated connection between Ferguson's PSLQ algorithm and at least one of Ferguson's sculptures. It is known that the volumes of complements of certain knot figures (which volumes in R3 are infinite) are finite in hyperbolic space, and sometimes are given by certain explicit formulas. This is not true of all knots. Many of these hyperbolic complements of knots correspond to certain discrete quotient subgroups of matrix groups. One of Ferguson's sculptures, known as the ""Eight-Fold Way,"" is housed at the Mathematical Sciences Research Institute in Berkeley, California (see Figure 2.2, courtesy of Helaman Ferguson).   2.3. EXPERIMENTAL MATHEMATICS AND SCULPTURE 13  Figure 2.2: Ferguson's ""Eight-Fold Way"" and ""Figure-Eight Knot Complement"" (courtesy of Helaman Ferguson).   14  CHAPTER 2. EXPERIMENTAL MATHEMATICS IN ACTION  Another of Ferguson's well-known sculptures is the ""Figure-Eight Complement II"" (see Figure 2.2, courtesy of Helaman Ferguson). It has been known for some time that the hyperbolic volume V of the figure-eight knot complement is given by the formula V  =23   1 n 2n n  2n-1  n=1  k=n  1 k  (2.3.7) (2.3.8)  = 2.029883212819307250042405108549 . . .   In 1998, British physicist David Broadhurst conjectured that V / 3 is a rational linear combination of   Cj = n=0  (-1)n . 27n (6n + j )2  (2.3.9)  Indeed, it is, as Broadhurst [18] found using a PSLQ program:  18 3 (-1)n 18 24 V= - - 9 n=0 27n (6n + 1)2 (6n + 2)2 (6n + 3) - 6 2 + (6n + 4)2 (6n + 5) 2  2  .  (2.3.10)  You can verify this yourself, using for example the Mathematician's Toolkit, available at http://www.expmath.info. Just type the following lines of code: v = 2 * sqrt[3] * sum[1/(n * binomial[2*n,n]) * sum[1/k, \ {k, n, 2*n-1}], {n, 1, infinity}] pslq[v/sqrt[3], table[sum[(-1)^n/(27^n*(6*n+j)^2), \ {n, 0, infinity}], {j, 1, 6}]]  When this is done you will recover the solution vector (9, -18, 18, 24, 6, -2, 0). A proof that formula (2.3.10) holds, together with a number of other identities for V , is given in the Volume 1, Section 2 Commentary. As we shall see in Section 3.1, constants given by a formula of the general type given in (2.3.10), namely a ""BBP-type"" formula, possess some remarkable properties, among them the fact that you can calculate the n-th digit (base-3 digit in this case) of such constants by means of a simple algorithm, without having to compute any of the first n - 1 digits.   2.4. RECOGNITION OF EULER SUMS  15  2.4  Recognition of Euler Sums  [From Volume 1, Section 2.5] In April 1993, Enrico Au-Yeung, an undergraduate at the University of Waterloo, brought to the attention of one of us (Borwein) the curious result [11]   1+ k=1  1 1 +  + 2 k  2  k  -2  = 4.59987 . . . (2.4.11)    17 17 4  (4) = . 4 360  The function  (s) in (2.4.11) is the classical Riemann zeta function,    (s) = n=1  1 . ns  Bernoulli showed that for even integers,  (2n) is a rational multiple of  2n [15]. (Bernoulli's result is proved in Section 3.2 of the second volume of this work.) Au-Yeung had computed the sum in (2.4.11) to 500,000 terms, giving an accuracy of 5 or 6 decimal digits. Suspecting that his discovery was merely a numerical coincidence, Borwein sought to compute the sum to a higher level of precision. Using Fourier analysis and Parseval's equation, he obtained 1 2  0  t ( - t) log (2 sin ) dt = 2 2 2    ( (n  n=1  n 12 k=1 k ) + 1)2  .  (2.4.12)  The idea here is that the series on the right of (2.4.12) permits one to evaluate (2.4.11), while the integral on the left can be computed using the numerical quadrature facility of Mathematica or Maple. When he did this, he was surprised to find that the conjectured identity holds to more than 30 digits. We should add here that by good fortune, 17/360 = 0.047222 . . . has period one and thus can plausibly be recognized from its first six digits, so that Au-Yeung's numerical discovery was not entirely far-fetched. What Borwein did not know at the time was that Au-Yeung's suspected identity follows directly from a related result proved by De Doelder in 1991 [28]. In fact, it had cropped up even earlier as a problem in the American   16  CHAPTER 2. EXPERIMENTAL MATHEMATICS IN ACTION  Mathematical Monthly, but the story goes back further still. Some historical research showed that Euler considered these summations. In response to a letter from Goldbach, he examined sums that are equivalent to   1+ k=1  1 1 +  + m m 2 k  (k + 1)  -n  .  (2.4.13)  The great Swiss mathematician was able to give explicit values for certain of these sums in terms of the Riemann zeta function. For example, he found an explicit formula for the case m = 1, n  2. Sums of this general form are nowadays known as ""Euler sums"" or ""Euler-Zagier sums."" High precision calculations of many of these sums, together with considerable investigations involving heavy use of Maple's symbolic manipulation facilities, eventually yielded numerous new results. Below are just a few of the interesting results that were first discovered numerically and have since been established analytically [12]. Since these results were first obtained in 1994, many more specific identities have been discovered, and a growing body of general formulas and other results have been proven. These results, together with the underlying numerical and symbolic techniques used in their derivation, are discussed further in Chapter 3 of the second volume.   1+ k=1   1 1 +  + 2 k  2  (k + 1) 3  -4  = =  37 6  -  2 (3) 22680  k=1  1 1 1 + +  + 2 k  3 (3) +  (k + 1)  -6  1 11 4 37 6 197  (9) +  2  (7) -   (5) -   (3) 24 2 120 7560 +1    1- k=1  1 +    + (-1)k 2 4 Li 5  1 k  2  (k + 1)  -3  =  1 17 11 4 7 log5 (2) -  (5) -  log(2) +  (3) log2 (2) 30 32 720 4 1 1 +  2 log3 (2) -  2  (3), (2.4.14) 18 8 where Lin (x) = k>0 xk /k n denotes the polylogarithm function. 1 2 -   2.5. QUANTUM FIELD THEORY  17  2.5  Quantum Field Theory  [From Volume 1, Section 2.6] In another recent development, David Broadhurst (who discovered the identity (2.3.10) for Ferguson's Clay Math Award sculpture) has found, using similar methods, that there is an intimate connection between Euler sums and constants resulting from evaluation of Feynman diagrams in quantum field theory [19, 20]. In particular, the renormalization procedure (which removes infinities from the perturbation expansion) involves multiple zeta values, which we will discuss in detail in Chapter 3 of the second volume. Broadhurst's recent results are even more remarkable. He has shown [18], using PSLQ computations, that in each of ten cases with unit or zero mass, the finite part of the scalar 3-loop tetrahedral vacuum Feynman diagram reduces to four-letter ""words"" that represent iterated integrals in an alphabet of seven ""letters"" comprising the single 1-form  = dx/x and the six 1-forms k =  dx/(-k - x), where  = (1 + -3)/2 is the primitive sixth root of unity, and k runs from 0 to 5. A four-letter word here is a four-dimensional iterated integral, such as U =  (2 3 0 ) = 1 dx1 x1 dx2 x1 0 x2 0  x 0  2  dx3 (-1 - x3 )  x 0  3  dx4 = (1 - x4 )  j >k>0  (-1)j j 3k  +k  .  There are 74 such four-letter words. Only two of these are primitive terms occurring in the 3-loop Feynman diagrams: U , above, and V = Re[ (2 3 1 )] = j >k>0  (-1)j cos(2 k /3) . j 3k  The remaining terms in the diagrams reduce to products of constants found in Feynman diagrams with fewer loops. These ten cases are shown in Figure 2.3. In these diagrams, dots indicate particles with nonzero rest mass. The formulas that have been found, using PSLQ, for the corresponding constants are given in Table 2.1. In the Table the constant C = k>0 sin( k /3)/k 2 .   18  CHAPTER 2. EXPERIMENTAL MATHEMATICS IN ACTION  t t t  t  rr   t  sr  V1  t t ts  s t  rr   r t   V2A  t t s t  t  rr   t  sr  V2  t t ts  s t  rr   t  sr  N  V3  t t s t   s rr  st  r t   T  V  3S  t t  t s  rr st  s r  t  V3  t t s  t  s rr  st   s r t  L  V4A  t t  ts s  s rr  st   r t  V4  t t s  ts s  s rr  st   r t  N  V  t t s  ts s  s rr  st   s r t  5  V6  Figure 2.3: The ten tetrahedral configurations.  V V V V V V V V V V  1 2A 2N 3T 3S 3L 4A 4N 5 6  = = = = = = = = = =  6 6 6 6 6 6 6 6 6 6             (3) (3) (3) (3) (3) (3) (3) (3) (3) (3)  + - - - - - - - - -  3 (4) 5 (4) 13  (4) - 8U 2 9 (4) 11  (4) - 4C 2 2 15  (4) - 6C 2 4 77  (4) - 6C 2 12 14 (4) - 16U 469  (4) + 8 C 2 - 16V 27 3 13 (4) - 8U - 4C 2  Table 2.1: Formulas found by PSLQ for the ten tetrahedral diagrams.   2.6. DEFINITE INTEGRALS AND INFINITE SERIES  19  2.6  Definite Integrals and Infinite Series  [From Volume 1, Section 2.7] We mention here one particularly useful application of experimental mathematics methodology: evaluating definite integrals and sums of infinite series by means of numerical calculations. In one sense, there is nothing new here, since mathematicians have utilized computers to compute the approximate numerical value of definite integrals and infinite series since the dawn of computing. What we suggest here, however, is a slightly different approach: Use advanced numerical quadrature techniques and series summations methods, extended to the realm of high-precision arithmetic, and then use the computed values (typically accurate to tens or even hundreds of decimal digits) as input to a computerbased constant recognition tool, which hopefully can recognize the constant as a simple expression involving known mathematical constants. We will discuss techniques for computing definite integrals and sums of series to high precision in Section 7.4 of the second volume of this work. For the time being, we simply note that both Mathematica and Maple have incorporated some reasonably good numerical facilities for this purpose, and it is often sufficient to rely on these packages when numerical values are needed. For our first example, we use Maple or Mathematica to compute the following three integrals to over 100 decimal digit accuracy: 1 0  t2 log(t) dt = (t2 - 1)(t4 + 1)  0.180671262590654942792308128981671615337114571018296766266 240794293758566224133001770898254150483799707740 . . .  /4 0  t2 dt = sin2 (t)  0.843511841685034634002620051999528151651689086421444293697 112596906587355669239938399327915596371348023976 . . . (2.6.15)   20  0  CHAPTER 2. EXPERIMENTAL MATHEMATICS IN ACTION x sin x dx = 1 + cos2 x  2.467401100272339654708622749969037783828424851810197656603 337344055011205604801310750443350929638057956006 . . . (2.6.16) (the third of these is from [32]). Both Maple and Mathematica attempt to evaluate these definite integrals analytically. In each case, however, while the results appear to be technically correct, they are not very useful, in that they are either rather lengthy, or involve advanced functions and complex entities. We suspect that there are considerably simpler closed-form versions. Indeed, using the Inverse Symbolic Calculator (ISC) tool (a constant recognition facility) at http://www.cecm.sfu.ca/pro jects/ISC, we obtain the following, based solely on the numerical values above:  1 t2 log(t) dt  2 (2 - 2) = 2 4 32 0 (t - 1)(t + 1)  /4 0  0  t2 dt  2  log(2) =- + +G 16 4 sin2 (t) x sin x dx 2 = , 1 + cos2 x 4  (2.6.17)  where G denotes Catalan's constant   G= n=0  (-1)n . (2n + 1)2  2.7  Commentary and Additional Examples  [From Volume 1, Chapter 2 Commentary] 1. Putnam problem 1995B4. Determine a simple expression for = 8  2207 - 2207 -  1 1 2207 - 1 2207 -     .  (2.7.18)   2.7. COMMENTARY AND ADDITIONAL EXAMPLES  21  Hint: Calculate this limit to 15 decimal place accuracy, using ordinary double-precision arithmetic. Then use the ISC tool, with the ""integer relation algorithm"" option, to recognize the constant as a simple algebraic number. The result can be proved by noting that  8 = 2207 - 1/ 8 , so  that  4 +  -4 = 47. Answer: (3 + 5)/2. 2. Two radical expressions. (From [34, pg. 81, 84]). Express 3  cos  2 + 7 2 + 9  3  cos  4 + 7 4 + 9  3  cos  6  7 8  9  3  cos  3  cos  3  cos  as radicals. Hint: Calculate to high precision, then use the ISC tool to find the polynomial they satisfy.   Answers: 3 1 (5 - 3 3 7) and 3 3 3 9 - 3. 2 2 3. H. S. M. (Donald) Coxeter (19072003). The renowned Canadian geometer H. S. M. Coxeter passed away in late March 2003. Coxeter was known for making extensive use of physical models in his research. A portion of his collection is on display at the University of Toronto, where he worked for 67 years. The model shown in Figure 2.4 now resides at York University in Toronto. Among his numerous published books, Regular Complex Polytopes, for example, is lavishly illustrated with beautiful and often intricate figures. He was a friend of Maurits C. Escher, the graphic artist. In a 1997 paper, Coxeter showed that Escher, despite knowing no mathematics, had achieved ""mathematical perfection"" in his etching ""Circle Limit III."" ""Escher did it by instinct,"" Donald Coxeter noted, ""I did it by trigonometry."" Two sculptures based on Coxeter's work decorate the Fields Institute in Toronto. One, hanging from the ceiling, is a three-dimensional pro jection of a four-dimensional regular polytope whose 120 faces are dodecahedrons as shown in Figure 2.5.   22  CHAPTER 2. EXPERIMENTAL MATHEMATICS IN ACTION  Figure 2.4: Donald Coxeter's own kaleidoscope (courtesy Asia Weiss).  Figure 2.5: A pro jection of a four dimensional polytope.   Chapter 3 Pi and Its Friends I am ashamed to tell you to how many figures I carried these computations, having no other business at the time. Issac Newton, personal journal, 1666 The desire, as well as the need, to calculate ever more accurate values of  , the ratio of the circumference of a circle to its diameter, has challenged mathematicians for many centuries. In recent years,  computations have provided some fascinating examples of computational mathematics.  3.1  Computing Individual Digits of Pi  [From Volume 1, Section 3.4] An outsider might be forgiven for thinking that essentially everything of interest with regards to  has been discovered. But even insiders are sometimes surprised by a new discovery. Prior to 1996, almost all mathematicians believed that if you want to determine the d-th digit of  , you have to generate the entire sequence of the first d digits. (For all of their sophistication and efficiency, the schemes described above all have this property.) But it turns out that this is not true, at least for hexadecimal (base 16) or binary (base 2) digits of  . In 1996, Peter Borwein, Simon Plouffe, and one of the present authors (Bailey) found an algorithm for computing individual hexadecimal or binary digits of  [7]. To be precise, this algorithm: 23   24  CHAPTER 3. PI AND ITS FRIENDS  (1) directly produces a modest-length string of digits in the hexadecimal or binary expansion of  , beginning at an arbitrary position, without needing to compute any of the previous digits; (2) can be implemented easily on any modern computer; (3) does not require multiple precision arithmetic software; (4) requires very little memory; and (5) has a computational cost that grows only slightly faster than the digit position. Using this algorithm, for example, the one millionth hexadecimal digit (or the four millionth binary digit) of  can be computed in less than a minute on a 2001-era computer. The new algorithm is not fundamentally faster than best-known schemes for computing all digits of  up to some position, but its elegance and simplicity are nonetheless of considerable interest. This scheme is based on the following remarkable new formula for  : Theorem 3.1.1   = i=0  1 16i  4 2 1 1 - - - 8i + 1 8i + 4 8i + 5 8i + 6  .  (3.1.1)  Pro of. First note that for any k < 8,  1/ 2 0  xk-1 dx = 1 - x8 =   1/ 2   x 0  k-1+8i  dx (3.1.2)  1 2k/2  i=0   i=0  16i  1 . (8i + k )  Thus one can write   i=0  1 16i  4 2 1 1 - - - 8i + 1 8i + 4 8i + 5 8i + 6    1/ 2 4 2 - 8x3 - 4 2x4 - 8x5 = dx, 1 - x8 0  (3.1.3)   3.1. COMPUTING INDIVIDUAL DIGITS OF PI which on substituting y = 1 0  25    2x becomes 1 0  16 y - 16 dy = 4 - 2 y3 + 4 y - 4 y  4y dy - 2-2 y  1 0  4y - 8 dy y - 2y + 2 2  = .  (3.1.4) 2  However, in presenting this formal derivation, we are disguising the actual route taken to the discovery of this formula. This route is a superb example of experimental mathematics in action. It all began in 1995, when Peter Borwein and Simon Plouffe of Simon Fraser University observed that the following well-known formula for log 2 permits one to calculate isolated digits in the binary expansion of log 2:   log 2 = k=0  1 . k 2k  (3.1.5)  This scheme is as follows. Suppose we wish to compute a few binary digits beginning at position d + 1 for some integer d > 0. This is equivalent to calculating {2d log 2}, where {} denotes fractional part. Thus we can write d  {2 log 2} = k=0 d  d  2d-k k 2d -k    + k=d+1  2d-k k   = k=0  mod k k  + k=d+1  2d-k k  .  (3.1.6)  We are justified in inserting ""mod k "" in the numerator of the first summation, because we are only interested in the fractional part of the quotient when divided by k . Now the key observation is this: The numerator of the first sum in Equation (3.1.6), namely 2d-k mod k , can be calculated very rapidly by means of the binary algorithm for exponentiation, performed modulo k . The binary algorithm   26  CHAPTER 3. PI AND ITS FRIENDS  for exponentiation is merely the formal name for the observation that exponentiation can be economically performed by means of a factorization based on the binary expansion of the exponent. For example, we can write 317 = ((((32 )2 )2 )2 )  3, thus producing the result in only 5 multiplications, instead of the usual 16. According to Knuth, this technique dates back at least to 200 bce [35, pg. 461]. In our application, we need to obtain the exponentiation result modulo a positive integer k . This can be done very efficiently as follows: Algorithm 1 Binary algorithm for exponentiation modulo k . To compute r = bn mod k , where r, b, n and k are positive integers: First set t to be the largest power of two such that t  n, and set r = 1. Then A: if n  t then r  br mod k ; n  n - t; endif t  t/2 if t  1 then r  r2 mod k ; go to A; endif 2 Note that the above algorithm is performed entirely with positive integers that do not exceed k 2 in size. Thus ordinary 64-bit floating-point or integer arithmetic, available on almost all modern computers, suffices for even rather large calculations. 128-bit floating-point arithmetic (double-double or quad precision), available at least in software on many systems (see Volume 1, Section 6.2), suffices for the largest computations currently feasible. We can now present the algorithm for computing individual binary digits of log 2. Algorithm 2 Individual digit algorithm for log 2. To compute the (d + 1)-th binary digit of log 2: Given an integer d > 0, (1) calculate each numerator of the first sum in Equation (3.1.6), using Algorithm 1, implemented using ordinary 64-bit integer or floating-point arithmetic; (2) divide each numerator by the respective value of k , again using ordinary floatingpoint arithmetic; (3) sum the terms of the first summation, while discarding any integer parts; (4) evaluate the second summation as written using floating-point arithmetic--only a few terms are necessary since it rapidly converges; and (5) add the result of the first and second summations, discarding any integer part. The resulting fraction, when expressed in binary, gives the first few digits of the binary expansion of log 2 beginning at position d + 1. 2   3.1. COMPUTING INDIVIDUAL DIGITS OF PI  27  As soon as Borwein and Plouffe found this algorithm, they began seeking other mathematical constants that shared this property. It was clear that any constant  of the form   = k=0  p(k ) , q (k )2k  (3.1.7)  where p(k ) and q (k ) are integer polynomials, with deg p < deg q and q having no zeroes at nonnegative integer arguments, is in this class. Further, any rational linear combination of such constants also shares this property. Checks of various mathematical references eventually uncovered about 25 constants that possessed series expansions of the form given by equation (3.1.7). As you might suppose, the question of whether  also shares this property did not escape these researchers. Unfortunately, exhaustive searches of the mathematical literature did not uncover any formula for  of the requisite form. But given the fact that any rational linear combination of constants with this property also shares this property, Borwein and Plouffe performed integer relation searches to see if a formula of this type existed for  . This was done, using computer programs written by one of the present authors (Bailey), which implement the ""PSLQ"" integer relation algorithm in high-precision, floating-point arithmetic [30, 5]. We discuss the PSLQ algorithm and related techniques more in Volume 1, Section 6.3. In particular, these three researchers sought an integer relation for the real vector (1 , 2 ,    , n ), where 1 =  and (i , 2  i  n) is the collection of constants of the requisite form gleaned from the literature, each computed to several hundred decimal digit precision. To be precise, they sought an n-long vector of integers (ai ) such that i ai i = 0, to within a very small ""epsilon."" After a month or two of computation, with numerous restarts using new  vectors (when additional formulas were found in the literature) the identity (3.1.1) was finally uncovered. The actual formula found by the computation was:  = 4F (1/4, 5/4; 1; -1/4) + 2 arctan(1/2) - log 5, (3.1.8)  where F (1/4, 5/4; 1; -1/4) = 0.955933837 . . . is a hypergeometric function evaluation. Reducing this expression to summation form yields the new  formula:   = i=0  1 16i  4 2 1 1 - - - 8i + 1 8i + 4 8i + 5 8i + 6  .  (3.1.9)   28  CHAPTER 3. PI AND ITS FRIENDS  It should be clear at this point that the scheme for computing individual hexadecimal digits of  is very similar to Algorithm 2. For completeness, we state it as follows: Algorithm 3 Individual digit algorithm for  . To compute the (d + 1)-th hexadecimal digit of  : Given an integer d > 0, we can write {16d  } = {4{16d S1 } - 2{16d S4 } - {16d S5 } - {16d S6 }}, where   (3.1.10)  S  j  = k=0  1 . 16k (8k + j )  (3.1.11)  Now apply Algorithm 2, with d  {16 Sj } = k=0 d  d  16d-k 8k + j 16 d-k    + k=d+1  16d-k 8k + j   = k=0  mod 8k + j 8k + j  + k=d+1  16d-k 8k + j  (3.1.12)  instead of equation (3.1.6), to compute {16d Sj } for j = 1, 4, 5, 6. Combine these four results, discarding integer parts, as shown in (3.1.10). The resulting fraction, when expressed in hexadecimal notation, gives the hex digit of  in position d + 1, plus a few more correct digits. 2 As with Algorithm 2, multiple-precision arithmetic software is not required-- ordinary 64-bit or 128-bit floating-point arithmetic suffices even for some rather large computations. We have omitted here some numerical details for large computations--see [7]. Sample implementations in both C and Fortran-90 are available from the web site http://www.expmath.info. Needless to say, Algorithm 3 has been implemented by numerous researchers. In 1997, Fabrice Bellard of INRIA computed 152 binary digits of  starting at the trillionth binary digit position. The computation took 12 days on 20   3.1. COMPUTING INDIVIDUAL DIGITS OF PI Hex Digits Beginning at This Position 26C65E52CB4593 17AF5863EFED8D ECB840E21926EC 85895585A0428B 921C73C6838FB2 9C381872D27596 07E45733CC790B E6216B069CB6C1  29  Position 106 107 108 109 1010 1011 1.25  1012 2.5  1014  Table 3.1: Computed hexadecimal digits of  .  workstations working in parallel over the Internet. His scheme is actually based on the following variant of 3.1.9:   =4 k=0  (-1)k 4k (2k + 1)   -  1 64  k=0  (-1)k 1024k  32 8 1 + + 4k + 1 4k + 2 4k + 3  .  (3.1.13)  This formula permits individual hex or binary digits of  to be calculated roughly 43% faster than (3.1.1). A year later, Colin Percival, then a 17-year-old student at Simon Fraser University, utilized a network of 25 machines to calculate binary digits in the neighborhood of position 5 trillion, and then in the neighborhood of 40 trillion. In September 2000, he found that the quadrillionth binary digit is ""0,"" based on a computation that required 250 CPU-years of run time, carried out using 1,734 machines in 56 countries. Table 3.1 gives some results known as of this writing. One question that immediately arises in the wake of this discovery is whether or not there is a formula of this type and an associated computational scheme to compute individual decimal digits of  . Searches conducted by numerous researchers have been unfruitful. Now it appears that there is no nonbinary formula of this type--this is ruled out by a new result co-authored by one of the present authors (see Volume 1, Section 3.7) [14].   30  CHAPTER 3. PI AND ITS FRIENDS  3.2  Commentary and Additional Examples  [From Volume 1, Chapter 3 Commentary] 1. An arctan series for pi. Find rational coefficients ai such that the identity  = a1 arctan 1 1 + a2 arctan 390112 485298 1 1 +a3 arctan + a4 arctan 683982 1984933 1 1 +a5 arctan + a6 arctan 2478328 3449051 1 1 +a7 arctan + a8 arctan 18975991 22709274 1 1 +a9 arctan + a10 arctan 24208144 201229582 1 +a11 arctan 2189376182  holds [3, pg. 75]. Also show that an identity with even simpler coefficients exists if arctan 1/239 is included as one of the terms on the RHS. Hint: Use an integer relation program (see Volume 1, Section 6.3), or try the tools at one of these sites: http://www.cecm.sfu.ca/pro jects/IntegerRelations or http://www.expmath.info. 2. Biblical pi. 1 Kings 7:23 and 2 Chronicles 4:2 describe a circular pool in Solomon's temple ""ten cubits from brim to brim,"" and 30 cubits in circumference, so that  = 3. In spite of the clearly informal context, this discrepancy has been a source of consternation among Biblical literalists for centuries. For example, an 18th-century German Bible commentary attempted to explain away this discrepancy using the imaginative suggestion that the circular pool in Solomon's temple (clearly described in 2 Chron. 4:2 as ""round in compass"") was instead hexagonal in shape [9, pg. 7576].   Chapter 4 Sequences, Series, Pro ducts and Integrals Several years ago I was invited to contemplate being marooned on the proverbial desert island. What book would I most wish to have there, in addition to the Bible and the complete works of Shakespeare? My immediate answer was: Abramowitz and Stegun's Handbook of Mathematical Functions. If I could substitute for the Bible, I would choose Gradsteyn and Ryzhik's Table of Integrals, Series and Products. Compounding the impiety, I would give up Shakespeare in favor of Prudnikov, Brychkov and Marichev's Tables of Integrals and Series. . . On the island, there would be much time to think about waves on the water that carve ridges on the sand beneath and focus sunlight there; shapes of clouds; subtle tints in the sky. . . With the arrogance that keeps us theorists going, I harbor the delusion that it would be not too difficult to guess the underlying physics and formulate the governing equations. It is when contemplating how to solve these equations--to convert formulations into explanations--that humility sets in. Then, compendia of formulas become indispensable. Michael Berry, ""Why Are Special Functions Special?"", 2001 In the first volume, we presented numerous examples of experimental mathematics in action. In particular, we examined how a computational-experimental approach could be used to identify constants and sequences, evaluate definite 31   32  CHAPTER 4. SEQUENCES, SERIES, PRODUCTS AND INTEGRALS  integrals and infinite series, discover new identities involving fundamental constants and functions of mathematics, provide a more intuitive approach to mathematical proofs, and formulate conjectures that can lead to important advances in the field. In this chapter, we introduce our discussion with a number of additional intriguing examples in the realm of sequences, series, products and integrals.  4.1  Pi Is Not 22/7  [From Volume 2, Section 1.1] We first consider an example from the early history of  , as described in Chapter 3 of the first volume. Even Maple or Mathematica ""knows""  = 22/7, since 1  0< 0  (1 - x)4 x4 22 dx = - , 2 1+x 7  (4.1.1)  though it would be prudent to ask ""why"" it can perform the evaluation and ""whether"" we should trust it? Assume we trust it. Then the integrand is strictly positive on the interior of the interval of integration, and the answer in (4.1.1) is necessarily an area and thus strictly positive, despite millennia of claims that  is 22/7. Of course, 22/7 is one of the early continued fraction approximations to  . The first four are 3, 22/7, 333/106, 355/113. In this case, computing the indefinite integral provides immediate reassurance. We obtain t 0  x4 (1 - x)4 17 26 4 dx = t - t + t5 - t3 + 4 t - 4 arctan (t) . (4.1.2) 2 1+x 7 3 3  This is easily confirmed by differentiation, and the Fundamental Theorem of Calculus substantiates (4.1.1). In fact, one can take this idea a bit further. We note that 1 0  x4 (1 - x)4 dx =  1 , 630  (4.1.3)   4.1. PI IS NOT 22/7  33  Figure 4.1: A pictorial proof of Archimedes' inequality and we observe that 1 2 1 0  x4 (1 - x)4 dx < 0  1  (1 - x)4 x4 dx < 1 + x2  1 0  x4 (1 - x)4 dx. (4.1.4)  On combining this with (4.1.1) and (4.1.3), we straightforwardly derive 223/71 < 22/7 - 1/630 <  < 22/7 - 1/1260 < 22/7, and so re-obtain Archimedes' famous computation 3 10 10 <<3 71 70 (4.1.5)  (illustrating that it is sometimes better not to fully reduce a fraction to lowest terms). This derivation of the estimate above seems first to have been written down in Eureka, the Cambridge student journal in 1971 [25]. The integral in (4.1.1) was apparently shown by Kurt Mahler to his students in the mid-1960s, and it had appeared in a mathematical examination at the University of Sydney in November, 1960. Figure 4.1 (also in the Color Supplement) shows the estimate graphically illustrated. The three 10  10 arrays color the digits of the first hundred digits of 223/71,  , and 22/7. One sees a clear pattern on the right (22/7), a more subtle structure on the left (223/71), and a ""random"" coloring in the middle ( ). It is tempting to ask if there is a clean general way to mimic (4.1.1) for more general rational approximations, or even continued fraction convergents. This   34  CHAPTER 4. SEQUENCES, SERIES, PRODUCTS AND INTEGRALS  is indeed possible to some degree, as discussed by Beukers in [10]. The most satisfactory result is an  - for n  1, in (4.1.6). 75920 - Unlike bn = cn 1 2n  t  (1 - t2 )  2n  (1 + it)3  n+1 n+1  + (1 - it)3  n+1  0  (1 + t2 )3  dt,  (4.1.6)  where the integers an , bn and cn are implicitly defined by the integral The first three integrals evaluate to 14 - 44, 968 - 45616/15, and 1669568/7, so again we start with  - 22/7. Beukers' preliminary attempts in [10], such as the seemingly promising 1n 0  t (1 - t)n dt, (t2 + 1)n+1  this set of approximates actually produces an explicit if weak irrationality estimate [15, 10]: for large n, - pn 1  1.0499 . qn qn p 1  21.04 q q  As Beukers sketches, one consequence of this explicit sequence - ...  for all integers p, q with sufficiently large q . (Here 21.04 . . . = 1 + 1/0.0499. In fact, in 1993 Hata by different methods had improved the number 21.4 to 8.02.) While it is easy to discover ""natural"" results like 1 5 1 0  x (1 - x)2 7 - log (2) , 3 dx = 10 (1 + x)  (4.1.7)  the fact that 7/10 is again a convergent to log 2 seems to be largely a happenstance. For example, 1 0 1 0  x12 (1 - x)12 431302721 dx = - 16 (1 + x2 ) 137287920 1 x12 (1 - x)12 dx = 16 1081662400  leads to the true, if inelegant, estimate that 5902037233/1878676800 <  < 224277414953/71389718400, where the interval is of size 1.39  10-9 .   4.2. HIGH PRECISION FRAUD  35  4.2  High Precision Fraud  [From Volume 2, Section 1.4] Consider the sums   n=1  n tanh( ) 10n  =  ?  1 , 81  an evaluation that is wrong, but valid to 268 decimal places, and   n=1  n tanh( /2) 10n  =  ?  1 , 81  which is valid to ""only"" 12 places. Both series actually evaluate to transcendental numbers. What underlies these ""fraudulent"" evaluations? The ""quick"" reason is that tanh( ) and tanh( /2) are almost integers, with, e.g., 0.99 < tanh( ) < 1. Therefore, n tanh( ) will be equal to n - 1 for many n; precisely for n = 1,    , 268. Since   n=1  n-1 1 = , n 10 81  this explains the evaluations. Looking more closely at this argument, one is directly led to continued fractions as the deeper reason behind the frauds. For any irrational positive , we can write  = [a0 , a1 ,    , an , an+1 ,    ] 1 = a0 + , 1 a1 + 1 a2 + a3 +    with integral an and a0  0, an  1 for n  1. This is hard to compute by hand, but easy even on a small computer or calculator. For the parameters in our series, we get tanh( ) = [0, 1, 267, 4, 14, 1, 2, 1, 2, 2, 1, 2, 3, 8, 3, 1,    ] (4.2.8)   36 and  CHAPTER 4. SEQUENCES, SERIES, PRODUCTS AND INTEGRALS  tanh   2  = [0, 1, 11, 14, 4, 1, 1, 1, 3, 1, 295, 4, 4, 1, 5, 17, 7,    ]. (4.2.9)  It cannot be a coincidence that the integers 267 and 11 (each equal to the number of places of agreement with 1/81 in the respective formula) appear in these expansions! There must be a connection between series of the type n z n and the continued fraction expansion of an irrational . In fact, consider the infinite continued fraction approximations for  generated by p q n+1 n+1  = pn an+1 + pn-1 , = qn an+1 + qn-1 , 2n  p0 = a0 =  , q 0 = 1, 2n+1  p-1 = 1 , q-1 = 0. decreases to  and  Then for n  0, p2n /q  increases to , while p2n+1 /q ) < -  1 qn (qn + q Let further n  n+1  pn 1 . < qn qn qn+1  = qn  - pn . Then from the above, it follows that | n+1  |<  1 qn + q  n+1  < | n| <  1 q n+1   1.  All of this is standard and may be found in [33], [43], or [39]. Our aim now is to show a relationship between the above series and the continued fraction expansion of . A first key is the following lemma, which we will not prove here since it requires some knowledge about linear Diophantine equations (see [16], from which this material is taken). Lemma 4.2.1 For any irrational  > 0 and n, N  N, we have n + n + N N  = =  n for n < q N n + (-1) for n = q  N +1 N +1  , .  Theorem 4.2.2 For irrational  > 0,   n=1  p0 z + n z = (1 - z )2 n    (-1)n n=0  z qn z qn+1 (1 - z qn ) (1 - z  qn+1  )  .   4.2. HIGH PRECISION FRAUD Pro of. Let   37  G (z , w) = n=1  zn w  n  ,  (4.2.10)  for |z |, |w| < 1. Then for N > 0, qN  (1 - z w  qN  pN  ) G (z , w) -  n=1  znw  n  = n=1   z z n=1 qN qN  n+qN  w w  (n+qN )  -w n+  n +pN  = =z =z  n+qN  n +pN  w  N  - n  -1 qN +1  +1  +qN +qN  w w  qN pN  +1   +pN  w N  (-1)  N  - 1 + O(z qN  +qN +1  ) (4.2.11)  +1  +1  +pN  (-1)  w-1 + O (z w  +1  +qN +1  ),  since qN +1  = N +1 + pN +1 = pN +1 if N is odd, and = pN +1 - 1 if N is even. qN n n and QN = 1 - z qN wpN . Then AN = Now write PN = n=1 z w QN PN +1 - QN +1 PN is a polynomial of degree at most qN + qN +1 in z , and therefore it follows from (4.2.11) that AN = Q (QN G - PN ) - QN (QN +1 G - PN w - 1 qN pN qN +1 pN +1 . w = (-1)N zwz w N +1 +1  )  This in turn implies PN +1 PN - QN +1 QN = AN QN QN = (-1)N +1  w - 1 z qN wpN z qN +1 w w QN QN +1  pN  +1  .  Next summing from zero to infinity, and noting that (4.2.11) implies that G - PN /QN tends to 0 as N tends to infinity, shows that G (z , w) = z wp0 1 - zw p0  -  1-w w    (-1) n=0  n  z qn wpn z qn+1 wpn+1 (1 - z qn wpn ) (1 - z qn+1 w  pn+1  )  .   38  CHAPTER 4. SEQUENCES, SERIES, PRODUCTS AND INTEGRALS  Now differentiating with respect to w and then letting w tend to 1 proves the assertion. 2 This theorem was first proved (for   (0, 1)) by Mahler in [37]. Example 4.2.3  = tanh( ). In this case, qn = 1, 1, 268, 1073,    for n = 0, 1, 2, 3,    , and thus   n tanh( ) z n=1  n  =  z2 z 269 - (1 - z )2 (1 - z )(1 - z  268  )  +  .  Therefore, 1 - 2  10- 81  269   n=1  n tanh( ) 10n    1 + 2  10 81  -269  , 2  and similarly for  = tanh(  ). 2 Example 4.2.4  = e  163/9  .  163/9  With one of our favorite transcendental numbers,  = e 1653264929,    ], we get the incorrect evaluation   ne 163/9 ? = 1280640, n 2 n=1  = [640320,  which is, however, correct to at least half a billion digits.  2  4.3  Knuth's Series Problem  [From Volume 2, Section 1.5] We give an account here of the solution, by one of the present authors (Borwein) to a problem recently posed by Donald E. Knuth of Stanford University in the American Mathematical Monthly (Problem 10832, Nov. 2000):   4.3. KNUTH'S SERIES PROBLEM Problem: Evaluate   39  S= k=1  kk 1 - k k !e 2 k  .  Solution: We first attempted to obtain a numerical value for S . Using Maple, we produced the approximation S  -0.08406950872765599646. Based on this numerical value, the Inverse Symbolic Calculator, available at the URL http://www.cecm.sfu.ca/pro jects/ISC, with the ""Smart Lookup"" feature, yielded the result 2 1 S  - -  3 2 1 2 . (4.3.12)  Calculations to even higher precision (50 decimal digits) confirmed this approximation. Thus within a few minutes we ""knew"" the answer. Why should such an identity hold? One clue was provided by the surprising speed with which Maple was able to calculate a high-precision value of this slowly convergent infinite sum. Evidently, the Maple software knew something that we did not. Peering under the covers, we found that Maple was using the Lambert W function, which is the functional inverse of w(z ) = z ez . Another clue was the appearance of  (1/2) in the above experimental identity, together with an obvious allusion to Stirling's formula in the original problem. This led us to conjecture the identity    k=1  1 P (1/2, k - 1)  - (k - 1)! 2 2 k  1 = 2  1 2  ,  (4.3.13)  where P (x, n) denotes the Pochhammer function x(x + 1)    (x + n - 1), and where the binomial coefficients in the LHS of (4.3.13) are the same as those of  the function 1/ 2 - 2x. Maple successfully evaluated this summation, as shown on the RHS. We now needed to establish that   k=1  P (1/2, k - 1) kk  - k k !e (k - 1)! 2  2 =-. 3   40  CHAPTER 4. SEQUENCES, SERIES, PRODUCTS AND INTEGRALS  Guided by the presence of the Lambert W function   W (z ) = k=1  (-k )k-1 z k , k!  an appeal to Abel's limit theorem suggested the conjectured identity lim dW (-z /e) 1 + dz 2 - 2z = 2/3.  z 1  Here again, Maple was able to evaluate this summation and establish the identity. 2  4.4  Commentary and Additional Examples  [From Volume 2, Chapter 1 Commentary] 1. Putnam problem 1999A3. Consider the power series expansion 1 = 1 - 2x - x2 a n xn . n0  Prove that for each integer n  0, there is an integer m such that a2 + a2 n n Answer: It transpires that a2 + a2 n n +1 +1  = am .  = a2n+1 ,  (4.4.14)  which remains to be proven. Hint: The first 15 coefficients are 1, 2, 5, 12, 29, 70, 169, 408, 985, 2378, 5741, 13860, 33461, 80782, 195025, and the desired squares are 5, 29, 169, 985, 5741, 33461, 195025,   4.4. COMMENTARY AND ADDITIONAL EXAMPLES  41  which is more than enough to spot the pattern. To prove this either explicitly use the closed form for n+1   n+1 1 an =  - - 2+1 , 1+ 2 22 or show that both sides of (4.4.14) satisfy the same recursion (and initial conditions). 2. Putnam problem 2000A4. Show that the improper integral M  I = lim  M   sin(x) sin(x2 ) dx  (4.4.15)  0  exists. Hint: Numerical experimentation shows that a limit of approximately 0.4917 is reached. The existence of the limit can be rigorously established in two ways: (a) Since the integrand equals cos(x2 - x) - cos(x2 + x))/2, it suffices to M show that limM  0 cos(x + x2 ) dx exists. After a change of variables, it suffices to consider n- 1 (k+1/2) (k-1/2)  k=0  cos (u)  du. 1 + 4u  This converges by the alternating series test. (b) Use Cauchy's theorem to integrate the entire functions exp(ix2  ix) over a triangular path with vertices at 0, M and (1 + i)M . Easy estimates show that the integrals over the vertical and the diagonal edges converge. 3. Two exp ected distances. These results originate with James D. Klein. (a) The expected distance between two random points on different sides of the unit square: 2 3 1 0 0 1  x2 + y 2 dx dy +  1 3  1 0 0  1  1 + (y - u)2 du dy  = 0.869009055274534463884970594345406624856719 . . . =  2 1 5 + 2 + log 1 + 2 . 99 9   42  CHAPTER 4. SEQUENCES, SERIES, PRODUCTS AND INTEGRALS (b) The expected distance between two random points on different faces of the unit cube: 4 5 1 + 5 1 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1  x2 + y 2 + (z - w)2 dw dx dy dz 1 + (y - u)2 + (z - w)2 du dw dy dz  = 0.92639005517404672921816358654777901444496019010734 . . . = 4 17  2 7 + 2- 3-  75 75 25 75   7 7 + log 1 + 2 + log 7 + 4 3 . 25 25  (c) Show that the first term in (b) is   2 F (1/2, -n + 2; 3/2; 1/2) 5 n=2 (2 n + 1)  (n + 2)  (5/2 - n)  2 1 4 2 + log 2+1 -  15 5 75 and the second term is   F (1, 1/2, -1/2 - n, -n - 1; 2, 1/2 - n, 3/2; -1) 10 n=0 (2 n + 1)  (n + 2)  (3/2 - n) +   2 2 1 -+ + log 2+1 . 25 50 10 This allows one to numerically compute the expectation to high precision and to express both of the individual integrals in terms of the same set of constants. These expectations have actually been checked by computer simulations. Hint: Reduce the first integral to a three dimensional one, and use the binomial theorem on both.   Chapter 5 Partitions and Powers I'll be glad if I have succeeded in impressing the idea that it is not only pleasant to read at times the works of the old mathematical authors, but this may occasionally be of use for the actual advancement of science. Constantin Carath dory, speaking to an MAA meeting in 1936 eo In this chapter, we address the theory of additive partitions and the theory of representations as sums of squares, both from an experimental perspective. Each has a distinguished history. We will show that computational techniques can accelerate both solution and understanding of these problems. What's more, these techniques have a number of interesting applications, including, for instance, Madelung's constant in physical chemistry.  5.1  Partition Functions  [From Volume 2, Section 4.1] The number of additive partitions of n, p(n), is formally generated by P (q ) = 1 + n1  p(n)q n =  (1 - q n )-1 . n1  (5.1.1)  One ignores ""0"" and permutations. Thus p(5) = 7 since 5 = 4+1=3+2=3+1+1=2+2+1 = 2 + 1 + 1 + 1 = 1 + 1 + 1 + 1 + 1. 43 (5.1.2)   44  CHAPTER 5. PARTITIONS AND POWERS  Figure 5.1: A Ferrer diagram Additive partitions are less tractable than multiplicative ones as there is no analogue of unique prime factorization nor the corresponding structure. Formula (5.1.1) is easily seen by expanding (1 - q n )-1 and comparing coeffi  cients. It is relatively easy to deduce that 2 n < p(n) < e 2n/3 for n > 3 (see [38]), and that the series is absolutely convergent for |q | < 1. We return to the analytic behavior of this series below. Partitions provide a wonderful example of why Keith Devlin calls mathematics ""the science of patterns"" [26]. Many geometric representations exist. For example, the partition 5 = 4 + 1 can be represented as a point at (0, 0) and four points at (0, 1), (1, 1), (2, 1), (3, 1). Read with axis reversed, this identifies 1 + 4 with 2 + 1 + 1 + 1 and so on. See Figure 5.1, which identifies 1 + 1 + 1 + 2 + 3 + 4 and 6 + 3 + 2 + 1. Such techniques provide alternate ways to prove results such as the number of partitions of n with al l parts odd is the number of partitions of n into distinct parts, (see Volume 2, Chapter 4, Exercise 1). A modern computational temperament leads to: Question: How hard is p(n) to compute--in 1900 (for MacMahon the ""father of combinatorial analysis"") or in 2000 (for Maple or Mathematica)?   5.1. PARTITION FUNCTIONS  45  Answer: The computation of p(200) = 3972999029388 took MacMahon months and intelligence. Now, however, we can use the most naive approach: Computing 200 terms of the series for the inverse product in (5.1.1) instantly produces the result using either Mathematica or Maple. Obtaining the result p(500) = 2300165032574323995027 is not much more difficult, using the Maple code > N:=500; coeff(series(1/product(1-q^n,n=1..N+1),q,N+1),q,N); 2300165032574323995027 2  5.1.1  The ""Exact"" Formula for the Partition Function  [From Volume 2, Section 4.1.3] One of the signal achievements of early twentieth century analysis was Hardy and Ramanujan's precise asymptotic for p(n) [21]. It is based in part on an analysis of the Dedekind  -function  (q ) = eiz/12 n1 (1 - e2inz ). The function  is closely related to Q(q ), and 3 (q ) discussed in the next section, and satisfies a modular equation. Their asymptotic is eK n p(n) =  4 3 2 n  1+O  1  n  ,  (5.1.3)  where K =  2/3 and n = n - 1/24. This was subsequently refined by Rademacher to   2   d  sinh k 3 x - 1 k (n) k p(n) =   dx 1  2 k=1 x - 24 where k   1 24    x=n  ,  (5.1.4)  k (n) = (h,k)=1    h,k  e-2   inh/k  ,  and h,k = exp( i h,k =  h,k  ) with m m 1 - - k k 2 hm hm 1 - - k k 2 .  k -1  m=1   46  CHAPTER 5. PARTITIONS AND POWERS   If order n terms are appropriately used, the nearest integer is p(n). A mere five terms of this expansion provides p(200)  3972999029387.86108 and six terms yields p(500)  2300165032574323995027.196661. As we have seen, the underlying asymptotic is  1  e p(n)  4n 3 2n/3  .  Later Erd made an ""elementary"" derivation of the Hardy-Ramanujan formula os (5.1.3). A recent discussion of this formula is given by Almkvist and Wilf in [2]. It is interesting to speculate how much corresponding beautiful mathematics is not done when computation becomes too easy--both Maple and Mathematica have good built-in partition functions.  5.2  Singular Values  [From Volume 2, Section 4.2] The Jacobian theta functions are a very rich source mine for experimentation-- both as a tool to learning classical theory and to discover new phenomena. Further details of what follows are given fully in [15]. For our purposes, we consider only the three classical -functions:   3 (q ) = n=-   qn , (-1)n q n , n=-  2  2  (5.2.5)  4 (q ) = 2 (q ) =  q n=-  (n+1/2)2  ,  2 for |q |  1. Note that 3 is the generating function for the number of ways of writing a number as a sum of two squares, counting order and sign. Similarly, 2 2 counts sums of two odd squares. A beautiful result of Jacobi's is 4 4 4 3 (q ) = 2 (q ) + 4 (q ).  (5.2.6)   5.3. SOME FIBONACCI SUMS  47  2 2 2 2 If we write k = 2 /3 and k = 4 /3 , we note that k 2 + (k )2 = 1. It transpires that 2 (i) 3 (q 2 ) = 2 2 4 (q ) + 3 (q ) 2 2 (ii) 4 (q 2 ) = 4 (q ) 3 (q ).  (5.2.7)  Now (5.2.6) and (5.2.7) can be proved in many ways and can be ""verified"" symbolically in many more.  5.3  Some Fib onacci Sums  [From Volume 2, Section 4.4] Theta functions turn up in quite unexpected places as we now show. The Fibonacci sequence, namely 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144,    , takes its name from its first appearance in print, which seems to have been in the famous book Liber Abaci, published by Leonardo Fibonacci (also known as Leonardo of Pisa) in 1202. He asked: How many pairs of rabbits can be produced from a single pair in a year if every month each pair begets a new pair which from the second month on becomes productive? Lest one thinks the problem is imprecise, Fibonacci describes the solution in the text and in the margin. There one finds written vertically Parium 1 Primus 2 Secundus 3 Tercius 5 Quartus 8 Quintus 13 Sestus 21 Septimus 34 Octauus 55 Nonus 89 Decimus 144 Undecimus 233 Duodecimus 377. We leave it to the reader to decide that this indeed leads to the Fibonacci sequence, but we do note that ""the proof is left as an exercise"" seems to have occurred first in De Triangulis Omnimodis by Regiomontanus, written in 1464 (but published in 1533). He is quoted as saying, ""This is seen to be the converse of the preceding. Moreover, it has a straightforward proof, as did the preceding. Whereupon I leave it to you for homework.""   48  CHAPTER 5. PARTITIONS AND POWERS  Among its many other contributions such as popularizing Hindu-Arabic notation in the west, Liber Abaci contains methods for extracting cube roots, for solving quadratics, and the lovely identity (a2 + b2 )(c2 + d2 ) = (ac  bd)2 + (ad bc)2 , which shows the product of sums of two squares is such a sum. The Fibonacci sequence occurs in many contexts both serious and quirky. For example, 144 is the only Fibonacci square. A moment's inspection shows that it is generated by F0 = 1, F 1 = 1, F n+1  = Fn + Fn-1 .  (5.3.8)  It grows quickly (like rabbits) and is monotonic. In particular, Fn+2 > 2 Fn . If we look computationally at Fn+1 /Fn , for n = 10, 20, 30, 40, we obtain the numerical values 1.61818181818, 1.61803399852, 1.61803398875, 1.61803398875, which either the h uman eye or a constant recognition facility reveals to be the Golden Mean  = ( 5 + 1)/2, to the precision used. Indeed, the standard theory of two term linear recurrence relations leads to n n 1+ 5 1 1- 5 Fn =  - , (5.3.9) 2 2 5  where (1 - 5)/2 is the other root of x2 = x + 1. It is easy to check that the sequence in (5.3.9) satisfies the recursion in (5.3.8), and has the correct initial conditions. Since |g | < 1, it is also easy to see that Fn+1 /Fn  , as claimed, and to deduce many other identities such as 2 Fn+1 Fn-1 = Fn + (-1)n for n  2. There is a slightly less well known companion Lucas sequence, named after the French number theorist Edouard Lucas (18421891): L0 = 2, L 1 = 1, n  L  n+1  = Ln + Ln-1 , n  (5.3.10)  which is correspondingly solved by  5+1 Ln = 2  +   1- 5 2  .  (5.3.11)  As both Fibonacci and Lucas sequences are built of geometric sequences, it is k clear that we can easily evaluate sums like N=1 Fn , for positive integer k . What n happens for negative integers is more interesting. A preparatory lemma is useful ([15]):   5.3. SOME FIBONACCI SUMS Lemma 5.3.1 For 0 <  <  with  = 1,   49  n=1   1 n+  1 +   n  = n=1   n 1+  2n  2 = 3 ( ),  (5.3.12) (5.3.13)  n=0    2n+1  2n+1  = n=0   2n+1 12 = 2 ( 2 ). 1 +  2n+1 4  Pro of. The proof of the first formula is a consequence of the result on theta functions in Volume 1, 4.3.1. This relies on confirming that   n=1  n 1+   2n  = n=0  (-1)n   2n+1 . 1 -  2n+1  (5.3.14)  (Try expanding both sides as double sums.) The second formula then follows by applying the first to 2 and  2 , and then 2 2 subtracting that result from the first to obtain (3 ( ) - 3 ( 2 ))/4, which equals 2 2 ( 2 )/4. 2 Two immediate consequences are   1 F2 n+1   =  n=0   52  42  n=1  1 12 =  L2n 43   3- 5 2  3- 5 1 +. 2 4  (5.3.15) (5.3.16)  Two somewhat more elaborate derivations, (see [15], Section 3.7), lead to   n=1   1 5 = 2 Fn 24 1 1 = 2 Ln 8  n=1    3- 5 3- 5 4 4 2 - 4 2 2  3- 5 4 3 -1 . 2  +1  (5.3.17) (5.3.18)  Since it is known that the classical theta functions are transcendental for algebraic values q , 0 < |q | < 1, we discover the far-from-obvious result that the   50  CHAPTER 5. PARTITIONS AND POWERS  left-hand side of each of (5.3.15), (5.3.16), (5.3.18) is a transcendental number, as probably is (5.3.17). Moreover, since both the initial sums and especially the theta functions are easy to compute numerically, we can hunt for other such identities using integer relation methods. In this way, we find:    (-1)n 5 3- 5 3- 5 4 4 = 2 - 2 - 2 4 , (5.3.19) 2 Fn 48 2 2 n=1 and a host of more recondite identities. By contrast, a remarkable elementary identity is   1 (2k - 1) 5 = , F + F2k-1 2 F2k-1 n=0 2n+1  (5.3.20)  - for k = 1, 2, 3,    . So while  F2n1 is transcendental,  (F2n+1 + 1)-1 = +1 n=0 n=0  5/2. If we compute the corresponding continued fractions of the two sums, we obtain the quite different results [1, 1, 4, 1, 2, 3, 6, 2, 1, 3, 1, 189, 1, 3, 12] and [1, 8, 2, 8, 2, 8, 2, 8, 2, 8] in partial confirmation.  5.4  Commentary and Additional Examples  [From Volume 2, Chapter 4 Commentary] 1. A combinatorial determinant problem. Find the determinant of  n n n           p n+1 p n+2 p n p n+1 p n+2 p n+3 p n p+1 n+1 p+1 n+2 p+1 n+3 p+1 p+1 n+1 p+1 n+2 p+1 n p+2 n+1 p+2 n+2 p+2 n+3 p+2 p+2 n+1 p+2 n+2 p+2 n p+3 n+1 p+3 n+2 p+3 n+3 p+3              5.4. COMMENTARY AND ADDITIONAL EXAMPLES  51  and its q -dimensional extension as a function of n, p, q . (Taken from [32].) Solution: The pattern is clear from the first few cases on simplifying in Maple or Mathematica. 2. A sum-of-p owers determinant. Find the determinant of  1 1 1 1 4 4 4 4 k=0 k k=0 k k=0 k k=0 k  1 2 2 2  k4 k4 k4 k4  k=0 k=0 k=0   k=0  1 2 3 3 4 4 4 4  k=0 k k=0 k k=0 k   k=0 k 1 2 3 4 4 4 4 4 k=0 k k=0 k k=0 k k=0 k and its q -dimensional extension. (Taken from [32].) Solution: The first few instances of this sequence are 1, 4, 216, 331776, 24883200000, 139314069504000000, which can be quickly identified as (q !)q , using the Sloane online sequence recognition tool. This fact can be proved by taking cofactors on the last row, and observing that only the final two entries have nonzero cofactors with value (q - 1)!q-1 . 3. Putnam problem 1994B4. Let dn be the greatest common divisor of 32 the entries of An -I where A = . Show that dn   with n. Hint: 43 Observe numerically, then prove by induction, that An has determinant 1 an bn and is of the form . Hence, (an - 1)|2b2 . Then write An n 2bn an explicitly via the Cayley-Hamilton theorem, which tells us that An+1 = 6 An - An-1 . 4. Crandall's integral representation for Madelung's constant. The following identity is both beautiful and effective--though less effective for computational purposes than Benson's formula. For example, 60 digits of M3 (1) can be obtained in seconds in Maple or Mathematica using Benson's identity, while using the numerical quadrature tools of Section 6.1 (see also   52  CHAPTER 5. PARTITIONS AND POWERS Volume 2, Section 7.4) to compute the integral to the same 60 digits takes roughly one hour runtime on a 2003-era computer. Richard Crandall's 3 formula is derived in [23] from the Andrews formula for 2 . It is 2 M3 (1) = -  1   r dr 0 -  1 + 2/(1 + r2(1-sin ) ) d (1 + r1+cos  )(1 + r1-cos  ) (5.4.21)  = -1.7475645946332 . . .  5. A p olygon problem. Count (i) the number of ways a polygon with n + 2 sides can be cut into n triangles; (ii) the number of ways in which parentheses can be placed in a sequence of numbers to be multiplied, two at a time; and (iii) the number of paths of length 2n through an n-by-n grid that do not rise above the main diagonal (Dijk paths). Hint: In each case the sequence starts 1, 2, 5, 14, 42, 132, 429, 1430, 4862.  -2 The ""gfun"" package returns the generating function 4 1 + 1 - 4 x and the recursion (4n + 6)u(n) = (n + 3)u(n + 1), which gives rise to n the Catalan numbers (1/(n + 1)) 2n named after Eug Charles Catalan ene (18141894). 6. Fib onacci and Lucas numbers in terms of hyp erb olic functions. Show that 2 Fn =  i 5 where  = log -n  sinh(n)  and   Ln = 2 i-n cosh(n),  5+1 2   +i . 2  Many Fibonacci formulas are then easy to obtain from the addition for2 mulas for sinh and cosh--for example consider 5 Fn - L2 . (See [27], which n should be consulted whenever one ""discovers"" a result in classical number theory.)   Chapter 6 Numerical Techniques I I Another thing I must point out is that you cannot prove a vague theory wrong . . . Also, if the process of computing the consequences is indefinite, then with a little skill any experimental result can be made to look like the expected consequences. Richard Feynman, 1964, from Gary Taubes, The (Political) Science of Salt, 1998 In this chapter, we will examine in more detail some additional underlying computational techniques that are useful in experimental mathematics. In particular, we shall briefly examine techniques for theorem proving, prime number computations, polynomial root finding, numerical quadrature, and infinte series summation. As in the first volume, we focus here on practical algorithms and techniques. In some cases, there are known techniques that have superior efficiencies or other characteristics, but for various reasons are not considered suitable for practical implementation. We acknowledge the existence of such algorithms but do not, in most cases, devote space to them.  6.1  Numerical Quadrature  [From Volume 2, Section 7.4] Experimental mathematicians very frequently find it necessary to calculate definite integrals to high precision. Recall the examples given in Chapters 1 53   54  CHAPTER 6. NUMERICAL TECHNIQUES II  and 5 of the first volume, wherein we were able to experimentally identify certain definite integrals as analytic expressions, based only on their high-precision numerical value. To briefly reprise one example, we were inspired by a recent problem in the American Mathematical Monthly [1]. By using one of the quadrature routines to be described below, together with a PSLQ integer relation detection program, we found that if C (a) is defined by  1 arctan( x2 + a2 ) dx  , (6.1.1) C (a) = x2 + a2 (x2 + 1) 0 then C (0) =  log 2/8 + G/2    C (1) =  /4 -  2/2 + 3 2 arctan( 2)/2  C ( 2) = 5 2 /96,  (6.1.2)  where G = k0 (-1)k /(2k + 1)2 is Catalan's constant. The third of these results is the result from the Monthly. These particular results then led to the following general result, among others:     arctan( x2 + a2 ) dx   2 arctan( a2 - 1) - arctan( a4 - 1) . = 2 a2 - 1 x2 + a2 (x2 + 1) 0 (6.1.3) The commercial packages Maple and Mathematica both include rather good high-precision numerical quadrature facilities. However, these packages do have some limitations, and in many cases much faster performance can be achieved with custom-written programs. And in general it is beneficial to have some understanding of quadrature techniques, even if you rely on software packages to perform the actual computation. We describe here three state-of-the-art, highly efficient techniques for numerical quadrature. You can try programming these schemes yourself, or you can refer to the C++ and Fortran-90 programs available at http://www.expmath.info.  6.1.1  Error Function Quadrature  [From Volume 2, Section 7.4.2]   6.1. NUMERICAL QUADRATURE  55  The second scheme we will discuss here is known as ""error function"" or ""erf "" quadrature. While error function quadrature is not as efficient as Gaussian quadrature for continuous, bounded, well-behaved functions on finite intervals, it often produces highly accurate results even for functions with (integrable) singularities or vertical derivatives at one or both endpoints of the interval. In contrast, Gaussian quadrature typically performs very poorly in such instances. The error function quadrature scheme and the tanh-sinh scheme to be described in the next section are based on the Euler-Maclaurin summation formula, which can be stated as follows [4, pg. 280]. Let m  0 and n  1 be integers, and define h = (b - a)/n and xj = a + j h for 0  j  n. Further, assume that the function f (x) is at least (2m + 2)-times continuously differentiable on [a, b]. Then b n  f (x) dx = h a j =0 m  f (xj ) -  h (f (a) + f (b)) 2 (2i-1)  - i=1  h2i B2i f (2i)!  (b) - f  (2i-1)  (a) - E ,  (6.1.4)  where B2i denote the Bernoulli numbers, and E= h2 m+2  (b - a)B2m+2 f (2m + 2)!  (2m+2)  ( )  ,  (6.1.5)  for some   (a, b). In the circumstance where the function f (x) and all of its derivatives are zero at the endpoints a and b, the second and third terms of the Euler-Maclaurin formula are zero. Thus the error in a simple step-function approximation to the integral, with interval h, is simply E . But since E is then less than a constant times h2m+2 /(2m + 2)!, for any m, we conclude that the error goes to zero more rapidly than any power of h. In the case of a function defined on (-, ), the Euler-Maclaurin summation formula still applies to the resulting doubly infinite sum approximation, provided as before that the function and all of its derivatives tend to zero for large positive and negative arguments. This principle is utilized in the error function and tanh-sinh quadrature scheme by transforming the integral of f (x) on a finite interval, which we will take to be (-1, 1) for convenience, to an integral on (-, ) using the change of variable x = g (t). Here g (x) is some monotonic function with the property that   56  CHAPTER 6. NUMERICAL TECHNIQUES II  g (x)  1 as x  , and g (x)  -1 as x  -, and also with the property that g (x) and all higher derivatives rapidly approach zero for large arguments. In this case we can write, for h > 0, 1    f (x) dx = -1 -  f (g (t))g (t) dt = h -  wj f (xj ),  (6.1.6)  where xj = g (hj ) and wj = g (hj ). If the convergence of g (t) and its derivatives to zero is sufficiently rapid for large |t|, then even in cases where f (x) has a vertical derivative or an integrable singularity at one or both endpoints, the resulting integrand f (g (t))g (t) will be a smooth bell-shaped function for which the Euler-Maclaurin summation formula applies, as described above. In such cases we have that the error in the above approximation decreases faster than any power of h. The summation above is typically carried out to limits (-N , N ), beyond which the terms of the summand are less than the ""epsilon"" of the multiprecision arithmetic being used. The error function integration scheme uses the function g (t) = erf (t) and  2 g (t) = (2/  )e-t . Note that g (t) is merely the bell-shaped probability density function, which is well known to converge rapidly to zero, together with all of its derivatives, for large arguments. The error function erf (x) can be computed to high precision as 1 - erfc(x), using the following formula given by Crandall [22, pg. 85] (who in turn attributes it to a 1968 paper by Chiarella and Reichel): e-t t erfc(t) =  2 2 2  1 +2 t2  k 1  e-k  k 2 2 + t2  22  +  2 1 - e2   t/  + E,  (6.1.7)  where |E | < e- / . The parameter  > 0 here is chosen small enough to ensure that the error E is sufficiently small. We summarize this scheme with the following algorithm statement. Here np is the precision level in digits, and is the ""epsilon"" level, which is typically 10-np . Algorithm 4 Error function complement [erfc] evaluation. Initialize: Set  :=  / np log(10), and set nt := np log(10)/ . 2 Set t2 := e- , t3 := t2 , and t4 := 1. 2   6.1. NUMERICAL QUADRATURE For k := 1 to nt do: set t4 := t2  t4 , Ek := t4 , t2 := t2  t3 ; enddo. Evaluation of function, with argument x: Set t1 := 0, t2 := x2 , t3 := e-t2 and t4 := /(1000  t3 ). For k := 1 to nt do: set t5 := Ek /(k 2 2 + t2 ) and t1 := t1 + t5 . If |t5 | < t4 then exit do; enddo. Set erfc(x) := t3 x/  (1/t2 + 2t1 ) + 2/(1 - e2x/ ).  57  2  We now state the algorithm for error function quadrature. As with the Gaussian scheme, m levels or phases of abscissas and weights are precomputed in the error function scheme. Then we perform the computation, increasing the level by one (each of which approximately doubles the computation, compared to the previous level), until an acceptable level of estimated accuracy is obtained (see Volume 2, Section 7.4.4 for an efficient error estimation). In the following, is the ""epsilon"" level of the multiprecision arithmetic being used. Algorithm 5 Error function quadrature. Initialize: Set h := 22-m . For k := 0 to 20  2m do:  Set t := k h, xk := 1 - erfc(t) and wk := 2/   e If |xk - 1| < then exit do; enddo. Set nt = k (the value of k at exit).  -t  2  .  Perform quadrature for a function f (x) on (-1, 1): Set S := 0 and h := 4. For k := 1 to m (or until successive values of S are identical to within ) do: h := h/2. For i := 0 to nt step 2m-k do: If ( mod (i, 2m-k+1 ) = 0 or k = 1) then If i = 0 then S := S + w0 f (0) else S := S + wi (f (-xi ) + f (xi )) endif. endif; enddo; endo. Result = hS . 2   58  CHAPTER 6. NUMERICAL TECHNIQUES II  6.2  Commentary and Additional Examples  [From Volume 2, Chapter 7 Commentary] 1. Evaluation of integrals. Evaluate the following integrals, by numerically computing them and then trying to recognize the answers, either by using the Inverse Symbolic Calculator at http://www.cecm.sfu.ca/pro jects/ISC, or by using a PSLQ facility, such as that built into the Experimental Mathematician's Toolkit, available at http://www.expmath.info. These examples are taken from Gradsteyn and Ryzhik [31]. All of the answers are simple one- or few-term expressions involving familiar mathemat ical constants such as  , e, 2, 3, log 2,  (3), G (Catalan's constant), and  (Euler's constant). We recognize that many of these can be evaluated analytically using symbolic computing software (depending on the available versions). The intent here is to provide exercises for numerical quadrature and constant recognition facilities. (a) 0  (b) 0  x2 dx  (1 + x4 ) 1 - x4   xe-x 1 - e-2x dx   1  (6.2.8) (6.2.9) (6.2.10) (6.2.11) (6.2.12) (6.2.13) (6.2.14)  (c) 0  x2 dx x e -1 x tan x dx   /4  (d) 0  /2  (e) 0  /4  x2 dx 1 - cos x ( /4 - x tan x) tan x dx  (f ) 0  /2  (g) 0  log2 (cos x) dx  Answers: (a)  /8, (b)  (1+2 log 2)/8, (c) 4 (log2 2+ 2 /12), (d) ( log 2)/8+ G/2, (e) - 2 /4 +  log 2 + 4G, (f ) (log 2)/2 +  2 /32 -  /4 + ( log 2)/8, (g)  /2(log2 2 +  2 /12).   6.2. COMMENTARY AND ADDITIONAL EXAMPLES  59  Figure 6.1: Newton-Julia set for p(x) = x3 - 1. 2. Julia sets. Figure 6.1 is a color-coded plot of the number of iterations required for convergence (to some accuracy ) of Newton's iteration (in the complex plane) for the cubic polynomial p(x) = x3 - 1. The filamentary structure shown is a Julia set, a set of measure zero separating disconnected regions. 3. Chaitin on randomness. It seems apropos to end with Greg Chaitin's views in ""The Creative Life: Science vs Art,"" an article available at the URL http://www.cs.umaine.edu/~chaitin/cdg.html. The message is that mathematics is quasi-empirical, that mathematics is not the same as physics, not an empirical science, but I think it's more akin to an empirical science than mathematicians would like to admit.   60  CHAPTER 6. NUMERICAL TECHNIQUES II Mathematicians normally think that they possess absolute truth. They read God's thoughts. They have absolute certainty and all the rest of us have doubts. Even the best physics is uncertain, it is tentative. Newtonian science was replaced by relativity theory, and then--wrong!--quantum mechanics showed that relativity theory is incorrect. But mathematicians like to think that mathematics is forever, that it is eternal. Well, there is an element of that. Certainly a mathematical proof gives more certainty than an argument in physics or than experimental evidence, but mathematics is not certain. This is the real message of G odel's famous incompleteness theorem and of Turing's work on uncomputability. You see, with G odel and Turing the notion that mathematics has limitations seems very shocking and surprising. But my theory just measures mathematical information. Once you measure mathematical information you see that any mathematical theory can only have a finite amount of information. But the world of mathematics has an infinite amount of information. Therefore it is natural that any given mathematical theory is limited, the same way that as physics progresses you need new laws of physics. Mathematicians like to think that they know all the laws. My work suggests that mathematicians also have to add new axioms, simply because there is an infinite amount of mathematical information. This is very controversial. I think mathematicians, in general, hate my ideas. Physicists love my ideas because I am saying that mathematics has some of the uncertainties and some of the characteristics of physics. Another aspect of my work is that I found randomness in the foundations of mathematics. Mathematicians either don't understand that assertion or else it is a nightmare for them . . .   Bibliography [1] Zafar Ahmed. Definitely An Integral. American Mathematical Monthly, 109:670671, 2002. [2] Gerg Almkvist and Herbert S. Wilf. On the Coefficients in the Hardy-Ramanujan-Rademacher Formula for p(n). Journal of Number Theory, 50:329334, 1995. [3] Jorg Arndt and Christoph Haenel. Pi Unleashed. Springer-Verlag, Heidelberg, 2001. [4] Kendall E. Atkinson. An Introduction to Numerical Analysis. John Wiley and Sons, Hoboken, NJ, 1989. [5] David H. Bailey. A Fortran-90 Based Multiprecision System. ACM Transactions on Mathematical Software, 21:379387, 1995. [6] David H. Bailey. Integer Relation Detection. Computing in Science and Engineering, 2(1):2428, 2000. [7] David H. Bailey, Peter B. Borwein, and Simon Plouffe. On The Rapid Computation of Various Polylogarithmic Constants. Mathematics of Computation, 66:903913, 1997. [8] David H. Bailey and David J. Broadhurst. Parallel Integer Relation Detection: Techniques and Applications. Mathematics of Computation, 70:17191736, 2000. [9] Petr Beckmann. A History of Pi. St. Martin's Press, New York, 1971. [10] Frits Beukers. A Rational Approach to Pi. Nieuw Archief voor Wiskunde, 5:372379, 2000. [11] D. Borwein and J. M. Borwein. On Some Intriguing Sums Involving  (4). Proceedings of the American Mathematical Society, 123:111118, 1995.  61   62  Bibliography  [12] D. Borwein, J. M. Borwein, and R. Girgensohn. Explicit Evaluation of Euler Sums. Proceedings of the Edinburgh Mathematical Society, 38:273294, 1995. [13] Jonathan Borwein, Peter Borwein, and K. Dilcher. Pi, Euler Numbers and Asymptotic Expansions. American Mathematical Monthly, 96:681687, 1989. [14] Jonathan M. Borwein, David Borwein, and William F. Galway. Finding and Excluding b-ary Machin-Type BBP Formulae. http://www.cecm.sfu.ca/preprints/2002pp.html, 2002. [15] Jonathan M. Borwein and Peter B. Borwein. Pi and the AGM: A Study in Analytic Number Theory and Computational Complexity. CMS Series of Mongraphs and Advanced books in Mathematics, John Wiley, Hoboken, NJ, 1987. [16] Jonathan M. Borwein and Peter B. Borwein. Strange Series Evaluations and High Precision Fraud. American Mathematical Monthly, 99:622640, 1992. [17] David Bressoud. Proofs and Confirmations: The Story of the Alternating Sign Matrix Conjecture. Mathematical Association of America, Washington, 1999. [18] David J. Broadhurst. Massive 3-loop Feynman Diagrams Reducible to SC Primitives of Algebras of the Sixth Root of Unity. http://lanl.arxiv.org/abs/hep-th/9803091, 1998. [19] David J. Broadhurst, John A Gracey, and Dirk Kreimer. Beyond the Triangle and Uniqueness Relations: Non-Zeta Counterterms at Large N from Positive Knots. Zeitschrift fur Physik, C75:559574, 1997. [20] David J. Broadhurst and Dirk Kreimer. Association of Multiple Zeta Values with Positive Knots via Feynman Diagrams up to 9 Loops. Physics Letters, B383:403412, 1997. [21] K. Chandrasekharen. Arithmetical Functions. Springer-Verlag, Heidelberg, 1970. [22] Richard E. Crandall. Topics in Advanced Scientific Computation. Springer-Verlag, Heidelberg, 1996. [23] Richard E. Crandall. New Representations for the Madelung Constant. Experimental Mathematics, 8:367379, 1999. [24] Richard E. Crandall and Jason S. Papadopoulos. On the Implementation of AKS-Class Primality Tests. http://developer.apple.com/hardware/ve/acgresearch.html, 2003.   Bibliography [25] D. P. Dalzell. On 22/7 and 355/113. Eureka, 34:1013, 1971.  63  [26] Keith Devlin. Mathematics: The Science of Patterns: The Search for Order in Life, Mind and the Universe. W. H. Freeman, New York, 1997. [27] Leonard Eugene Dickson. History of the Theory of Numbers, volume 13. Chelsea Publishing (1952), Carnegie Institute (1991), American Mathematical Society, Providence, 1999. [28] J. De Doelder. On Some Series Containing  (x) -  (y ) and ( (x) -  (y ))2 for Certain Values of x and y . Journal of Computational and Applied Mathematics, 37:125141, 1991. [29] Claire Ferguson. Helaman Ferguson: Mathematics in Stone and Bronze. Meridian Creative Group, Erie, PA, 1994. [30] Helaman R. P. Ferguson, David H. Bailey, and Stephen Arno. Analysis of PSLQ, An Integer Relation Finding Algorithm. Mathematics of Computation, 68:351369, 1999. [31] I. S. Gradshteyn and I. M. Ryzhik. Table of Integrals, Series, and Products, Fifth Edition. Academic Press, New York, 1994. [32] Francois Gu enard and Henri Lemberg. La M de Exp etho erimentale en Math ematiques. Scopos, Springer-Verlag, Heidelberg, 2001. [33] Godfrey H. Hardy. Ramanujan. Chelsea, New York, 1978.  sa. [34] J. Herman, R. Ku cera, and J. Sim Equations and Inequalities: Elementary Problems and Theorems in Number Theory, volume 1. CMS Books, Springer-Verlag, Heidelberg, 2000. [35] Donald E. Knuth. The Art of Computer Programming, volume 2. Addison-Wesley, Boston, 1998. [36] I. S. Kotsireas and K. Karamanos. Exact computation of the bifurcation point b4 of the logistic map and the bailey-broadhurst conjectures. International Journal of Bifurcation and Chaos, to appear. [37] K. Mahler. Arithmetische Eigenschaften der Losungen einer Klasse von Funktionalgleichungen. Mathematik Annalen, 101:342366, 1929. [38] Ivan Niven, Herbert S. Zuckerman, and Hugh L. Montgomery. An Introduction to the Theory of Numbers. John Wiley, Hoboken, NJ, 1991.   64  Index  [39] Oskar Perron. Die Lehre von den Kettenbruchen. Chelsea, New York, 1950. [40] George Polya. Mathematical Discovery: On Understanding, Learning and Teaching Problem Solving (Combined Edition). John Wiley, Hoboken, NJ, 1981. [41] Ed Regis. Who Got Einstein's Office? Addison-Wesley, Boston, 1986. [42] George Szpiro. Does the Proof Stack Up? Nature, 424:1213, Jul. 3, 2003. [43] H. S. Wall. Analytic Theory of Continued Fractions. Chelsea, New York, 1948.   Index Abel, Niels Henrik, 40 additive partitions, see partitions aeronautical engineering, 2 Archimedes, 33 Archimedes' inequality1 , 33 Au-Yeung, Enrico, 15 BBP-type formulas, 25 Bellard, Fabrice, 28 Benson's formula, 51 Berlinski, David, 1 Bernoulli numbers, 55 Berry, Michael2 , 31 Beukers, Frits, 34 biology, 2 Borwein, Peter, 23, 25 Broadhurst, David, 14, 17 Carath dory, Constantin, 43 eo Catalan, Eug ene Catalan numbers, 52 Catalan's constant, 54 Catalan, Eug` ene Catalan's constant, 20 Chaitin, Gregory, 59, 60 computer algebra systems, 2, 19 constant recognition, 19 continued fractions, 3538 numerical accuracy, 35 1 2  cosmology, 2 Coxeter, H. S. M. (Donald), 21, 21 4-D polytope, 22 kaleidoscope, 22 Crandall, Richard, 3, 56 Madelung's constant, 52 De Doelder, 15 Dedekind  -function, 45 Devlin, Keith, 44 Dijk paths, 52 economics, 2 Erd Paul, 46 os, Escher, Maurits C., 21 Euler, Leonhard Euler numbers, 89 Euler sums, 1516 quantum field theory, 17 Euler-Maclaurin summation formula, 5556 Euler-Zagier sums, see Euler sums Eureka (journal), 33 experimental mathematics, 2 Ferguson, Claire, 12 Ferguson, Helaman, 6, 1114 sculptures, 13, 1214 Ferguson, Samuel, 6 Fermat's Last Theorem, 4 Ferrer diagram, 44 Feynman, Richard, 53  In this Index,  denotes a figure. In this Index,  denotes a quote.  65   66 Feynman diagrams, 17 Fibonacci, Leonardo, 47 Fibonacci numbers, 52 Fibonacci sequence, 4750 fluid dynamics, 2 Gamma function, 42 G odel, Kurt, 1 Goldbach, Christian, 16 Gregory, James Gregory's series, 7, 9 Hadamard, Jacques, 5 Hales, Thomas, 6 Hamming, Richard, 7 Hardy, G. H., 5, 45 Hata, 34 Hilbert, David, 6 hypergeometric function calculation of  , 27 integer relation algorithm, 1011 LLL, 11 polynomials, 10 PSLQ, 10 integer relation algorithms, 50 Jacobi, Carl Jacobian theta functions, 46, 46 Julia sets, 59 Kanada, Yasumasa, 3 Kepler, Johanes, 6 Klein, James D., 41 Knuth, Donald, 26 Knuth, Donald E., 38 Lambert W function, 39 LLL, 11 logistic iteration, 9, 912  Index Bifurcation plot, 10 Lucas, Edouard, 48 Lucas numbers, 52 Lucas sequence, 48 MacMahon, Percy, 44 Madelung's constant, 52 Mahler, Kurt, 33 Mayer, Ernst, 3 Milnor, John, 5 Moore, Gordon Moore's Law, 2 Newton's method, 59 Newton, Isaac, 23 North, Joseph Roy, 7 Papadopoulos, Jason, 3 Parseval's equation, 15 partitions, 43 Percival, Colin, 3, 29 physics, 2 pi continued fraction, 32 estimates, 32 pictorial representations, 33  algorithms for calculating BBP algorithm, 2329 Biblical references, 30 calculations of, 3 formulas for calculating, 24, 27, 30 Plouffe, Simon, 8, 23, 25 Pochhammer function, 39 polylogarithms Euler sums, 16 PSLQ, 10, 12, 54 calculation of  , 27 Feynman diagrams, 17, 18 psychology, 2   Index quadrature, 53 error function quadrature, 5457 quantum physics, 2 Rademacher, Hans, 45 Ramanujan, Srinivasa, 45 Regiomontanus, Johann, 47 Riemann zeta function, 15, 39 Euler sums, 16 Feynman diagrams, 18 Sloane, Neil, 8 sociology, 2 Stirling, James Stirling's formula, 39 supernovas, 2 tangent numbers, 89 Taubes, Gary, 53 theta function Fibonacci sums, 4850 Turing, Alan, 1, 60 von Neumann, John, 1 Wiles, Andrew, 4  67"
GX049-00-14373160	"Profile Hidden Markov Model Analysis     [  Program Manual  |  User's Guide  |  Data Files  |  Databases  ]            Sensitive Database Searching and Identifying  Sequence Domains            Introduction         Profile analysis has long been  a useful tool in finding  and aligning distantly related sequences  and in  identifying known sequence domains in  new sequences.  Basically, a  profile is a description of  the  consensus of a multiple sequence  alignment.  It uses a  position-specific scoring system to capture  information about the degree of  conservation at various positions in  the multiple alignment.  This  makes it a much more  sensitive and specific method for  database searching than pairwise methods,  such as those used by   BLAST  or  FastA , that use  position-independent scoring.     Hidden Markov modeling, a technique  that has been used for  years in speech recognition, is  now being  applied to many types of  problems in molecular sequence analysis.   In particular, this technique  can  produce profiles that are an  improvement over traditionally constructed profiles.     Profile hidden Markov models (HMMs)  have several advantages over standard  profiles.  Profile  HMMs have a formal probabilistic  basis and have a consistant  theory behind gap and insertion  scores,  in contrast to standard profile  methods which use heuristic methods.   HMMs apply a statistical  method to estimate the true  frequency of a residue at  a given position in the  alignment from its  observed frequency while standard profiles  use the observed frequency itself  to assign the score for  that residue.  This means  that a profile HMM derived  from only 10 to 20  aligned sequences can be of  equivalent quality to a standard  profile created from 40 to  50 aligned sequences.  In  general,  producing good profile HMMs requires  less skill and manual intervention  than producing good  standard profiles.     The HMMER (pronounced  hammer ) package  developed by Sean Eddy of  Washington University in St.  Louis, Missouri, is a set  of programs that allow you  to create and manipulate profile  HMMs and  databases of profile HMMs (HmmerBuild,  HmmerConvert), perform sensitive searches of  sequence  and profile HMM databases, (HmmerSearch  and HmmerPfam) and create multiple  sequence  alignments efficiently (HmmerAlign).  In  collaboration with Dr. Eddy, GCG has  incorporated these  programs into the Wisconsin Package.        What is a Profile HMM?   - A Simplified Description         A profile HMM is a  linear state machine consisting of  a series of nodes, each  of which corresponds  roughly to a position (column)  in the alignment from which  it was built.  If  we ignore gaps, the  correspondence is exact -- the  profile HMM has a node  for each column in the  alignment, and each  node can exist in one  state, a match state.   (The word ""match"" here implies  that there is a position  in  the model for every position  in the sequence to be  aligned to the model.)       A profile HMM has several  types of probabilities associated with  it.  One type is  the  transition     probability  -- the probability of  transitioning from one state to  another.  In a simple  ungapped model,  the probability of a  transition from one match state  to the next match  state is 1.0 and the  path through the model is  strictly linear, moving from the  match state of node n  to  the match state of node  n+1.     There are also  emissions probabilities   associated with each match state,  based on the probability of  a  given residue existing at that  position in the alignment.   For example, for a fairly  well-conserved  column in a protein alignment,  the emissions probability for the  most common amino acid may  be  0.81, while for each of  the other 19 amino acids  it may be 0.01.    If you follow  a path through the model  to generate  a sequence consistent with the  model, the probability of any  sequence that is generated depends  on  the transition and emissions probabilities  at each node.     In order to model real  sequences, we also need to  consider the possibility that gaps  might occur when  a model is aligned to  a sequence.  Two types  of gaps may arise.   The first type occurs when  the  sequence contains a region that  is not present in the  model (an insertion in the  sequence).  The second  type occurs when there is  a region in the model  that is not present in  the sequence (a deletion in  the  sequence).  To handle these  cases, each node in the  profile HMM must now have  three states:  the  match state, an insert state,  and a delete state.   The model also needs more  types of transition  probabilities:  match->match, match->insert, match->delete, insert->match, etc.           Aligning a sequence to a  profile HMM is done by  a dynamic programming algorithm that  finds the  most probable path that the  sequence may take through the  model, using the transition and  emissions  probabilities to score each possible  path.     In general, if the sequence  is equivalent to the consensus  of the original alignment, the  path through  the model will pass from  match state to match state  in a linear fashion.   If the sequence contains a  deletion relative to the consensus,  the path passes through one  or more delete states before  transitioning to the next match  state; if the sequence contains  an insertion relative to the  consensus,  the path passes through an  insert state between two match  states.     For example, if a sequence  contains an insert that occurs  between nodes 5 and 6  of the model, the  path transitions from the node  5 match state to an  insert state.  It remains  in the insert state and  ""consumes"" residues in the sequence  until it reaches the residue  in the sequence that corresponds  to  node 6 in the model.   At this point the  path transitions from the insert  state to the node 6  match state.     Similarly, if the sequence contains  a deletion so that it  has no residues corresponding to  nodes 12  through 15 of the model,  the path transitions from the  node 11 match state into  a delete state, then  transitions through additional delete states  until it can transition to  the match state of node  16 of the  model.     Profile HMMs can be aligned  to a sequence either globally  (the whole profile HMM aligns  to the  sequence) or locally (only part  of the profile HMM need  be aligned with the sequence).   The alignment  type is actually part of  the model, so you must  specify whether the model is  to be global or local  at the  time the model is  built ,  not at the time the  model is used.  (See  HmmerBuild documentation for more  details.)        Most Common Uses for Profile  HMMs         Because a profile HMM can  serve as a representation of  a sequence family or sequence  domain, the  most common application is to  compare profile HMMs and sequences.   These types of comparisons  are  more likely to identify distant  homologs than sequence vs. sequence comparisons  used in most  database search programs.     For example, you can use  HmmerPfam to compare your sequence  to a database of profile  HMMs  representing known sequence families and  known sequence domains.  A  match to one of these  profile  HMMs can help you identify  your sequence and determine its  function.  The curated Pfam  (""Protein  families"") database contains a large  number of global profile HMMs  representing known protein  families, while the PfamFrag database  contains local profile HMMs for  these same families.     Similarly, you can create a  profile HMM representing a domain  or sequence family in which  you are  interested, then use this profile  HMM as a query to  search a sequence database with  HmmerSearch to  see if any other sequences  possess this domain.     Another use for profile HMMs  is to create a multiple  alignment of a large number  of sequences more  quickly than by using standard  methods.  HmmerAlign uses a  small seed alignment of representative  sequences to create a profile  HMM which is then used  as a template for aligning  the full set of  sequences.        Overview of the HMMER Programs         There are nine programs in  the GCG adaptation of the  HMMER package.  The following  five are used  to create and manipulate profile  HMMs:     HmmerBuild  -- creates a profile  HMM from a set of  pre-aligned sequences.  The profile  HMM can be  appended to a file containing  other profile HMMs in order  to create an HMM database  file.     HmmerCalibrate  -- calibrates an existing  profile HMM or profile HMM  database so that searches  performed with it will be  more sensitive.     HmmerConvert  -- converts a profile  HMM created by HmmerBuild into  other formats.     HmmerIndex  -- indexes a profile  HMM database so that profile  HMMs can be retrieved from  it easily  with HmmerFetch.     HmmerFetch  -- extracts a profile  HMM from an indexed profile  HMM database into a file.    The remaining four programs are  used for analyzing data:     HmmerSearch  -- searches a sequence  database with a profile HMM  query.     HmmerPfam  -- searches a profile  HMM database with a sequence  query.  The profile HMM  database  file may be one you  created as well as the  Pfam database created by Eddy  and collaborators.     HmmerAlign  -- efficiently creates a  large multiple alignment from a  small seed alignment and a  collection of unaligned sequences.     HmmerEmit  -- randomly generates sequences  that match a given profile  HMM.        Pfam Acknowledgement        Pfam - A database of  protein domain family alignments and  HMMs Copyright (C) 1996-2000 The  Pfam Consortium.        References          1.  Eddy, S.R., et al. (1996).   Hidden Markov Models.   Current  Opinion in Structural Biology ,      6 ; 361-365.     2.  Durbin, R., Eddy, S., Krogh,  A., and Mitchison, G. (1998).    Biological Sequence Analysis.       Probabilistic Models of Proteins and  Nucleic Acids , Cambridge University Press,  Cambridge, UK.     3.  Eddy, S.R. (1998).  Profile hidden  Markov models.   Bioinformatics ,  14 ; 755-763.                   Printed: February 5, 2001  15:21 (1162)         [  Program Manual  |  User's Guide  |  Data Files  |  Databases  ]       Technical Support:  support-us@accelrys.com  or  support-eu@accelrys.com      Copyright (c) 1982-2002 Accelrys Inc.  A subsidiary of Pharmacopeia, Inc.  All rights reserved.    Licenses and Trademarks Wisconsin Package is a trademark and GCG and the GCG logo are registered trademarks of Accelrys Inc.    All other product names mentioned in this documentation may be trademarks, and if so, are trademarks or registered trademarks of their respective holders and are used in this documentation for identification purposes only.       www.accelrys.com/bio"
GX255-61-7916521	"Date of revision: Saturday, July 21, 2001  Structural aspects of the fivefold quasicrystalline Al-Cu-Fe surface from STM and dynamical LEED studies  T. Caia, F. Shib, Z. Shena, M. Giererbc, A.I. Goldmand, M.J. Kramer,e C.J. Jenksa, T.A. Lograssoe, D.W. Delaneye, P.A. Thiela, and M.A. Van Hove bfg  a  Department of Chemistry and Ames Laboratory,  b  Materials Sciences Division, Lawrence Berkeley National Laboratory, University of California, Berkeley, CA 94720  c  Institut fr Kristallographie und Mineralogie der Universitt Mnchen, Theresienstr. 41, D-80333 Mnchen, Germany d  Department of Physics and Astronomy and Ames Laboratory,  e  Department of Materials Science and Engineering and Ames Laboratory, Iowa State University, Ames, IA 50011 f  Advanced Light Source, Lawrence Berkeley National Laboratory, University of California, Berkeley, CA 94720   Shi, Cai, et al. 2 g  Department of Physics, Univ. of California, Davis, CA 95616  Submitted to Surface Science, May 2001   Shi, Cai, et al. 3  Abstract We investigate the atomic structure of the fivefold surface of an icosahedral AlCu-Fe alloy, using scanning tunneling microscopy (STM) imaging and a special dynamical low energy-electron diffraction (LEED) method. STM indicates that the step heights adopt (primarily) two values in the ratio of tau, but the spatial distribution of these two values does not follow a Fibonacci sequence, thus breaking the ideal bulk-like quasicrystalline layer stacking order perpendicular to the surface. The appearance of screw dislocations in the STM images is another indication of imperfect quasicrystallinity. On the other hand, the LEED analysis, which was successfully applied to Al-Pd-Mn in a previous study, is equally successful for Al-Cu-Fe. Similar structural features are found for both materials, in particular for interlayer relaxations and surface terminations. Although there is no structural periodicity, there are clear atomic planes in the bulk of the quasicrystal, some of which can be grouped in recurring patterns. The surface tends to form between these grouped layers in both alloys. For Al-Cu-Fe, the step heights measured by STM are consistent with the thicknesses of the grouped layers favored in LEED. These results suggest that the fivefold Al-Cu-Fe surface exhibits a quasicrystalline layering structure, but with stacking defects.  Keywords: Theoretical methods, models & techniques: 1. Electron-solid interactions, scattering, diffraction   Shi, Cai, et al. 4  Experimental sample preparation and characterization methods: 2. Scanning Tunneling Microscopy (STM) 3. Electron-solid diffraction--Low Energy Electron Diffraction Phenomena: 4. Step formation and bunching Elemental and chemical identity: 5. Aluminum Compounds: 6. Alloys Surfaces: 7. Single crystal surfaces--Low index single crystal surfaces   Shi, Cai, et al. 5  1. Introduction The atomic structure at the surfaces of quasicrystals is a matter both of technological and fundamental interest. On the technological side, the highly-ordered but non-periodic bulk structure of these metallic alloys is tied to an unusual combination of physical properties. These properties have already led to some applications as coatings and composites, and may lead to more.[1, 2] On the fundamental side, a basic question is how the bulk structure of the three-dimensional quasicrystals (i.e., the icosahedral phases) responds to the two-dimensional truncation enforced by a surface. Another is how the bulk and surface structures relate to the macroscopic physical properties. In the present paper, we employ two techniques to gain insights into the atomic structure at the surface of one particular quasicrystal, icosahedral (i-) Al-Cu-Fe. The first technique used here is STM, which provides real-space information on some aspects of surface structure. The first report of an STM image of the 5f surface of this particular alloy, i-Al-Cu-Fe, was given by Becker et al., [3]. However, all subsequent work on surfaces of icosahedral materials was done with i-Al-Pd-Mn, and in these later studies a very detailed level of analysis emerged. Schaub, et al.[4-6] observed a stepterrace structure that displayed geometric characteristics expected for a bulk-terminated surface, namely, arrangements of features, both lateral and vertical, in Fibonacci sequences. Later, we provided an interpretation of the fine structure on the terraces in terms of the cluster structure of the bulk.[7] Very recently, Ledieu et al. have analyzed   Shi, Cai, et al. 6  the fine structure in terms of bulk tilings.[8] While these approaches and analyses differ, there is one main conclusion common to all of the three more recent studies (Schaub's, ours, and Ledieu's): The fine structure on the terraces of i-Al-Pd-Mn is consistent with bulk structural models. Hence, the horizontal structure, i.e. the structure within the surface plane, must be close or identical to that of the bulk. The step heights measured via STM present a different situation. While Schaub et al. reported only two values of step heights on i-Al-Pd-Mn, we found three; furthermore, we found the frequency of step heights as a function of height to be much different (qualitatively) than did Schaub et al., suggesting a significant difference in the layerstacking in the two studies. [4-7] Analysis of step height data are important for two reasons: First, they are a test of the vertical `perfection' of the quasicrystalline surface, i.e. of whether the layers are stacked in a bulk-like sequence; and second, the LEED I-V model predicts specific step heights by predicting separations between favored terminations. Hence, the step heights measured in STM can be cross-checked against the results of the LEED structure model. In this paper, we will report and analyze the step heights on 5f i-Al-Cu-Fe. In the second technique, we exploit the fact that surfaces of the icosahedral quasicrystals display sharp and dense LEED patterns.[5, 6, 9-16] Dynamical LEED analysis of experimental intensity-voltage (IV) curves can then be used to derive atomic structure, but for quasicrystals a special challenge comes from the non-periodic ordering of the atoms in the bulk. (A number of techniques, including LEED, have confirmed that the fivefold surfaces retain fivefold symmetry.[5-7, 9-20] Hence, it is reasonable to assume that a bulk-terminated surface is a good starting point in the structural analysis.)   Shi, Cai, et al. 7  It is impossible to define a unit cell as in periodic crystals; instead there is an infinite variety of local structures. Therefore an exact dynamical calculation of LEED IV curves for quasicrystals is not feasible, and certain approximations are required to make the analysis possible. Our previous study of the fivefold surface of i-Al-Pd-Mn [12, 21] showed that a successful dynamical analysis of a fivefold (5f) surface could be achieved. The main modification to the LEED theory was the ""average neighborhood approximation,"" according to which the scattering properties of atoms with similar neighborhoods were assumed to be identical. The success of the approximation was partly due to the fact that a few types of local structures repeat throughout a quasicrystal and thus dominate in the diffraction. For example, more than 50% of the atoms on the 5f surfaces of Al-Pd-Mn form pentagons. In addition, the structure of LEED IV curves primarily reflects local structure, mainly because of the short free mean path of the diffracting electrons. [22] It is reasonable to expect that the same analysis will work for i-Al-Cu-Fe. The bulk structures of i-Al-Cu-Fe and i-Al-Pd-Mn are very similar, with minor deviations due mainly to differences in composition: the changed composition fractions result primarily in substitution of chemical identities, and secondarily in a small number of added or deleted atoms in some sites. This paper presents the outcome of that analysis, for the 5f surface. The organization of the paper follows. First, experimental details, both of LEED and STM, are presented in Sec. 2. The theory of the dynamical LEED of quasicrystals is summarized in Sec. 3. In Sec. 4, the experimental STM data, including step height distributions, are presented. In Sec. 5, the dynamical LEED analysis on Al-Cu-Fe is   Shi, Cai, et al. 8  described. In Sec. 6, the results are compared with those for Al-Pd-Mn and conclusions are drawn.  2. Experimental Description. Samples were grown, characterized, and prepared as discussed in previous papers.[23, 24] The two samples used in STM were adjacent slices from the same single grain, whose composition was determined via inductively coupled plasma-atomic emission spectroscopy (ICP-AES) to be Al 61.5  Cu24.8Fe 63.4  13.7  . The composition of the sample 12.6  used in LEED was determined similarly to be Al  Cu24.0Fe  . In both the STM and  LEED experiments, we cleaned the sample in ultrahigh vacuum with cycles of ion etching and annealing. In the STM experiments, we used He ions, whereas in the LEED experiments, we used Ar ions. [13] Etching with Ar is known to shift the surface composition far away from the icosahedral region of the phase diagram in the Al-rich quasicrystals. [5, 11, 13, 14, 25-29] Helium was chosen in the STM experiments (done more recently than the LEED experiments) to reduce preferential etching of Al.[30] A fresh sample, introduced from air, was typically cleaned by sputtering for 30 minutes, followed by 30 minutes of annealing, starting at 450 K and stepping up by 50 K when significant carbon and oxygen were no longer detected by AES at a given temperature. The sample was heated to a maximum temperature of 875 K. STM and LEED experiments were performed in three separate ultrahigh vacuum chambers. The two STM chambers were each equipped with an Omicron STM, instrumentation for Auger electron spectroscopy (AES), a mass spectrometer, ion   Shi, Cai, et al. 9  sputtering gun, sample heating capability, and a manifold for introduction of selected pure gases by backfilling. A typical base pressure during STM measurements was 2 to 6 x 10 -11  Torr. The STM samples were each about 3x4 mm2 in area, and 1.5 mm thick.  Before each STM measurement, the sample was sputtered for 30 minutes with He (1.0 KeV beam voltage, 8-10 A from sample to ground with no bias), annealed at the stated temperature for one to two hours, and cooled down to room temperature. The typical tunneling current for the STM measurements was 0.3-0.5 nA, and the tunneling voltage was 1.0 V. In the course of measuring the step heights with STM, we used two different scanners. Comparison revealed that it was important to calibrate the piezoelectrics accurately. We used Ag(100) as the standard, with known atomic step heights of 2.04 . Without this in-house calibration, errors up to 40% would have resulted. The LEED chamber and measurements have been described previously.[13, 21]  3. Experimental STM Data. After annealing at temperatures lower than 825 K, STM reveals a rough morphology with cluster-like protrusions. These protrusions have different sizes, varying from 1 nm to 2.5 nm in diameter (Fig. 1a). Terraces start to appear at about 825 K, though still dotted by clusters (Fig. 1b). At higher annealing temperatures (850  875 K), a step-terrace morphology predominates (Fig. 1c, 1d). This cluster-to-terrace sequence is similar to the progression of structures that we, and others, have reported already for 5f[7, 31, 32] and 3f [33] surfaces of i-Al-Pd-Mn. Commonly, the well-annealed surfaces   Shi, Cai, et al. 10  exhibit apparent screw dislocations, as shown by the black arrow in Fig. 1e, and by the white arrow in Fig. 1f. It is also common that the large terraces contain broad but shallow pentagonal pits (black arrows in Fig. 1f). These two features--the screw dislocations, and the pentagonal pits--are distinctive, since we never observe screw dislocations and pentagonal pits on the 5f Al-Pd-Mn surfaces. The fine structure on the terraces after high temperature annealing is shown in Fig. 1g-h. It consists of many flower-like features that are approximately 18  in diameter, (most obvious in Fig. 1h) and of small black holes arranged in Fibonacci pentagrids (most obvious in Fig. 1g). The flowers are presumably the same as the ""daisies"" noted earlier by Becker, et al..[3] The origin of the fine structure on this surface will be discussed elsewhere. The corrugation in Fig. 1g is about 0.5  peak-to-peak. (Statistical analysis gives 0.25 for the root mean square, and 0.18 as the arithmetic mean.) The main point is that the corrugation on the terraces is much smaller than the step heights, and hence does not interfere significantly in the following analysis. We also analyzed the step heights carefully. Determination of individual step heights was problematic, for two reasons. First was the sloping background evident in Fig. 2, which could not be corrected satisfactorily by planing the entire image--either due to STM drift, or to a meandering, large-scale curvature of the surface. Second was the fact that the step height determined from a single line profile varied significantly, depending upon the exact point where the profile cut across the step. Hence, we devised a procedure to obtain a statistical average of heights measured along a continuous step, within localized regions of the image. For background correction, we used standard Omicron software to plane the original image. For individual step heights, we used the   Shi, Cai, et al. 11  Omicron software to construct histograms of pixel intensities in the immediate vicinity of each step. These histograms showed the frequency of z-values versus z within the local area bridging two neighboring terraces. This resulted in a histogram with two peaks, one corresponding to the lower terrace and the other to the upper terrace. We took the separation of the peaks as the step height across the two terraces. A typical histogram for a single step height measurement spanned roughly 300 nm2. An example of one such histogram is shown in Fig. 3, and corresponds to the pixel height distribution encompassed within the rectangular box of Fig. 1c. We avoided steps originating obviously from screw dislocations in the analysis of the step height distributions. Steps originating from screw dislocations predominantly displayed heights of 2.5 . The entire set of values of step heights is illustrated in the distribution of Fig. 4. We see three step heights: 2.5 , 4.0  and 6.2-6.6 . These values form consecutive ratios of 1.6 and 1.55-1.65, i.e. close to the golden mean, =1.618. The step height of 2.5  is found much less frequently than the other two values. For instance, in Fig. 4, we have 30 steps with height of 6.2 , compared to 5 with a height of 2.4 . Line profiles across series of terraces display sequences of step heights such as those illustrated in Fig. 2. Steps are labeled as low (L) and high (H), corresponding to heights of about 4  and 6 , respectively. If the surface were perfectly bulk-terminated, one would expect to see a Fibonacci sequence of L and H steps, in which sequences such as L-L-L would be forbidden.[34] The fact that we observe the forbidden L-L-L   Shi, Cai, et al. 12  sequences indicates that the surface cannot be perfectly bulk-terminated; instead, it appears that layers--or, more likely, groups of layers--are stacked imperfectly. The observation of a relatively high density of screw dislocations on this surface (on the order of 3 dislocations per 200  x 200 ) is also consistent with imperfections in the layer stacking sequence. In a regular periodic lattice, as one spirals around a screw dislocation, all layers line up properly again after each turn, preserving a simple sequence of equal step heights. But in a quasicrystal, because of the non-periodic spacings between layers, most layers do not line up correctly when spiraling around a screw dislocation, leading to mismatches and thus stacking errors. For example, if one propagates a step of height L around a screw dislocation (as in spiral growth), and if that step height is maintained, it will create a sequence of heights L-L-L-..., adding one height L at each turn. This simple example, if continued, leads to a periodic sequence. The exact relationship between screw dislocations and the non-Fibonacci step height sequences is not clear at this stage. Among other things, such a relationship must explain how the screw dislocations generate 2.5  step heights in their immediate vicinity, whereas the non-Fibonacci sequences include 4.0 and 6.2-6.6  steps (Fig. 2); for example, a 2.5  step may join another step and increase or decrease its height by 2.5  (Fig. 1e). It is nonetheless true that the screw dislocations and the non-Fibonacci step heights are both manifestations of imperfect quasicrystalline stacking, such as locally periodic stacking. Periodic sequences of interlayer spacings have in fact been observed by highresolution transmission electron microscopy (TEM) in defect areas of bulk i-Al-Cu-Fe samples. Similar periodic regions have been observed by TEM in the types of samples we   Shi, Cai, et al. 13  use, and are associated with strain fields. Strain fields arise because the samples are first prepared via liquid-assisted growth, then hot-isostatically-pressed to reduce porosity, and finally annealed again to reduce strain from the pressing. Strain fields become particularly abundant after the hot isostatic-pressing step, as shown by comparing the TEM images of Fig. 5a and 5b. Fig. 5a is a TEM image of an as-grown sample, whereas Fig. 5b shows a sample immediately after pressing. The strain fields are reduced considerably, although not eliminated, after the final annealing. Fig. 5c is a TEM image after the final treatment, showing a region that is locally perfect. We speculate that remnant strain fields from the hot-pressing procedure may play a role in the imperfections at our surfaces.  4. Sketch of dynamical LEED theory for quasicrystalline surfaces In spite of their non-periodicity, quasicrystals often produce LEED patterns with a set of well-defined spots, as shown in Fig. 6. The LEED patterns can be even sharper and clearer than for normal crystals; quasicrystals are thus structurally very different from amorphous materials. The sharp LEED pattern is evidence that quasicrystals are a class of ordered materials with particular long-range order and orientation symmetries; in fact they can be described with self-similar models (i.e. as structures that can be scaled up by constant factors to yield similar structures at different length scales). It has been proven in many studies that the Fourier transform of a quasicrystal structure is well ordered in reciprocal space, although again not periodic.[34] In LEED, the Fourier transform corresponds to single scattering theory. Just as with periodic crystals, multiple scattering does not change the LEED pattern in reciprocal lattice, but modifies the LEED spot   Shi, Cai, et al. 14  intensities. The LEED IV curves for these patterns have the same qualitative appearance as for a normal crystal, and are illustrated in Fig. 7 and Fig. 8, where diffraction spot intensity is plotted as a function of momentum transfer parallel to the surface, k||. As a starting point for the structural LEED analysis, we need a source of bulk atomic coordinates with icosahedral symmetry. Quasicrystals are most conveniently represented as a periodic bcc lattice in 6 dimensions. The ""atoms"" in that lattice are replaced by atomic hypersurfaces. Tricontahedral atomic hypersurfaces, instead of the usual spherical ones, are used in our models for Al-Cu-Fe and Al-Pd-Mn, in order to yield reasonable bond lengths. [35] The three-dimensional structure is generated as a particular projection from that six-dimensional bcc lattice into a 3-dimensional space, and the two-dimensional surface is obtained by terminating the three-dimensional lattice. The bulk structure of Al-Cu-Fe is determined by x-ray and neutron diffraction, which fix the atomic hypersurfaces. Once the 3-dimensional bulk structure of Al-Cu-Fe is determined, a 5f surface is obtained by rotating the bulk structure so that the surface plane is perpendicular to the 5f axis. The resulting atomic structure along the surface normal shows well-defined and separated atomic planes, with one, two or three chemical types of atoms in each plane. (We shall use the term ""plane"" for sets of coplanar atoms and ""layer"" for more general sets of atoms, including groups of atomic ""planes"", as in ""composite layers""). The interplanar spacings vary systematically according to Fibonacci sequences and the Golden mean. As a result of the non-periodicity, each plane is different. If we assume that the surface terminates along such atomic planes, there is, strictly speaking, an infinite number of inequivalent terminations. Within each atomic plane, although there is no   Shi, Cai, et al. 15  repeating unit cell, one finds repeating local 5f rings throughout the plane. The atomic density of different planes varies strongly in the range from 100 to 800 atoms in an area of 100  x 100 . The smallest size of the local rings varies with the atomic density. These planes are found to belong to subsets of planes with very similar compositions, atomic densities and geometry. Thus, the infinite variety of planes actually has a relatively small number of distinct types of structure, and we shall exploit this property. The major difficulty in the LEED analysis of quasicrystals comes from multiple scattering. In a single scattering LEED theory, which assumes that all electrons are scattered only once before they leave the surface, the outgoing wave amplitude from an atom depends only on the chemical identity and the position of the atom. In dynamical LEED, however, the outgoing wave amplitude from an atom is composed not only of that directly scattered wave, but also of waves that have undergone other scattering events within the surface. The latter events depend not only on the chemical identity and position of the scatterer, but also on the environment of the scatterer. In principle, the environment of every atom is different from that of every other atom, measured on an infinitely large scale. Therefore, the total wave amplitude of outgoing electrons from any given atom is different from that from all other atoms. In this sense, there are an infinite number of different atoms that have to be taken account in a dynamical LEED analysis of quasicrystals. This is fundamentally different from the dynamical LEED theory of periodic crystals, in which the number of different atoms is finite, due to the repeating unit cells. As a result, certain approximations have to be made in a realistic dynamical LEED analysis of quasicrystals. In this paper, we apply efficient approximations that   Shi, Cai, et al. 16  were tested in a previous LEED analysis of an i-Al-Pd-Mn surface;[21] these are sketched in the following. The basic idea is that the local environments of atoms tend to have only a few basic structural arrangements, as described above. Due to the limited mean free path of propagating electrons within the surface, the LEED IV curves are determined primarily by the scattering within such local environments. The different approximations depend on how we treat the similar local environments in a quasicrystal lattice. More specifically, the approximations in our theory take into account that the atoms are explicitly arranged in atomic planes parallel to the 5f surface in a bulk terminated quasicrystal. The atoms within a given atomic plane are typically evenly distributed in space. The atomic density measured per unit area is uniform within a plane, but fluctuates from plane to plane, in the range 0.01-0.13 atoms/2. The ""effective"" atomic scattering properties (including all multiple scattering events) are more alike within an atomic plane than between different atomic planes, as the average scattering properties of atoms in an atomic plane depend largely on the composition and the atomic density of that plane. Therefore, as an approximation, we assume that all atoms within a certain atomic plane are equivalent, but different from atoms in other planes. This approximation, in the single scattering theory, implies that the scattering properties of an atom are averaged over chemical identities. It leads to the average tmatrix approximation (ATA),[36] [37] [38] according to which the scattering matrices t of the individual atoms within one plane are replaced by an averaged scattering t-matrix:   Shi, Cai, et al. 17  t = c Al t  Al  + cCut  Cu  + c Fe t Fe ,  where Ci are the relative concentrations within the plane. Next, as an approximation in the multiple scattering LEED theory, the variable environments of the atoms in a particular plane are replaced by a fixed, simplified average geometry, referred to as ""average neighborhood approximation"" (ANA). More specifically, the final wave amplitude from an atom, with all multiple scattering events considered, is replaced by an averaged wave amplitude over all atoms in the plane. The ATA is applied before the wave amplitude is averaged. For more details, see Ref. [21]. With these approximations, we can perform the calculation with a relatively small number of atoms, equivalent to the number of atomic planes, with different planedependent scattering properties. If we take atomic planes as deep as 10  into account, we obtain about 12 planes, i.e. 12 atoms with different scattering properties. The incident beam is damped into the surface due to inelastic scattering events, so that the contributions of deeper atoms do not influence the IV curves significantly. The calculation of the averaged propagator matrix G which describes electron propagation within and between closely-spaced planes, is performed by averaging in an area of 100  x 100 . The averaged wave amplitude depends strongly on the atomic density of the plane. For a plane with high atomic density, the averaged amplitude is affected by multiple scattering events within the plane more than that between planes. Inversely, for a plane with low atomic density, it is affected more by the multiple scattering events between atomic planes than by that within the plane.   Shi, Cai, et al. 18  The ANA, in which all atoms in a particular atomic plane have the same averaged effective scattering properties, can be improved in several respects. First, it is possible to use the ANA without the ATA. In this case, a single coplanar atomic plane is divided into several mono-atomic planes such that each has only one chemical species. All atoms in one mono-atomic plane are then assumed to have the same scattered wave amplitude. The computation time increases because one has to deal with more mono-atomic planes, i.e. more atoms in the calculation. It has been demonstrated for Al-Pd-Mn that most IV curves are little affected by the use of the ATA: thus one can safely apply ATA and gain computational time. The ANA may be further improved if one divides an atomic plane into several subplanes in which the atoms in each subplane have exactly the same local environment within and out of the plane. For example, one can assume that the local environments of two atoms are similar only if the nearest-neighbor distances and the number of nearest-neighbors are equal. All the atoms are thus sorted into a few classes of atoms, each with the same local neighborhoods. Notice that the number of different atoms increases sharply if the size of the local neighborhood increases. In the LEED calculation for Al-Pd-Mn 5f surfaces, it was shown that the IV curves are also very similar under this approximation. This at least shows that the ANA is reliable for the 5f quasicrystal surfaces. However, the approximation was found to be less successful for the surfaces of Al-Pd-Mn with lower symmetry, such as the two- and three-fold surfaces. [39]  5. LEED structural analysis   Shi, Cai, et al. 19  Some of the experimental LEED IV curves are shown in Fig. 7 and Fig. 8. Normal incidence is established by optimizing the agreement between curves for different, but symmetry-equivalent, diffraction spots, as shown in Fig. 7. Each curve shown in Fig. 8 is a symmetry-averaged composite, normalized to approximately the same value. Different curves represent different sample treatments. We varied sample treatments in order to check the robustness of the IV data. In all, we reproduced the complete set of IV curves 9 times. We conclude that the IV curves are not sensitive to sample preparation conditions, within the range of sputtering conditions (15-30 minutes with Ar+) and annealing conditions (1-2 hours at 800-850 K) tested. This is similar to our previous findings for 5f i-Al-Pd-Mn, where the experimental IV curves were also found to be very robust. In the LEED analysis of the 5f Al-Cu-Fe surface, we first explored individual bulk terminations, with possible relaxations of the top few interplanar spacings. Later we also considered combinations of terminations. For the calculation, we used relativistic phase shifts of Al, Cu and Fe, which have also been used in the LEED analysis of the (110) surface of crystalline -Al1-x(CuFe)x with a similar composition. [40] Thermal effects were included through the usual Debye-Waller factor. The Pendry R- factor was used for comparison between theory and experiment. We thus started by analyzing a large set of individual terminations of the 5f AlCu-Fe surfaces, namely those that exist within a rectangular box with surface dimensions of 100  x 100  and a depth of 50 . Those terminations run through all the atomic planes along the surface normal in the entire depth of the box. For each termination, the top 12 atomic planes are chosen as a composite layer for LEED calculation. At first, in a   Shi, Cai, et al. 20  rough search, we only allowed the relaxation of the topmost interplanar spacing using a grid search, i.e. the R-factor was calculated on a grid of points in the range from -0.3 to 0.3 . The resulting R-factors for all the terminations that we examined are plotted in Fig. 9. It is seen that the R-factor varies from about 0.5 to 0.9 for different terminations. The R-factor drops if more planes are allowed to relax, but it is found that the change in R-factors is much smaller than the fluctuations between R-factors for different terminations. So the R-factors shown in Fig. 9 are reliable as a basis for further analysis. It is also interesting to find that the different terminations yield very similar optimized structural parameters. Next, we focused on the most promising surface terminations as given by the Rfactors shown in Fig. 9. We interpret the ""better"" terminations of Fig. 9 to be all present on the real surface, forming an array of terraces separated by steps of variable height. The favored step heights can be understood in terms of groups of closely-spaced planes. Even though there is no periodic repetition in the quasicrystalline bulk structure, certain groups of planes occur frequently (even if not identically). We recognize three main sets of grouped planes, each group containing 3, 5 or 9 atomic planes (they can be identified in Fig. 9, while the 5- and 9-plane cases are also illustrated in Figure 10; note that there sometimes exist additional planes of very low atomic density between these planes, resulting in 11-plane groups, for instance). Within each group, the distance between successive atomic planes is smaller than 0.78 . The different groups are mutually separated by at least a distance of 1.5-1.6 . The two large groups with 5 and 9 planes share some common features. Each group has an approximate central symmetry   Shi, Cai, et al. 21  around the middle plane. Both outer surfaces of such a group have two planes separated by only 0.48 ; these planes have the highest combined atomic density. It is found that the surface tends to form between these grouped planes, i.e. by splitting through the relatively large spacings of 1.5-1.6 , and thus exposing the closely-spaced pairs of planes. Since the outermost plane within these pairs has a high aluminum concentration and the other an Al:Cu ratio of about 50:50, these pairs are enriched in aluminum compared to the bulk average. Their combined surface atomic density is close to that of the densely-packed Al(111) crystalline surface. The step heights between terraces with such terminations are obtained by measuring the distances between two kinds of terminations, which are 3.99  for 5-plane steps (Fig. 10 shows one such step) and 6.47  for 9-plane steps; 3-plane steps give 2.40  heights. (Spacing relaxations, if identical for the terminations involved, do not change these distances). The next stage in our LEED analysis mixes different terminations. In a full LEED analysis of quasicrystals, one can explicitly take into account the coexistence of terraces with different types of terminations. Since they are all assumed to contribute to the LEED IV curves, one needs to mix their contributions. Assuming that the area of each terrace is large compared to the coherence width of the electron beam (which is justified by the STM observations in Section 3), then only the reflected intensities from different terraces need to be mixed, as opposed to reflected amplitudes. As shown both in the LEED analysis and in the STM data (Sec. 3), the frequencies of the three kinds of terminations are different. To reduce the number of fit parameters, we take into account that the terminations in 5-plane groups appear to have the better R-factors, and mix only this kind of termination. Since they have essentially the same structure perpendicular to the   Shi, Cai, et al. 22  surface, the number of fit-parameters can be reduced to a few, representing the number of interplanar spacings that one wishes to fit. We thus mixed the 6 terminations that yielded the best R-factors (indicated by the short arrows in Fig. 9) and gave them equal weights. We also allowed relaxation of the top 5 interplanar spacings. This was done by the linear LEED approximation.[41] In this very efficient approach, the wave fields are calculated for one reference and several trial structures. Then the Powell optimization scheme is applied to optimize the structure. The linear LEED approximation is valid for this material if the deviations between reference and trial structures are smaller than about 0.2  (which was always the case in our analysis). As a result, the R-factor could be reduced to 0.39, which we take as our optimum fit. The comparison between the experimental and theoretical IV curves for this best-fit structure is shown in Fig. 11, and the structure itself in Fig. 12 (again, this represents an average best-fit structure for the 5-plane terminations). The topmost interplanar spacing contracts from the bulk value by 0.100.08 , going from 0.48  to 0.38, while the second interplanar spacing expands, the third interplanar spacing contracts and the deeper spacings are close to bulk values, cf. Fig. 12. Since the third plane has a low atomic density, the error bar for the second and third interplanar spacings is large (comparable to the changes from the bulk values). We assumed the bulk composition for each layer, since the earlier work with AlPd-Mn showed little sensitivity of LEED to these compositions.[21] Thus, on average, the bulk composition of the outermost atomic plane is 90% Al and 10% Fe atoms, while the second atomic plane contains 45% Al, 45% Cu and 10% Fe atoms. The average   Shi, Cai, et al. 23  lateral density of the two top planes taken together is 0.14 atoms/2, a value very close to that of a single plane of Al(111), 0.141 atoms/2. This indicates a very densely packed surface double-layer. We notice that the optimized structure for a single termination is exactly the same as that for mixed terminations. This again indicates that the selected terminations are very similar in structure. From Fig. 9 it is to be noted that some 9- and 3-plane terminations give R-factors that are not much worse than those for 5-plane terminations. While the LEED analysis cannot be used to state that these terminations are also present on the surface or with what relative abundance, at least the LEED analysis is consistent with the presence of some 9- and 3-plane terminations on the surface: the LEED results are therefore also compatible with the STM data, which indicate the presence of several step heights. Finally, we note that other authors have suggested that three step heights may be a normal situation on 5fold surfaces of icosahedral quasicrystals. Fradkin [42, 43] has shown that three step heights are to be expected on supercooled quasicrystalline samples along the fivefold axis. The relative population of these three step heights depends upon the extent of supercooling, i.e. the extent of deviation from equilibrium. At the exact points where one population disappears, the ratio of the other two populations should be  = 1.61.... In Fig. 4, the 2.4  population is very small, and in the context of the Fradkin model, one might think that it is on the verge of disappearing. The ratio of the three populations is about 9%:37%:54%, i.e. the two largest populations are in the ratio of about 1.5. While this value is not exactly , it is close enough to be qualitatively in accord with the hypothesis of Fradkin.[42, 43]   Shi, Cai, et al. 24  Another prediction has been made recently by Gratias, et al., [44] who, on the basis of the specific structural model for i-Al-Cu-Fe, forecast that steps should adopt three values of heights with predictable frequencies. These heights (frequencies) are, relative to the longest step, L: L (31%); L/ (51%); and L/2 (19%), i.e. the two largest populations should be in the ratio of 1.65. Again, we see that our experimental value of 1.5 is only slightly lower than expected. Also, our experimental values of the step heights (2.5, 4.0, and 6.2-6.6 ) follow the predicted ratio of L, L/, and L/2 exactly.  6. Conclusions We have analyzed the atomic-scale structure of the 5f surface of an Al-Cu-Fe quasicrystal by STM imaging and a special dynamical LEED analysis. Basically, the surface is bulk-terminated with presumably a bulk-like layer-dependent composition. Our analysis finds that the surface tends to form between different groups of closely-spaced planes, i.e. by splitting the quasicrystal through the larger interplanar spacings that separate those groups. In particular, we find that those grouped planes can be sorted into three sets, with 3, 5 and 9 planes, respectively. The interplanar spacings are the same for the same set of grouped planes. The 5-plane and 9-plane groups are similar in that the top pair of planes have the highest combined atomic density, about 0.14 atoms/2--very close to that of the Al(111) surface--and are rich in Al. The sets of 3, 5, and 9 planes correspond to step heights of 2.47, 3.99, and 6.47 , respectively. The step height frequencies seen in STM hence favor the 9-plane steps, then the 5- plane steps and finally the 3-plane steps (Fig. 4). LEED does not measure step   Shi, Cai, et al. 25  heights directly: the R-factors favor 5-plane terminations, then 9- and 3-plane terminations. The LEED R-factors contain information both on the quality of fit of individual coexisting terminations, and on the frequency of occurrence of the different terminations; however, it is not clear how one could extract these two aspects separately from the data. Nonetheless, the fact that LEED and STM favor the same sets of step heights indicates that the two techniques are in overall agreement on the types of terminations present at the surface. The measured distribution is also roughly consistent with the predictions of Fradkin,[42, 43] and of Gratias, et al.[44] The sequence of step heights in STM (Fig. 2) suggests that there are defects in layer stacking, relative to a perfect bulk quasicrystalline termination. LEED shows that groups of planes are separated by distances of 1.4-1.6  with no atoms in between, as illustrated in Fig. 10. We speculate that ""mistakes"" in the quasicrystalline lattice, normal to the surface, occur between these groups of planes. In other words, we hypothesize that quasicrystalline order is maintained well within favored groups of planes, where atoms interconnect densely throughout the structure, but less well between groups of planes. Perhaps these mistakes occur as the surface regrows after ion bombardment. Or perhaps they are engendered by defects in the bulk structure incorporated during sample preparation (Fig. 5). In either case, this hypothesis provides some understanding for the observation that there is stronger evidence for defects perpendicular to the surface than for defects within, or very near to, the surface plane. Finally, it is interesting to compare the present results for 5f i-Al-Cu-Fe, and those reported previously for 5f i-Al-Pd-Mn. The main difference is that the i-Al-Cu-Fe surface contains screw dislocations and pentagonal pits, neither of which has been   Shi, Cai, et al. 26  observed on i-Al-Pd-Mn, in our laboratory and under analogous conditions. Our general impression is that the i-Al-Cu-Fe surface contains more defects, perhaps a remnant of the hot-pressing step which is a part of sample preparation, as discussed in Section 3. On the other hand, there are several similarities between the two types of surfaces. Both have been analyzed successfully by dynamical LEED theory. Similar R-factors are obtained for both materials. The preferred terminations, atomic densities, and chemical compositions are similar. As a result, the surface in both cases is aluminum-rich. The implied step heights are also very comparable. And the outermost interplanar spacing is contracted on both materials by 0.1 , leaving deeper spacings bulk-like within the error bars. Further discussion of geometric aspects of these surface structures can be found in ref. [21], given that the two materials are structurally so similar.  Acknowledgments This work was supported by the Director, Office of Science, Office of Basic Energy Sciences, Materials Sciences Division of the U.S. Department of Energy under Contract Nos. DE-AC03-76SF00098 and W-405-Eng-82.   Shi, Cai, et al. 27  References. 1. P. A. Thiel, J. M. Dubois, Materials Today 2 (1999) 3.  2.  Z. M. Stadnik, Physical Properties of Quasicrystals, M. Cardona, P.  Fulde, K. v. Klitzing, R. Merlin, H.-J. Queisser, H. Strmer (Series Ed.) Springer Series in Solid-State Sciences, Vol. 126, Springer-Verlag, Berlin, 1999.  3.  R. S. Becker, A. R. Kortan, F. A. Thiel, H. S. Chen, J. Vac. Sci. Technol.  B 9 (1991) 867.  4.  T. M. Schaub, D. E. Brgler, H.-J. Gntherodt, J. B. Suck, Phys. Rev.  Lett. 73 (1994) 1255.  5.  T. M. Schaub, D. E. Brgler, H.-J. Gntherodt, J. B. Suck, M. Audier,  Appl. Phys. A 61 (1995) 491.  6. (1994) 93.  T. M. Schaub, D. E. Brgler, H.-J. Gntherodt, J.-B. Suck, Z. Phys. B 96   Shi, Cai, et al. 28  7. (1999) 14688.  Z. Shen, C. Stoldt, C. Jenks, T. Lograsso, P. A. Thiel, Phys. Rev. B 60  8. J. Ledieu, R. McGrath, R.D. Diehl, T.A. Lograsso, D.W. Delaney, Z. Papadopolos, G. Kasner, Surface Sci. Lett. (2001) in press.  9.  T. M. Schaub, D. E. Brgler, H.-J. Gntherodt, J. B. Suck, M. Audier,  ""Fivefold symmetric LEED patterns measured on icosahdral Al68Pd23Mn9,"" in: C. Janot, R. Mosseri (Ed.) Proceedings of the 5th International Conference on Quasicrystals (ICQ5), Vol. World Scientific, Singapore, 1995, p. 132.  10.  X. Wu, S. W. Kycia, C. G. Olson, P. J. Benning, A. I. Goldman, D. W.  Lynch, Phys. Rev. Lett. 75 (1995) 4540.  11.  S.-L. Chang, W. B. Chin, C.-M. Zhang, C. J. Jenks, P. A. Thiel, Surf. Sci.  337 (1995) 135.  12.  M. Gierer, M. A. Van Hove, A. I. Goldman, Z. Shen, S.-L. Chang, C. J.  Jenks, C.-M. Zhang, P. A. Thiel, Phys. Rev. Lett. 78 (1997) 467.   Shi, Cai, et al. 29  13.  Z. Shen, P. J. Pinhero, T. A. Lograsso, D. W. Delaney, C. J. Jenks, P. A.  Thiel, Surface Sci. 385 (1997) L923.  14.  J. Ledieu, A. Munz, T. Parker, R. McGrath, R. D. Diehl, D. W. Delaney,  T. A. Lograsso, Surface Science 433-435 (1999) 666.  15.  G. Cappello, A. Dechelette, F. Schmithsen, J. Chevrier, F. Comin, A.  Stierle, V. Formoso, M. de Boissieu, T. Lograsso, C. Jenks, D. Delaney, ""Characterization and properties of the AlPdMn 5 surface,"" in: J. M. Dubois, P. A. Thiel, A. P. Tsai, K. Urban (Ed.) MRS Proceedings: Quasicrystals, Materials Research Society Symposium Proceedings Vol. 553, MRS, Boston, 1999, p. 243.  16.  Z. Shen, W. Raberg, M. Heinzig, C. J. Jenks, V. Fourne, M. A. V. Hove,  T. A. Lograsso, D. Delaney, T. Cai, P. C. Canfield, I. R. Fisher, A. I. Goldman, M. J. Kramer, P. A. Thiel, Surface Science 450 (2000) 1.  17.  M. Erbudak, H.-U. Nissen, E. Wetli, M. Hochstrasser, S. Ritsch, Phys.  Rev. Lett. 72 (1994) 3037.  18.  D. Naumovic, P. Aebi, L. Schlapbach, C. Beeli, ""A real-space and  chemically-sensitive study of the i-AlPdMn pentagonal surface,"" in: A. I. Goldman, D. J.   Shi, Cai, et al. 30  Sordelet, P. A. Thiel, J. M. Dubois (Ed.) New Horizons in Quasicrystals: Research and Applications, Vol. World Scientific, Singapore, 1997, p. 86.  19.  D. Naumovic, P. Aebi, L. Schlapbach, C. Beeli, T. A. Lograsso, D. W.  Delaney, ""Study of the 5-fold and 2-fold i-AlPdMn surfaces by full-hemispherical x-ray photoelectron diffraction,"" in: S. Takeuchi, T. Fujiwara (Ed.) Proceedings of the 6th International Conference on Quasicrystals (ICQ6), Vol. World Scientific, Singapore, 1998, p. 749.  20.  R. Bastasz, C. J. Jenks, T. A. Lograsso, A. R. Ross, P. A. Thiel, J. A.  Whaley, ""Low-Energy Ion Scattering Measurements from an Al-Pd-Mn Quasicrystal,"" in: E. Belin-Ferr, P. A. Thiel, K. Urban, A.-P. Tsai (Ed.) MRS Conference Proceedings: Quasicrystals, Vol. MRS, Warrendale, NJ, 2001,  21.  M. Gierer, M. A. Van Hove, A. I. Goldman, Z. Shen, S.-L. Chang, P. J.  Pinhero, C. J. Jenks, J. W. Anderegg, C.-M. Zhang, P. A. Thiel, Phys. Rev. B 57 (1998) 7628.  22.  D. K. Saldin, J. B. Pendry, M. A. Van Hove, G. A. Somorjai, Phys. Rev.  B31 (1985) 1216.   Shi, Cai, et al. 31  23.  C. J. Jenks, P. J. Pinhero, Z. Shen, T. A. Lograsso, D. W. Delaney, T. E.  Bloomer, S.-L. Chang, C.-M. Zhang, J. W. Anderegg, A. H. M. Z. Islam, A. I. Goldman, P. A. Thiel, ""Preparation of icosahedral AlPdMn and AlCuFe samples for LEED studies,"" in: S. Takeuchi, T. Fujiwara (Ed.) Proceedings of the 6th International Conference on Quasicrystals (ICQ6), Vol. World Scientific, Singapore, 1998, p. 741.  24.  C. J. Jenks, D. W. Delaney, T. E. Bloomer, S.-L. Chang, T. A.  Lograsso, Z. Shen, C.-M. Zhang, P. A. Thiel, Appl. Surf. Sci. 103 (1996) 485.  25. 95.  S.-L. Chang, J. W. Anderegg, P. A. Thiel, J. Non-cryst. Solids 195 (1996)  26. 891.  S. Suzuki, Y. Waseda, N. Tamura, K. Urban, Scripta Materialia 35 (1996)  27.  D. Rouxel, M. Gavatz, P. Pigeat, B. Weber, P. Plaindoux, ""Auger electron 25.5  microprobe analysis of surface of Al62Cu  Fe  12.5  quasicrystal. First steps of oxidation.,""  in: A. I. Goldman, D. J. Sordelet, P. A. Thiel, J. M. Dubois (Ed.) New Horizons in Quasicrystals: Research and Applications, Vol. World Scientific, Singapore, 1997, p. 173.   Shi, Cai, et al. 32  28. 302.  D. Naumovic, P. Aebi, C. Beeli, L. Schlapbach, Surf. Sci. 433-435 (1999)  29.  F. Schmithsen, G. Cappello, M. De Boissieu, M. Boudard, F. Comin, J.  Chevrier, Surface sci. 444 (2000) 113.  30.  C. J. Jenks, J. W. Burnett, D. W. Delaney, T. A. Lograsso, P. A. Thiel,  Applied Surface Science 157 (2000) 23.  31.  J. Ledieu, A. W. Munz, T. M. Parker, R. McGrath, R. D. Diehl, D. W.  Delaney, T. A. Lograsso, ""Clustered, terraced, and mixed surface phases of the Al70Pd21Mn9 quasicrystal,"" in: J. M. Dubois, P. A. Thiel, A.-P. Tsai, K. Urban (Ed.) MRS Proceedings: Quasicrystals, Materials Research Society Symposium Proceedings Vol. 553, Materials Research Society, Warrendale, Pennsylvania, 1999, p. 237.  32.  G. Cappello, Ph.D. Thesis, Universit Joseph Fourier--Grenoble I (2000).  33.  D. Rouxel, T. Cai, C. J. Jenks, T. Lograsso, A. Ross, P. A. Thiel, Surface  Sci. 461 (2000) L521.   Shi, Cai, et al. 33  34.  C. Janot, Quasicrystals: A Primer, C. J. Humphreys, P. B. Hirsch, N. F.  Mott, R. J. Brook (Series Ed.) Monographs on the Physics and Chemistry of Materials, Vol. 48, Clarendon Press, Oxford, 1992.  35.  A. Katz, D. Gratias, J. Non-Cryst. Solids 153-154 (1993) 187.  36.  Y. Gauthier, Surf. Rev. Lett. 3 (1996) 1663.  37.  F. Jona, K. O. Legg, H. D. Shih, D. W. Jepsen, P. M. Marcus, Phys. Rev.  Lett. 40 (1978) 1466.  38.  P. J. Rous, Surf. Sci. 244 (1991) L137.  39.  M. Gierer, F. Shi, Z. Shen, P. Thiel, C. Jenks, A. Goldman, M. A. Van  Hove, (1998) unpublished results.  40.  F. Shi, Z. Shen, D. W. Delaney, A. I. Goldman, C. J. Jenks, M. J. Kramer,  T. Lograsso, P. A. Thiel, M. A. Van Hove, Surface Science 411 (1998) 86.  41.  A. Wander, J. B. Pendry, M. A. Van Hove, Phys. Rev. B 46 (1992) 9847.   Shi, Cai, et al. 34  42.  M. A. Fradkin, ""Structure selection and generation of phasons at the  growing surface of quasicrystal,"" in: G. Chapuis, W. Paciorek (Ed.) Aperiodic Crystals, Vol. World Scientific, Singapore, 1995,  43.  M. A. Fradkin, JETP Letters 69 (1999) 570.  44.  D. Gratias, F. Puyraimond, M. Quiquandon, A. Katz, Phys. Rev. B 63 (2001)  024202.   Shi, Cai, et al. 35  Figure Captions. Fig. 1. Micrographs of the 5f Al-Cu-Fe surface, after different thermal treatments and at different magnifications. (a) T < 825 K, 30x30 nm2 (b) T  825 K, 50x50 nm2 (c) T > 825 K, 100x100 nm2 (d) T > 825 K, 200x200 nm2 (e) T > 825 K, 100 x 100 nm2 . Black arrow shows the origin of a screw dislocation. (f) T > 825 K, 100x100 nm2 . White arrow points to a screw dislocation; black arrows point to the pentagonal facets. (g) T  825 K, 50x50 nm2. (h) T  825 K, 15x15 nm2. The circle outlines a the flower. Fig. 2. Line profiles across consecutive terraces, from the data of Figs. 1c-d. The labels (i)-(iii) indicate the specific profiles in Figs. 1c-d. Steps are labelled as H, high (6.2-6.6 ) or L, low (4.0 ). Fig. 3. Histogram illustrating the method of step height measurement. The histogram shows the distribution of pixel heights, in the area encompassed by the rectangle in Fig. 1c. The two maxima correspond to the upper and lower terraces, respectively. Their separation is the step height. Fig. 4. Frequency of step heights. Steps originating at screw dislocations are not included in this distribution. The total number of observations is 278. The height 2.5A is the average of 23 data points, and the calculated standard deviation is 0.2. The height 4.0 is the average of 99 data points, with 0.4 standard deviation. The height 6.2 is from 142 data points, with 0.3 standard deviation. Fig. 5. TEM images of i-Al-Cu-Fe samples after different treatments. (a) Along the 2f axis, as grown. (b) Along the 2f axis, after hot isostatic pressing. Arrows point to periodic regions in the lattice; one such region is enclosed in a box to show it clearly. The   Shi, Cai, et al. 36  bands of periodic lattice align along the 5f directions. (c) Along the 5f direction, after pressing and annealing to 800C. The inset shows a selected area diffraction pattern. The lack of streaking between diffraction spots, and the high degree of perfection in the TEM image, indicate the absence or very low density of ""periodic"" defect regions. Contrast variations across the images in (a) and (c) are due to variations in foil thickness; in (b), the contrast variation is due to this and also to the strain fields associated with the periodic regions. Fig. 6. LEED pattern of the surface after annealing at 850 K for 1 hour. The incident beam energy is 70 eV. Fig. 7. LEED IV curves from the 5f surface of Al-Cu-Fe, taken after preparing the surface by sputtering at 15 minutes and annealing at 800 K for 1 hour, then cooling to 120 K for data acquisition. Diffraction spot intensity is plotted as a function of momentum transfer parallel to the surface, k||. Each box encloses four symmetryequivalent curves. At normal incidence, the curves should be identical. The values of k||, based upon x-ray diffraction data for the bulk, are: bottom two boxes-- 1.647 -1; middle two boxes--2.665 -1; top two boxes--4.312 -1. Fig. 8. LEED IV curves from the 5f surface of Al-Cu-Fe, showing the effect of different annealing temperatures on the IV curves. Diffraction spot intensity is plotted as a function of momentum transfer parallel to the surface, k||. Within each box, the lower and upper curves were measured after annealing for one hour to 800 and 850 K, respectively, then cooling to 120 K for data acquisition.   Shi, Cai, et al. 37  Fig. 9. R-factors for different terminations of bulk icosahedral Al-Cu-Fe, depending on the surface height z of the termination. (For each atomic plane shown as a vertical bar, the surface consists of this plane and all planes with higher z values; planes with lower z values are cut away.) The terminations giving the best R-factors are marked by arrows. In the lower part, the atomic planes are shown, at their respective depths z, as bars with thickness proportional to the atomic density in each layer. The colors of the bars suggest the chemical composition of each layer, according to the color codes shown below the figure. Fig. 10. Side view (parallel to the surface, which is at top) showing several atomic planes: the five upper ones form a 5-plane layer, the left part of which is cut away to suggest a step down to a 9-plane layer (the structure at the step edge is completely unknown). The colors identify chemical elements: Al red, Cu blue, and Fe green. The larger spacing between the thicker layers is believed to be the preferred place where a surface forms. Fig. 11. Comparison of the IV curves measured experimentally (solid lines) and generated from the best structural model (dashed lines). The experimental curves were measured after annealing at 800 K for 1 hour. The values of k||, based upon x-ray diffraction data for the bulk, are, for the bottom two boxes-- 1.647 -1; middle two boxes--2.665 -1; top two boxes--4.312 -1. Fig. 12. Most favored termination for the five-fold Al-Cu-Fe surface, from LEED. Individual atomic planes are shown as colored bars (color coded to suggest their   Shi, Cai, et al. 38  approximate chemical composition). The optimized and bulk interplanar spacings are shown (at right), as well as a possible step height (at left)."
GX255-34-5555531	"Title: Divine Ratios: A Study of the Fibonacci Sequence and Golden Ratio Link to Outcomes:  Problem Solving Students will demonstrate their ability to solve problems in mathematics, including problems with open-ended answers, problems which are solved in a cooperative atmosphere, and problems which are solved with the use of technology.  Communication Students will demonstrate their ability to communicate mathematically through oral and written analysis of group test results, representing and using numbers in a variety of equivalent forms.  Reasoning Students will reason mathematically by making conjectures, gathering evidence, and building arguments to support their conclusions. Students will draw on their knowledge of graphs and ratios to analyze their data and draw conclusions about correlations between groups of data. Students will plot ordered pairs to represent the relationship of the length to the width in selected rectangles. Students will select the appropriate metric unit, choose the tool to find measurements, and apply those measurements to interdisciplinary problems. Students will collect, organize, display, and interpret data through bar graphs and two dimensional graphs. They will then write an evaluative paragraph about the results. Students will write and solve simple proportions using the appropriate operation and a calculator. Students will apply number theory concepts, such as the Fibonacci Sequence and the Golden Ratio. Students will generalize a correlation between graphs of two different data sets.   Connections   Geometry  Measurement   Statistics   Arithmetic Operations  Number Relationships  Patterns/ Relationships Brief Overview:  This activity introduces the Fibonacci Sequence and relates the ratio of nth term to the previous term to a linear equation. They will compare the ratio of the length to the width in the selected rectangles to the ratios of n to n+1 (excluding the first three terms) found in the Fibonacci Sequence. Students then will correlate the ratio of the length of their upper body to the length of their lower body and the Golden Ratio.   Grade/Level: Grades 6 - 8 (General Math with extensions to Social Studies and Art) Duration/Length: This activity should take 3 or 4 days. The activities for the second day may take longer than anticipated. Prerequisite Knowledge: Students need to have a basic knowledge of :     ratio and proportions graphing metric measurement fractions, decimals and percents  Objectives:          Describe and list terms of the Fibonacci Sequence Generate subsequent values in a sequence Find length using metric measurement Express ratios as fractions and decimals Convert ratios to fractions, decimals, and percent Compare ratios in a variety of ways Construct two-dimensional graphs Communicate hypotheses and conclusions Justify conjectured correlations and conclusions  Materials/Resources/Printed Materials:         Calculator Pencil & Paper Meter Sticks Graph Paper Chart Paper for Tally Sheet Resource Material on Fibonacci Computer Software for Data Analysis (Optional) Index Cards  Development/Procedures: Day 1:  Introduce the historical figure Fibonacci.  Give the students the first five numbers of the Fibonacci Sequence (1,1,2,3,5,...) and have them hypothesize the next three numbers (8,13,21) in the sequence.  Have the students explain their methods for finding the next numbers.    Share the standard Fibonacci Sequence with the class. (Any number in the Fibonacce Sequence except for the first two is the sum of its two predecessors. Ex.:. 8 + 13 = 21, 13 + 21 = 34)  Have students list the ratio of nth term to previous term, starting with the second number in the sequence, (nth term:n-1 term) up to 233. (1:1) = 1 (2:1) = 2 (3:2) = 1.5 (5:3) = 1.666 (8:5) = 1.6 (13:8) = 1.62   Have the students graph ratios (n-1 term:nth term) up to 89 (if you have graphing calculators or computer software available, extend the graph to 233). SPECIAL NOTE : When you plot on a coordinate plane, the nth term MUST be the Y coordinate. Title: Fibonacci Sequence Plot.  Assign Worksheet #1 for homework (Survey Sheet, prerequisite to following day's work). Day 2:  Record students' survey results from Worksheet #1 on overhead or chart paper.  Total results from each individual rectangle. Note : Figures B and E have Golden Ratios. Note : Squares are a subset of rectangles (Figure F).  Use data to make a bar graph comparing popularity of the various rectangles from Worksheet #1 on overhead or chart paper (Computer Software - optional).  Discuss with students which rectangles received the greatest number of votes. Introduce the historical background on the ""Golden Ratio"" in architecture and art (""Golden Ratio"" is 2:1 +  5 or approx. 1:1.618). Examples: The Parthenon, Great Pyramids of Ghiza, and Last Supper painting by Salvador Dali.  Determine the ratio (shorter side:longer side) of each rectangle on Worksheet #1. Round to three decimal places (calculator is recommended).  Plot the ordered pairs representing the ratios just determined onto the Fibonacci Sequence Plot from Day 1. Analyze for correlations between the individual ratios (rectangle) and Fibonacci Sequence ratios. SPECIAL NOTE : When you plot your ordered pairs, the width MUST be the X-coordinate.  Assign homework to write a paragraph comparing the results of each individual's survey to the plot just completed. Then compare the class results to this plot. Day 3:  Share sampling of previous night's homework. Analyze results through class discussion.  Measure body parts as indicated on Worksheet #2.    Record A2 and B2 from Worksheet #2 on an index card.  Line students up by least to greatest using the A2 value.  Record all A2 and B2 values in order of line-up on a class chart.  Plot the ordered pairs from the chart on the Fibonacci Sequence Plot from Day 1.  Discuss in small group any correlation that they observed. Note: A line parallel to the Fibonacci Sequence Plot may be drawn to assist with analysis.  Write paragraph analyzing the correlation between the body ratios and the ""Golden Ratio"" as illustrated on the Fibonacci Sequence Plot.  Assign homework to complete chart on bottom of Worksheet #2 and complete the paragraph from the previous step. Evaluation: Worksheet #1 Total 15% Complete Survey Expressing Ratios Rectangle Survey Table Total 15% Measurement Activity Chart Total 15% Class Activity Total 15% Individual Activity 5% 5% 5% 10% 5% 15% 15% Criteria (complete/incomplete) (1% per correct response) (complete/incomplete) (complete/incomplete) (complete/incomplete) (complete/incomplete) (complete/incomplete)  Worksheet #2  Fibonacci Sequence Plot #1 Fibonacci Sequence Plot #2  Written Response #1 Total 20% Paragraph should include: 1) Summary of individual and class observations. 2) Observation of any correlations between expected ratio (approx. 1.618) and actual data. 3) Conclusion justified with data. Written Response #2 Total 20% Paragraph should include: 1) Summary of individual and class observations. 2) Observation of any correlations between expected ratio (approx. 1.618) and actual data. 3) Conclusion justified with data.   Extensions/Follow Up: Students will create a Collage of Human Figures and Golden Rectangles using the following procedure. Construct and cut out four different sized golden rectangles from one 9"" X 12"" piece of white construction paper. Trace a duplicate of each rectangle, using a different color of construction paper for each. Cut out the colored rectangles and set aside. On one side of each white rectangle, create a textured pattern using several different colored crayons. Turn each white rectangle over to the blank side and draw a human figure which is as tall as the rectangle's length while keeping in mind the proportions and golden rectangles discovered in the initial activity. Cut out the four human figures. Assemble the human figures and rectangles in an arrangement that is ""golden to their eyes"" and glue individual parts on an 8"" X 11"" black background paper. Mount the finished design on a 9"" X 12"" white background paper. Materials needed: White Construction paper (9"" X 12"") for each student, black construction paper (8"" X 11"") for each student, at least four different colors of construction paper, pencils, rulers, scissors and glue. Students will research a famous building from an ancient culture such as the Parthenon, the Great Pyramids of Ghiza, or the Great Temple of Karnak to find out what role the Golden Ratio plays in its architecture. Students will perform the activity titled ""Scaling Up the Human Body: Adding vs. Multiplying, Distortion vs. Proportion,"" from the book Equals Investigations: Growth Patterns. (ISBN 0-912511-57-5, published by Lawrence Hall of Science in 1994) Authors: Crissy Daniel Col. E. Brooke Lee Montgomery County Edgar W. Jones Sligo Middle School Montgomery County Marcia Rosenblum Col. E. Brooke Lee Montgomery County   Worksheet #1  Student Survey  Name:_______________________  A 1"" x 1.25""  B 1.13"" x 1.83""  2"" x 0.63"" C 0.25"" x 1.38""  D E 1.63"" x 2.63""  F  G  2.75"" x 0.88"" 0.69"" x 2.00"" H  0.75"" x 0.75""  Directions: Survey five different people. Ask the participant to identify the two rectangles that they find most pleasing to the eye. Record the results on a separate sheet of paper. After completing the survey, record the total for each rectangle on the table below. Rectangle Survey Table  Rectangle A __________ Rectangle B __________ Rectangle C __________ Rectangle D __________  Rectangle E __________ Rectangle F __________ Rectangle G __________ Rectangle H __________   Worksheet #2 ARE OUR BODIES GOLDEN ? Data Recording Sheet  Name: ____________________  Measure the following lengths to the nearest centimeter.  Data Set I A = Top of head to middle of throat B = Middle of throat to navel  ____________________________ ____________________________ ____________________________  A2 = A+B = Top of head to navel Data Set II C = Knee cap to floor D = Navel to knee cap  ____________________________ ____________________________ ____________________________  B2 = C+D = Navel to floor Complete the chart using data from above.  Ratio  Fraction  Decimal  Percent  A:B  A:A2  B:A2  C:D  C:B2  D:B2"
GX263-99-6685844	NTC QUARTERLY REPORT - PERIOD ENDING MARCH 31,1993  PAGE 33  phases randomly determined, 3)light slubs, and 4)light 4. Recruit a graduate student to work jointly on data acquisition and controls. and dark slubs. Of the along-end uniformities studied, the normally 5. Utilize equipment developed in year one to study dosing of chemicals and dyes. distributed one interfered with streak discernment most effectively. Even so this interference was effective only when the along-end variability was 5 or more times larger 6. To improve and validate theoretical dyeing and control models through data acquired from designed than the end-to-end variability. Random-phase sinusoidal experiments. variability produced severe chevroning, and failed to cover streaks at all, while simulated slubs produced effects which 7. Design a database management system to facilitate were similar to, albeit slightly less effective than, those technology transfer and user access to data by expressgenerated by normally distributed noise. ing results in terms of standard data structures. As is evident, there is a practically infinite list of pos- a. Explore joint efforts with researchers at the Swiss Fedsible types and magnitudes of along-end noise. We are eral Institute of Technology to use supercomputers in continuing to look for good ideas in this regard, because studying the dyeing process. this type of approach is conceivably commercially viable, SUMMARY: assuming an effective solution can be found. At this time, five faculty and six students are working Real-Time Data Acquisition, Theoretical on the project. The faculty are K. Beck, R. McGregor, W. Modeling, And Adaptive Control Of Batch Jasper, G. Lee, and B. Smith. The students are G. Berkstresser, IV, M. Arora, M. Lefeber, J. Lu, M. Kashif, and J. Dyeing Processes Peterson. Also, Bill Hunter (Adjunct Professor of Textile File: S92ClO Chemistry) of Ciba, and other interested industrial parties are collaborating. During this reporting period, activities PI(s): Brent Smith focused mainly on collection and analysis of data, equipQUARTERLY REPORT ENDING: March 3 1,1993 ment fabrication and upgrading. Within the main project, there are essentially three subprojects: dye process modelREPORT COMPILED BY: Brent Smith ling; real-time data acquisition and analysis; and control OBJECTIVES: systems development. Work progresses well in all areas, and poster and plenary session presentations of all progress Longterm Goals were made at the NTC national meeting in Auburn, AL. 1. To develop novel state-of-the-art methods for real-time Dyeing experiments were continued to validate and sensing of dye concentration upgrade certain aspects of the models. These models are 2. To utilize those methods for the generation of a database for improving and testing theoretical models of the dyeing process To adaptively control the dyeing process through the use of advanced control algorithms and/or neural networks To investigate dye and chemical dosing as means of controlling the dyeing process, minimizing effluent discharge, and reusing dyebaths proving very useful in predicting the ultimate state the dyeing system (thus the ultimate color obtained) from state variables. This information is being integrated into the control protocols to give real-time estimates of final dye shade, thus facilitating real-time multi-channel adaptive control. The non-dimensional equilibrium model for the dyeing of ionic fibers by ionic dyes was extended to include polrionic dyes, and mixtures of dyes. Data obtained from the literature were fitted to the model. Very good least-squares non-linear fits to the data were obtained. Experimental work is also being done on the control of the dyeing of nylon by acid dyes. The dyeing will be controlled by dosing the acid dye into the dyebath. Kinetic models will be developed and will be used to design the control strategies for the dyeing process. Flow injection analysis (FIA) data from pilot plant dyeings done at Ciba in Greensboro using three different calibration models showed that FIA peak measurements are linear in the range of 0 to 2 absorbance units (AU), with excellent agreement between FIA peak and extended backslope calibration sets. There was good agreement between  3.  4.  Year Two Goals 1. To further develop flow injection analysis (FIA) and investigate a planar waveguide as tools for measuring dye concentrations in mixtures. Enhance database development by installing equip ment for sampling multiple dyebaths:. Close the loop in the current lab dyeing system so that the process is computer controlled.  2. 3.   NTC QUARTERLY REPORT - PERIOD ENDING MARCH 31,1993  PAGE 34  concentrations generated from the FIA data and color predictions based on reflectance measurements on the dyed goods. Several dyeings were done, with similar results. This work will be confirmed by running the same formulas on our own in-house laboratory dyeing equipment. A new piston-type pump is being evaluated as a constant flow FIA carrier pump. A problem which has not yet has been solved is to develop a more satisfactory dynamic mixer. As presently configured, the volume of the commercial chamber is 320=, however, a chamber volume of about 100 m is needed, in order obtain a comparable level of precision. Further design modifications are being carried out. FIA curves for 16 dye combinations have been determined and are being analyzed to give a firm direction to use in optimizing system parameters, once the mixer configuration is established. The FIA system was modified so that its utility in analyzing disperse dyes can be determined. Further progress was made for the research of fuzzy logic control of dyeing processes. Based on previous work, some modified control schemes have been developed, such as self-learning scheme for control rules, and self-tuning scheme of fuzzy calibration. All these schemes were tested through computer simulation. Also, the work to combine fuzzy logic and optimization to control complex multi-input-multi-output (MIMO) systems has been started. Considerable progress has been made in the analysis of dye mixtures, and especially in identifying appropriate strategies (eg. matrix mathematics, neural networks) to handle dye/dye and dye/salt interactions. Some of this work will be presented at a regional American Chemical Society Meeting in April.  Design, test, and operate an integrated environment which enables fundamental changes in design, manufacturing and business operations. 1. Establish the architecture of a CIM network to provide a generic design for industry. These architectures will be demonstrated from the Textile Design Laboratory, the Management systems Laboratory and the Model Manufacturing Facility @&IF). Continue establishing CIM network in limited number of processing areas. Define philosophies and strategies for the FIA complex which will be supported by appropriate monitoring, control and analysis tools.. Create the ability to integrate the FTA activities by defining and developing information transmission and retrieval systems for activities within and among textile companies. Develop systematic decision making techniques for evaluating investments for controlling and managing operations in CIM technologies.  2.  3.  4.  SUMMARY: The CIM project teams continue to seek ways to integrate and leverage the work of the various projects into a whole consistent with the overall project objectives. The display board developed for the Auburn meeting to show the integration of the projects is an example. The overall vision of the project is becoming clearer, especially as it relates to system simulation work by other research teams and technology transfer to industry. We are planning a major week-long workshop for late June to which forty CIM and FTA experts worldwide are invited. Their purpose is to reshape the view of CIM technology and its capability to transform the competitive capability of the domestic fiber-textile-apparel complex. 1. INFORMATION INTEGRATION TECHNOLOGY:  CIM In The FTA Industrial Complex File: S92Cll  PRINCIPAL INVESTIGATOR: Perry Grady (PG) amd Sa, Winchester (SW) - Team Leaders; D. R. Buchanan (DB), North Carolina State; J. Cuculo (JC), North Carolina State; Alan Donaldson (AD), North Carolina State; G. L. Hodge (GH) , North Carolina State; W. Jasper (WJ), North Carolina State; T. Little (TL), North Carolina State; G. Mock (GM), North Carolina State; M. Mohamed (MM), North Carolina State; J. P. Rust (JR), North Carolina State; A. Seyam (AS), North Carolina State; G. Smith (GS), North Carolina State; Paul Tucher (PT), North Carolina State QUARTERLY REPORT ENDING: March 3 1,1993 REPORT COMPILED BY: SAM WINCHESTER OBJECTIVE:  Overall Design of CIM System - The development of an overall CIM system prototype continues using a centralized data base for all process areas. Strategies for information flow within and among process areas are being assessed. Interest from industry continues to increase indicating the significance of this work to textile company operations. Computer Integration of Short-Staple Spinning Enterprise - A practical method for the use of neural networks in analysis and control applications was begun with emphasis on the quantity and form of data required as well as the capabilities and training methodologies for neural network architectures. We established an understanding of the theoretical significance of trained neural network weights and biases in model building. Methods are being
GX004-68-9122379	"Profile Hidden Markov Model Analysis     [  Program Manual  |  User's Guide  |  Data Files  |  Databases  ]            Sensitive Database Searching and Identifying  Sequence Domains            Introduction         Profile analysis has long been  a useful tool in finding  and aligning distantly related sequences  and in  identifying known sequence domains in  new sequences.  Basically, a  profile is a description of  the  consensus of a multiple sequence  alignment.  It uses a  position-specific scoring system to capture  information about the degree of  conservation at various positions in  the multiple alignment.  This  makes it a much more  sensitive and specific method for  database searching than pairwise methods,  such as those used by   BLAST  or  FastA , that use  position-independent scoring.     Hidden Markov modeling, a technique  that has been used for  years in speech recognition, is  now being  applied to many types of  problems in molecular sequence analysis.   In particular, this technique  can  produce profiles that are an  improvement over traditionally constructed profiles.     Profile hidden Markov models (HMMs)  have several advantages over standard  profiles.  Profile  HMMs have a formal probabilistic  basis and have a consistant  theory behind gap and insertion  scores,  in contrast to standard profile  methods which use heuristic methods.   HMMs apply a statistical  method to estimate the true  frequency of a residue at  a given position in the  alignment from its  observed frequency while standard profiles  use the observed frequency itself  to assign the score for  that residue.  This means  that a profile HMM derived  from only 10 to 20  aligned sequences can be of  equivalent quality to a standard  profile created from 40 to  50 aligned sequences.  In  general,  producing good profile HMMs requires  less skill and manual intervention  than producing good  standard profiles.     The HMMER (pronounced  hammer ) package  developed by Sean Eddy of  Washington University in St.  Louis, Missouri, is a set  of programs that allow you  to create and manipulate profile  HMMs and  databases of profile HMMs (HmmerBuild,  HmmerConvert), perform sensitive searches of  sequence  and profile HMM databases, (HmmerSearch  and HmmerPfam) and create multiple  sequence  alignments efficiently (HmmerAlign).  In  collaboration with Dr. Eddy, GCG has  incorporated these  programs into the Wisconsin Package.        What is a Profile HMM?   - A Simplified Description         A profile HMM is a  linear state machine consisting of  a series of nodes, each  of which corresponds  roughly to a position (column)  in the alignment from which  it was built.  If  we ignore gaps, the  correspondence is exact -- the  profile HMM has a node  for each column in the  alignment, and each  node can exist in one  state, a match state.   (The word ""match"" here implies  that there is a position  in  the model for every position  in the sequence to be  aligned to the model.)       A profile HMM has several  types of probabilities associated with  it.  One type is  the  transition     probability  -- the probability of  transitioning from one state to  another.  In a simple  ungapped model,  the probability of a  transition from one match state  to the next match  state is 1.0 and the  path through the model is  strictly linear, moving from the  match state of node n  to  the match state of node  n+1.     There are also  emissions probabilities   associated with each match state,  based on the probability of  a  given residue existing at that  position in the alignment.   For example, for a fairly  well-conserved  column in a protein alignment,  the emissions probability for the  most common amino acid may  be  0.81, while for each of  the other 19 amino acids  it may be 0.01.    If you follow  a path through the model  to generate  a sequence consistent with the  model, the probability of any  sequence that is generated depends  on  the transition and emissions probabilities  at each node.     In order to model real  sequences, we also need to  consider the possibility that gaps  might occur when  a model is aligned to  a sequence.  Two types  of gaps may arise.   The first type occurs when  the  sequence contains a region that  is not present in the  model (an insertion in the  sequence).  The second  type occurs when there is  a region in the model  that is not present in  the sequence (a deletion in  the  sequence).  To handle these  cases, each node in the  profile HMM must now have  three states:  the  match state, an insert state,  and a delete state.   The model also needs more  types of transition  probabilities:  match->match, match->insert, match->delete, insert->match, etc.           Aligning a sequence to a  profile HMM is done by  a dynamic programming algorithm that  finds the  most probable path that the  sequence may take through the  model, using the transition and  emissions  probabilities to score each possible  path.     In general, if the sequence  is equivalent to the consensus  of the original alignment, the  path through  the model will pass from  match state to match state  in a linear fashion.   If the sequence contains a  deletion relative to the consensus,  the path passes through one  or more delete states before  transitioning to the next match  state; if the sequence contains  an insertion relative to the  consensus,  the path passes through an  insert state between two match  states.     For example, if a sequence  contains an insert that occurs  between nodes 5 and 6  of the model, the  path transitions from the node  5 match state to an  insert state.  It remains  in the insert state and  ""consumes"" residues in the sequence  until it reaches the residue  in the sequence that corresponds  to  node 6 in the model.   At this point the  path transitions from the insert  state to the node 6  match state.     Similarly, if the sequence contains  a deletion so that it  has no residues corresponding to  nodes 12  through 15 of the model,  the path transitions from the  node 11 match state into  a delete state, then  transitions through additional delete states  until it can transition to  the match state of node  16 of the  model.     Profile HMMs can be aligned  to a sequence either globally  (the whole profile HMM aligns  to the  sequence) or locally (only part  of the profile HMM need  be aligned with the sequence).   The alignment  type is actually part of  the model, so you must  specify whether the model is  to be global or local  at the  time the model is  built ,  not at the time the  model is used.  (See  HmmerBuild documentation for more  details.)        Most Common Uses for Profile  HMMs         Because a profile HMM can  serve as a representation of  a sequence family or sequence  domain, the  most common application is to  compare profile HMMs and sequences.   These types of comparisons  are  more likely to identify distant  homologs than sequence vs. sequence comparisons  used in most  database search programs.     For example, you can use  HmmerPfam to compare your sequence  to a database of profile  HMMs  representing known sequence families and  known sequence domains.  A  match to one of these  profile  HMMs can help you identify  your sequence and determine its  function.  The curated Pfam  (""Protein  families"") database contains a large  number of global profile HMMs  representing known protein  families, while the PfamFrag database  contains local profile HMMs for  these same families.     Similarly, you can create a  profile HMM representing a domain  or sequence family in which  you are  interested, then use this profile  HMM as a query to  search a sequence database with  HmmerSearch to  see if any other sequences  possess this domain.     Another use for profile HMMs  is to create a multiple  alignment of a large number  of sequences more  quickly than by using standard  methods.  HmmerAlign uses a  small seed alignment of representative  sequences to create a profile  HMM which is then used  as a template for aligning  the full set of  sequences.        Overview of the HMMER Programs         There are nine programs in  the GCG adaptation of the  HMMER package.  The following  five are used  to create and manipulate profile  HMMs:     HmmerBuild  -- creates a profile  HMM from a set of  pre-aligned sequences.  The profile  HMM can be  appended to a file containing  other profile HMMs in order  to create an HMM database  file.     HmmerCalibrate  -- calibrates an existing  profile HMM or profile HMM  database so that searches  performed with it will be  more sensitive.     HmmerConvert  -- converts a profile  HMM created by HmmerBuild into  other formats.     HmmerIndex  -- indexes a profile  HMM database so that profile  HMMs can be retrieved from  it easily  with HmmerFetch.     HmmerFetch  -- extracts a profile  HMM from an indexed profile  HMM database into a file.    The remaining four programs are  used for analyzing data:     HmmerSearch  -- searches a sequence  database with a profile HMM  query.     HmmerPfam  -- searches a profile  HMM database with a sequence  query.  The profile HMM  database  file may be one you  created as well as the  Pfam database created by Eddy  and collaborators.     HmmerAlign  -- efficiently creates a  large multiple alignment from a  small seed alignment and a  collection of unaligned sequences.     HmmerEmit  -- randomly generates sequences  that match a given profile  HMM.        Pfam Acknowledgement        Pfam - A database of  protein domain family alignments and  HMMs Copyright (C) 1996-2000 The  Pfam Consortium.        References          1.  Eddy, S.R., et al. (1996).   Hidden Markov Models.   Current  Opinion in Structural Biology ,      6 ; 361-365.     2.  Durbin, R., Eddy, S., Krogh,  A., and Mitchison, G. (1998).    Biological Sequence Analysis.       Probabilistic Models of Proteins and  Nucleic Acids , Cambridge University Press,  Cambridge, UK.     3.  Eddy, S.R. (1998).  Profile hidden  Markov models.   Bioinformatics ,  14 ; 755-763.                   Printed: February 5, 2001  15:21 (1162)         [  Program Manual  |  User's Guide  |  Data Files  |  Databases  ]       Documentation Comments:  doc-comments@gcg.com  Technical Support:  help@gcg.com      Copyright (c) 1982-2001  Genetics Computer Group  Inc.  A subsidiary of Pharmacopeia, Inc.  All rights reserved.    Licenses and Trademarks Wisconsin Package is a trademark of  Genetics Computer Group , Inc.  GCG and the GCG logo are registered trademarks of  Genetics Computer Group , Inc.    All other product names mentioned in this documentation may be trademarks, and if so, are trademarks or registered trademarks of their respective holders and are used in this documentation for identification purposes only.       www.gcg.com"
GX005-26-11849146	Baum Welch algorithm    (algorithm)       Definition:  An algorithm to find  hidden Markov model  parameters A, B, and    with the maximum likelihood of generating the given symbol sequence in the observation vector.       See also   Viterbi algorithm .       Note: Contributed by Arvind <uk_arvind@mail.utexas.edu> May 2002.      Author:  PEB   More information    See this  lesson on Hidden Markov Models  for more explanation.       Go to the  Dictionary of Algorithms and Data Structures  home page.       If you have suggestions, corrections, or comments, please get in touch with  Paul E. Black   (paul.black@nist.gov).      Entry modified Fri Aug  2 10:21:30 2002.  HTML page formatted Thu Apr 17 10:48:18 2003.      This page's URL is  http://www.nist.gov/dads/HTML/baumWelch.html
GX261-11-3967086	"1 Strategies and Tools for Whole Genome Alignments Olivier Couronne1,3, Alexander Poliakov1, Nicolas Bray1, Tigran Ishkhanov1, Dmitriy Ryaboy1, Edward Rubin1, Lior Pachter2,3, Inna Dubchak 1 1  Lawrence Berkeley National Laboratory, Berkeley, CA 94720, USA; 2Department of  Mathematics, University of California at Berkeley, Berkeley, CA 94720, USA.  Abstract The availability of the assembled mouse genome makes possible, for the first time, an alignment and comparison of two large vertebrate genomes. We have investigated different strategies of alignment for the subsequent analysis of conservation of genomes that are effective for different quality assemblies. These strategies were applied to the comparison of the working draft of the human genome with the Mouse Genome Sequencing Consortium assembly, as well as other intermediate mouse assemblies. Our methods are fast and the resulting alignments exhibit a high degree of sensitivity, covering more than 90% of known coding exons in the human genome. We have obtained such coverage while preserving specificity. With a view towards the end user, we have developed a suite of tools and websites for automatically aligning, and subsequently browsing and working with whole genome comparisons. We describe the use of these tools to identify conserved non-coding regions between the human and mouse genomes, some of which have not been identified by other methods.  1. Introduction The expectation behind the sequencing of the mouse genome is to gain a deeper understanding of the human genome through comparative analysis. Comparative genomic studies of selected regions have already resulted in interesting biological discoveries; from many examples we mention here the discovery of new genes (Pennacchio et al. 2001; Dehal et al. 2001) and the identification of conserved noncoding sequences with regulatory functions (Hardison et al. 1997; Oeltjen et al. 1997; Hardison et al. 2000;   2 Loots et al. 2000; Krivan and Wasserman 2001). These comparative genomic studies have been based on sequence alignments and have been successful because the evolutionary distance between mouse and man appears to be small enough so that genes and other functional elements have been conserved both in sequence (Batzoglou et al. 2000; Hardison et al. 1997) and function (Huxley 1997). On the other hand, sufficient time has elapsed so that non-functional sequence has diverged enough to yield a good ""signal to noise"" ratio. Alignments of whole genomes have already been undertaken for complete genomic sequences of various bacterial species (Tatusov et al. 1997; Delcher et al. 1999; Florea et al. 2000) where the problem w as feasible due to the small genomic size of these organisms (up to 4Mb). The recently published Fugu genome (Aparicio et al. 2002) has been aligned to the human genome using BLAST program, but the complexity of the problem was mitigated by the small size of the Fugu genome and its relatively simple repeat structure. A similar local alignment approach has been applied to the mouse genome by the Blastz group (Schwartz et al. 2002). Aligning large vertebrate genomes that are structurally complex poses a variety of problems not encountered on smaller scales. They are rich in repetitive elements (~50% in the human genome, I.H.G.S., 2001, Venter et al. 2001) and contain multiple segmental duplications (the human genome seems likely to contain about 5% segmental duplication, with most of this sequence in large blocks greater than 10 kb, Bailey et al. 2002). The sizes of the sequences is perhaps the biggest hurdle, since many alignment algorithms were designed for comparing single proteins and are extremely inefficient when processing large genomic intervals (Miller, 2001). The complexity of vertebrate genomes also increases the difficulty of identifying true orthologous DNA segments in alignments. Taking into account that there are sometimes near perfect matches between paralogous DNA regions it is necessary to statistically assess the identification of the most likely orthologous DNA segments, while minimizing the rate of misaligned regions. In this paper we describe our strategies and results for the human and mouse genomes. We have integrated both local and global alignment programs , and our study provides   3 the first quantitative analysis of how such strategies perform in tandem. The resulting implementation allows rapid and specific whole genome alignments and comparisons. The ultimate goal of genome alignment is to facilitate biological discovery, and with this in mind we have also integrated in the computational system a variety of browsing and analysis tools. We present visualization tools for browsing the alignments, as well as a web server that allows users to align their own sequences against completed genome assemblies.  2. Algorithms Finding the orthologous regions between two species computationally is a non-trivial task that has never been explored on a whole genome scale for vertebrate genomes. Local alignment tools find a lot of high scoring matching segments, in particular the orthologous segments, but in addition they identify many paralogous relationships, or even false positives alignments resulting from simple sequence repeats and other sequence artifacts (Chen et al., 2001). BLAST was successfully utilized in the study of Gibbs and coauthors (Chen et al., 2001) where high-quality rat WGS reads (covering 7.5% of the rat genome) were compared with the GoldenPath human genome assembly. The authors of the study investigated statistical significance of BLAST search results and parameters, but they did not focus on finding `true' orthologs and were mostly interested in higher sensitivity of alignment and completeness of coverage of coding exons. When compared with the human assembly more than 47.3% of all aligned reads produced between 2 and 12 hits (which correspond to medium represented elements), and 7.6% had more than 12 hits (likely containing repetitive elements). Unlike local alignment, global alignment methods require aligned features to be conserved in both order and orientation, and are therefore appropriate for aligning orthologous regions in the domain where this assumption applies. But whole genome rearrangements, duplications, inversions, and other evolutionary events restrict the resolution at which the order and orientation assumption of global alignment applies. In   4 the case of the human and mouse genomes, it appears that this assumption applies, on average, to regions less than 8 megabases in length (Mural et al. 2002). Our strategy is to use a fast local alignment method to find anchors on the base genome to identify regions of possible homology for a query sequence. The key element is then to be able to post-process these anchors in order to delimit a region of homology where the order and orientation seems conserved. These regions are then globally aligned. In the work presented here we used BLAT (Kent 2002) to find anchors and AVID (version 2.0, Bray et al. 2002) to generate global alignments (see Figure 1 for an overview of the pipeline and how they were combined). BLAT was designed for cDNA/DNA alignment and first used in Intronerator (Kent and Zahler, 2000). BLAT is not optimized for crossspecies alignments (Kent, 2002), but we chose this program because our tests demonstrated that it performed very well as an anchoring tool in our computational scheme. It is also about 500 times faster than other existing alignment tools. Heuristic for selecting candidate regions for global aligning ( post-processing of anchors). For each sequence, BLAT matches are sorted by score, and regions of possible homology are selected around the strongest matches which serve as anchors. All BLAT hits at most L bases apart are grouped together (here L is the length of the region being aligned,). For groups smaller than L/4, the regions were then extended out by min(50k, L/2-G) where G was the length of the group. For groups with G greater than L/4, the regions were extended out by min(50k,L/4). The groups obtained are compared and the ones with less than 30% of the score of the best group are rejected at this stage (see Figure 2). Various parameters for these heuristics were explored in order to obtain a method that would work for different size of sequences. This heuristic may identify multiple regions of possible homology of different size and score in the base genome. These regions are proposed as candidates to the alignment program. The score obtained by the global alignment is used to make the decision about which alignments to report or to reject. Strategies for different types of analyzed sequence . Different sequencing strategies, coupled with the various assembly methods used to build contigs and scaffolds, result in genomic sequence at different stages of completion and of different quality. There is a   5 significant number of BAC-size finished contigs particularly suitable for higher quality comparative analysis (Mardis et al. 2002), while whole genome shotgun generated assemblies result in shorter contigs and scaffolds. We developed different strategies for aligning sequences at different stages of completion by taking into account all available information, such as the scaffold or the map information, when available. Table 1 summarizes the computational schemes we developed for different types of sequences. In the case of finished BACs or individual sequences submitted by the user through the pipeline interface, no other information is available and we use a `contig' scheme where mapping is based solely on the found anchors followed by the global alignment stage with its scoring. When we align an assembly with the scaffold information available, anchors obtained for different contigs in a scaffold are analyzed together to select candidate positions . We map the whole scaffold, but have the flexibility to reorient and reorder each of the contigs in the scaffold at the alignment stage if necessary. The algorithm also allows us to break the scaffold by selecting more than one candidate region, so that some of the contigs can be aligned to a different place. These last two features allow our alignment to be tolerant to scaffold assembly errors. For an advanced assembly scaffold information is very reliable. In MGSCv3 the quality of assembly was high enough that contigs and scaffolds were mapped to the mouse chromosomes (Waterson et al. 2002). For such cases we chopped the mouse chromosomes into large sections before aligning them. The chromosomes were chopped around the contig boundaries in order not to split them. We tested different sizes and found that fragments averaging 250kbp in length give us the best sensitivity. 3. Results. Here we present the results of alignment of the Mouse Genome Sequencing Consortium assembly MGSCv3 with the June 2002 Human Genome freeze (NCBI build 30). Alignments on this freeze as well as the December 2001 freeze are available at http://pipeline.lbl.gov. The human genome sequence was soft-masked, so that repeats were not considered at the anchoring level, although the global alignments generated at later stages do extend into repeats.   6  Sensitivity. For the final alignment we calculated the level of coverage on known coding and non-coding functional features of the human genome (Table 2). The alignments were scored according to the procedure described in the paper on the mouse genome (Waterson et al. 2002). Three different evolutionary models were selected for scoring the alignments. For coding regions, a high stringency and high penalty for indels was chosen. In order to assess performance on less conserved regulatory regions we also applied less stringent filters. The overall coverage was computed, as well as the coverage of the RefSeq exons, upstream (100, 200 and 500bp) and downstream (200bp) regions, and UTR. About 2% of aligned base pairs of the human genome were covered by more than one mouse sequence fragment. Figure 3 shows an example of a chromosome 3 location where several copies of the mouse pseudogene of Laminin B receptor (LAMR1) from different chromosomes was aligned (laminin B receptor has multiple pseudogenes, http://www.ncbi.nlm.nih.gov/LocusLink/). Our alignment showed more than one million regions conserved at higher than 70% conservation over 100bp level. These features cover about 217 million base pairs. Only 61.6% of them are covered by at least one base pair of a BLAT hit. This means that about two fifths of the conserved features are found only at the global alignment stage. This result is critical because it proves that a local aligner such as BLAT set up with parameters for which its sensitivity is not optimal, but its speed is, can nevertheless be used as an anchoring system because the global alignment retrieves a lot of additional conserved regions outside the anchors (Figure 4). The amount of conserved non-coding sequence was extraordinarily high. At least 5.82% of the bases in the genome are conserved at the 70%/100bp threshold but do not overlap annotated RefSeq, mRNA, Genscan predictions or ESTs. Our scheme has the flexibility to align a query sequence to multiple regions in the genome. Among the conserved features (70% over 100bp) 6.6% of the total conserved sequences, came from secondary hits. These conserved regions may arise from genomic rearrangements or duplications.   7 Specificity. Measuring the specificity, ie how much alignments are correct, is considerably more difficult than measuring the sensitivity. To test the specificity of our method, we aligned a ""random"" mouse genome obtained by reversing (not complementing) the mouse sequences (as proposed by Arian Smit, MGSC communication). Figure 5 presents the ratio of the number of nucleotides on each human chromosome covered by alignments of the random mouse sequence and the number of nucleotides covered by the real one for each chromosome. Alignments were filtered out at different thresholds. For most of the chromosomes, this ratio is below 0.0005, meaning that less than 0.05% of the mouse versus human alignments can be accounted for by random sequence alignments even at low thresholds. This number is higher for certain chromosomes, especially short ones, partly because of numerical instability caused by the very small amount of alignment obtained on these chromosomes. Another test to estimate specificity is to measure the total coverage of the human chromosome 20 by alignments of sequences from all mouse chromosomes with the exception of chromosome 2. The human chromosome 20 being entirely syntenic with the mouse chromosome 2, we should expect to have, for example, only a few percentage of non syntenic coverage coming from pseudogenes. We found a coverage of only 5.6% for exons, with the tight filter, and 0.43% for upstream 100, with the medium filter (Table 3). It is interesting to note that most of these are covered more than once. An interesting example is the case of the Apolipoprotein(a) region. The expressed gene is confined to a subset of primates, as most mammal lack apo(a) (only hedgehogs produce an apo(a)-like protein) (Lawn et al. 1997). Figure 6 shows the coverage in this region by the mouse sequence utilizing two methods: Blastz (Schwartz et al. 2002) and the method presented here. Our method is the only one to predict that apoa(a) has no homology in the mouse, as it had been shown experimentally. This example is interesting because it uniquely demonstrates the importance of specificity.  We set up a database of conserved elements obtained by three different groups using different methods of genome alignment (local and global) and the same conservation cutoff (available at http://pipeline.lbl.gov/cgi-bin/cnc). The most interesting result of   8 comparing the three different sets of conserved non-coding sequences is that the sets overlap by no more than 80%. This suggests that a combination of strategies and methods could lead to a better overall whole genome alignment; this is similar to the situation that has been observed in gene finding (Rogic et al. 2000). An analysis of conservation was performed and every conserved region was classified as coding, noncoding, intronic, repetitive element, or UTR based on annotations associated with the human genome assembly. The alignments of the human and mouse sequences have revealed a significant number of conserved coding and non-coding elements. In addition to deciphering the coding component of the genome, the discovery of conserved noncoding sequences (CNCs) for their potential role in gene regulation is of particular interest. The identification of all such regions is complicated by the high level of conservation between as yet un-annotated coding regions (which can be viewed as non-coding false positives) and the variation in underlying mutation rates throughout the genome. As described in the mouse genome paper (Waterson et al., 2002), we believe the annotation of the genome is not missing vast numbers of genes, which suggests that most of the CNC bases identified do not code for proteins. Conserved sequences for the whole genome were calculated by identifying all regions at least 100bp long conserved at greater than 70% identity. In many cases this scheme has allowed for retrieving important regulatory sequences (Loots et al. 2000; Henkel et al. 1992). Alternatively, more sophisticated methods for retrieving ""significant"" conserved non-coding regions can be selected by the user, such as regions identified by scoring with evolutionary model based matrices (Waterson et al., 2002). 4.1 Implementation. 4.1 Database and software. The pipeline was built on a MySQL database platform selected for its compa tibility with major sources of annotation data like Ensembl (Hubbard et al. 2002) and the UCSC Genome Browser (Kent et al. 2002). The tables contain all the input sequences (either format, draft or finished), and all the data generated by the pipeline, repeats regions, anchors, alignments and regions of high conservation (both coding and non coding). The pipeline software consists of a combination of Perl, C and Java programs. It includes a   9 scheduler that gets control data from the database, builds a queue of jobs, and dispatches them to the computation nodes of the cluster for execution, and the main program that processes individual sequences. A Perl library acts as an interface between the database and the above programs. The use of a separate library allows the programs to function independently of the database schema. The library also improves on the standard Perl MySQL database interface package by providing auto-reconnect functionality and improved error handling. One of the main features of the pipeline is its modular design which allows us to be relatively independent of the specific choice of integrated programs; with slight modifications of input and output scripts, other alignment and visualization tools can substitute the ones mentioned above. The code source is available at http://pipeline.lbl.gov/downloads.shtml. 4.2 Performance. The whole alignment of the mouse and the human genomes presented here took 17 hours on a cluster of sixteen 2.2GHz Pentium IV CPUs (20 CPU days). For comparison, the Blastz alignment took an order of magnitude longer time (Waterson et al. 2002, Supp. Mat.). Our generated alignments represent 7.5GB of data, stored in binary format in a MySQL database and are available for download in AXT format at http://pipeline.lbl.gov/downloads.shtml. 4.3 Data presentation. Two schemes of data presentation on the whole genome scale are available to the user  the VISTA Genome Browser and the Text browser, both synchronized with the pipeline database. They can be accessed at the gateway Web site http://pipeline.lbl.gov. VISTA Genome Browser is a Java applet for interactively visualizing results of comparative sequence analysis in a VISTA format on the scale of whole chromosomes. It has a number of options, such as zoom, extraction of a region to be displayed, userdefined parameters for conservation level, and options for selecting sequence elements to study (Figure 7). VISTA Genome Browser is realized as a dynamic web-interface synchronized with the central MySQL.   10 The Text browser is the most direct front end to the central MySQL database. It allows a user to examine detailed information about each mouse sequence aligned to the selected region on the human genome. For each aligned region exact location of alignments on the human and mouse genomes, the sequences, alignments, coordinates of conserved regions, and a lot of other information are easily retrieved. The pipeline annotation of conserved regions is DAS compatible (Distributed Annotation System, Dowell et al. 2001; Fumoto et al. 2002) and can be viewed through the Ensembl browser at http://www.ensembl.org (step-by-step instructions for viewing the data are available at http://pipeline.lbl.gov/das.shtml).  4.3 Web-based server to align and compare user-submitted sequences with a base genome. As described above, we developed alignment methods for sequences of different quality and length against the whole genome assembly. Our computational system is open for user queries through a web interface accessible from http://pipeline.lbl.gov. Comparative analysis can be done against the base human or mouse genomes. This server accepts sequences in either finished or draft format. Contigs in draft sequences are ordered and oriented according to their alignment with the base genome (Figure 7). The server also accepts GENBANK accession number and connects automatically to GENBANK to retrieve the sequence. The user is provided with detailed results of the comparative analysis, including the alignments, VISTA pictures and the ability to interactively navigate the Vista Genome Browser. A typical query sequence of up to a few hundred kilobases is processed in seconds. Based on current usage (5000 requests/month) we have determined that the average query size is 150kb.  5. Discussion. As we have pointed out in this paper, an alignment of the whole human and mouse genomes represents both an algorithmic challenge, and yet holds the promise of significant biological understanding. We expect that the methodology for aligning the   11 human and mouse genomes will change over time, eventually leading to a ""true"" alignment of the genomes which correctly identifies orthologous relationships between genes and nucleotides, and in which parologous genes, and duplications within genomes are correctly handled. It seems to us that a dramatic improvement in the alignment of the human and mouse genomes will be possible with more genomes available. Significant initial results from the alignment of the human and mouse genomes are that coding regions are highly conserved as expected, but an additional large portion of the genome (roughly as much sequence as is coding) is highly conserved with unknown function. This conserved sequence is arguably not coding and cannot all be explained by neutral evolution (Waterston et al, 2002). It is interesting to note that comparisons of three species (Dubchak et al. 2000) show that many human-mouse conserved regions are also present in the dog, suggesting that they may indeed be functional. Unfortunately, current methods for predicting transcription factor binding sites and other regulatory elements are not accurate enough to classify the conserved regions (Fickett and Wasserman 2000). Our studies of alignment efficiency with respect to different contig sizes should be useful for dynamic alignment tools that rapidly align query sequences to genomes, and for devising strategies for combined local and global alignment. It is important to note that we specifically designed our method in such a way that anchor selection is a standalone module, so that different methods can be used without difficulty. For example, it is possible that other whole genome local alignment methods such as PatternHunter (Ma et al, 2002) or Blastz (http://bio.cse.psu.edu/) could also be very effective at selecting anchors. We plan to test different local and global programs, and novel methods for combining them to optimize performance and accuracy of our comparative analysis scheme. Unfortunately, it still remains an open problem to devise accurate criteria for judging the accuracy of an alignment. The sensitivity is not that difficult to measure (one can, for example, check to see how many exons were aligned), but the specificity (a measure of how much incorrect alignment there is), is considerably harder to estimate as we have discussed.   12 The alignments described in this paper are currently being used by a number of projects, including a comparative-based annotation of genes in the human and mouse genome (Pachter et al. 2002) and a study of the rearrangement history of the genomes (P. Pevzner, unpublished). These projects will in turn lead to better alignments, and eventually, in conjunction with more genomes, a more complete understanding of genome evolution. Acknowledgements. We thank the Mouse Genome Sequencing Consortium for the possibility to work with the mouse genome during the sequencing phases and in the subsequent analysis phase. The analysis group, comprising many individuals and teams from around the world, was particularly helpful not only in providing crucial suggestions and advice as the project unfolded, but also in contributing many independent ideas. Special thanks go to Jim Kent who coordinated the alignment efforts of the mouse sequencing consortium analysis group and designed the filtering methods for calculating alignment coverage. Thanks also to the Penn State Group (Laura Elnitsky, Ross Hardison, Webb Miller, Scott Schwartz and others) and the Pattern Hunter Group (Ming Li, Mike Zody and others) who developed different alignment strategies with which we compared throughout. We thank Ivan Ovcharenko for initiating the project and developing the prototype. We also thank Serafim Batzoglou for his help with generating simulated reads and assemblies for our test sets. The project was partially supported by a Program for Genomic Applications (PGA) grant from the National Heart Lung and Blood Institute.  Footnotes. 3  Corresponding authors  E-MAIL ocouronne@lbl.gov; Tel (510)486-6030; Fax (510)486-5717 lpachter@math.berkeley.edu; Tel (510)642-2028; Fax (510)642-8204   13 References: Altschul, S.F., Gish, W., Miller, W., Myers, E.W., and Lipman, D.J. 1990. Basic local alignment search tool. J. Mol. Biol. 215: 403-410. Aparicio, S., Chapman, J., Stupka, E., Putnam, N., Chia , J.M., Dehal, P., Christoffels , A., Rash, S., Hoon , S., Smit , A., et al. 2002. Whole-genome shotgun assembly and analysis of the genome of Fugu rubripes. Science 297: 1301-1310. Bailey, J.A. et al. 2002. Recent Segemental Duplication in the Human Genome. 2002. Science 297: 1003-1007. Batzoglou, S., Jaffe, D.B., Stanley, K., Butler, J., Gnerre, S., Mauceli, E., Berger, B., Mesirov, J.P., and Lander, E.S. 2002. ARACHNE: a whole-genome shotgun assembler. Genome Res. 12: 177-189. Batzoglou, S., Pachter, L., Mesirov, J.P., Berger, B., Lander, E.S. 2000. Human and mouse gene structure: comparative analysis and application to exon prediction. Genome Res. 10: 950-958. Bedell, J.A., Korf, I., and Gish, W. 2000. MaskerAid: a performance enhancement to RepeatMasker. Bioinformatics 16: 1040-1041 Bray, N., Dubchak, I., and Pachter, L. AVID: A global alignment program for large genomic sequences. Submitted. Chen, R., Bouck, J.B., Weinstock, G.M. and Gibbs, R.A. 2001. Comparing Vertebrate Whole-Genome Shotgun Reads to the Human Genome . Genome Res. 11: 1807-1816 Dehal, P., Predki, P., Olsen, A.S., Kobayashi, A., Folta, P., Lucas, S., Land, M., Terry, A., Ecale Zhou, C.L., Rash, S., et al. 2001. Human chromosome 19 and related regions in mouse: conservative and lineage-specific evolution. Science. 293: 104-111 Delcher, A.L., Kasif, S., Fleischmann, R.D., Peterson, J., White, O., and Salzberg SL. 1999. Alignment of whole genomes. Nucleic Acids Res. 27: 2369-2376. Dowell, R.D., Jokerst, R.M., Day, A., Eddy, S.R., and Stein, L. 2001. The Distributed Annotation System. BMC Bioinformatics. 2: 7   14 Dubchak, I., Brudno, M., Pachter, L.S., Loots,G.G., Mayor,C., Rubin, E.M., and Frazer, K.A.. 2000. Active conservation of noncoding sequences revealed by 3-way species comparisons. Genome Research 10: 1304-1306. Fickett, J.W. and Wasserman, W.W. 2000. Discovery and modeling of transcriptional regulatory regions. Curr. Opinion Biotechnol. 11: 19-24 Florea, L., Riemer ,C., Schwartz, S., Zhang, Z., Stajonovic, N., Miller, W., and McClelland, M. 2000. Web-based visualization tools for bacterial genome alignments. Nucelic Acids Res. 28: 3486-3496. Fumoto, M., Miyazaki, S., and Sugawara, H. Genome Information Broker (GIB): data retrieval and comparative analysis system for completed microbial genomes and more. Nucleic Acids Res. 2002 30: 66-68. Hardison, R.C., Oeltjen, J., and Miller, W. 1997. Long human-mouse sequence alignments reveal novel regulatory elements: a reason to sequence the mouse genome. Genome Res. 7:959-66. Hardison RC. 2000. Conserved noncoding sequences are reliable guides to regulatory elements. Trends Genet. 16: 369-372. Hattori, M., Fujiyama, A., Taylor, T.D., Watanabe, H., Yada, T., Park, H.S., Toyoda, A., Ishii K, Totoki Y, Choi DK, et al. 2000. The DNA sequence of human chromosome 21. Nature 405: 311-319. Henkel, G., Weiss, D.L., McCoy, R., Deloughery , T., Tara, D., and Brown, M.A. 1992. A DNase I-hypersensitive site in the second intron of the murine IL-4 gene defines a mast cell-specific enhancer. Immunology 149: 3239-3246. Hubbard, T., Barker, D., Birney, E., Cameron G., Chen, Y., Clark, L., Cox, T., Cuff, J., Curwen, V., Down, T., et al. 2002. The Ensembl genome database project. Nucleic Acids Res. 30: 38-41. Huxley, C. 1997. Mammalian artificial chromosomes and chromosome transgenics. TIG 13: 345-347.   15 International Human Genome Sequencing (I.H.G.S.) Consortium. 2001. Initial sequencing and analysis of the human genome. International Human Genome Sequencing Consortium. Nature 409: 860-921 Kent, J. 2002. BLAT - The BLAST-Like Alignment Tool. Genome Res. 12: 656-664 Kent,W.J, Sugnet, C.W. Furey,T.S. , Roskin,K.M., Pringle,T.H. Alan M. Zahler, A.M., Haussler, D. 2002. The Human Genome Browser at UCSC. Genome Res. 6: 996-1006. Kent W.J., and Zahler, A.M. 2000. The intronerator: exploring introns and alternative splicing in Caenorhabditis elegans. Nucleic Acids Res. 28: 91-93. Krivan, W. and Wasserman, W.W. 2001. A predictive model for regulatory sequences directing liver-specific transcription. Genome Res 11: 1559-1566. Lawn, R.M., Schwartz, K., Patthy, L. Convergent evolution of apolipoprotein(a) in primates and hedgehog. Proc. Natl. Acad. Sci, 94: 11992-11997. Loots, G.G., Locksley, R.M., Blankespoor, C.M., Wang, Z.E., Miller, W., Rubin, E.M., and Frazer, K.A. 2000. Identification of a coordinate regulator of cytokines 4, 13, and 5 by cross-species sequence comparisons. Science 288: 136-140. Ma, B., Tromp, J., and Li, M. 2002. PatternHunter: faster and more sensitive homology search. Bioinformatics 18: 440-445. Mardis, E., McPherson, J., Martienssen, R., Wilson, R.K., and McCombie, W.R. 2002. What is finished, and why does it matter. Genome Res. 12: 669-671. Mayor, C., Brudno, M., Schwartz, J.R., Poliakov, A., Rubin, E.M., Frazer, K.A., Pachter, L., and Dubchak, I. 2000. VISTA: Visualizing global DNA sequence alignments of arbitrary length. Bioinformatics 16: 1046-1047. Miller W. 2001. Comparison of genomic DNA sequences: solved and unsolved problems. Bioinformatics 17: 391-397 Mural, R. et al. A comparison of whole genome derived mouse chromosome 16 and the human genome. 2002. Science 296: 1661-1671   16 Oeltjen, J.C., Malley, T.M., Muzny, D.M., Miller, W., Gibbs, R.A., and Belmont, J.W. 1997. Large-scale comparative sequence analysis of the human and murine Bruton's tyrosine kinase loci reveals conserved regulatory domains. Genome Res. 7: 315-329. Pachter, L. , Alexandersson, M., and Cawley, S. 2002. Applications of Generalized Pair Hidden Markov Models to Alignment and Gene Finding Problems. J. Comp. Biol. 9: 389-399 Pennacchio, L.A., Olivier, M., Hubacek, J.A., Cohen, J.C., Cox, D.R., Fruchart , J.C., Krauss, R.M., and Rubin, E.M. 2001. An apolipoprotein influencing triglycerides in humans and mice revealed by comparative sequencing. Science 294: 169-173. Rogic, S., Ouellette F. and Mackworth A.K. 2002 Improving gene recognition accuracy by combining predictions from two gene-finding program. Bioinformatics 2002 18: 1034-1045. Smith, T.F. and Waterman, M.S. 1981. Identification of common molecular subsequences. J. Mol. Biol. 147: 195-197 Schwartz, S., Kent, W.J., Smit, A., Zhang, Z., Baertsch, R., Hardison, R., Haussler, D., Miller, W. Human-mouse alignment with Blastz (submitted) Tatusov, R.L., Koonin, E.V., and Lipman, D.J. 1997. A genomic perspective on protein families. Science 278: 631-637. Venter, J.C. et al. 2001. The sequence of the human genome. Science 291: 1304-1351. Waterston et al. 2002. Initial sequencing and comparative analysis of the mouse genome. Nature, in press.  Website references.  http://pipeline.lbl.gov, Comparative analysis pipeline gateway at Lawrence Berkeley National laboratory.   17 http://pipeline.lbl.gov/cgi-bin/cnc, Database of conserved sequences from LBNL, PSU, and UCSC. http://pipeline.lbl.gov/tradeoff/, Results of the study on specificity and sensitivity of different anchoring techniques. http://bio.math.berkeley.edu/avid/ Avid website http://repeatmasker.genome.washington.edu/cgi-bin/RepeatMasker, RepeatMasker. http://www.tigr.org/tdb/tgi/software/, TIGR`s standalone low complexity (""dust"") filter http://genome.ucsc.edu/, UCSC web site from which human genome assemblies used in the study where downloaded from   18  Figure 1. General scheme of the pipeline. The pipeline processes individual contigs, supercontigs or long fragments of assemblies. Figure 2. Heuristic for selecting anchors to determinate candidates regions for global alignment. Figure 3. location at chr3:38787874-38793594 on the Human Genome, June 2002 (hg12/ncbi30) where LAMR1 gene is covered by the alignments of sequences from different Mouse chromosomes. Figure 4. The global alignment of the mouse finished sequence NT_002570 against the region found by BLAT anchors revealed conserved coding and non coding elements not found by the BLAT program. The anchoring scheme is sensitive enough to provide the global alignment with the correct homology candidate. The location found for this Mouse finished contig on the Human genome, June 2002 (hg12/ncbi30) is chr20:4297459042993423. Figure 5. The ratio of the number of nucleotides on each human chromosome covered by alignments of the random mouse sequence and the number of nucleotides covered by the real mouse sequence for each chromosome. Threshold definition is described in the alignment section of Waterson et al, 2002. Figure 6. Apolipoprotein(a) region. The expressed gene is confined to a subset of primates, as most mammal lack apo(a) (only hedgehogs produce an apo(a)-like protein) (Lawn et al. 1997). This figure shows the coverage in this region by the mouse sequence utilizing Blastz (Schwartz et al. 2002) and the method presented here. Our method is the only one to predict that apoa(a) has no homology in the mouse, as it had been shown experimentally. Figure 7. results of a on-line submission of a draft unannotated platypus sequence to the genome alignment web server. The gene has been correctly identified. It is interesting to note the general lack of conservation in non-coding regions, except for a few highly conserved islands. The submission was done directly with the GENBANK accession number AC130185 and was completed in less than 30 seconds.   19  Table 1: Alignment strategies for different types of assemblies.  Method Contigs Scaffold  Scheme of alignment Individual contigs contigs can be reoriented and reordered  Examples Finished BACs Arachne October 2001 Phusion November 2001 Celera chromosome 16 MGSC v3  Chopped pieces  mouse chromosomes are chopped in 250 kb and aligned to the Human Genome   20 Table2. percentage of bases pairs covered for known coding and non-coding functional features of the human genome (see text for details).  Overall coverage Feature Coverage Exons UTR Upstream 500 Upstream 200 Upstream 100 Downstream 200  matrix loose threshold=2500 22.15%  matrix medium threshold=2500 7.26%  matrix loose threshold=3400 4.48%  90.93% 72.21% 56.08% 65.94% 70.83% 53.42%  88.19% 34.43% 23.35% 33.01% 38.94% 17.62%  85.76% 23.96% 15.19% 22.61% 27.38% 10.85%   21  Table 3. specificity test: coverage on human chromosome 20 only by all the mouse chromosomes except chromosome 2 (see text for details).  Overall coverage Features Coverage exons UTR upstream 500 upstream 200 upstream 100 downstream 200  matrix loose threshold=2500 0.49%  matrix medium threshold=2500 0.29%  matrix tight threshold=3400 0.22%  5.57% 3.85% 0.10% 0.24% 0.46% 1.59%  5.36% 2.71% 0.09% 0.22% 0.43% 0.91%  5.06% 1.84% 0.08% 0.19% 0.35% 0.23%"
GX090-15-4802477	"Next   Previous   Contents     2. Traditional Artificial Intelligence         Traditional AI is based around the ideas of logic, rule systems, linguistics, and the concept of rationality.  At its roots are programming languages such as Lisp and Prolog. Expert systems are the largest successful example of this paradigm.  An expert system consists of a detailed knowledge base and a complex rule system to utilize it.  Such systems have been used for such things as medical diagnosis support and credit checking systems.      2.1 AI class/code libraries        These are libraries of code or classes for use in programming within the artificial intelligence field.  They are not meant as stand alone applications, but rather as tools for building your own applications.              ACL2     Web site:   www.telent.net/cliki/ACL2     ACL2 (A Computational Logic for Applicative Common Lisp) is a theorem prover for industrial applications. It is both a mathematical logic and a system of tools for constructing proofs in the logic.  ACL2 works with GCL (GNU Common Lisp).          AI Search II     WEB site:   www.bell-labs.com/topic/books/ooai-book/     Submitted by:   Peter M. Bouthoorn   Basically, the library offers the programmer a set of search algorithms that may be used to solve all kind of different problems. The idea is that when developing problem solving software the programmer should be able to concentrate on the representation of the problem to be solved and should not need to bother with the implementation of the search algorithm that will be used to actually conduct the search. This idea has been realized by the implementation of a set of search classes that may be incorporated in other software through  C++ 's features of derivation and inheritance.  The following search algorithms have been implemented:        depth-first tree and graph search.   breadth-first tree and graph search.   uniform-cost tree and graph search.   best-first search.   bidirectional depth-first tree and graph search.   bidirectional breadth-first tree and graph search.   AND/OR depth tree search.   AND/OR breadth tree search.       This library has a corresponding book, ""  Object-Oriented Artificial Instelligence, Using C++ "".          Chess In Lisp (CIL)     FTP site:   chess.onenet.net/pub/chess/uploads/projects/       The CIL (Chess In Lisp) foundation is a Common Lisp implementaion of all the core functions needed for development of chess applications.  The main purpose of the CIL project is to get AI researchers interested in using Lisp to work in the chess domain.            DAI     Web site:   starship.python.net/crew/gandalf/DNET/AI/     A library for the Python programming language that provides an object oriented interface to the CLIPS expert system tool. It  includes an interface to COOL (CLIPS Object Oriented Language) that allows:    Investigate COOL classes   Create and manipulate with COOL instances   Manipulate with COOL message-handler's   Manipulate with Modules           HTK     Web site:   htk.eng.cam.ac.uk     The Hidden Markov Model Toolkit (HTK) is a portable toolkit for building and manipulating hidden Markov models.  HTK consists of a set of library modules and tools available in C source form. The tools provide sophisticated facilities for speech analysis, HMM training, testing and results analysis. The software supports HMMs using both continuous density mixture Gaussians and discrete distributions and can be used to build complex HMM systems.  The HTK release contains extensive documentation and examples.        LK     Web site:   www.cs.utoronto.ca/~neto/research/lk/     LK is an implementation of the Lin-Kernighan heuristic for the Traveling Salesman Problem and for the minimum weight perfect matching problem. It is tuned for 2-d geometric instances, and has been applied to certain instances with up to a million cities. Also included are instance generators and Perl scripts for munging TSPLIB instances.   This implementation introduces ``efficient cluster compensation'', an experimental algorithmic technique intended to make the Lin-Kernighan heuristic more robust in the face of clustered data.          Nyquist     Web site:   www.cs.cmu.edu/afs/cs.cmu.edu/project/music/web/music.html       The Computer Music Project at CMU is developing computer music and interactive performance technology to enhance human musical experience and creativity. This interdisciplinary effort draws on Music Theory, Cognitive Science, Artificial Intelligence and Machine Learning, Human Computer Interaction, Real-Time Systems, Computer Graphics and Animation, Multimedia, Programming Languages, and Signal Processing. A paradigmatic example of these interdisciplinary efforts is the creation of interactive performances that couple human musical improvisation with intelligent computer agents in real-time.          PDKB     Web site:   lynx.eaze.net/~pdkb/web/   SourceForge site:   sourceforge.net/project/pdkb     Public Domain Knowledge Bank (PDKB) is an Artificial Intelligence Knowledge Bank of common sense rules and facts. It is based on the Cyc Upper Ontology and the MELD language.          Python Fuzzy Logic Module     FTP site:   ftp://ftp.csh.rit.edu/pub/members/retrev/     A simple python module for fuzzy logic. The file is 'fuz.tar.gz' in this directory. The author plans to also write a simple genetic algorithm and a neural net library as well. Check the 00_index file in this directory for release info.        QUANT1     Web site:   linux.irk.ru/projects/QUANT/     QUANT/1 stands for type QUANTifier. It aims to be an alternative to Prolog-like (Resulutional-like) systems. Main features include a lack of necessity for eliminating Quantifiers, scolemisation, ease of comprehension, large scale formulae operation, acceptance of nonHorn formulaes, and Iterative deeping. The actual library implemented in this project is called ATPPCF (Automatic Theorem Prover in calculus of Positively Constructed Formulae).  ATPPCF will be a library (inference engine) and an extension of the Predicate Calculus Language as a new logical language. The library will be incorporable in another software such as TCL, Python, Perl. The engine's primary inference method will be the ""search of inference in language of Positively Constructed Formulas (PCFs)"" (a subset of Predicate Calculus well translated in both directions). The language will be used as scripting language to the engine. But there will be possibility to replace it with extensions languages of main software.          Screamer     Web site:   www.cis.upenn.edu/~screamer-tools/home.html     Screamer is an extension of Common Lisp that adds support for nondeterministic programming. Screamer consists of two levels. The basic nondeterministic level adds support for backtracking and undoable side effects.  On top of this nondeterministic substrate, Screamer provides a comprehensive constraint programming language in which one can formulate and solve mixed systems of numeric and symbolic constraints. Together, these two levels augment Common Lisp with practically all of the functionality of both Prolog and constraint logic programming languages such as CHiP and CLP(R). Furthermore, Screamer is fully integrated with Common Lisp. Screamer programs can coexist and interoperate with other extensions to Common Lisp such as CLOS, CLIM and Iterate.          ThoughtTreasure     Web site:   www.signiform.com/tt/htm/tt.htm     ThoughtTreasure is a project to create a database of commonsense rules for use in any application. It consists of a database of a little over 100K rules and a C API to integrate it with your applications. Python, Perl, Java and TCL wrappers are already available.                    2.2 AI software kits, applications, etc.            These are various applications, software kits, etc. meant for research in the field of artificial intelligence. Their ease of use will vary, as they were designed to meet some particular research interest more than as an easy to use commercial package.          ASA - Adaptive Simulated Annealing     Web site:   www.ingber.com/#ASA-CODE   FTP site:   ftp.ingber.com/       ASA (Adaptive Simulated Annealing) is a powerful global optimization C-code algorithm especially useful for nonlinear and/or stochastic systems.    ASA is developed to statistically find the best global fit of a nonlinear non-convex cost-function over a D-dimensional space. This algorithm permits an annealing schedule for 'temperature' T decreasing exponentially in annealing-time k, T = T_0 exp(-c k^1/D). The introduction of re-annealing also permits adaptation to changing sensitivities in the multi-dimensional parameter-space. This annealing schedule is faster than fast Cauchy annealing, where T = T_0/k, and much faster than Boltzmann annealing, where T = T_0/ln k.            Babylon     FTP site:   ftp.gmd.de/gmd/ai-research/Software/Babylon/     BABYLON is a modular, configurable, hybrid environment for developing expert systems. Its features include objects, rules with forward and backward chaining, logic (Prolog) and constraints. BABYLON is implemented and embedded in Common Lisp.        cfengine     Web site:   www.iu.hioslo.no/cfengine/     Cfengine, or the configuration engine is a very high level language for building expert systems which administrate and configure large computer networks. Cfengine uses the idea of classes and a primitive form of intelligence to define and automate the configuration of large systems in the most economical way possible. Cfengine is design to be a part of computer immune systems.        CLEARS     Web site:   www.coli.uni-sb.de/~clears/     The CLEARS system is an interactive graphical environment for computational semantics. The tool allows exploration and comparison of different semantic formalisms, and their interaction with syntax. This enables the user to get an idea of the range of possibilities of semantic construction, and also where there is real convergence between theories.        CLIG     Web site:   www.ags.uni-sb.de/~konrad/clig.html     CLIG is an interactive, extendible grapher for visualizing linguistic data structures like trees, feature structures, Discourse Representation Structures (DRS), logical formulas etc. All of these can be freely mixed and embedded into each other. The grapher has been designed both to be stand-alone and to be used as an add-on for linguistic applications which display their output in a graphical manner.        CLIPS     Web site:   www.jsc.nasa.gov/~clips/CLIPS.html   FTP site:   cs.cmu.edu/afs/cs.cmu.edu/project/ai-repository/ai/areas/expert/systems/clips     CLIPS is a productive development and delivery expert system tool which provides a complete environment for the construction of rule and/or object based expert systems.      CLIPS provides a cohesive tool for handling a wide variety of knowledge with support for three different programming paradigms: rule-based, object-oriented and procedural.  Rule-based programming allows knowledge to be represented as heuristics, or ""rules of thumb,"" which specify a set of actions to be performed for a given situation. Object-oriented programming allows complex systems to be modeled as modular components (which can be easily reused to model other systems or to create new components).  The procedural programming capabilities provided by CLIPS are similar to capabilities found in languages such as C, Pascal, Ada, and LISP.        EMA-XPS - A Hybrid Graphic Expert System Shell     Web site:   wmwap1.math.uni-wuppertal.de:80/EMA-XPS/       EMA-XPS is a hybrid graphic expert system shell based on the ASCII-oriented shell Babylon 2.3 of the German National Research Center for Computer Sciences (GMD). In addition to Babylon's AI-power (object oriented data representation, forward and backward chained rules - collectible into sets, horn clauses, and constraint networks) a graphic interface based on the X11 Window System and the OSF/Motif Widget Library has been provided.        FOOL & FOX     FTP site:   ntia.its.bldrdoc.gov/pub/fuzzy/prog/       FOOL stands for the Fuzzy Organizer OLdenburg. It is a result from a project at the University of Oldenburg. FOOL is a graphical user interface to develop fuzzy rulebases.  FOOL will help you to invent and maintain a database that specifies the behavior of a fuzzy-controller or something like that.    FOX is a small but powerful fuzzy engine which reads this database, reads some input values and calculates the new control value.        FUF and SURGE     Web site:   www.dfki.de/lt/registry/generation/fuf.html   FTP site:   ftp.cs.columbia.edu/pub/fuf/     FUF is an extended implementation of the formalism of functional unification grammars (FUGs) introduced by Martin Kay specialized to the task of natural language generation. It adds the following features to the base formalism:    Types and inheritance.    Extended control facilities (goal freezing, intelligent backtracking).    Modular syntax.     These extensions allow the development of large grammars which can be processed efficiently and can be maintained and understood more easily.  SURGE is a large syntactic realization grammar of English written in FUF. SURGE is developed to serve as a black box syntactic generation component in a larger generation system that encapsulates a rich knowledge of English syntax. SURGE can also be used as a platform for exploration of grammar writing with a generation perspective.        The Grammar Workbench     Web site:   www.cs.kun.nl/agfl/GWB.html             The Grammar Workbench, or GWB for short, is an environment for the comfortable development of Affix Grammars in the AGFL-formalism. Its purposes are:     to allow the user to input, inspect and modify a grammar;    to perform consistency checks on the grammar;    to compute grammar properties;    to generate example sentences;    to assist in performing grammar transformations.            GSM Suite     Web site:   www.slip.net/~andrewm/gsm/     The GSM Suite is a set of programs for using Finite State Machines in a graphical fashion. The suite consists of programs that edit, compile, and print state machines. Included in the suite is an editor program, gsmedit, a compiler, gsm2cc, that produces a C++ implementation of a state machine, a PostScript generator, gsm2ps, and two other minor programs. GSM is licensed under the GNU Public License and so is free for your use under the terms of that license.        Illuminator     Web site:   documents.cfar.umd.edu/resources/source/illuminator.html     Illuminator is a toolset for developing OCR and Image Understanding applications.  Illuminator has two major parts: a library for representing, storing and retrieving OCR information, heretofore called dafslib, and an X-Windows ""DAFS"" file viewer, called illum. Illuminator and DAFS lib were designed to supplant existing OCR formats and become a standard in the industry. They particularly are extensible to handle more than just English.  The features of this release:     5 magnification levels for images   flagged characters and words   unicode support -- American, British, French, German,  Greek, Italian, MICR, Norwegian, Russian, Spanish, Swedish,  keyboards    reads DAFS, TIFF's, PDA's (image only)   save to DAFS, ASCII/UTF or Unicode   Entity Viewer - shows properties, character choices,  bounding boxes image fragment for a selected entity, change  type, change content, hierarchy mode           Isabelle     Web site:   isabelle.in.tum.de     Isabelle is a popular generic theorem prover developed at Cambridge University and TU Munich. Existing logics like Isabelle/HOL provide a theorem proving environment ready to use for sizable applications. Isabelle may also serve as framework for rapid prototyping of deductive systems. It comes with a large library including Isabelle/HOL (classical higher-order logic), Isabelle/HOLCF (Scott's Logic for Computable Functions with HOL), Isabelle/FOL (classical and intuitionistic first-order logic), and Isabelle/ZF (Zermelo-Fraenkel set theory on top of FOL).        Jess, the Java Expert System Shell     Web site:   herzberg.ca.sandia.gov/jess/     Jess is a clone of the popular CLIPS expert system shell written entirely in Java. With Jess, you can conveniently give your applets the ability to 'reason'. Jess is compatible with all versions of Java starting with version 1.0.2. Jess implements the following constructs from CLIPS: defrules, deffunctions, defglobals, deffacts, and deftemplates.          learn     FTP site:   sunsite.unc.edu/pub/Linux/apps/cai/     Learn is a vocable learning program with memory model.         LISA     Web site:   lisa.sourceforge.net     LISA (Lisp-based Intelligent Software Agents) is a production-rule system heavily influenced by JESS (Java Expert System Shell). It has at its core a reasoning engine based on the Rete pattern matching algorithm. LISA also provides the ability to reason over ordinary CLOS objects.        NICOLE     Web site:   nicole.sourceforge.net     NICOLE (Nearly Intelligent Computer Operated Language Examiner) is a theory or experiment that if a computer is given enough combinations of how words, phrases and sentences are related to one another, it could talk back to you. It is an attempt to simulate a conversation by learning how words are related to other words. A human communicates with NICOLE via the keyboard and NICOLE responds back with its own sentences which are automatically generated, based on what NICOLE has stored in it's database. Each new sentence that has been typed in, and NICOLE doesn't know about, is included into NICOLE's database, thus extending the knowledge base of NICOLE.        Otter: An Automated Deduction System     Web site:   www-unix.mcs.anl.gov/AR/otter/     Our current automated deduction system  Otter is designed to prove theorems stated in first-order logic with equality.  Otter's inference rules are based on resolution and paramodulation, and it includes facilities for term rewriting, term orderings, Knuth-Bendix completion, weighting, and strategies for directing and restricting searches for proofs.   Otter can also be used as a symbolic calculator and has an embedded equational programming system.        PVS     Web site:   pvs.csl.sri.com/     PVS is a verification system: that is, a specification language integrated with support tools and a theorem prover. It is intended to capture the state-of-the-art in mechanized formal methods and to be sufficiently rugged that it can be used for significant applications. PVS is a research prototype: it evolves and improves as we develop or apply new capabilities, and as the stress of real use exposes new requirements.          RIPPER     Web site:   www.research.att.com/~wcohen/ripperd.html       Ripper is a system for fast effective rule induction. Given a set of data, Ripper will learn a set of rules that will predict the  patterns in the data. Ripper is written in ASCI C and comes with documentation and some sample problems.          SNePS     Web site:   www.cs.buffalo.edu/pub/sneps/WWW/   FTP site:   ftp.cs.buffalo.edu/pub/sneps/     The long-term goal of The SNePS Research Group is the design and construction of a natural-language-using computerized cognitive agent, and carrying out the research in artificial intelligence, computational linguistics, and cognitive science necessary for that endeavor. The three-part focus of the group is on knowledge representation, reasoning, and natural-language understanding and generation. The group is widely known for its development of the SNePS knowledge representation/reasoning system, and Cassie, its computerized cognitive agent.              Soar     Web site:   bigfoot.eecs.umich.edu/~soar/   FTP site:   cs.cmu.edu/afs/cs/project/soar/public/Soar6/       Soar has been developed to be a general cognitive architecture. We intend ultimately to enable the Soar architecture to:    work on the full range of tasks expected of an intelligent agent, from highly routine to extremely difficult, open-ended problems   represent and use appropriate forms of knowledge, such as procedural, declarative, episodic, and possibly iconic   employ the full range of problem solving methods    interact with the outside world and    learn about all aspects of the tasks and its performance on them.      In other words, our intention is for Soar to support all the capabilities required of a general intelligent agent. http://wwwis.cs.utwente.nl:8080/ tcm/index.html        TCM     Web site:   wwwis.cs.utwente.nl:8080/~tcm/index.html   FTP site:   ftp.cs.vu.nl/pub/tcm/       TCM (Toolkit for Conceptual Modeling) is our suite of graphical editors. TCM contains graphical editors for Entity-Relationship diagrams, Class-Relationship diagrams, Data and Event Flow diagrams, State Transition diagrams, Jackson Process Structure diagrams and System Network diagrams, Function Refinement trees and various table editors, such as a Function-Entity table editor and a Function Decomposition table editor.  TCM is easy to use and performs numerous consistency checks, some of them immediately, some of them upon request.          WEKA     Web site:   lucy.cs.waikato.ac.nz/~ml/       WEKA (Waikato Environment for Knowledge Analysis) is an state-of-the-art facility for applying machine learning techniques to practical problems. It is a comprehensive software ""workbench"" that allows people to analyse real-world data. It integrates different machine learning tools within a common framework and a uniform user interface. It is designed to support a ""simplicity-first"" methodology, which allows users to experiment interactively with simple machine learning tools before looking for more complex solutions.              Next   Previous   Contents"
