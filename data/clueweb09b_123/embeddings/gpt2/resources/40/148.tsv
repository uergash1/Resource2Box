id	content
GX169-26-14225454	arXiv: chao-dyn/9806001  v1   1 Jun 1998     QUASICRYSTALLINE PATTERN FORMATION IN FLUID SUBSTRATES AND PHYLLOTAXIS   A. Mary Selvam       1. INTRODUCTION       The botanical elements which constitute plants are branches, leaves, petals, stamens, sepals, florets, etc. These plant elements begin their existence as primordia in the neighborhood of the undifferentiated shoot apex (extremity). Extensive observations in botany show that in more than 90% of plants studied worldwide (Jean, 1984, 1989, 1990, 1992a,b, 1994; Douady and Couder, 1991, 1992, 1993, 1995; Ryan et al, 1991; Bursill et al, 1992; Needham et al, 1993; Stewart, 1995) primordia emerge as protuberances at locations such that the angle subtended at the apical center by two successive primordia is equal to the golden angle  j   =2 p  / t   2  corresponding to approximately 137.5 °  where  t  is the golden mean  (1+ Ö  5)/2 = 1.618 . The golden mean is the most irrational number and is associated with the Fibonacci mathematical sequence 1, 1, 2, 3, 5, 8, ...... where each term is the sum of the two previous terms and the ratio of each term to the previous term approaches the golden mean  t  .  t  is the most irrational number in the sense that rational approximations converge very slowly to  t  as compared to other irrational numbers. Irrational numbers are numbers such as  which has an infinite number of non-periodic decimals. Rational approximations such as  p/q  where  p  and  q  are integers are used to represent   irrational numbers (Kappraff, 1992). The surprisingly precise geometrical placement of plant primordia results in the observed 'phyllotactic patterns', namely, the familiar spiral patterns found in the arrangement of leaves on a stem, in florets of composite flowers, the pattern of scales on pineapple and pine cone, etc. The word 'phyllotaxis' is of Greek origin ('phyllon', leaf and 'taxis', arrangement) and literally means, the study of the disposition of leaves on the stem. Phyllotaxis, in a broader sense now includes the study of arrangement of all plant elements which originate as primordia. Botanists, physicists and mathematicians alike have been fascinated by such universal rhythms governing plant growth and the field of phyllotaxis has a long history of more than 150 years. A comprehensive review of phyllotaxis has been given by Jean (1984,1994).     The emergence of primordia on the shoot is basically a branching (bifurcating) process. Such repeated (selfsimilar) branching on all scales is exhibited in the branching structures of roots, shoots and veins on leaf in the plant kingdom (Arber, 1950). Selfsimilar spatial patterns where the internal small scale structure resembles the large scale is ubiquitous to nature (Stevens, 1974; Freeman, 1987, 1989; Jean, 1994) and belongs to the category of ‘fractals’ (Mandelbrot, 1977; Basu, 1990) in the new science of  nonlinear dynamics and chaos  (Gleick, 1987). Objects in nature are selfsimilar fractals (West and Schlesinger, 1989; Schroeder, 1990; West, 1990; Goldberger et al, 1990) in the sense that ‘fractals’ denotes fractional Euclidean shape and ‘selfsimilarity’ denotes repetition on all length scales of the fundamental irregular shapes, namely, wrinkles or folds and bifurcations or branchings in spatial pattern. Tessier et al(1993) have documented and discussed the fractal nature of space-time fluctuations of atmospheric flows. The fractal dimension  D  is given by the relation            (1)   where  M  is the mass contained within a distance  R  from a point in the fractal object. The fractal dimension  D  is therefore obtained as the slope of the straight line portion of the log-log plot of  M  versus  R.  A constant value for the fractal dimension  D  indicates uniform stretching on a logarithmic scale. Eq. (1) may also be written in the form           (2)   where  ‘A’  is a constant. This type of scaling was used by D’Arcy Thompson (Thompson, 1963) in scaling anatomical structures. It appears quite often in the form of allometric growth laws in botany as well as in biology (Deering and West, 1992; Jean, 1994). This particular kind of scaling has been successfully used in biology for over a century. However, the significance of the existence of such allometric growth laws have not always been properly appreciated. This scaling often refers to a process that has ‘infinite’ levels of substructure that repeat at an ever decreasing cascade to even smaller scales over which one averages to obtain allometric growth law.    Objects in nature in general exhibit multifractal structure, i.e. the fractal dimension  D  is different for different length scale range  R . The dimension of a naturally occurring fractal is a quantitative measure of a qualitative property of a structure that is selfsimilar over some region of space or interval of time. For example, a tree is composed of the fundamental branching structure on all length scales.   Selfsimilarity indicates long-range correlations, i.e. the amplitude of short- and long-term fluctuations are related to each other by a non-dimensional scale factor alone. Voss (1993) has identified selfsimilar fractal structure of DNA-base sequence of plants indicating existence of long-range correlations at the molecular level. Long-range spatiotemporal correlations are generic to dynamical systems in nature and are now identified as signatures of self-organized criticality (Bak et al, 1988; Bak and Chen, 1991). Dynamical systems are systems which change with time.   Biological auto-organization and pattern formation have been studied over the past 40 years as non-equilibrium thermodynamic phenomena (Turing, 1952).Biological systems exhibit high degree of co-operation in the form of long-range communication. The concept of co-operative existence of fluctuations in the organization of coherent structures have been identified as self-organized co-operative phenomena (Nicolis and Prigogine, 1977; Prigogine, 1980; Prigogine and Stengers, 1988; Insinnia, 1992) and synergetics (Haken, 1980).    Mary Selvam (1990) has developed a cell dynamical system model for atmospheric flows which shows that the observed long-range spatiotemporal correlations namely, self-organized criticality are intrinsic to quantumlike mechanics governing turbulent flow dynamics. The model concepts are independent of the exact details, such as the chemical, physical, physiological, etc. properties of the dynamical systems(Rosen, 1993) and therefore is a general systems theory (Peacocke, 1989; Klir, 1993; Jean, 1994) applicable for all dynamical systems in nature.    Selfsimilar structures (space-time) incorporate the Fibonacci numbers and therefore the golden mean manifested as fivefold symmetry and spiral (Fibonacci) symmetry ( Stoddart, 1988; Hargittai, 1992; Hargittai and Pickover, 1992; Jean, 1994). Nature abounds in symmetrical structures from the macro- to the microscopic scales (Tarasov, 1986). Phyllotactic patterns in the plant kingdom are the clearest examples of selfsimilar structures or self-organized criticality. Therefore 'phyllotacticlike' patterns, namely patterns with fivefold and spiral symmetry underlie self-organized criticality in dynamical systems in nature.     In this paper the cell dynamical system model concepts developed for atmospheric flows are applied for explaining the observed phyllotactic patterns in the plant kingdom. It is shown that 'phyllotaxis like' patterns are signatures of self-organized criticality in dynamical systems in nature.The cell dynamical system model is based on the concept that spatial integration of fluctuations give rise to quasiperiodic structures incorporating Fibonacci sequence. The Fibonacci sequence is generated by cumulative summation process (Stewart ,1992). The other less frequent series (Jean ,1994) observed in phyllotaxis can be generated by similar cumulative summation process (see section 4.2) and therefore consistent with cell dynamical system model concepts.    The contents of this chapter are organized as follows. Section 2. discusses the identification of quasicrystalline structure of the quasiperiodic Penrose tiling pattern in phyllotactic patterns. Nonlinear dynamics and chaos in iterative processes with reference to phyllotaxis is discussed in Section 3. Section 4 deals with the traditional concepts of quantum mechanics, its limitations, and applications of quantumlike mechanics in phyllotaxis. Section 5 closes with discussions and conclusions.           2. QUASICRYSTALLINE STRUCTURE FOR PHYLLOTAXIS : QUASIPERIODIC PENROSE TILING PATTERN       The regular arrangement of plant parts resemble the newly identified (since 1984) quasicrystalline order in condensed matter physics (Nelson, 1986). An understanding of the physical mechanism underlying the spontaneous organization of mathematically precise patterns in macroscale plant development will benefit crystallographers investigating quasicrystalline order in the arrangement of atoms at the molecular level (Bursill et al, 1992). Traditional (last 100 years) crystallography has defined a crystalline structure as an arrangement of atoms that is periodic in three dimensions. Crystals have lattice structure with identical arrangement of atoms (Von Baeyer, 1990; Lord, 1991) with space filling cubes or hexagonal prisms. Five-fold symmetry was prohibited in classical crystallography. In 1984, an alloy of alluminium and magnesium was discovered which exhibited the symmetry of an icosahedron with five fold axis. At the same time Paul Steinhardt of the University of Pennsylvania and his student Dov Levine (Von Baeyer, 1990) had quite independently identified similar geometrical structure, now called quasicrystals. These developments were based on the important work on the mathematics of tilings done by Roger Penrose and others beginning in the 1970s (Penrose , 1974, 1979; Steinhardt and Ostlund, 1987). Penrose discovered a nonperiodic tiling of the plane, using two types of tiles, which is a quasiperiodic crystal with pentagonal symmetry (DiVincenzo, 1989). It is generally accepted that a quasicrystal can be understood as a systematic (but not periodic) filling of space by unit cells of more than one kind. Such extended structures in space can be orderly and systematic without being periodic. Penrose tiling pattern are two dimensional quasicrystals.    The geometric pattern is selfsimilar and exhibits long-range correlations and is quasiperiodic. Mary Selvam(1990) has shown that small scale fluctuations (turbulence) in fluid flows self-organize to form the quasiperiodic Penrose tiling pattern (Fig.1) with fractal selfsimilar geometry to spatial pattern and long-range temporal correlations for temporal fluctuations. Self-organized criticality is exhibited as the Penrose tiling pattern for spatial geometry which then incorporates temporal correlations for dynamical processes. The generating spiral for Penrose tiling pattern (Fig.1) is traced by progressively increasing radii whose length follow the Fibonacci mathematical series , the angular turning between successive radii being equal to  2 p  / t   2  . The generating spiral is the same for phyllotaxis and Penrose tiling and therefore plant part placement follows the precise geometry of the quasiperiodic Penrose tiling pattern in the spatial domain.     The universality in the botanical arrangement of leaves, florets etc. comes from identical growth processes which can be translated into simple assumptions for an iterative growth process (Douady and Couder, 1993). Selfsimilarity underlies all growth processes in nature. Jean (1994) has emphasized the selfsimilar geometry of botanical elements. Selfsimilar structures are generated by iteration (repetition) of simple rules for growth processes on all scales of space and time. Such iterative processes are simulated mathematically by numerical computations such as    Xn+1 = F(Xn)          (3)   where  Xn+1  , the value of the variable at ( n+1 )th computational step is a function  F  of its earlier value  Xn  . Mathematical models of real world dynamical systems are basically such iterative computational schemes implemented on finite precision digital computers. Computer precision imposes a limit (finite precision) on the accuracy (number of decimals) for numerical representation of  X . Since  X  is a real number (infinite number of decimals) finite precision introduces round-off error in iterative computations from the first stage of computation. The model iterative dynamical system generates computational structures which represent the cumulative sum of round-off error at successive iterations. Computed growth patterns exhibit selfsimilar fractal structure which incorporates the golden mean (Stewart, 1992). Fibonacci series underlying the golden mean characterizes cumulative summation of bifurcations, i.e., iterative (repeated) branching process. Phyllotaxis is basically a branching process with rhythmic branching out of primordia from the shoot. Phyllotaxis may therefore be studied under the broader perspective of the new science of  nonlinear dynamics and chaos  which seeks to understand the physics of such selfsimilar patterns generated by iterative processes in computed and real world systems.       3. NONLINEAR DYNAMICS AND CHAOS IN ITERATIVE PROCESSES        Mathematical models of real world dynamical systems such as atmospheric flows are based on Newtonian continuum dynamics and consist of nonlinear equations which do not have analytical solutions. Numerical solutions of such nonlinear model equations are obtained using numerical integration schemes which are basically iterative computations such as Eq. (3) (Section 2) which amplify exponentially with time the round-off and other initialization errors and give unrealistic solutions. Deterministic equations such as Eq. (3) which are precisely defined and mathematically formulated give chaotic solutions because of sensitive dependence on initial conditions. Finite precision computer realization of mathematical models of dynamical systems therefore exhibit deterministic chaos(Gleick, 1987). Historically, though deterministic chaos was identified nearly a century ago by Poincare(1892), it became an intensive field of research in recent years (since 1980’s) following Lorenz’s (Lorenz, 1963) study of sensitive dependence on initial conditions of a simple mathematical model of atmospheric flows.   Mary Selvam (1993) has compared round-off error to turbulent (small-scale) fluctuations in fluid flows and has shown that round-off error approximately doubles on an average for each iteration. Therefore round-off error will propagate to the mainstream computation within 50 iterations in single precision (7th decimal place accuracy) computations and thereafter the computed domain represents cumulative sum of round-off error growth. The computed domain, when resolved as a function of computational accuracy has overall logarithmic spiral geometry with the quasiperiodic Penrose tiling pattern for internal structure (Fig.1), namely quasicrystalline structure incorporating the golden mean. There is a very close similarity between the geometrical patterns generated during iterative computations and those found in nature (Jurgen et al., 1990; Stewart, 1992). Iterative computations generate patterns strongly reminiscent of plant forms and clearly these curious configurations show that the rules responsible for the construction of elaborate living tissue structures could be absurdly simple (Dewdney, 1986).     In summary, cumulative integration (summation) of fluctuations or bifurcations result in selfsimilar structures which can be resolved into the quasicrystalline geometry of Penrose tiling pattern.    Phyllotaxis is basically a branching (bifurcating) process at the level of primordia initiation in the neighborhood of the shoot apex. A hierarchy of branching structures representing cumulative summation of bifurcations incorporate the golden mean in the spatial geometry . The spatial geometry of quasicrystalline structures such as that of phyllotaxis, fluid flows and round-off error growth result from cumulative summation of persistent primary perturbations appropriate to the dynamical process. Quasicrystalline order implies long-range spatial and temporal correlations or self-organized criticality (section 2.0). Recent studies by Voss (1993) show that long-range correlations indicating selfsimilar spatial geometry are present at the level of DNA in plants. Plant phyllotaxis may therefore result from quasicrystalline ordering of morphogenetic chemical fields at the subcellular level in fluid substrates.        4.   QUANTUM MECHANICS : CONCEPTS AND LIMITATIONS FOR REAL WORLD SYSTEMS         Quantum mechanical laws govern the subatomic dynamics of quantum systems such as the photon or electron. Since its invention some sixty years ago, quantum theory has been developed to describe successfully the behavior of subatomic particles, the properties of atomic nucleus and the structure and properties of molecules and solids (Rae, 1988). Quantum physics, applicable to the microscopic building blocks of matter, however, fails to describe the behavior of bulk matter, i.e., macroscale real world dynamical systems such as atmospheric flows, plant growth, functions of physiological systems, etc. Classical mechanical laws based on Newton's laws of motion are traditionally (more than 200 years) used to describe macroscale dynamical behavior. Because quantum mechanics is the fundamental theory of nature, it should also encompass classical physics. That is, applied to macroscopic phenomena, quantum mechanics should reach a limit at which it becomes equivalent to classical mechanics. Yet until recently, the exact nature of this transition had not been fully elucidated (Nauenberg et al, 1994).    One of the most challenging problems in physics is how to obtain macroscopic behavior and macroscopic variables out of (quantum) microscopic dynamics. The problem of emergence of macroscopic variables out of microscopic dynamics is of crucial relevance in biology, even more so than in Physics (Vitiello, 1992).    Quantum mechanics describes the behavior of a subatomic particle in terms of a group of waves, i.e. a wavetrain that can be built up of a large number of sine waves of slightly differing frequency. Where the waves together produce an amplitude  y  , this region advances with a group velocity that can represent the velocity of a particle whose position is represented by the region of amplitude  y  . Variable  y   2 , the square of the wave amplitude, is proportional to the probability of finding a particle at the coordinates where  y  is evaluated (Kerwin, 1963).The wave function  y  , named Schrodinger's wave function, describes a subatomic particle in terms of probabilities of occurrence at different locations on the wave train, thereby creating wave-particle duality aspect for the particle. Though quantum mechanical laws are successful in describing subatomic phenomena, the following inconsistencies are yet to be resolved (Maddox, 1988).   (1) The interpretation of Schrodinger's wave function  y  as quantities whose squared amplitudes give the probability density that a particle will be at a particular place (if the arguments of the wave function are in space coordinates). Such a declaration that algebraically additive amplitudes must be squared to obtain probability densities is unsatisfactory in the absence of physically consistent and mathematically rigorous proof.   (2) The unresolved issue of nonlocality in quantum mechanics whereby the spatially separated parts of a quantum system (photon, electron, etc.) respond as a unified whole to local perturbation.   (3) Energy propagation and interchange in quantum systems occur in discrete quanta or packets of energy content  h n   where  h  is a universal constant of nature (Planck's constant) and  n  is the frequency in cycles per second of the electromagnetic radiation. The exact physical mechanism responsible for the manifestation of subatomic phenomena as discrete packets of energy propagating as waves, i.e., the wave-particle duality is not yet identified.   (5) Finally, quantum mechanical laws, which govern the ultimate structure of matter, cannot be interpreted in terms of macroscale real world phenomena.       4.1. Quantumlike mechanics in fluid flows : a physically consistent theory        In the following it is shown that atmospheric flow structure follows laws similar to quantum mechanical laws for subatomic dynamics. The apparent inconsistencies of quantum mechanical laws described above are explained in terms of the physically consistent characteristics inherent to eddy circulation patterns in atmospheric flow.   (1)  Atmospheric flows consist of a continuum of eddy fluctuations, which give rise to alternating regions of updrafts and downdrafts (Fig. 2). Under favorable conditions of moisture supply in the environment, convection and cloud formation occur in the updraft regions while subsidence and cloud dissipation occur in adjacent downdraft regions, thereby forming discrete regions of weather activity. The square of the eddy amplitude  y   2 , represents the eddy energy (kinetic) and is proportional to the intensity of the weather system at the location where  y  is measured. In Fig. 2, a fixed location O in the path of the wavetrain experiences progressive increase in cloud formation (height and thickness of clouds) and weather activity, reaching a peak corresponding to maximum amplitude of the wavetrain at point P, followed by gradual dissipation of clouds. The signature of the passage of the wavetrain will be recorded in the meteorological parameters, such as rainfall, temperature, etc., at location O.     Variable  y   2  represents the kinetic energy of eddies. Since the large eddy is but the sum total of the enclosed smaller scales, the large eddy energy content is equal to the sum of all its individual component eddy energies, and therefore, by the Central Limit Theorem (Mood and Graybill, 1963) in  Statistics  , the kinetic energy distribution is normal, i.e. the eddy energy probability density distribution is represented by the square of the eddy amplitude (i.e., the variance).   (2) Nonlocality is intrinsic to the instantaneously adjusting bi-directional energy flow structure of atmospheric eddies, e.g., the updrafts and downdrafts of the complete eddy circulation (Fig. 2) are in steady state momentum balance.   (3) The atmospheric eddy energy is made up of the sum of discrete quanta or packets of energy in individual small eddy circulations . This concept is somewhat analogous to quanta of electromagnetic radiation.   (4) The apparent wave-particle duality is physically consistent in the context of atmospheric flows since the bi-directional energy flow structure of a complete atmospheric eddy results in the formation of clouds in updraft regions and dissipation of clouds in downdraft regions (Fig. 2) thereby giving rise to the observed discrete cellular geometry to cloud structure. The wave-particle duality of quantum mechanical phenomena may therefore be associated with bimodal (i.e. formation and dissipation, respectively) of phenomenological form for the energy manifestation in the corresponding bi-directional energy flow structure.    The continuously evolving atmospheric eddy continuum traces out the quasi-periodic Penrose tiling pattern (Fig.1) where, as a natural consequence the eddy growth is associated with an increase in phase angle and associated long range correlations. Therefore, long-range space-time correlations are inherent to quantumlike mechanics in atmospheric flows. As mentioned earlier ( Section 1) long-range space-time correlations are ubiquitous to macroscale real world dynamical system in nature and such non-local connections are identified as signatures of self-organized criticality.    Self-organized criticality manifested as selfsimilar geometrical structure to space-time fluctuation pattern is therefore a signature of quantumlike mechanics in real world macroscale dynamical system.    The cell dynamical system model for atmospheric flows is applicable to all dynamical systems and may provide a unifying theory for subatomic scale to macroscale dynamics.    The above described analogy of quantumlike mechanics for atmospheric flows is similar to the concept of a subquantum level of fluctuations whose space-time organization gives rise to the observed manifestation of subatomic phenomena, i.e., quantum system as order out of chaos phenomena (Grossing, 1989).    The macroscale atmospheric flow structure may therefore provide physically consistent interpretation for the apparent inconsistencies of quantum mechanical laws thereby unifying the laws of natural phenomena.       4.2. Quantumlike mechanics and phyllotaxis        Phyllotaxis is basically a rhythmic process for placement of primordia. Rhythmic and periodic phenomena are intrinsic to plant and animal development (Barlow, 1994). Phyllotaxis, particularly is the precisely controlled periodic appearance of primordia on the shoot apex.     Jean (1990) mentions the following Fibonacci-type sequences in phyllotaxis :      (4)   where  J  ³  1  represents the number of genetic spirals in the system,  t  ³  2  are integers. The corresponding divergence angles  q  are :             (5)    Common values for the divergence, in degrees are 137.51, 99.5 and 68.7. The above Fibonacci-type sequences in phyllotaxis (Eqs. 4 & 5) are generated by a cumulative summation process. The cell dynamical system model for fluid flows (Mary Selvam, 1990) shows that cumulative summation (integration) of fluctuations give rise to quasicrystalline structures incorporating the golden mean. The observed phyllotactic patterns are therefore consistent with model concepts.    From elementary considerations of the observed precise quasicrystalline geometry of phyllotaxis, the following macroscale manifestation of quantumlike mechanical laws may be derived.    Any two successive primordia subtend an angle equal to  2 p   q   (   radians) at the shoot apex. The angular domain of each primordium is therefore equal to  2 p   q   . The fractional angular domain of each primordium for one complete cycle of rotation is then  q  . The Schrodinger wavefunction  y  , which represents amplitude of perturbation corresponds to this fractional angular domain  q  of each primordium. The variable  y   2 , equal to  q   2  represents the variance of the fractional angular domain of each primordium.    Since the angular domain  2 p   q   corresponds to one primordium, the probability  p 1  of occurrence of a primordium for one complete cycle of rotation in any one direction is equal to  q  . Therefore, considering either clockwise or anti-clockwise direction of rotation, the probability  p  of occurrence of a primordium is equal to  p 1 2  =  q   2  .    Therefore,  y   2 ,  square of amplitude  y  of primodium domain also represents probability of occurrence of primodium similar to quantum mechanical laws which govern sub-atomic dynamics (Section 4.0).   Also, long-range spatiotemporal correlations, namely non-local connections are intrinsic to the quasicrystalline structure of phyllotaxis. Therefore quantumlike mechanical laws may be applicable to phyllotaxis.    The appearance of primordia at regular intervals may be compared to the formation of clouds regularly spaced in updraft regions of eddy circulations in atmospheric flows (Fig. 2). Self-organization of small scale fluctuation in the atmosphere outside coupled to the water medium inside the plant stem may generate Schrodinger wavefunction  y  -like concentration of morphogenetic chemical fields which trigger the birth of primordia. Such a concept is analogous to the field theory of phyllotaxis (Schoute, 1913; Richards, 1948; Wardlaw 1949; Steeves and Sussex, 1989) and also to the general morphogenetic field theory of Sheldrake (1985). The observed universality of selfsimilar space-time structures in nature may be attributed to selforganization of fluctuations in the basic fluid form of the substrate, the self-organization process being independent of the exact nature of the substrate.           5. DISCUSSIONS AND CONCLUSIONS          In a majority of plants studied worldwide (Jean, 1994), the angle subtended at the shoot apex by two successive primordia, called the divergence angle is equal to the golden angle  2 p  / t   2  corresponding to approximately 137.5 °  .    Universal occurrence of divergence angle equal to the golden angle in plant kingdom indicates dynamical growth processes which are independent of exact details (chemical, physical, physiological, genetic etc.) of the plant. Phyllotaxis like patterns occur commonly in other living and nonliving systems (Jean, 1994). The bifurcating(branching) tree-like fractal (selfsimilar) structure underlie phyllotactic patterns.    Fractal (selfsimilar) geometry to the spatial pattern is ubiquitous to dynamical systems in nature. Selfsimilarity implies long-range correlation in space and time and is now (since 1988) identified as signature of self-organized criticality. Such non-local connection are intrinsic to subatomic dynamics of quantum systems which follow quantum mechanical laws . Therefore, macroscale dynamical system in nature exhibit signatures of quantumlike mechanics.    Phyllotactic patterns possess the quasicrystalline structure of the quasiperiodic Penrose tiling pattern. Mary Selvam (1990,1993) , Mary Selvam et al(1992), Mary Selvam and Joshi (1995) have shown that quasicrystalline structure of the quasiperiodic Penrose tiling pattern underlie iterative growth processes such as turbulent(small scale) fluctuations in fluid flows and round-off error growth in numerical integration schemes. Selfsimilarity in spatial structure extends down to the level of DNA molecules in plants (Voss, 1993). It is possible that plant growth processes may be coupled to the turbulent fluid environments of air and water respectively in the exterior and interior of the plant body down to sub-cellular levels resulting in the spontaneous generation of the quasicrystalline structure in plant growth pattern. Recent studies show that Phyllotaxislike symmetrical patterns are formed spontaneously by chemotactic bacteria growing in certain fluid substrates (Budrene and Berg, 1995). Quasicrystalline structure intrinsic to fluids may help formation of such symmetrical structures. The spontaneous formation of quasiperiodic structures in fluids has been recently demonstrated (Mukerjee, 1995).    In conclusion, self-organization of space-time fluctuations of all scales in fluid substrates contribute to manifested forms and functions of diverse phenomena from micro- to macroscales. The concept of unified rhythms in manifestation of diverse phenomenological forms in the universe has been expressed by poet Rabindranath Tagore (1967) as follows.     The birth and death of the leaves are the rapid whirls of the eddy whose wider circles move slowly among stars.             8. ACKNOWLEDGEMENTS       The author is grateful to Dr. A.S.R. Murty and Mrs.A.A.Shiralkar for their keen interest and encouragement during the course of this study.Thanks are due to Mr. M.I.R. Tinmaker for typing the manuscript.       9. REFERENCES        Arber A. (1950) :  The Natural Philosophy of Plant Form , Cambridge University Press, London.   Bak P.C., Tang C. and Wiesenfeld, K. (1988) : Self-organized criticality,  Phys. Rev. Ser. A38 , 364-374.   Bak P.C. and Chen, K. (1991) : Self-organized criticality,  Sci. Amer. 264 , 26-33.   Barlow P.W. (1994): Rhythm, periodicity and polarity as bases for morphogenesis in plants,  Biol. Rev. 69,  475-525.   Basu A. (1990) : A new geometry of nature,  Science Reporter May , 9-18.   Budrene E.O. and Berg H. C. (1995) : Dynamics of formation of symmetrical patterns by chemotactic bacteria,  Nature 376 , 49-53.   Bursill L. A., Rouse J.L. and Needham A. (1992) : Sunflower quasicrystallography. In  Spiral Symmetry , Hargittai I. and Pickover C.A. (eds), World Scientific, Singapore, pp. 295-322.   Deering W. and West B.J. (1992) : Fractal physiology,  IEEE Engineering in Medicine and Biology June , 40-46.   Dewdney, A. K. (1986) : Computer recreations,  Sci. Am. 255 , 14-23.   DiVincenzo, D. P. (1989) : Perfect quasicrystals ?,  Nature 340 , 504-505.   Douady, S. and Couder, Y. (1991): Phyllotaxis as a self-organized growth process. In Proc. NATO ARW  Growth Patterns in Physical Science and Biology , Granada, Spain, 7-11 Oct. 1991.   Douady, S. and Couder, Y.(1992): Phyllotaxis as a physical self-organized growth process,  Phys. Rev. Lett. 68(13),  2098-2101.   Douady S. and Couder, Y. (1993) : Phyllotaxis as a self-organized growth process, In  Growth Patterns in Physical Sciences and Biology , J.M. Garcia-Ruiz  et al.  (eds), Plenum Press, New York.   Douady S. and Couder, Y. (1995) : Phyllotaxis as a dynamical self-organizing process,  J. Theor. Biol.  (in press).   Freeman G.R. (1987): Introduction. In  Kinetics of Nonhomogenous Processes ,   Freeman, G.R. (ed), John Wiley and Sons, Inc., NY, pp. 1-18.   Freeman G.R. (1989) :Introduction. In  KNP89 : Kinetics of Nonhomogeneous Processes (KNP) and Nonlinear Dynamics,   Can. J. Phys. 68 , 655-659.   Goldberger A. L., Rigney D. R. and West B.J. (1990) : Chaos and fractals in human physiology,  Vigyan, Sci. Am. (Indian Edition) Feb ., 41-47.   Gleick J. (1987) :  Chaos : Making a New Science , Viking, New York.   Grossing G. (1989) : Quantum systems as order out of chaos phenomena,  Il Nuovo Cimento 103B , 497-510.   Haken H. (1980) :  Synergetics : An Introduction , Springer, Berlin.   Hargittai I. and Pickover C.A. (eds.) (1992) :  Spiral Symmetry , World Scientific, Singapore.   Hargittai I. (ed.) (1992) :  Fivefold Symmetry , World Scientific, Singapore.   Insinnia E.M. (1992) : Synchronicity and coherent excitations in microtubules,  Nanobiology 1 (2), 191-208.   Jean R. V. (1984) :  Mathematical Approach to Patterns and Form in Plant Growth , Wiley Interscience, New York.   Jean R. V. (1989) : Phyllotaxis : a reappraisal,  Can. J. Bot. 67 , 3103-3107.   Jean R. V. (1990): A synergic approach to plant pattern generation,  Math. Biosci. 98 , 13-47.   Jean R. V. (1992a): Nomothetical modelling of spiral symmetry in biology. In  Fivefold Symmetry , Hargittai I. (ed.), World Scientific, Singapore, pp. 505-528.   Jean R. V. (1992b): On the origins of spiral symmetry in plants. In  Spiral Symmetry,  Hargittai I. (ed.), World Scientific, Singapore, pp. 323 - 351.   Jean R. V. (1994) :  Phyllotaxis : A systemic Study in Plant Morphogenesis , Cambridge University Press, NY, USA.   Jurgen H., Peitgen H-O. and Saupe D. (1990) : The language of fractals.  Sci. Amer. 263,  40-49.   Kappraff, J. (1992): The relationship between mathematics and mysticism of the golden mean through history. In  Fivefold Symmetry , Hargittai I.(ed.), World Scientific, Singapore, pp. 33-65.   Kerwin L. (1963) :  Atomic Physics : An Introduction,  Holt, Rinehart and Winston, New York.   Klir G.J. (1993) : Systems science : a guided tour,  J. Biol. Systems 1,  27-58.   Lord E.A. (1991) : Quasicrystals and penrose patterns,  Current Science 61(5)  , 313-319.   Lorenz E.N. (1963) : Deterministic non-periodic flow,  J. Atmos. Sci. 20,  130-141.   Maddox J. (1988) : Licence to slang Copenhagen ?,  Nature 332,  581.   Mandelbrot B.B. (1977) :  Fractals : Form, Chance and Dimension , W.H.Freeman, San Francisco.   Mary Selvam A. (1990) : Deterministic chaos, fractals and quantumlike mechanics in atmospheric flows,  Can. J. Phys. 68,  831-841.   Mary Selvam A., Pethkar J.S. and Kulkarni M.K. (1992) : Signatures of a universal spectrum for atmospheric interannual variability in rainfall time series over the indian region,  Int'l. J. Climatol. 12,  137-152.   Mary Selvam A. (1993) : Universal quantification for deterministic chaos in dynamical systems,  Appl. Math. Modelling 17,  642-649.   Mary Selvam A. and Joshi R.R. (1995) : Universal spectrum for interannual variability in  COADS  global air and sea-surface temperatures,  Int'l. J. Climatol. 15,  613-623.   Mood A.M. and Graybill F.A. (1963) :  Introduction to the Theory of Statistics,  McGraw-Hill, New York.   Mukerjee M. (1995) : Quasimodal,  Sci. Amer. 273 (2) ,  18.   Nauenberg M., Stroud C. and Yeazell J. (1994) : The classical limit of an atom,  Sci. Amer. 270,  24-29.   Needham A. R., Rouse J.L. and Bursill L. A. (1993) : Chirality and phyllotaxis of  Helianthus Tuberosus  :   A Multihead Sunflower,  Current Topics in Bot. Research 1,  79-81.   Nelson D. R. (1986) : Quasicrystals,  Sci. Amer. 255,  42-51.   Nicolis G. and Prigogine I. (1977) :  Self-Organization in Non-Equilibrium Systems,  Wiley, New York.   Peacocke A. R. (1989) :  The Physical Chemistry of Biological Organization , Clarendon Press, Oxford, U.K.    Penrose R. (1974) : The role of asthetics in pure and applied mathematical research,  Bull. Inst. Math. Appl. 10 , 266- 271, reprinted in  The Physics of Quasicrystals , P.J. Steinhardt and S. Ostlund (eds.), World Scientific,Singapore.   Penrose R. (1979) : Pentaplexity,  Math. Intell. 2 (1) ,  32-37.   Poincare H. (1892) :  Les Methodes Nouvelles de la Mecanique Celeste , Gautheir-Villars, Paris.   Prigogine I. (1980) :  From Being to Becoming,  Freeman, San Francisco, CA, USA.   Prigogine I. and Stengers I. (1988) :  Order Out of Chaos,  3rd ed. Fontana Paperbacks, London.   Rae, A. (1988) :  Quantum Physics : Illusion or Reality? , Cambridge University Press, New York.   Richards F.J. (1948) : The geometry of phyllotaxis and its origin. In  Symp. Soc. Exper. Biol. 2 . Cambridge University Press, Cambridge, pp. 217-245.   Rosen R. (1993) : Some random thoughts about chaos and some chaotic thoughts about randomness,  J. Biol. Systems 1 (1), 19-26.    Ryan G.W., Rouse J.L. and Bursill L.A. (1991) : Quantitative analysis of sunflower seed packing,  J. Theor. Biol. 147 , 303-328.   Schoute J.C. (1913) : Beitrage zur Blattstellunglehre,  Rec. Trav. Bot. Neerl 10,  153-235.   Sheldrake R. (1985) :  A New Science of Life : The Hypothesis of Formative Causation,  Anthony Bland, London.   Schroeder M. (1990) :  Fractals, Chaos and Power Laws,  W.H. Freeman and Co., N.Y.   Steeves T. A. and Sussex I. M. (1989) :  Patterns in Plant Development , 2nd ed. Cambridge University Press, New York.   Steinhardt P.J. and Ostlund S. (eds.) (1987) :  The Physics of Quasicrystals,  World Scientific, Singapore.   Stevens P.S. (1974) :  Patterns in Nature,  Little, Brown and Co. Inc., Boston, USA.   Stewart I. (1992) : Where do nature's patterns come from ?,  Nature 135, 14.    Stewart I. (1995) : Daisy, daisy, give your answer do,  Sci. Amer. 272,  76-79.   Stoddart F. (1988) : Unnatural product synthesis,  Nature 334,  10-11.   Tagore R. (1967) :  Stray Birds , Macmillan and Company Limited, U.K.   Tarasov L. (1986) :  This Amazingly Symmetrical World,  Mir Publishers, Moscow.   Tessier Y., Lovejoy S. and Schertzer D. (1993) : Universal multifractals : theory and observations for rain and clouds,  J. Appl. Meteor. 32,  223-250.   Thompson D.W. (1963) :  On Growth and Form,  2nd ed. Cambridge University Press, Cambridge.   Turing A.M. (1952) : The chemical basis of morphogenesis,  Phil. Trans. Roy Soc.   (London) B237 , 37-52.   Vitiello G. (1992) : Coherence and electromagnetic fields in living matter,  Nanobiology 1,  221-228.   Von Baeyer H. (1990) : Impossible crystals,  Discover February,  69-78.   Voss R.F. (1993) :  1/f  Noise and fractals in DNA-base sequences. In  Applications of Fractals and Chaos,  Crilly A.J. Earnshaw R.A. and Jones H. (eds.), Springer-Verlag, New York, 7-20.   Wardlaw C.W. (1949) : Experiments on organogenesis in ferns,  Growth (Suppl.) 13,  93-131.   West B.J. and Shlesinger M.F. (1989) : On the ubiquity of  1/f  noise,  Int'l. J. Mod. Phys. B3 (6), 795-819.   West B.J. (1990) : Fractal forms in physiology,  Int’l. J. Mod. Phys.   B4 (10), 1629-1669.                    LEGEND       Fig. 1  : The quasiperiodic Penrose tiling pattern.   Fig. 2  : Wave trains and cloud formation in atmospheric flows.                             Fig. 1  : The quasiperiodic Penrose tiling pattern.                                         Fig. 2  : Wave trains and cloud formation in atmospheric flows.
GX238-04-3201358	"Mathematics by Exp eriment: Plausible Reasoning in the 21st Century and Exp eriments in Mathematics: Computational Paths to Discovery Jonathan M. Borwein Centre for Experimental and Constructive Mathematics Department of Mathematics Simon Fraser University David H. Bailey Lawrence Berkeley National Laboratory Roland Girgensohn Zentrum Mathematik, Technische Universit Munchen at  Copyright c 2003 September 30, 2003   i  Preface This document is an adapted selection of excerpts from two newly published books, Mathematics by Experiment: Plausible Reasoning in the 21st Century, and Experimentation in Mathematics: Computational Paths to Discovery, published by AK Peters, Natick, Massachussetts. We have gleaned from these two volumes material that explains what experimental mathematics is all about, as well as some of the more engaging examples of experimental mathematics in action. The experimental methodology that we describe in these books provides a compelling way to generate understanding and insight; to generate and confirm or confront conjectures; and generally to make mathematics more tangible, lively and fun for both the professional researcher and the novice. We have concentrated primarily on examples from analysis and number theory, but there are numerous excursions into other areas of mathematics as well. Much of this material is gleaned from existing sources, but there is a significant amount of material that, as far as we are aware, has not yet appeared in the literature. Each of the two volumes is targeted to a fairly broad cross-section of mathematically trained readers. Most of the first volume should be readable by anyone with solid undergraduate coursework in mathematics. Most of the second volume should be readable by persons with upper-division undergraduate or graduate-level coursework. Some programming experience is useful, but not required. Borwein's work is supported by the Canada Research Chair Program and the Natural Sciences and Engineering Council of Canada. Bailey's work is supported by the Director, Office of Computational and Technology Research, Division of Mathematical, Information, and Computational Sciences of the U.S. Department of Energy, under contract number DE-AC03-76SF00098. Jonathan M. Borwein David H. Bailey Roland Girgensohn jborwein@cecm.sfu.ca dhbailey@lbl.gov girgen@cecm.sfu.ca   ii  Chapters of the Two Volumes Volume Chapter Title No. Pages 1 1 What is Experimental Mathematics? 52 2 Experimental Mathematics in Action 66 3 Pi and Its Friends 48 4 Normality of Numbers 34 5 The Power of Constructive Proofs I 44 6 Numerical Techniques I 32 7 Making Sense of Experimental Math 26 Bibliography and Index 25 Total 327 2 1 Sequences, Series, Products and Integrals 76 2 Fourier Series and Integrals 66 3 Zeta Functions and Multizeta Valaues 58 4 Partitions and Powers 46 5 Primes and Polynomials 40 6 The Power of Constructive Proofs II 40 7 Numerical Techniques II 40 Bibliography and Index 26 Total 392  Exp erimental Mathematics Web Site The authors have established a web site containing an updated collection of links to many of the URLs mentioned in the two volumes, plus errata, software, tools, and other web useful information on experimental mathematics. This can be found at the following URL: http://www.expmath.info   Contents 1 What is Exp erimental Mathematics? 1.1 Background . . . . . . . . . . . . . . . 1.2 Proof versus Truth . . . . . . . . . . . 1.3 Paradigm Shifts . . . . . . . . . . . . . 1.4 Commentary and Additional Examples 2 Exp 2.1 2.2 2.3 2.4 2.5 2.6 2.7 1 1 3 4 6 7 7 9 12 15 17 19 20  . . . .  . . . .  . . . . . . . . . . .  . . . . . . . . . . .  . . . . . . . . . . .  . . . . . . . . . . .  . . . . . . . . . . .  . . . . . . . . . . .  . . . . . . . . . . .  . . . . . . . . . . .  . . . . . . . . . . .  . . . . . . . . . . .  . . . . . . . . . . .  . . . . . . . . . . .  . . . . . . . . . . .  erimental Mathematics in Action A Curious Anomaly in the Gregory Series Bifurcation Points in the Logistic Iteration Experimental Mathematics and Sculpture Recognition of Euler Sums . . . . . . . . . Quantum Field Theory . . . . . . . . . . . Definite Integrals and Infinite Series . . . . Commentary and Additional Examples . .  3 Pi and Its Friends 23 3.1 Computing Individual Digits of Pi . . . . . . . . . . . . . . . . . . 23 3.2 Commentary and Additional Examples . . . . . . . . . . . . . . . 30 4 Sequences, Series, Pro ducts and Integrals 4.1 Pi Is Not 22/7 . . . . . . . . . . . . . . . . 4.2 High Precision Fraud . . . . . . . . . . . . 4.3 Knuth's Series Problem . . . . . . . . . . . 4.4 Commentary and Additional Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 32 35 38 40  5 Partitions and Powers 43 5.1 Partition Functions . . . . . . . . . . . . . . . . . . . . . . . . . . 43 iii   iv 5.1.1 The ""Exact"" Formula Singular Values . . . . . . . Some Fibonacci Sums . . . . Commentary and Additional for the Partition .......... .......... Examples . . . . Function ..... ..... ..... . . . .  CONTENTS . . . . . . . . . . . . . . . . . . . . 45 46 47 50 53 53 54 58 61 64  5.2 5.3 5.4  6 Numerical Techniques I I 6.1 Numerical Quadrature . . . . . . . . . . . . . . . . . . . . . . . . 6.1.1 Error Function Quadrature . . . . . . . . . . . . . . . . . . 6.2 Commentary and Additional Examples . . . . . . . . . . . . . . . Bibliography Index   Chapter 1 What is Exp erimental Mathematics? The computer has in turn changed the very nature of mathematical experience, suggesting for the first time that mathematics, like physics, may yet become an empirical discipline, a place where things are discovered because they are seen. David Berlinski, ""Ground Zero: A Review of The Pleasures of Counting, by T. W. Koerner,"" 1997 If mathematics describes an ob jective world just like physics, there is no reason why inductive methods should not be applied in mathematics just the same as in physics. Kurt G odel, Some Basic Theorems on the Foundations, 1951  1.1  Background  [From Volume 1, Section 1.1] One of the greatest ironies of the information technology revolution is that while the computer was conceived and born in the field of pure mathematics, through the genius of giants such as John von Neumann and Alan Turing, until recently this marvelous technology had only a minor impact within the field that gave it birth. 1   2  CHAPTER 1. WHAT IS EXPERIMENTAL MATHEMATICS?  This has not been the case in applied mathematics, as well as in most other scientific and engineering disciplines, which have aggressively integrated computer technology into their methodology. For instance, physicists routinely utilize numerical simulations to study exotic phenomena ranging from supernova explosions to big bang cosmology--phenomena that in many cases are beyond the reach of conventional laboratory experimentation. Chemists, molecular biologists, and material scientists make use of sophisticated quantum-mechanical computations to unveil the world of atomic-scale phenomena. Aeronautical engineers employ large-scale fluid dynamics calculations to design wings and engines for jet aircraft. Geologists and environmental scientists utilize sophisticated signal processing computations to probe the earth's natural resources. Biologists harness large computer systems to manage and analyze the exploding volume of genome data. And social scientists--economists, psychologists, and sociologists--make regular use of computers to spot trends and inferences in empirical data. Perhaps the most important advancement in bringing mathematical research into the computer age is the development of broad spectrum mathematical software products, such as Mathematica and Maple. These days, many mathematicians are highly skilled with these tools and use them as part of their day-to-day research work. As a result, we are starting to see a wave of new mathematical results discovered partly or entirely with the aid of computer-based tools. Further developments in hardware (the gift of Moore's Law of semiconductor technology), software tools, and the increasing availability of valuable Internetbased facilities, are all ensuring that mathematicians will have their day in the computational sun. This new approach to mathematics--the utilization of advanced computing technology in mathematical research--is often called experimental mathematics. The computer provides the mathematician with a ""laboratory"" in which he or she can perform experiments: analyzing examples, testing out new ideas, or searching for patterns. Our books are about this new, and in some cases not so new, way of doing mathematics. To be precise, by experimental mathematics, we mean the methodology of doing mathematics that includes the use of computations for: (1) gaining insight and intuition; (2) discovering new patterns and relationships; (3) using graphical displays to suggest underlying mathematical principles; (4) testing and especially falsifying conjectures; (5) exploring a possible result to see if it is worth formal proof; (6) suggesting approaches for formal   1.2. PROOF VERSUS TRUTH  3  proof; (7) replacing lengthy hand derivations with computer-based derivations; (8) confirming analytically derived results. Note that the above activities are, for the most part, quite similar to the role of laboratory experimentation in the physical and biological sciences. In particular, they are very much in the spirit of what is often termed ""computational experimentation"" in physical science and engineering, which is why we feel the qualifier ""experimental"" is particularly appropriate in the term experimental mathematics.  1.2  Pro of versus Truth  [From Volume 1, Sections 1.3] In any discussion of an experimental approach to mathematical research, the questions of reliability and standards of proof justifiably come to center stage. We certainly do not claim that computations utilized in an experimental approach to mathematics by themselves constitute rigorous proof of the claimed results. Rather, we see the computer primarily as an exploratory tool to discover mathematical truths, and to suggest avenues for formal proof. Nonetheless, we feel that in many cases computations constitute very strong evidence, evidence that is at least as compelling as some of the more complex formal proofs in the literature. Prominent examples include: (1) the determina24 tion that the Fermat number F24 = 22 + 1 is composite, by Crandall, Mayer, and Papadopoulos [24]; (2) the recent computation of  to more than one trillion decimal digits by Yasumasa Kanada and his team; and (3) the Internet-based computation of binary digits of  beginning at position one quadrillion organized by Colin Percival. These are among the largest computations ever done, mathematical or otherwise (the  computations are described in greater detail in Volume 1, Chapter 3). Given the numerous possible sources of error, including programming bugs, hardware bugs, software bugs, and even momentary cosmic-ray induced glitches (all of which are magnified by the sheer scale of these computations), one can very reasonably question the validity of these results. But for exactly such reasons, computations such as these typically employ very strong validity checks. In the case of computations of digits of  , it has been customary for many years to verify a result either by repeating the computation using a different algorithm, or by repeating with a slightly different   4  CHAPTER 1. WHAT IS EXPERIMENTAL MATHEMATICS?  index position. For example, if one computes hexadecimal digits of  beginning at position one trillion (we shall see how this can be done in Chapter 3), then this can be checked by repeating the computation at hexadecimal position one trillion minus one. It is easy to verify (see Algorithm 3 in Section 3.1) that these two calculations take almost completely different tra jectories, and thus can be considered ""independent."" If both computations generate 25 hexadecimal digits beginning at the respective positions, then 24 digits should perfectly overlap. If these 24 hexadecimal digits do agree, then we can argue that the probability that these digits are in error, in a very strong (albeit heuristic) sense, is roughly one part in 1624  7.9  1028 , a figure much larger even than Avogadro's number (6.022  1022 ). Percival's actual computation of the quadrillionth binary digit (i.e., the 250 trillionth hexadecimal digit) of  was verified by a similar scheme, which for brevity we have simplified here. Independent checks and extremely high numerical confidence levels still do not constitute formal proofs of correctness. What's more, we shall see in Section 1.4 of the second volume (and in Section 4.2 of this document) some examples of ""high-precision frauds,"" namely ""identities"" that hold to high precision, yet are not precisely true. Even so, one can argue that many computational results are as reliable, if not more so, than a highly complicated piece of human mathematics. For example, perhaps only 50 or 100 people alive can, given enough time, digest al l of Andrew Wiles' extraordinarily sophisticated proof of Fermat's Last Theorem. If there is even a one percent chance that each has overlooked the same subtle error (and they may be psychologically predisposed to do so, given the numerous earlier results that Wiles' result relies on), then we must conclude that computational results are in many cases actually more secure than the proof of Fermat's Last Theorem.  1.3  Paradigm Shifts  [From Volume 1, Section 1.4] We acknowledge that the experimental approach to mathematics that we propose will be difficult for some in the field to swallow. Many may still insist that mathematics is all about formal proof, and from their viewpoint, computations have no place in mathematics. But in our view, mathematics is not ultimately about formal proof; it is instead about secure mathematical knowledge. We   1.3. PARADIGM SHIFTS  5  are hardly alone in this regard--many prominent mathematicians throughout history have either exemplified or explicitly espoused such a view. Jacques Hadamard (18651963) was perhaps the greatest mathematician to think deeply and seriously about cognition in mathematics. He nicely declared: The ob ject of mathematical rigor is to sanction and legitimize the conquests of intuition, and there was never any other ob ject for it. (J. Hadamard, from E. Borel, ""Lecons sur la theorie des fonctions,"" 1928, quoted in [40]) G. H. Hardy was another of the 20th century's towering figures in mathematics. In addition to his own mathematical achievements in number theory, he is well known as the mentor of Ramanujan. In his Rouse Ball lecture in 1928, Hardy emphasized the intuitive and constructive components of mathematical discovery: I have myself always thought of a mathematician as in the first instance an observer, a man who gazes at a distant range of mountains and notes down his observations. . . . The analogy is a rough one, but I am sure that it is not altogether misleading. If we were to push it to its extreme we should be led to a rather paradoxical conclusion; that we can, in the last analysis, do nothing but point; that proofs are what Littlewood and I call gas, rhetorical flourishes designed to affect psychology, pictures on the board in the lecture, devices to stimulate the imagination of pupils. This is plainly not the whole truth, but there is a good deal in it. The image gives us a genuine approximation to the processes of mathematical pedagogy on the one hand and of mathematical discovery on the other; it is only the very unsophisticated outsider who imagines that mathematicians make discoveries by turning the handle of some miraculous machine. Finally the image gives us at any rate a crude picture of Hilbert's metamathematical proof, the sort of proof which is a ground for its conclusion and whose ob ject is to convince. [17, Preface] As one final example, in the modern age of computers, we quote John Milnor, a contemporary Fields medalist:   6  CHAPTER 1. WHAT IS EXPERIMENTAL MATHEMATICS? If I can give an abstract proof of something, I'm reasonably happy. But if I can get a concrete, computational proof and actually produce numbers I'm much happier. I'm rather an addict of doing things on computer, because that gives you an explicit criterion of what's going on. I have a visual way of thinking, and I'm happy if I can see a picture of what I'm working with. [41, page 78]  1.4  Commentary and Additional Examples  [From Volume 1, Chapter 1 Commentary] 1. Hales' computer-assisted pro of of Kepler's conjecture. In 1611, Kepler described the stacking of equal-sized spheres into the familiar arrangement we see for oranges in the grocery store. He asserted that this packing is the tightest possible. This assertion is now known as the Kepler conjecture, and has persisted for centuries without rigorous proof. Hilbert included the Kepler conjecture in his famous list of unsolved problems in 1900. In 1994, Thomas Hales, now at the University of Pittsburgh, proposed a five-step program that would result in a proof: (a) treat maps that only have triangular faces; (b) show that the face-centered cubic and hexagonal-close packings are local maxima in the strong sense that they have a higher score than any Delaunay star with the same graph; (c) treat maps that contain only triangular and quadrilateral faces (except the pentagonal prism); (d) treat maps that contain something other than a triangle or quadrilateral face; (e) treat pentagonal prisms. In 1998, Hales announced that the program was now complete, with Samuel Ferguson (son of Helaman Ferguson) completing the crucial fifth step. This pro ject involved extensive computation, using an interval arithmetic package, a graph generator, and Mathematica. As this book was going to press, the Annals of Mathematics has decided to publish Hales' paper, but with a cautionary note, because although a team of referees is ""99% certain"" that the computer-assisted proof is sound, they have not been able to verify every detail [42]. One wonders if every other article in this journal has implicitly been certified to be correct with more than 99% certainty.   Chapter 2 Exp erimental Mathematics in Action The purpose of computing is insight, not numbers. Richard Hamming, Numerical Methods for Scientists and Engineers, 1962 In this chapter, we will present a few particularly engaging examples of modern experimental mathematics in action. We invite those readers with access to some of the computational tools we mention below to personally try some of these examples.  2.1  A Curious Anomaly in the Gregory Series  [From Volume 1, Section 2.2] In 1988, Joseph Roy North of Colorado Springs observed that Gregory's series for  ,   =4 k=1  (-1)k+1 = 4(1 - 1/3 + 1/5 - 1/7 +    ), 2k - 1  (2.1.1)  when truncated to 5,000,000 terms, gives a value that differs strangely from the true value of  . Here is the truncated Gregory value and the true value of  : 7   8  CHAPTER 2. EXPERIMENTAL MATHEMATICS IN ACTION  3.14159245358979323846464338327950278419716939938730582097494182230781640... 3.14159265358979323846264338327950288419716939937510582097494459230781640... 2 -2 10 -122 2770  The series value differs, as one might expect from a series truncated to 5,000,000 terms, in the seventh decimal place--a ""4"" where there should be a ""6."" But the next 13 digits are correct! Then, following another erroneous digit, the sequence is once again correct for an additional 12 digits. In fact, of the first 46 digits, only four differ from the corresponding decimal digits of  . Further, the ""error"" digits appear to occur in positions that have a period of 14, as shown above. Such anomalous behavior begs explanation. Once observed, it is natural (and easy given a modern computer algebra system) to ask if something similar happens with the logarithm. Indeed it does, as the following value obtained by truncating the series log 2 = 1 - 1/2 + 1/3 - 1/4 +    shows: 0.69314708055995530941723212125817656807551613436025525140068000949418722... 0.69314718055994530941723212145817656807550013436025525412068000949339362... 1 -1 2 -16 272 -7936  Here again, the ""erroneous"" digits appear in locations with a period of 14. In the first case, the differences from the ""correct"" values are (2, -2, 10, -122, 2770), while in the second case the differences are (1, -1, 2, -16, 272, -7936). We note that each integer in the first set is even; dividing by two, we obtain (1, -1, 5, -122, 1385). How can we find out exactly what is going on here? A great place to start is by enlisting the help of an excellent resource for the computational mathematician: Neil Sloane and Simon Plouffe's Internet-based integer sequence recognition tool, available at http://www.research.att.com/~njas/sequences. This tool has no difficulty recognizing the first sequence as ""Euler numbers"" and the second as ""tangent numbers."" Euler numbers and tangent numbers are defined in terms of the Taylor's series for sec x and tan x, respectively:   sec x = k=0   (-1)k E2k x (2k )!  2k  tan x = k=0  (-1)k+1 T2k+1 x (2k + 1)!  2k+1  .  (2.1.2)   2.2. BIFURCATION POINTS IN THE LOGISTIC ITERATION  9  Indeed, this discovery, made originally through the print version of the sequence recognition tool available more than a decade ago, led to a formal proof that these sequences are indeed the source of the ""errors"" in these sequences. The precise result is that the following asymptotic expansions hold:  -2 2 log 2 - k=1 N /2  k=1 N /2  (-1)k+1  2k - 1 (-1)k k +1    m=0  E2m N 2m+1   (2.1.3) (2.1.4)  1  + N  m=1  T2m-1 . N 2m  Now the genesis of the anomaly mentioned above is clear: North, in computing  by Gregory's series, had by chance truncated the series at 5,000,000 terms, which is exactly one-half of a fairly large power of ten. Indeed, setting N = 10, 000, 000 in Equation (2.1.3) shows that the first hundred or so digits of the truncated series value are small perturbations of the correct decimal expansion for  . And the asymptotic expansions show up on the computer screen, as we observed above. Similar phenomena occur for other constants. (See [13] for proofs of (2.1.3) and (2.1.4), together with some additional details.)  2.2  Bifurcation Points in the Logistic Iteration  [From Volume 1, Section 2.3] One of the classic examples of a chaotic iteration is known as the logistic iteration: Fix a real number r > 0, select x0 in the unit interval (0, 1), and then iterate x k+1  = rxk (1 - xk ).  (2.2.5)  This is termed the ""logistic"" iteration because of its roots in computational ecology: It mimics the behavior of a biological population, which, if it becomes too numerous, exhausts its available food supply and then falls back to a smaller population, possibly oscillating in an irregular manner over many generations. For values of r < 1, the iterates (xk ) quickly converge to zero. For 1 < r < 3, the iterates converge to a single nonzero limit point. At r = 3, a bifurcation occurs: For 3 < r < 3.449489 . . . = 1 + 6, the iterates oscillate between two   10  CHAPTER 2. EXPERIMENTAL MATHEMATICS IN ACTION  Figure 2.1: Bifurcation in the logistic iteration.  distinct  limit points. A second bifurcation occurs at r = 1 + 6. In particular, for 1 + 6 < r < 3.544090359 . . ., the iterates oscillate in a periodic fashion between four distinct limit points. This pattern of limit point bifurcation and period doubling occurs at successively shorter intervals, until r > 3.5699457 . . ., when iterates behave in a completely chaotic manner. This behavior is shown in Figure 2.1. Until recently, the identity of the third bifurcation point, namely the constant b3 = 3.544090359 . . ., was not known. It is fairly straightforward, by means of recursive substitutions of Equation (2.2.5), to demonstrate that this constant must be algebraic, but the bound on the degree of the integer polynomial that b3 satisfies is quite large and thus not very useful. A tool that can be used in such situations is an integer relation algorithm. This is an algorithm which, when given n real numbers (x1 , x2 ,    , xn ), returns integers (a1 , a2 ,    , an ), not all zero, such that a1 x1 +a2 x2 +  +an xn = 0 (if such a solution exists). Such computations must be done using very high precision arithmetic, or else the results are not numerically significant. At present the best algorithm for integer relation detection appears to be the ""PSLQ"" algorithm   2.2. BIFURCATION POINTS IN THE LOGISTIC ITERATION  11  of mathematician-sculptor Helaman Ferguson [30, 6, 8], although the ""LLL"" algorithm is also often used. We discuss integer relation detection in greater depth in Volume 1, Chapter 6. For the time being we mention the Internetbased integer relation tool at http://www.cecm.sfu.ca/pro jects/IntegerRelations and the Experimental Mathematician's Toolkit at http://www.expmath.info. One straightforward application of an integer relation tool is to recover the polynomial satisfied by an algebraic number. If you suspect that a constant , whose numerical value can be calculated to high precision, is algebraic of degree n, then you can test this conjecture by computing the (n + 1)-long vector (1, , 2 ,    , n ), and then using this vector as input to an integer relation calculation. If it finds a solution vector (a0 , a1 , a2 ,    , an ) with a sufficiently high degree of numerical accuracy, then you can be fairly confident that these integers are precisely the coefficients of the polynomial satisfied by . In the present example, where  = b3 , a predecessor algorithm to PSLQ recovered the polynomial 0 = 4913 + 2108t2 - 604t3 - 977t4 + 8t5 + 44t6 + 392t7 - 193t8 - 40t9 +48t10 - 12t11 + t12 . (2.2.6) You might like to try to rediscover this polynomial by using the Internet-based tool mentioned above. To do this requires a high-precision value of b3 . Its value correct to 120 decimal digits is: 3.5440903595 5192285361 5965986604 8045405830 9984544457 3675457812 2530305842 9428588630 1225625856 6424891799 9626089927 7589974545 If you do not wish to type this number in, you may find it by using Mathematica: FindRoot[4913 + 2108*t^2 - 604*t^3 - 977*t^4 + 8*t^5 + 44*t^6 + 392*t^7 - 193*t^8 - 40*t^9 + 48*t^10 - 12*t^11 + t^12 == 0, {t, 3.544}, WorkingPrecision -> 125]  or by using a similar command with the Experimental Mathematician's Toolkit. Recently, the fourth bifurcation point b4 = 3.564407266095 . . . was identified by a similar, but much more challenging, integer relation calculation. In particular, it was found that  = -b4 (b4 - 2) satisfies a certain integer polynomial of degree 120. The recovered coefficients descend monotonically from   12  CHAPTER 2. EXPERIMENTAL MATHEMATICS IN ACTION  25730  1.986  1072 down to 1. This calculation required 10,000 decimal digit precision arithmetic, and more than one hour on 48 processors of a parallel computer system. Full details can be found in [8]. The relation produced was recently verified by Konstantinos Karamanos, using the Magma computer algebra system [36].  2.3  Exp erimental Mathematics and Sculpture  [From Volume 1, Section 2.4] In the previous section, we mentioned the PSLQ algorithm, which was discovered in 1993 by Helaman Ferguson. This is certainly a signal accomplishment-- for example, the PSLQ algorithm (with associated lattice reduction algorithms) was recently named one of ten ""algorithms of the century"" by Computing in Science and Engineering [6]. Nonetheless Ferguson is even more well-known for his numerous mathematics-inspired sculptures, which grace numerous research institutes in the United States. Photos and highly readable explanations of these sculptures can be seen in a lovely book written by his wife, Claire [29]. Together, the Fergusons recently won the 2002 Communications Award, bestowed by the Joint Policy Board of Mathematics. The citation for this award declares that the Fergusons ""have dazzled the mathematical community and a far wider public with exquisite sculptures embodying mathematical ideas, along with artful and accessible essays and lectures elucidating the mathematical concepts."" There is a remarkable and unanticipated connection between Ferguson's PSLQ algorithm and at least one of Ferguson's sculptures. It is known that the volumes of complements of certain knot figures (which volumes in R3 are infinite) are finite in hyperbolic space, and sometimes are given by certain explicit formulas. This is not true of all knots. Many of these hyperbolic complements of knots correspond to certain discrete quotient subgroups of matrix groups. One of Ferguson's sculptures, known as the ""Eight-Fold Way,"" is housed at the Mathematical Sciences Research Institute in Berkeley, California (see Figure 2.2, courtesy of Helaman Ferguson).   2.3. EXPERIMENTAL MATHEMATICS AND SCULPTURE 13  Figure 2.2: Ferguson's ""Eight-Fold Way"" and ""Figure-Eight Knot Complement"" (courtesy of Helaman Ferguson).   14  CHAPTER 2. EXPERIMENTAL MATHEMATICS IN ACTION  Another of Ferguson's well-known sculptures is the ""Figure-Eight Complement II"" (see Figure 2.2, courtesy of Helaman Ferguson). It has been known for some time that the hyperbolic volume V of the figure-eight knot complement is given by the formula V  =23   1 n 2n n  2n-1  n=1  k=n  1 k  (2.3.7) (2.3.8)  = 2.029883212819307250042405108549 . . .   In 1998, British physicist David Broadhurst conjectured that V / 3 is a rational linear combination of   Cj = n=0  (-1)n . 27n (6n + j )2  (2.3.9)  Indeed, it is, as Broadhurst [18] found using a PSLQ program:  18 3 (-1)n 18 24 V= - - 9 n=0 27n (6n + 1)2 (6n + 2)2 (6n + 3) - 6 2 + (6n + 4)2 (6n + 5) 2  2  .  (2.3.10)  You can verify this yourself, using for example the Mathematician's Toolkit, available at http://www.expmath.info. Just type the following lines of code: v = 2 * sqrt[3] * sum[1/(n * binomial[2*n,n]) * sum[1/k, \ {k, n, 2*n-1}], {n, 1, infinity}] pslq[v/sqrt[3], table[sum[(-1)^n/(27^n*(6*n+j)^2), \ {n, 0, infinity}], {j, 1, 6}]]  When this is done you will recover the solution vector (9, -18, 18, 24, 6, -2, 0). A proof that formula (2.3.10) holds, together with a number of other identities for V , is given in the Volume 1, Section 2 Commentary. As we shall see in Section 3.1, constants given by a formula of the general type given in (2.3.10), namely a ""BBP-type"" formula, possess some remarkable properties, among them the fact that you can calculate the n-th digit (base-3 digit in this case) of such constants by means of a simple algorithm, without having to compute any of the first n - 1 digits.   2.4. RECOGNITION OF EULER SUMS  15  2.4  Recognition of Euler Sums  [From Volume 1, Section 2.5] In April 1993, Enrico Au-Yeung, an undergraduate at the University of Waterloo, brought to the attention of one of us (Borwein) the curious result [11]   1+ k=1  1 1 +  + 2 k  2  k  -2  = 4.59987 . . . (2.4.11)    17 17 4  (4) = . 4 360  The function  (s) in (2.4.11) is the classical Riemann zeta function,    (s) = n=1  1 . ns  Bernoulli showed that for even integers,  (2n) is a rational multiple of  2n [15]. (Bernoulli's result is proved in Section 3.2 of the second volume of this work.) Au-Yeung had computed the sum in (2.4.11) to 500,000 terms, giving an accuracy of 5 or 6 decimal digits. Suspecting that his discovery was merely a numerical coincidence, Borwein sought to compute the sum to a higher level of precision. Using Fourier analysis and Parseval's equation, he obtained 1 2  0  t ( - t) log (2 sin ) dt = 2 2 2    ( (n  n=1  n 12 k=1 k ) + 1)2  .  (2.4.12)  The idea here is that the series on the right of (2.4.12) permits one to evaluate (2.4.11), while the integral on the left can be computed using the numerical quadrature facility of Mathematica or Maple. When he did this, he was surprised to find that the conjectured identity holds to more than 30 digits. We should add here that by good fortune, 17/360 = 0.047222 . . . has period one and thus can plausibly be recognized from its first six digits, so that Au-Yeung's numerical discovery was not entirely far-fetched. What Borwein did not know at the time was that Au-Yeung's suspected identity follows directly from a related result proved by De Doelder in 1991 [28]. In fact, it had cropped up even earlier as a problem in the American   16  CHAPTER 2. EXPERIMENTAL MATHEMATICS IN ACTION  Mathematical Monthly, but the story goes back further still. Some historical research showed that Euler considered these summations. In response to a letter from Goldbach, he examined sums that are equivalent to   1+ k=1  1 1 +  + m m 2 k  (k + 1)  -n  .  (2.4.13)  The great Swiss mathematician was able to give explicit values for certain of these sums in terms of the Riemann zeta function. For example, he found an explicit formula for the case m = 1, n  2. Sums of this general form are nowadays known as ""Euler sums"" or ""Euler-Zagier sums."" High precision calculations of many of these sums, together with considerable investigations involving heavy use of Maple's symbolic manipulation facilities, eventually yielded numerous new results. Below are just a few of the interesting results that were first discovered numerically and have since been established analytically [12]. Since these results were first obtained in 1994, many more specific identities have been discovered, and a growing body of general formulas and other results have been proven. These results, together with the underlying numerical and symbolic techniques used in their derivation, are discussed further in Chapter 3 of the second volume.   1+ k=1   1 1 +  + 2 k  2  (k + 1) 3  -4  = =  37 6  -  2 (3) 22680  k=1  1 1 1 + +  + 2 k  3 (3) +  (k + 1)  -6  1 11 4 37 6 197  (9) +  2  (7) -   (5) -   (3) 24 2 120 7560 +1    1- k=1  1 +    + (-1)k 2 4 Li 5  1 k  2  (k + 1)  -3  =  1 17 11 4 7 log5 (2) -  (5) -  log(2) +  (3) log2 (2) 30 32 720 4 1 1 +  2 log3 (2) -  2  (3), (2.4.14) 18 8 where Lin (x) = k>0 xk /k n denotes the polylogarithm function. 1 2 -   2.5. QUANTUM FIELD THEORY  17  2.5  Quantum Field Theory  [From Volume 1, Section 2.6] In another recent development, David Broadhurst (who discovered the identity (2.3.10) for Ferguson's Clay Math Award sculpture) has found, using similar methods, that there is an intimate connection between Euler sums and constants resulting from evaluation of Feynman diagrams in quantum field theory [19, 20]. In particular, the renormalization procedure (which removes infinities from the perturbation expansion) involves multiple zeta values, which we will discuss in detail in Chapter 3 of the second volume. Broadhurst's recent results are even more remarkable. He has shown [18], using PSLQ computations, that in each of ten cases with unit or zero mass, the finite part of the scalar 3-loop tetrahedral vacuum Feynman diagram reduces to four-letter ""words"" that represent iterated integrals in an alphabet of seven ""letters"" comprising the single 1-form  = dx/x and the six 1-forms k =  dx/(-k - x), where  = (1 + -3)/2 is the primitive sixth root of unity, and k runs from 0 to 5. A four-letter word here is a four-dimensional iterated integral, such as U =  (2 3 0 ) = 1 dx1 x1 dx2 x1 0 x2 0  x 0  2  dx3 (-1 - x3 )  x 0  3  dx4 = (1 - x4 )  j >k>0  (-1)j j 3k  +k  .  There are 74 such four-letter words. Only two of these are primitive terms occurring in the 3-loop Feynman diagrams: U , above, and V = Re[ (2 3 1 )] = j >k>0  (-1)j cos(2 k /3) . j 3k  The remaining terms in the diagrams reduce to products of constants found in Feynman diagrams with fewer loops. These ten cases are shown in Figure 2.3. In these diagrams, dots indicate particles with nonzero rest mass. The formulas that have been found, using PSLQ, for the corresponding constants are given in Table 2.1. In the Table the constant C = k>0 sin( k /3)/k 2 .   18  CHAPTER 2. EXPERIMENTAL MATHEMATICS IN ACTION  t t t  t  rr   t  sr  V1  t t ts  s t  rr   r t   V2A  t t s t  t  rr   t  sr  V2  t t ts  s t  rr   t  sr  N  V3  t t s t   s rr  st  r t   T  V  3S  t t  t s  rr st  s r  t  V3  t t s  t  s rr  st   s r t  L  V4A  t t  ts s  s rr  st   r t  V4  t t s  ts s  s rr  st   r t  N  V  t t s  ts s  s rr  st   s r t  5  V6  Figure 2.3: The ten tetrahedral configurations.  V V V V V V V V V V  1 2A 2N 3T 3S 3L 4A 4N 5 6  = = = = = = = = = =  6 6 6 6 6 6 6 6 6 6             (3) (3) (3) (3) (3) (3) (3) (3) (3) (3)  + - - - - - - - - -  3 (4) 5 (4) 13  (4) - 8U 2 9 (4) 11  (4) - 4C 2 2 15  (4) - 6C 2 4 77  (4) - 6C 2 12 14 (4) - 16U 469  (4) + 8 C 2 - 16V 27 3 13 (4) - 8U - 4C 2  Table 2.1: Formulas found by PSLQ for the ten tetrahedral diagrams.   2.6. DEFINITE INTEGRALS AND INFINITE SERIES  19  2.6  Definite Integrals and Infinite Series  [From Volume 1, Section 2.7] We mention here one particularly useful application of experimental mathematics methodology: evaluating definite integrals and sums of infinite series by means of numerical calculations. In one sense, there is nothing new here, since mathematicians have utilized computers to compute the approximate numerical value of definite integrals and infinite series since the dawn of computing. What we suggest here, however, is a slightly different approach: Use advanced numerical quadrature techniques and series summations methods, extended to the realm of high-precision arithmetic, and then use the computed values (typically accurate to tens or even hundreds of decimal digits) as input to a computerbased constant recognition tool, which hopefully can recognize the constant as a simple expression involving known mathematical constants. We will discuss techniques for computing definite integrals and sums of series to high precision in Section 7.4 of the second volume of this work. For the time being, we simply note that both Mathematica and Maple have incorporated some reasonably good numerical facilities for this purpose, and it is often sufficient to rely on these packages when numerical values are needed. For our first example, we use Maple or Mathematica to compute the following three integrals to over 100 decimal digit accuracy: 1 0  t2 log(t) dt = (t2 - 1)(t4 + 1)  0.180671262590654942792308128981671615337114571018296766266 240794293758566224133001770898254150483799707740 . . .  /4 0  t2 dt = sin2 (t)  0.843511841685034634002620051999528151651689086421444293697 112596906587355669239938399327915596371348023976 . . . (2.6.15)   20  0  CHAPTER 2. EXPERIMENTAL MATHEMATICS IN ACTION x sin x dx = 1 + cos2 x  2.467401100272339654708622749969037783828424851810197656603 337344055011205604801310750443350929638057956006 . . . (2.6.16) (the third of these is from [32]). Both Maple and Mathematica attempt to evaluate these definite integrals analytically. In each case, however, while the results appear to be technically correct, they are not very useful, in that they are either rather lengthy, or involve advanced functions and complex entities. We suspect that there are considerably simpler closed-form versions. Indeed, using the Inverse Symbolic Calculator (ISC) tool (a constant recognition facility) at http://www.cecm.sfu.ca/pro jects/ISC, we obtain the following, based solely on the numerical values above:  1 t2 log(t) dt  2 (2 - 2) = 2 4 32 0 (t - 1)(t + 1)  /4 0  0  t2 dt  2  log(2) =- + +G 16 4 sin2 (t) x sin x dx 2 = , 1 + cos2 x 4  (2.6.17)  where G denotes Catalan's constant   G= n=0  (-1)n . (2n + 1)2  2.7  Commentary and Additional Examples  [From Volume 1, Chapter 2 Commentary] 1. Putnam problem 1995B4. Determine a simple expression for = 8  2207 - 2207 -  1 1 2207 - 1 2207 -     .  (2.7.18)   2.7. COMMENTARY AND ADDITIONAL EXAMPLES  21  Hint: Calculate this limit to 15 decimal place accuracy, using ordinary double-precision arithmetic. Then use the ISC tool, with the ""integer relation algorithm"" option, to recognize the constant as a simple algebraic number. The result can be proved by noting that  8 = 2207 - 1/ 8 , so  that  4 +  -4 = 47. Answer: (3 + 5)/2. 2. Two radical expressions. (From [34, pg. 81, 84]). Express 3  cos  2 + 7 2 + 9  3  cos  4 + 7 4 + 9  3  cos  6  7 8  9  3  cos  3  cos  3  cos  as radicals. Hint: Calculate to high precision, then use the ISC tool to find the polynomial they satisfy.   Answers: 3 1 (5 - 3 3 7) and 3 3 3 9 - 3. 2 2 3. H. S. M. (Donald) Coxeter (19072003). The renowned Canadian geometer H. S. M. Coxeter passed away in late March 2003. Coxeter was known for making extensive use of physical models in his research. A portion of his collection is on display at the University of Toronto, where he worked for 67 years. The model shown in Figure 2.4 now resides at York University in Toronto. Among his numerous published books, Regular Complex Polytopes, for example, is lavishly illustrated with beautiful and often intricate figures. He was a friend of Maurits C. Escher, the graphic artist. In a 1997 paper, Coxeter showed that Escher, despite knowing no mathematics, had achieved ""mathematical perfection"" in his etching ""Circle Limit III."" ""Escher did it by instinct,"" Donald Coxeter noted, ""I did it by trigonometry."" Two sculptures based on Coxeter's work decorate the Fields Institute in Toronto. One, hanging from the ceiling, is a three-dimensional pro jection of a four-dimensional regular polytope whose 120 faces are dodecahedrons as shown in Figure 2.5.   22  CHAPTER 2. EXPERIMENTAL MATHEMATICS IN ACTION  Figure 2.4: Donald Coxeter's own kaleidoscope (courtesy Asia Weiss).  Figure 2.5: A pro jection of a four dimensional polytope.   Chapter 3 Pi and Its Friends I am ashamed to tell you to how many figures I carried these computations, having no other business at the time. Issac Newton, personal journal, 1666 The desire, as well as the need, to calculate ever more accurate values of  , the ratio of the circumference of a circle to its diameter, has challenged mathematicians for many centuries. In recent years,  computations have provided some fascinating examples of computational mathematics.  3.1  Computing Individual Digits of Pi  [From Volume 1, Section 3.4] An outsider might be forgiven for thinking that essentially everything of interest with regards to  has been discovered. But even insiders are sometimes surprised by a new discovery. Prior to 1996, almost all mathematicians believed that if you want to determine the d-th digit of  , you have to generate the entire sequence of the first d digits. (For all of their sophistication and efficiency, the schemes described above all have this property.) But it turns out that this is not true, at least for hexadecimal (base 16) or binary (base 2) digits of  . In 1996, Peter Borwein, Simon Plouffe, and one of the present authors (Bailey) found an algorithm for computing individual hexadecimal or binary digits of  [7]. To be precise, this algorithm: 23   24  CHAPTER 3. PI AND ITS FRIENDS  (1) directly produces a modest-length string of digits in the hexadecimal or binary expansion of  , beginning at an arbitrary position, without needing to compute any of the previous digits; (2) can be implemented easily on any modern computer; (3) does not require multiple precision arithmetic software; (4) requires very little memory; and (5) has a computational cost that grows only slightly faster than the digit position. Using this algorithm, for example, the one millionth hexadecimal digit (or the four millionth binary digit) of  can be computed in less than a minute on a 2001-era computer. The new algorithm is not fundamentally faster than best-known schemes for computing all digits of  up to some position, but its elegance and simplicity are nonetheless of considerable interest. This scheme is based on the following remarkable new formula for  : Theorem 3.1.1   = i=0  1 16i  4 2 1 1 - - - 8i + 1 8i + 4 8i + 5 8i + 6  .  (3.1.1)  Pro of. First note that for any k < 8,  1/ 2 0  xk-1 dx = 1 - x8 =   1/ 2   x 0  k-1+8i  dx (3.1.2)  1 2k/2  i=0   i=0  16i  1 . (8i + k )  Thus one can write   i=0  1 16i  4 2 1 1 - - - 8i + 1 8i + 4 8i + 5 8i + 6    1/ 2 4 2 - 8x3 - 4 2x4 - 8x5 = dx, 1 - x8 0  (3.1.3)   3.1. COMPUTING INDIVIDUAL DIGITS OF PI which on substituting y = 1 0  25    2x becomes 1 0  16 y - 16 dy = 4 - 2 y3 + 4 y - 4 y  4y dy - 2-2 y  1 0  4y - 8 dy y - 2y + 2 2  = .  (3.1.4) 2  However, in presenting this formal derivation, we are disguising the actual route taken to the discovery of this formula. This route is a superb example of experimental mathematics in action. It all began in 1995, when Peter Borwein and Simon Plouffe of Simon Fraser University observed that the following well-known formula for log 2 permits one to calculate isolated digits in the binary expansion of log 2:   log 2 = k=0  1 . k 2k  (3.1.5)  This scheme is as follows. Suppose we wish to compute a few binary digits beginning at position d + 1 for some integer d > 0. This is equivalent to calculating {2d log 2}, where {} denotes fractional part. Thus we can write d  {2 log 2} = k=0 d  d  2d-k k 2d -k    + k=d+1  2d-k k   = k=0  mod k k  + k=d+1  2d-k k  .  (3.1.6)  We are justified in inserting ""mod k "" in the numerator of the first summation, because we are only interested in the fractional part of the quotient when divided by k . Now the key observation is this: The numerator of the first sum in Equation (3.1.6), namely 2d-k mod k , can be calculated very rapidly by means of the binary algorithm for exponentiation, performed modulo k . The binary algorithm   26  CHAPTER 3. PI AND ITS FRIENDS  for exponentiation is merely the formal name for the observation that exponentiation can be economically performed by means of a factorization based on the binary expansion of the exponent. For example, we can write 317 = ((((32 )2 )2 )2 )  3, thus producing the result in only 5 multiplications, instead of the usual 16. According to Knuth, this technique dates back at least to 200 bce [35, pg. 461]. In our application, we need to obtain the exponentiation result modulo a positive integer k . This can be done very efficiently as follows: Algorithm 1 Binary algorithm for exponentiation modulo k . To compute r = bn mod k , where r, b, n and k are positive integers: First set t to be the largest power of two such that t  n, and set r = 1. Then A: if n  t then r  br mod k ; n  n - t; endif t  t/2 if t  1 then r  r2 mod k ; go to A; endif 2 Note that the above algorithm is performed entirely with positive integers that do not exceed k 2 in size. Thus ordinary 64-bit floating-point or integer arithmetic, available on almost all modern computers, suffices for even rather large calculations. 128-bit floating-point arithmetic (double-double or quad precision), available at least in software on many systems (see Volume 1, Section 6.2), suffices for the largest computations currently feasible. We can now present the algorithm for computing individual binary digits of log 2. Algorithm 2 Individual digit algorithm for log 2. To compute the (d + 1)-th binary digit of log 2: Given an integer d > 0, (1) calculate each numerator of the first sum in Equation (3.1.6), using Algorithm 1, implemented using ordinary 64-bit integer or floating-point arithmetic; (2) divide each numerator by the respective value of k , again using ordinary floatingpoint arithmetic; (3) sum the terms of the first summation, while discarding any integer parts; (4) evaluate the second summation as written using floating-point arithmetic--only a few terms are necessary since it rapidly converges; and (5) add the result of the first and second summations, discarding any integer part. The resulting fraction, when expressed in binary, gives the first few digits of the binary expansion of log 2 beginning at position d + 1. 2   3.1. COMPUTING INDIVIDUAL DIGITS OF PI  27  As soon as Borwein and Plouffe found this algorithm, they began seeking other mathematical constants that shared this property. It was clear that any constant  of the form   = k=0  p(k ) , q (k )2k  (3.1.7)  where p(k ) and q (k ) are integer polynomials, with deg p < deg q and q having no zeroes at nonnegative integer arguments, is in this class. Further, any rational linear combination of such constants also shares this property. Checks of various mathematical references eventually uncovered about 25 constants that possessed series expansions of the form given by equation (3.1.7). As you might suppose, the question of whether  also shares this property did not escape these researchers. Unfortunately, exhaustive searches of the mathematical literature did not uncover any formula for  of the requisite form. But given the fact that any rational linear combination of constants with this property also shares this property, Borwein and Plouffe performed integer relation searches to see if a formula of this type existed for  . This was done, using computer programs written by one of the present authors (Bailey), which implement the ""PSLQ"" integer relation algorithm in high-precision, floating-point arithmetic [30, 5]. We discuss the PSLQ algorithm and related techniques more in Volume 1, Section 6.3. In particular, these three researchers sought an integer relation for the real vector (1 , 2 ,    , n ), where 1 =  and (i , 2  i  n) is the collection of constants of the requisite form gleaned from the literature, each computed to several hundred decimal digit precision. To be precise, they sought an n-long vector of integers (ai ) such that i ai i = 0, to within a very small ""epsilon."" After a month or two of computation, with numerous restarts using new  vectors (when additional formulas were found in the literature) the identity (3.1.1) was finally uncovered. The actual formula found by the computation was:  = 4F (1/4, 5/4; 1; -1/4) + 2 arctan(1/2) - log 5, (3.1.8)  where F (1/4, 5/4; 1; -1/4) = 0.955933837 . . . is a hypergeometric function evaluation. Reducing this expression to summation form yields the new  formula:   = i=0  1 16i  4 2 1 1 - - - 8i + 1 8i + 4 8i + 5 8i + 6  .  (3.1.9)   28  CHAPTER 3. PI AND ITS FRIENDS  It should be clear at this point that the scheme for computing individual hexadecimal digits of  is very similar to Algorithm 2. For completeness, we state it as follows: Algorithm 3 Individual digit algorithm for  . To compute the (d + 1)-th hexadecimal digit of  : Given an integer d > 0, we can write {16d  } = {4{16d S1 } - 2{16d S4 } - {16d S5 } - {16d S6 }}, where   (3.1.10)  S  j  = k=0  1 . 16k (8k + j )  (3.1.11)  Now apply Algorithm 2, with d  {16 Sj } = k=0 d  d  16d-k 8k + j 16 d-k    + k=d+1  16d-k 8k + j   = k=0  mod 8k + j 8k + j  + k=d+1  16d-k 8k + j  (3.1.12)  instead of equation (3.1.6), to compute {16d Sj } for j = 1, 4, 5, 6. Combine these four results, discarding integer parts, as shown in (3.1.10). The resulting fraction, when expressed in hexadecimal notation, gives the hex digit of  in position d + 1, plus a few more correct digits. 2 As with Algorithm 2, multiple-precision arithmetic software is not required-- ordinary 64-bit or 128-bit floating-point arithmetic suffices even for some rather large computations. We have omitted here some numerical details for large computations--see [7]. Sample implementations in both C and Fortran-90 are available from the web site http://www.expmath.info. Needless to say, Algorithm 3 has been implemented by numerous researchers. In 1997, Fabrice Bellard of INRIA computed 152 binary digits of  starting at the trillionth binary digit position. The computation took 12 days on 20   3.1. COMPUTING INDIVIDUAL DIGITS OF PI Hex Digits Beginning at This Position 26C65E52CB4593 17AF5863EFED8D ECB840E21926EC 85895585A0428B 921C73C6838FB2 9C381872D27596 07E45733CC790B E6216B069CB6C1  29  Position 106 107 108 109 1010 1011 1.25  1012 2.5  1014  Table 3.1: Computed hexadecimal digits of  .  workstations working in parallel over the Internet. His scheme is actually based on the following variant of 3.1.9:   =4 k=0  (-1)k 4k (2k + 1)   -  1 64  k=0  (-1)k 1024k  32 8 1 + + 4k + 1 4k + 2 4k + 3  .  (3.1.13)  This formula permits individual hex or binary digits of  to be calculated roughly 43% faster than (3.1.1). A year later, Colin Percival, then a 17-year-old student at Simon Fraser University, utilized a network of 25 machines to calculate binary digits in the neighborhood of position 5 trillion, and then in the neighborhood of 40 trillion. In September 2000, he found that the quadrillionth binary digit is ""0,"" based on a computation that required 250 CPU-years of run time, carried out using 1,734 machines in 56 countries. Table 3.1 gives some results known as of this writing. One question that immediately arises in the wake of this discovery is whether or not there is a formula of this type and an associated computational scheme to compute individual decimal digits of  . Searches conducted by numerous researchers have been unfruitful. Now it appears that there is no nonbinary formula of this type--this is ruled out by a new result co-authored by one of the present authors (see Volume 1, Section 3.7) [14].   30  CHAPTER 3. PI AND ITS FRIENDS  3.2  Commentary and Additional Examples  [From Volume 1, Chapter 3 Commentary] 1. An arctan series for pi. Find rational coefficients ai such that the identity  = a1 arctan 1 1 + a2 arctan 390112 485298 1 1 +a3 arctan + a4 arctan 683982 1984933 1 1 +a5 arctan + a6 arctan 2478328 3449051 1 1 +a7 arctan + a8 arctan 18975991 22709274 1 1 +a9 arctan + a10 arctan 24208144 201229582 1 +a11 arctan 2189376182  holds [3, pg. 75]. Also show that an identity with even simpler coefficients exists if arctan 1/239 is included as one of the terms on the RHS. Hint: Use an integer relation program (see Volume 1, Section 6.3), or try the tools at one of these sites: http://www.cecm.sfu.ca/pro jects/IntegerRelations or http://www.expmath.info. 2. Biblical pi. 1 Kings 7:23 and 2 Chronicles 4:2 describe a circular pool in Solomon's temple ""ten cubits from brim to brim,"" and 30 cubits in circumference, so that  = 3. In spite of the clearly informal context, this discrepancy has been a source of consternation among Biblical literalists for centuries. For example, an 18th-century German Bible commentary attempted to explain away this discrepancy using the imaginative suggestion that the circular pool in Solomon's temple (clearly described in 2 Chron. 4:2 as ""round in compass"") was instead hexagonal in shape [9, pg. 7576].   Chapter 4 Sequences, Series, Pro ducts and Integrals Several years ago I was invited to contemplate being marooned on the proverbial desert island. What book would I most wish to have there, in addition to the Bible and the complete works of Shakespeare? My immediate answer was: Abramowitz and Stegun's Handbook of Mathematical Functions. If I could substitute for the Bible, I would choose Gradsteyn and Ryzhik's Table of Integrals, Series and Products. Compounding the impiety, I would give up Shakespeare in favor of Prudnikov, Brychkov and Marichev's Tables of Integrals and Series. . . On the island, there would be much time to think about waves on the water that carve ridges on the sand beneath and focus sunlight there; shapes of clouds; subtle tints in the sky. . . With the arrogance that keeps us theorists going, I harbor the delusion that it would be not too difficult to guess the underlying physics and formulate the governing equations. It is when contemplating how to solve these equations--to convert formulations into explanations--that humility sets in. Then, compendia of formulas become indispensable. Michael Berry, ""Why Are Special Functions Special?"", 2001 In the first volume, we presented numerous examples of experimental mathematics in action. In particular, we examined how a computational-experimental approach could be used to identify constants and sequences, evaluate definite 31   32  CHAPTER 4. SEQUENCES, SERIES, PRODUCTS AND INTEGRALS  integrals and infinite series, discover new identities involving fundamental constants and functions of mathematics, provide a more intuitive approach to mathematical proofs, and formulate conjectures that can lead to important advances in the field. In this chapter, we introduce our discussion with a number of additional intriguing examples in the realm of sequences, series, products and integrals.  4.1  Pi Is Not 22/7  [From Volume 2, Section 1.1] We first consider an example from the early history of  , as described in Chapter 3 of the first volume. Even Maple or Mathematica ""knows""  = 22/7, since 1  0< 0  (1 - x)4 x4 22 dx = - , 2 1+x 7  (4.1.1)  though it would be prudent to ask ""why"" it can perform the evaluation and ""whether"" we should trust it? Assume we trust it. Then the integrand is strictly positive on the interior of the interval of integration, and the answer in (4.1.1) is necessarily an area and thus strictly positive, despite millennia of claims that  is 22/7. Of course, 22/7 is one of the early continued fraction approximations to  . The first four are 3, 22/7, 333/106, 355/113. In this case, computing the indefinite integral provides immediate reassurance. We obtain t 0  x4 (1 - x)4 17 26 4 dx = t - t + t5 - t3 + 4 t - 4 arctan (t) . (4.1.2) 2 1+x 7 3 3  This is easily confirmed by differentiation, and the Fundamental Theorem of Calculus substantiates (4.1.1). In fact, one can take this idea a bit further. We note that 1 0  x4 (1 - x)4 dx =  1 , 630  (4.1.3)   4.1. PI IS NOT 22/7  33  Figure 4.1: A pictorial proof of Archimedes' inequality and we observe that 1 2 1 0  x4 (1 - x)4 dx < 0  1  (1 - x)4 x4 dx < 1 + x2  1 0  x4 (1 - x)4 dx. (4.1.4)  On combining this with (4.1.1) and (4.1.3), we straightforwardly derive 223/71 < 22/7 - 1/630 <  < 22/7 - 1/1260 < 22/7, and so re-obtain Archimedes' famous computation 3 10 10 <<3 71 70 (4.1.5)  (illustrating that it is sometimes better not to fully reduce a fraction to lowest terms). This derivation of the estimate above seems first to have been written down in Eureka, the Cambridge student journal in 1971 [25]. The integral in (4.1.1) was apparently shown by Kurt Mahler to his students in the mid-1960s, and it had appeared in a mathematical examination at the University of Sydney in November, 1960. Figure 4.1 (also in the Color Supplement) shows the estimate graphically illustrated. The three 10  10 arrays color the digits of the first hundred digits of 223/71,  , and 22/7. One sees a clear pattern on the right (22/7), a more subtle structure on the left (223/71), and a ""random"" coloring in the middle ( ). It is tempting to ask if there is a clean general way to mimic (4.1.1) for more general rational approximations, or even continued fraction convergents. This   34  CHAPTER 4. SEQUENCES, SERIES, PRODUCTS AND INTEGRALS  is indeed possible to some degree, as discussed by Beukers in [10]. The most satisfactory result is an  - for n  1, in (4.1.6). 75920 - Unlike bn = cn 1 2n  t  (1 - t2 )  2n  (1 + it)3  n+1 n+1  + (1 - it)3  n+1  0  (1 + t2 )3  dt,  (4.1.6)  where the integers an , bn and cn are implicitly defined by the integral The first three integrals evaluate to 14 - 44, 968 - 45616/15, and 1669568/7, so again we start with  - 22/7. Beukers' preliminary attempts in [10], such as the seemingly promising 1n 0  t (1 - t)n dt, (t2 + 1)n+1  this set of approximates actually produces an explicit if weak irrationality estimate [15, 10]: for large n, - pn 1  1.0499 . qn qn p 1  21.04 q q  As Beukers sketches, one consequence of this explicit sequence - ...  for all integers p, q with sufficiently large q . (Here 21.04 . . . = 1 + 1/0.0499. In fact, in 1993 Hata by different methods had improved the number 21.4 to 8.02.) While it is easy to discover ""natural"" results like 1 5 1 0  x (1 - x)2 7 - log (2) , 3 dx = 10 (1 + x)  (4.1.7)  the fact that 7/10 is again a convergent to log 2 seems to be largely a happenstance. For example, 1 0 1 0  x12 (1 - x)12 431302721 dx = - 16 (1 + x2 ) 137287920 1 x12 (1 - x)12 dx = 16 1081662400  leads to the true, if inelegant, estimate that 5902037233/1878676800 <  < 224277414953/71389718400, where the interval is of size 1.39  10-9 .   4.2. HIGH PRECISION FRAUD  35  4.2  High Precision Fraud  [From Volume 2, Section 1.4] Consider the sums   n=1  n tanh( ) 10n  =  ?  1 , 81  an evaluation that is wrong, but valid to 268 decimal places, and   n=1  n tanh( /2) 10n  =  ?  1 , 81  which is valid to ""only"" 12 places. Both series actually evaluate to transcendental numbers. What underlies these ""fraudulent"" evaluations? The ""quick"" reason is that tanh( ) and tanh( /2) are almost integers, with, e.g., 0.99 < tanh( ) < 1. Therefore, n tanh( ) will be equal to n - 1 for many n; precisely for n = 1,    , 268. Since   n=1  n-1 1 = , n 10 81  this explains the evaluations. Looking more closely at this argument, one is directly led to continued fractions as the deeper reason behind the frauds. For any irrational positive , we can write  = [a0 , a1 ,    , an , an+1 ,    ] 1 = a0 + , 1 a1 + 1 a2 + a3 +    with integral an and a0  0, an  1 for n  1. This is hard to compute by hand, but easy even on a small computer or calculator. For the parameters in our series, we get tanh( ) = [0, 1, 267, 4, 14, 1, 2, 1, 2, 2, 1, 2, 3, 8, 3, 1,    ] (4.2.8)   36 and  CHAPTER 4. SEQUENCES, SERIES, PRODUCTS AND INTEGRALS  tanh   2  = [0, 1, 11, 14, 4, 1, 1, 1, 3, 1, 295, 4, 4, 1, 5, 17, 7,    ]. (4.2.9)  It cannot be a coincidence that the integers 267 and 11 (each equal to the number of places of agreement with 1/81 in the respective formula) appear in these expansions! There must be a connection between series of the type n z n and the continued fraction expansion of an irrational . In fact, consider the infinite continued fraction approximations for  generated by p q n+1 n+1  = pn an+1 + pn-1 , = qn an+1 + qn-1 , 2n  p0 = a0 =  , q 0 = 1, 2n+1  p-1 = 1 , q-1 = 0. decreases to  and  Then for n  0, p2n /q  increases to , while p2n+1 /q ) < -  1 qn (qn + q Let further n  n+1  pn 1 . < qn qn qn+1  = qn  - pn . Then from the above, it follows that | n+1  |<  1 qn + q  n+1  < | n| <  1 q n+1   1.  All of this is standard and may be found in [33], [43], or [39]. Our aim now is to show a relationship between the above series and the continued fraction expansion of . A first key is the following lemma, which we will not prove here since it requires some knowledge about linear Diophantine equations (see [16], from which this material is taken). Lemma 4.2.1 For any irrational  > 0 and n, N  N, we have n + n + N N  = =  n for n < q N n + (-1) for n = q  N +1 N +1  , .  Theorem 4.2.2 For irrational  > 0,   n=1  p0 z + n z = (1 - z )2 n    (-1)n n=0  z qn z qn+1 (1 - z qn ) (1 - z  qn+1  )  .   4.2. HIGH PRECISION FRAUD Pro of. Let   37  G (z , w) = n=1  zn w  n  ,  (4.2.10)  for |z |, |w| < 1. Then for N > 0, qN  (1 - z w  qN  pN  ) G (z , w) -  n=1  znw  n  = n=1   z z n=1 qN qN  n+qN  w w  (n+qN )  -w n+  n +pN  = =z =z  n+qN  n +pN  w  N  - n  -1 qN +1  +1  +qN +qN  w w  qN pN  +1   +pN  w N  (-1)  N  - 1 + O(z qN  +qN +1  ) (4.2.11)  +1  +1  +pN  (-1)  w-1 + O (z w  +1  +qN +1  ),  since qN +1  = N +1 + pN +1 = pN +1 if N is odd, and = pN +1 - 1 if N is even. qN n n and QN = 1 - z qN wpN . Then AN = Now write PN = n=1 z w QN PN +1 - QN +1 PN is a polynomial of degree at most qN + qN +1 in z , and therefore it follows from (4.2.11) that AN = Q (QN G - PN ) - QN (QN +1 G - PN w - 1 qN pN qN +1 pN +1 . w = (-1)N zwz w N +1 +1  )  This in turn implies PN +1 PN - QN +1 QN = AN QN QN = (-1)N +1  w - 1 z qN wpN z qN +1 w w QN QN +1  pN  +1  .  Next summing from zero to infinity, and noting that (4.2.11) implies that G - PN /QN tends to 0 as N tends to infinity, shows that G (z , w) = z wp0 1 - zw p0  -  1-w w    (-1) n=0  n  z qn wpn z qn+1 wpn+1 (1 - z qn wpn ) (1 - z qn+1 w  pn+1  )  .   38  CHAPTER 4. SEQUENCES, SERIES, PRODUCTS AND INTEGRALS  Now differentiating with respect to w and then letting w tend to 1 proves the assertion. 2 This theorem was first proved (for   (0, 1)) by Mahler in [37]. Example 4.2.3  = tanh( ). In this case, qn = 1, 1, 268, 1073,    for n = 0, 1, 2, 3,    , and thus   n tanh( ) z n=1  n  =  z2 z 269 - (1 - z )2 (1 - z )(1 - z  268  )  +  .  Therefore, 1 - 2  10- 81  269   n=1  n tanh( ) 10n    1 + 2  10 81  -269  , 2  and similarly for  = tanh(  ). 2 Example 4.2.4  = e  163/9  .  163/9  With one of our favorite transcendental numbers,  = e 1653264929,    ], we get the incorrect evaluation   ne 163/9 ? = 1280640, n 2 n=1  = [640320,  which is, however, correct to at least half a billion digits.  2  4.3  Knuth's Series Problem  [From Volume 2, Section 1.5] We give an account here of the solution, by one of the present authors (Borwein) to a problem recently posed by Donald E. Knuth of Stanford University in the American Mathematical Monthly (Problem 10832, Nov. 2000):   4.3. KNUTH'S SERIES PROBLEM Problem: Evaluate   39  S= k=1  kk 1 - k k !e 2 k  .  Solution: We first attempted to obtain a numerical value for S . Using Maple, we produced the approximation S  -0.08406950872765599646. Based on this numerical value, the Inverse Symbolic Calculator, available at the URL http://www.cecm.sfu.ca/pro jects/ISC, with the ""Smart Lookup"" feature, yielded the result 2 1 S  - -  3 2 1 2 . (4.3.12)  Calculations to even higher precision (50 decimal digits) confirmed this approximation. Thus within a few minutes we ""knew"" the answer. Why should such an identity hold? One clue was provided by the surprising speed with which Maple was able to calculate a high-precision value of this slowly convergent infinite sum. Evidently, the Maple software knew something that we did not. Peering under the covers, we found that Maple was using the Lambert W function, which is the functional inverse of w(z ) = z ez . Another clue was the appearance of  (1/2) in the above experimental identity, together with an obvious allusion to Stirling's formula in the original problem. This led us to conjecture the identity    k=1  1 P (1/2, k - 1)  - (k - 1)! 2 2 k  1 = 2  1 2  ,  (4.3.13)  where P (x, n) denotes the Pochhammer function x(x + 1)    (x + n - 1), and where the binomial coefficients in the LHS of (4.3.13) are the same as those of  the function 1/ 2 - 2x. Maple successfully evaluated this summation, as shown on the RHS. We now needed to establish that   k=1  P (1/2, k - 1) kk  - k k !e (k - 1)! 2  2 =-. 3   40  CHAPTER 4. SEQUENCES, SERIES, PRODUCTS AND INTEGRALS  Guided by the presence of the Lambert W function   W (z ) = k=1  (-k )k-1 z k , k!  an appeal to Abel's limit theorem suggested the conjectured identity lim dW (-z /e) 1 + dz 2 - 2z = 2/3.  z 1  Here again, Maple was able to evaluate this summation and establish the identity. 2  4.4  Commentary and Additional Examples  [From Volume 2, Chapter 1 Commentary] 1. Putnam problem 1999A3. Consider the power series expansion 1 = 1 - 2x - x2 a n xn . n0  Prove that for each integer n  0, there is an integer m such that a2 + a2 n n Answer: It transpires that a2 + a2 n n +1 +1  = am .  = a2n+1 ,  (4.4.14)  which remains to be proven. Hint: The first 15 coefficients are 1, 2, 5, 12, 29, 70, 169, 408, 985, 2378, 5741, 13860, 33461, 80782, 195025, and the desired squares are 5, 29, 169, 985, 5741, 33461, 195025,   4.4. COMMENTARY AND ADDITIONAL EXAMPLES  41  which is more than enough to spot the pattern. To prove this either explicitly use the closed form for n+1   n+1 1 an =  - - 2+1 , 1+ 2 22 or show that both sides of (4.4.14) satisfy the same recursion (and initial conditions). 2. Putnam problem 2000A4. Show that the improper integral M  I = lim  M   sin(x) sin(x2 ) dx  (4.4.15)  0  exists. Hint: Numerical experimentation shows that a limit of approximately 0.4917 is reached. The existence of the limit can be rigorously established in two ways: (a) Since the integrand equals cos(x2 - x) - cos(x2 + x))/2, it suffices to M show that limM  0 cos(x + x2 ) dx exists. After a change of variables, it suffices to consider n- 1 (k+1/2) (k-1/2)  k=0  cos (u)  du. 1 + 4u  This converges by the alternating series test. (b) Use Cauchy's theorem to integrate the entire functions exp(ix2  ix) over a triangular path with vertices at 0, M and (1 + i)M . Easy estimates show that the integrals over the vertical and the diagonal edges converge. 3. Two exp ected distances. These results originate with James D. Klein. (a) The expected distance between two random points on different sides of the unit square: 2 3 1 0 0 1  x2 + y 2 dx dy +  1 3  1 0 0  1  1 + (y - u)2 du dy  = 0.869009055274534463884970594345406624856719 . . . =  2 1 5 + 2 + log 1 + 2 . 99 9   42  CHAPTER 4. SEQUENCES, SERIES, PRODUCTS AND INTEGRALS (b) The expected distance between two random points on different faces of the unit cube: 4 5 1 + 5 1 0 1 0 0 0 1 0 1 0 1 0 1 0 1 1  x2 + y 2 + (z - w)2 dw dx dy dz 1 + (y - u)2 + (z - w)2 du dw dy dz  = 0.92639005517404672921816358654777901444496019010734 . . . = 4 17  2 7 + 2- 3-  75 75 25 75   7 7 + log 1 + 2 + log 7 + 4 3 . 25 25  (c) Show that the first term in (b) is   2 F (1/2, -n + 2; 3/2; 1/2) 5 n=2 (2 n + 1)  (n + 2)  (5/2 - n)  2 1 4 2 + log 2+1 -  15 5 75 and the second term is   F (1, 1/2, -1/2 - n, -n - 1; 2, 1/2 - n, 3/2; -1) 10 n=0 (2 n + 1)  (n + 2)  (3/2 - n) +   2 2 1 -+ + log 2+1 . 25 50 10 This allows one to numerically compute the expectation to high precision and to express both of the individual integrals in terms of the same set of constants. These expectations have actually been checked by computer simulations. Hint: Reduce the first integral to a three dimensional one, and use the binomial theorem on both.   Chapter 5 Partitions and Powers I'll be glad if I have succeeded in impressing the idea that it is not only pleasant to read at times the works of the old mathematical authors, but this may occasionally be of use for the actual advancement of science. Constantin Carath dory, speaking to an MAA meeting in 1936 eo In this chapter, we address the theory of additive partitions and the theory of representations as sums of squares, both from an experimental perspective. Each has a distinguished history. We will show that computational techniques can accelerate both solution and understanding of these problems. What's more, these techniques have a number of interesting applications, including, for instance, Madelung's constant in physical chemistry.  5.1  Partition Functions  [From Volume 2, Section 4.1] The number of additive partitions of n, p(n), is formally generated by P (q ) = 1 + n1  p(n)q n =  (1 - q n )-1 . n1  (5.1.1)  One ignores ""0"" and permutations. Thus p(5) = 7 since 5 = 4+1=3+2=3+1+1=2+2+1 = 2 + 1 + 1 + 1 = 1 + 1 + 1 + 1 + 1. 43 (5.1.2)   44  CHAPTER 5. PARTITIONS AND POWERS  Figure 5.1: A Ferrer diagram Additive partitions are less tractable than multiplicative ones as there is no analogue of unique prime factorization nor the corresponding structure. Formula (5.1.1) is easily seen by expanding (1 - q n )-1 and comparing coeffi  cients. It is relatively easy to deduce that 2 n < p(n) < e 2n/3 for n > 3 (see [38]), and that the series is absolutely convergent for |q | < 1. We return to the analytic behavior of this series below. Partitions provide a wonderful example of why Keith Devlin calls mathematics ""the science of patterns"" [26]. Many geometric representations exist. For example, the partition 5 = 4 + 1 can be represented as a point at (0, 0) and four points at (0, 1), (1, 1), (2, 1), (3, 1). Read with axis reversed, this identifies 1 + 4 with 2 + 1 + 1 + 1 and so on. See Figure 5.1, which identifies 1 + 1 + 1 + 2 + 3 + 4 and 6 + 3 + 2 + 1. Such techniques provide alternate ways to prove results such as the number of partitions of n with al l parts odd is the number of partitions of n into distinct parts, (see Volume 2, Chapter 4, Exercise 1). A modern computational temperament leads to: Question: How hard is p(n) to compute--in 1900 (for MacMahon the ""father of combinatorial analysis"") or in 2000 (for Maple or Mathematica)?   5.1. PARTITION FUNCTIONS  45  Answer: The computation of p(200) = 3972999029388 took MacMahon months and intelligence. Now, however, we can use the most naive approach: Computing 200 terms of the series for the inverse product in (5.1.1) instantly produces the result using either Mathematica or Maple. Obtaining the result p(500) = 2300165032574323995027 is not much more difficult, using the Maple code > N:=500; coeff(series(1/product(1-q^n,n=1..N+1),q,N+1),q,N); 2300165032574323995027 2  5.1.1  The ""Exact"" Formula for the Partition Function  [From Volume 2, Section 4.1.3] One of the signal achievements of early twentieth century analysis was Hardy and Ramanujan's precise asymptotic for p(n) [21]. It is based in part on an analysis of the Dedekind  -function  (q ) = eiz/12 n1 (1 - e2inz ). The function  is closely related to Q(q ), and 3 (q ) discussed in the next section, and satisfies a modular equation. Their asymptotic is eK n p(n) =  4 3 2 n  1+O  1  n  ,  (5.1.3)  where K =  2/3 and n = n - 1/24. This was subsequently refined by Rademacher to   2   d  sinh k 3 x - 1 k (n) k p(n) =   dx 1  2 k=1 x - 24 where k   1 24    x=n  ,  (5.1.4)  k (n) = (h,k)=1    h,k  e-2   inh/k  ,  and h,k = exp( i h,k =  h,k  ) with m m 1 - - k k 2 hm hm 1 - - k k 2 .  k -1  m=1   46  CHAPTER 5. PARTITIONS AND POWERS   If order n terms are appropriately used, the nearest integer is p(n). A mere five terms of this expansion provides p(200)  3972999029387.86108 and six terms yields p(500)  2300165032574323995027.196661. As we have seen, the underlying asymptotic is  1  e p(n)  4n 3 2n/3  .  Later Erd made an ""elementary"" derivation of the Hardy-Ramanujan formula os (5.1.3). A recent discussion of this formula is given by Almkvist and Wilf in [2]. It is interesting to speculate how much corresponding beautiful mathematics is not done when computation becomes too easy--both Maple and Mathematica have good built-in partition functions.  5.2  Singular Values  [From Volume 2, Section 4.2] The Jacobian theta functions are a very rich source mine for experimentation-- both as a tool to learning classical theory and to discover new phenomena. Further details of what follows are given fully in [15]. For our purposes, we consider only the three classical -functions:   3 (q ) = n=-   qn , (-1)n q n , n=-  2  2  (5.2.5)  4 (q ) = 2 (q ) =  q n=-  (n+1/2)2  ,  2 for |q |  1. Note that 3 is the generating function for the number of ways of writing a number as a sum of two squares, counting order and sign. Similarly, 2 2 counts sums of two odd squares. A beautiful result of Jacobi's is 4 4 4 3 (q ) = 2 (q ) + 4 (q ).  (5.2.6)   5.3. SOME FIBONACCI SUMS  47  2 2 2 2 If we write k = 2 /3 and k = 4 /3 , we note that k 2 + (k )2 = 1. It transpires that 2 (i) 3 (q 2 ) = 2 2 4 (q ) + 3 (q ) 2 2 (ii) 4 (q 2 ) = 4 (q ) 3 (q ).  (5.2.7)  Now (5.2.6) and (5.2.7) can be proved in many ways and can be ""verified"" symbolically in many more.  5.3  Some Fib onacci Sums  [From Volume 2, Section 4.4] Theta functions turn up in quite unexpected places as we now show. The Fibonacci sequence, namely 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144,    , takes its name from its first appearance in print, which seems to have been in the famous book Liber Abaci, published by Leonardo Fibonacci (also known as Leonardo of Pisa) in 1202. He asked: How many pairs of rabbits can be produced from a single pair in a year if every month each pair begets a new pair which from the second month on becomes productive? Lest one thinks the problem is imprecise, Fibonacci describes the solution in the text and in the margin. There one finds written vertically Parium 1 Primus 2 Secundus 3 Tercius 5 Quartus 8 Quintus 13 Sestus 21 Septimus 34 Octauus 55 Nonus 89 Decimus 144 Undecimus 233 Duodecimus 377. We leave it to the reader to decide that this indeed leads to the Fibonacci sequence, but we do note that ""the proof is left as an exercise"" seems to have occurred first in De Triangulis Omnimodis by Regiomontanus, written in 1464 (but published in 1533). He is quoted as saying, ""This is seen to be the converse of the preceding. Moreover, it has a straightforward proof, as did the preceding. Whereupon I leave it to you for homework.""   48  CHAPTER 5. PARTITIONS AND POWERS  Among its many other contributions such as popularizing Hindu-Arabic notation in the west, Liber Abaci contains methods for extracting cube roots, for solving quadratics, and the lovely identity (a2 + b2 )(c2 + d2 ) = (ac  bd)2 + (ad bc)2 , which shows the product of sums of two squares is such a sum. The Fibonacci sequence occurs in many contexts both serious and quirky. For example, 144 is the only Fibonacci square. A moment's inspection shows that it is generated by F0 = 1, F 1 = 1, F n+1  = Fn + Fn-1 .  (5.3.8)  It grows quickly (like rabbits) and is monotonic. In particular, Fn+2 > 2 Fn . If we look computationally at Fn+1 /Fn , for n = 10, 20, 30, 40, we obtain the numerical values 1.61818181818, 1.61803399852, 1.61803398875, 1.61803398875, which either the h uman eye or a constant recognition facility reveals to be the Golden Mean  = ( 5 + 1)/2, to the precision used. Indeed, the standard theory of two term linear recurrence relations leads to n n 1+ 5 1 1- 5 Fn =  - , (5.3.9) 2 2 5  where (1 - 5)/2 is the other root of x2 = x + 1. It is easy to check that the sequence in (5.3.9) satisfies the recursion in (5.3.8), and has the correct initial conditions. Since |g | < 1, it is also easy to see that Fn+1 /Fn  , as claimed, and to deduce many other identities such as 2 Fn+1 Fn-1 = Fn + (-1)n for n  2. There is a slightly less well known companion Lucas sequence, named after the French number theorist Edouard Lucas (18421891): L0 = 2, L 1 = 1, n  L  n+1  = Ln + Ln-1 , n  (5.3.10)  which is correspondingly solved by  5+1 Ln = 2  +   1- 5 2  .  (5.3.11)  As both Fibonacci and Lucas sequences are built of geometric sequences, it is k clear that we can easily evaluate sums like N=1 Fn , for positive integer k . What n happens for negative integers is more interesting. A preparatory lemma is useful ([15]):   5.3. SOME FIBONACCI SUMS Lemma 5.3.1 For 0 <  <  with  = 1,   49  n=1   1 n+  1 +   n  = n=1   n 1+  2n  2 = 3 ( ),  (5.3.12) (5.3.13)  n=0    2n+1  2n+1  = n=0   2n+1 12 = 2 ( 2 ). 1 +  2n+1 4  Pro of. The proof of the first formula is a consequence of the result on theta functions in Volume 1, 4.3.1. This relies on confirming that   n=1  n 1+   2n  = n=0  (-1)n   2n+1 . 1 -  2n+1  (5.3.14)  (Try expanding both sides as double sums.) The second formula then follows by applying the first to 2 and  2 , and then 2 2 subtracting that result from the first to obtain (3 ( ) - 3 ( 2 ))/4, which equals 2 2 ( 2 )/4. 2 Two immediate consequences are   1 F2 n+1   =  n=0   52  42  n=1  1 12 =  L2n 43   3- 5 2  3- 5 1 +. 2 4  (5.3.15) (5.3.16)  Two somewhat more elaborate derivations, (see [15], Section 3.7), lead to   n=1   1 5 = 2 Fn 24 1 1 = 2 Ln 8  n=1    3- 5 3- 5 4 4 2 - 4 2 2  3- 5 4 3 -1 . 2  +1  (5.3.17) (5.3.18)  Since it is known that the classical theta functions are transcendental for algebraic values q , 0 < |q | < 1, we discover the far-from-obvious result that the   50  CHAPTER 5. PARTITIONS AND POWERS  left-hand side of each of (5.3.15), (5.3.16), (5.3.18) is a transcendental number, as probably is (5.3.17). Moreover, since both the initial sums and especially the theta functions are easy to compute numerically, we can hunt for other such identities using integer relation methods. In this way, we find:    (-1)n 5 3- 5 3- 5 4 4 = 2 - 2 - 2 4 , (5.3.19) 2 Fn 48 2 2 n=1 and a host of more recondite identities. By contrast, a remarkable elementary identity is   1 (2k - 1) 5 = , F + F2k-1 2 F2k-1 n=0 2n+1  (5.3.20)  - for k = 1, 2, 3,    . So while  F2n1 is transcendental,  (F2n+1 + 1)-1 = +1 n=0 n=0  5/2. If we compute the corresponding continued fractions of the two sums, we obtain the quite different results [1, 1, 4, 1, 2, 3, 6, 2, 1, 3, 1, 189, 1, 3, 12] and [1, 8, 2, 8, 2, 8, 2, 8, 2, 8] in partial confirmation.  5.4  Commentary and Additional Examples  [From Volume 2, Chapter 4 Commentary] 1. A combinatorial determinant problem. Find the determinant of  n n n           p n+1 p n+2 p n p n+1 p n+2 p n+3 p n p+1 n+1 p+1 n+2 p+1 n+3 p+1 p+1 n+1 p+1 n+2 p+1 n p+2 n+1 p+2 n+2 p+2 n+3 p+2 p+2 n+1 p+2 n+2 p+2 n p+3 n+1 p+3 n+2 p+3 n+3 p+3              5.4. COMMENTARY AND ADDITIONAL EXAMPLES  51  and its q -dimensional extension as a function of n, p, q . (Taken from [32].) Solution: The pattern is clear from the first few cases on simplifying in Maple or Mathematica. 2. A sum-of-p owers determinant. Find the determinant of  1 1 1 1 4 4 4 4 k=0 k k=0 k k=0 k k=0 k  1 2 2 2  k4 k4 k4 k4  k=0 k=0 k=0   k=0  1 2 3 3 4 4 4 4  k=0 k k=0 k k=0 k   k=0 k 1 2 3 4 4 4 4 4 k=0 k k=0 k k=0 k k=0 k and its q -dimensional extension. (Taken from [32].) Solution: The first few instances of this sequence are 1, 4, 216, 331776, 24883200000, 139314069504000000, which can be quickly identified as (q !)q , using the Sloane online sequence recognition tool. This fact can be proved by taking cofactors on the last row, and observing that only the final two entries have nonzero cofactors with value (q - 1)!q-1 . 3. Putnam problem 1994B4. Let dn be the greatest common divisor of 32 the entries of An -I where A = . Show that dn   with n. Hint: 43 Observe numerically, then prove by induction, that An has determinant 1 an bn and is of the form . Hence, (an - 1)|2b2 . Then write An n 2bn an explicitly via the Cayley-Hamilton theorem, which tells us that An+1 = 6 An - An-1 . 4. Crandall's integral representation for Madelung's constant. The following identity is both beautiful and effective--though less effective for computational purposes than Benson's formula. For example, 60 digits of M3 (1) can be obtained in seconds in Maple or Mathematica using Benson's identity, while using the numerical quadrature tools of Section 6.1 (see also   52  CHAPTER 5. PARTITIONS AND POWERS Volume 2, Section 7.4) to compute the integral to the same 60 digits takes roughly one hour runtime on a 2003-era computer. Richard Crandall's 3 formula is derived in [23] from the Andrews formula for 2 . It is 2 M3 (1) = -  1   r dr 0 -  1 + 2/(1 + r2(1-sin ) ) d (1 + r1+cos  )(1 + r1-cos  ) (5.4.21)  = -1.7475645946332 . . .  5. A p olygon problem. Count (i) the number of ways a polygon with n + 2 sides can be cut into n triangles; (ii) the number of ways in which parentheses can be placed in a sequence of numbers to be multiplied, two at a time; and (iii) the number of paths of length 2n through an n-by-n grid that do not rise above the main diagonal (Dijk paths). Hint: In each case the sequence starts 1, 2, 5, 14, 42, 132, 429, 1430, 4862.  -2 The ""gfun"" package returns the generating function 4 1 + 1 - 4 x and the recursion (4n + 6)u(n) = (n + 3)u(n + 1), which gives rise to n the Catalan numbers (1/(n + 1)) 2n named after Eug Charles Catalan ene (18141894). 6. Fib onacci and Lucas numbers in terms of hyp erb olic functions. Show that 2 Fn =  i 5 where  = log -n  sinh(n)  and   Ln = 2 i-n cosh(n),  5+1 2   +i . 2  Many Fibonacci formulas are then easy to obtain from the addition for2 mulas for sinh and cosh--for example consider 5 Fn - L2 . (See [27], which n should be consulted whenever one ""discovers"" a result in classical number theory.)   Chapter 6 Numerical Techniques I I Another thing I must point out is that you cannot prove a vague theory wrong . . . Also, if the process of computing the consequences is indefinite, then with a little skill any experimental result can be made to look like the expected consequences. Richard Feynman, 1964, from Gary Taubes, The (Political) Science of Salt, 1998 In this chapter, we will examine in more detail some additional underlying computational techniques that are useful in experimental mathematics. In particular, we shall briefly examine techniques for theorem proving, prime number computations, polynomial root finding, numerical quadrature, and infinte series summation. As in the first volume, we focus here on practical algorithms and techniques. In some cases, there are known techniques that have superior efficiencies or other characteristics, but for various reasons are not considered suitable for practical implementation. We acknowledge the existence of such algorithms but do not, in most cases, devote space to them.  6.1  Numerical Quadrature  [From Volume 2, Section 7.4] Experimental mathematicians very frequently find it necessary to calculate definite integrals to high precision. Recall the examples given in Chapters 1 53   54  CHAPTER 6. NUMERICAL TECHNIQUES II  and 5 of the first volume, wherein we were able to experimentally identify certain definite integrals as analytic expressions, based only on their high-precision numerical value. To briefly reprise one example, we were inspired by a recent problem in the American Mathematical Monthly [1]. By using one of the quadrature routines to be described below, together with a PSLQ integer relation detection program, we found that if C (a) is defined by  1 arctan( x2 + a2 ) dx  , (6.1.1) C (a) = x2 + a2 (x2 + 1) 0 then C (0) =  log 2/8 + G/2    C (1) =  /4 -  2/2 + 3 2 arctan( 2)/2  C ( 2) = 5 2 /96,  (6.1.2)  where G = k0 (-1)k /(2k + 1)2 is Catalan's constant. The third of these results is the result from the Monthly. These particular results then led to the following general result, among others:     arctan( x2 + a2 ) dx   2 arctan( a2 - 1) - arctan( a4 - 1) . = 2 a2 - 1 x2 + a2 (x2 + 1) 0 (6.1.3) The commercial packages Maple and Mathematica both include rather good high-precision numerical quadrature facilities. However, these packages do have some limitations, and in many cases much faster performance can be achieved with custom-written programs. And in general it is beneficial to have some understanding of quadrature techniques, even if you rely on software packages to perform the actual computation. We describe here three state-of-the-art, highly efficient techniques for numerical quadrature. You can try programming these schemes yourself, or you can refer to the C++ and Fortran-90 programs available at http://www.expmath.info.  6.1.1  Error Function Quadrature  [From Volume 2, Section 7.4.2]   6.1. NUMERICAL QUADRATURE  55  The second scheme we will discuss here is known as ""error function"" or ""erf "" quadrature. While error function quadrature is not as efficient as Gaussian quadrature for continuous, bounded, well-behaved functions on finite intervals, it often produces highly accurate results even for functions with (integrable) singularities or vertical derivatives at one or both endpoints of the interval. In contrast, Gaussian quadrature typically performs very poorly in such instances. The error function quadrature scheme and the tanh-sinh scheme to be described in the next section are based on the Euler-Maclaurin summation formula, which can be stated as follows [4, pg. 280]. Let m  0 and n  1 be integers, and define h = (b - a)/n and xj = a + j h for 0  j  n. Further, assume that the function f (x) is at least (2m + 2)-times continuously differentiable on [a, b]. Then b n  f (x) dx = h a j =0 m  f (xj ) -  h (f (a) + f (b)) 2 (2i-1)  - i=1  h2i B2i f (2i)!  (b) - f  (2i-1)  (a) - E ,  (6.1.4)  where B2i denote the Bernoulli numbers, and E= h2 m+2  (b - a)B2m+2 f (2m + 2)!  (2m+2)  ( )  ,  (6.1.5)  for some   (a, b). In the circumstance where the function f (x) and all of its derivatives are zero at the endpoints a and b, the second and third terms of the Euler-Maclaurin formula are zero. Thus the error in a simple step-function approximation to the integral, with interval h, is simply E . But since E is then less than a constant times h2m+2 /(2m + 2)!, for any m, we conclude that the error goes to zero more rapidly than any power of h. In the case of a function defined on (-, ), the Euler-Maclaurin summation formula still applies to the resulting doubly infinite sum approximation, provided as before that the function and all of its derivatives tend to zero for large positive and negative arguments. This principle is utilized in the error function and tanh-sinh quadrature scheme by transforming the integral of f (x) on a finite interval, which we will take to be (-1, 1) for convenience, to an integral on (-, ) using the change of variable x = g (t). Here g (x) is some monotonic function with the property that   56  CHAPTER 6. NUMERICAL TECHNIQUES II  g (x)  1 as x  , and g (x)  -1 as x  -, and also with the property that g (x) and all higher derivatives rapidly approach zero for large arguments. In this case we can write, for h > 0, 1    f (x) dx = -1 -  f (g (t))g (t) dt = h -  wj f (xj ),  (6.1.6)  where xj = g (hj ) and wj = g (hj ). If the convergence of g (t) and its derivatives to zero is sufficiently rapid for large |t|, then even in cases where f (x) has a vertical derivative or an integrable singularity at one or both endpoints, the resulting integrand f (g (t))g (t) will be a smooth bell-shaped function for which the Euler-Maclaurin summation formula applies, as described above. In such cases we have that the error in the above approximation decreases faster than any power of h. The summation above is typically carried out to limits (-N , N ), beyond which the terms of the summand are less than the ""epsilon"" of the multiprecision arithmetic being used. The error function integration scheme uses the function g (t) = erf (t) and  2 g (t) = (2/  )e-t . Note that g (t) is merely the bell-shaped probability density function, which is well known to converge rapidly to zero, together with all of its derivatives, for large arguments. The error function erf (x) can be computed to high precision as 1 - erfc(x), using the following formula given by Crandall [22, pg. 85] (who in turn attributes it to a 1968 paper by Chiarella and Reichel): e-t t erfc(t) =  2 2 2  1 +2 t2  k 1  e-k  k 2 2 + t2  22  +  2 1 - e2   t/  + E,  (6.1.7)  where |E | < e- / . The parameter  > 0 here is chosen small enough to ensure that the error E is sufficiently small. We summarize this scheme with the following algorithm statement. Here np is the precision level in digits, and is the ""epsilon"" level, which is typically 10-np . Algorithm 4 Error function complement [erfc] evaluation. Initialize: Set  :=  / np log(10), and set nt := np log(10)/ . 2 Set t2 := e- , t3 := t2 , and t4 := 1. 2   6.1. NUMERICAL QUADRATURE For k := 1 to nt do: set t4 := t2  t4 , Ek := t4 , t2 := t2  t3 ; enddo. Evaluation of function, with argument x: Set t1 := 0, t2 := x2 , t3 := e-t2 and t4 := /(1000  t3 ). For k := 1 to nt do: set t5 := Ek /(k 2 2 + t2 ) and t1 := t1 + t5 . If |t5 | < t4 then exit do; enddo. Set erfc(x) := t3 x/  (1/t2 + 2t1 ) + 2/(1 - e2x/ ).  57  2  We now state the algorithm for error function quadrature. As with the Gaussian scheme, m levels or phases of abscissas and weights are precomputed in the error function scheme. Then we perform the computation, increasing the level by one (each of which approximately doubles the computation, compared to the previous level), until an acceptable level of estimated accuracy is obtained (see Volume 2, Section 7.4.4 for an efficient error estimation). In the following, is the ""epsilon"" level of the multiprecision arithmetic being used. Algorithm 5 Error function quadrature. Initialize: Set h := 22-m . For k := 0 to 20  2m do:  Set t := k h, xk := 1 - erfc(t) and wk := 2/   e If |xk - 1| < then exit do; enddo. Set nt = k (the value of k at exit).  -t  2  .  Perform quadrature for a function f (x) on (-1, 1): Set S := 0 and h := 4. For k := 1 to m (or until successive values of S are identical to within ) do: h := h/2. For i := 0 to nt step 2m-k do: If ( mod (i, 2m-k+1 ) = 0 or k = 1) then If i = 0 then S := S + w0 f (0) else S := S + wi (f (-xi ) + f (xi )) endif. endif; enddo; endo. Result = hS . 2   58  CHAPTER 6. NUMERICAL TECHNIQUES II  6.2  Commentary and Additional Examples  [From Volume 2, Chapter 7 Commentary] 1. Evaluation of integrals. Evaluate the following integrals, by numerically computing them and then trying to recognize the answers, either by using the Inverse Symbolic Calculator at http://www.cecm.sfu.ca/pro jects/ISC, or by using a PSLQ facility, such as that built into the Experimental Mathematician's Toolkit, available at http://www.expmath.info. These examples are taken from Gradsteyn and Ryzhik [31]. All of the answers are simple one- or few-term expressions involving familiar mathemat ical constants such as  , e, 2, 3, log 2,  (3), G (Catalan's constant), and  (Euler's constant). We recognize that many of these can be evaluated analytically using symbolic computing software (depending on the available versions). The intent here is to provide exercises for numerical quadrature and constant recognition facilities. (a) 0  (b) 0  x2 dx  (1 + x4 ) 1 - x4   xe-x 1 - e-2x dx   1  (6.2.8) (6.2.9) (6.2.10) (6.2.11) (6.2.12) (6.2.13) (6.2.14)  (c) 0  x2 dx x e -1 x tan x dx   /4  (d) 0  /2  (e) 0  /4  x2 dx 1 - cos x ( /4 - x tan x) tan x dx  (f ) 0  /2  (g) 0  log2 (cos x) dx  Answers: (a)  /8, (b)  (1+2 log 2)/8, (c) 4 (log2 2+ 2 /12), (d) ( log 2)/8+ G/2, (e) - 2 /4 +  log 2 + 4G, (f ) (log 2)/2 +  2 /32 -  /4 + ( log 2)/8, (g)  /2(log2 2 +  2 /12).   6.2. COMMENTARY AND ADDITIONAL EXAMPLES  59  Figure 6.1: Newton-Julia set for p(x) = x3 - 1. 2. Julia sets. Figure 6.1 is a color-coded plot of the number of iterations required for convergence (to some accuracy ) of Newton's iteration (in the complex plane) for the cubic polynomial p(x) = x3 - 1. The filamentary structure shown is a Julia set, a set of measure zero separating disconnected regions. 3. Chaitin on randomness. It seems apropos to end with Greg Chaitin's views in ""The Creative Life: Science vs Art,"" an article available at the URL http://www.cs.umaine.edu/~chaitin/cdg.html. The message is that mathematics is quasi-empirical, that mathematics is not the same as physics, not an empirical science, but I think it's more akin to an empirical science than mathematicians would like to admit.   60  CHAPTER 6. NUMERICAL TECHNIQUES II Mathematicians normally think that they possess absolute truth. They read God's thoughts. They have absolute certainty and all the rest of us have doubts. Even the best physics is uncertain, it is tentative. Newtonian science was replaced by relativity theory, and then--wrong!--quantum mechanics showed that relativity theory is incorrect. But mathematicians like to think that mathematics is forever, that it is eternal. Well, there is an element of that. Certainly a mathematical proof gives more certainty than an argument in physics or than experimental evidence, but mathematics is not certain. This is the real message of G odel's famous incompleteness theorem and of Turing's work on uncomputability. You see, with G odel and Turing the notion that mathematics has limitations seems very shocking and surprising. But my theory just measures mathematical information. Once you measure mathematical information you see that any mathematical theory can only have a finite amount of information. But the world of mathematics has an infinite amount of information. Therefore it is natural that any given mathematical theory is limited, the same way that as physics progresses you need new laws of physics. Mathematicians like to think that they know all the laws. My work suggests that mathematicians also have to add new axioms, simply because there is an infinite amount of mathematical information. This is very controversial. I think mathematicians, in general, hate my ideas. Physicists love my ideas because I am saying that mathematics has some of the uncertainties and some of the characteristics of physics. Another aspect of my work is that I found randomness in the foundations of mathematics. Mathematicians either don't understand that assertion or else it is a nightmare for them . . .   Bibliography [1] Zafar Ahmed. Definitely An Integral. American Mathematical Monthly, 109:670671, 2002. [2] Gerg Almkvist and Herbert S. Wilf. On the Coefficients in the Hardy-Ramanujan-Rademacher Formula for p(n). Journal of Number Theory, 50:329334, 1995. [3] Jorg Arndt and Christoph Haenel. Pi Unleashed. Springer-Verlag, Heidelberg, 2001. [4] Kendall E. Atkinson. An Introduction to Numerical Analysis. John Wiley and Sons, Hoboken, NJ, 1989. [5] David H. Bailey. A Fortran-90 Based Multiprecision System. ACM Transactions on Mathematical Software, 21:379387, 1995. [6] David H. Bailey. Integer Relation Detection. Computing in Science and Engineering, 2(1):2428, 2000. [7] David H. Bailey, Peter B. Borwein, and Simon Plouffe. On The Rapid Computation of Various Polylogarithmic Constants. Mathematics of Computation, 66:903913, 1997. [8] David H. Bailey and David J. Broadhurst. Parallel Integer Relation Detection: Techniques and Applications. Mathematics of Computation, 70:17191736, 2000. [9] Petr Beckmann. A History of Pi. St. Martin's Press, New York, 1971. [10] Frits Beukers. A Rational Approach to Pi. Nieuw Archief voor Wiskunde, 5:372379, 2000. [11] D. Borwein and J. M. Borwein. On Some Intriguing Sums Involving  (4). Proceedings of the American Mathematical Society, 123:111118, 1995.  61   62  Bibliography  [12] D. Borwein, J. M. Borwein, and R. Girgensohn. Explicit Evaluation of Euler Sums. Proceedings of the Edinburgh Mathematical Society, 38:273294, 1995. [13] Jonathan Borwein, Peter Borwein, and K. Dilcher. Pi, Euler Numbers and Asymptotic Expansions. American Mathematical Monthly, 96:681687, 1989. [14] Jonathan M. Borwein, David Borwein, and William F. Galway. Finding and Excluding b-ary Machin-Type BBP Formulae. http://www.cecm.sfu.ca/preprints/2002pp.html, 2002. [15] Jonathan M. Borwein and Peter B. Borwein. Pi and the AGM: A Study in Analytic Number Theory and Computational Complexity. CMS Series of Mongraphs and Advanced books in Mathematics, John Wiley, Hoboken, NJ, 1987. [16] Jonathan M. Borwein and Peter B. Borwein. Strange Series Evaluations and High Precision Fraud. American Mathematical Monthly, 99:622640, 1992. [17] David Bressoud. Proofs and Confirmations: The Story of the Alternating Sign Matrix Conjecture. Mathematical Association of America, Washington, 1999. [18] David J. Broadhurst. Massive 3-loop Feynman Diagrams Reducible to SC Primitives of Algebras of the Sixth Root of Unity. http://lanl.arxiv.org/abs/hep-th/9803091, 1998. [19] David J. Broadhurst, John A Gracey, and Dirk Kreimer. Beyond the Triangle and Uniqueness Relations: Non-Zeta Counterterms at Large N from Positive Knots. Zeitschrift fur Physik, C75:559574, 1997. [20] David J. Broadhurst and Dirk Kreimer. Association of Multiple Zeta Values with Positive Knots via Feynman Diagrams up to 9 Loops. Physics Letters, B383:403412, 1997. [21] K. Chandrasekharen. Arithmetical Functions. Springer-Verlag, Heidelberg, 1970. [22] Richard E. Crandall. Topics in Advanced Scientific Computation. Springer-Verlag, Heidelberg, 1996. [23] Richard E. Crandall. New Representations for the Madelung Constant. Experimental Mathematics, 8:367379, 1999. [24] Richard E. Crandall and Jason S. Papadopoulos. On the Implementation of AKS-Class Primality Tests. http://developer.apple.com/hardware/ve/acgresearch.html, 2003.   Bibliography [25] D. P. Dalzell. On 22/7 and 355/113. Eureka, 34:1013, 1971.  63  [26] Keith Devlin. Mathematics: The Science of Patterns: The Search for Order in Life, Mind and the Universe. W. H. Freeman, New York, 1997. [27] Leonard Eugene Dickson. History of the Theory of Numbers, volume 13. Chelsea Publishing (1952), Carnegie Institute (1991), American Mathematical Society, Providence, 1999. [28] J. De Doelder. On Some Series Containing  (x) -  (y ) and ( (x) -  (y ))2 for Certain Values of x and y . Journal of Computational and Applied Mathematics, 37:125141, 1991. [29] Claire Ferguson. Helaman Ferguson: Mathematics in Stone and Bronze. Meridian Creative Group, Erie, PA, 1994. [30] Helaman R. P. Ferguson, David H. Bailey, and Stephen Arno. Analysis of PSLQ, An Integer Relation Finding Algorithm. Mathematics of Computation, 68:351369, 1999. [31] I. S. Gradshteyn and I. M. Ryzhik. Table of Integrals, Series, and Products, Fifth Edition. Academic Press, New York, 1994. [32] Francois Gu enard and Henri Lemberg. La M de Exp etho erimentale en Math ematiques. Scopos, Springer-Verlag, Heidelberg, 2001. [33] Godfrey H. Hardy. Ramanujan. Chelsea, New York, 1978.  sa. [34] J. Herman, R. Ku cera, and J. Sim Equations and Inequalities: Elementary Problems and Theorems in Number Theory, volume 1. CMS Books, Springer-Verlag, Heidelberg, 2000. [35] Donald E. Knuth. The Art of Computer Programming, volume 2. Addison-Wesley, Boston, 1998. [36] I. S. Kotsireas and K. Karamanos. Exact computation of the bifurcation point b4 of the logistic map and the bailey-broadhurst conjectures. International Journal of Bifurcation and Chaos, to appear. [37] K. Mahler. Arithmetische Eigenschaften der Losungen einer Klasse von Funktionalgleichungen. Mathematik Annalen, 101:342366, 1929. [38] Ivan Niven, Herbert S. Zuckerman, and Hugh L. Montgomery. An Introduction to the Theory of Numbers. John Wiley, Hoboken, NJ, 1991.   64  Index  [39] Oskar Perron. Die Lehre von den Kettenbruchen. Chelsea, New York, 1950. [40] George Polya. Mathematical Discovery: On Understanding, Learning and Teaching Problem Solving (Combined Edition). John Wiley, Hoboken, NJ, 1981. [41] Ed Regis. Who Got Einstein's Office? Addison-Wesley, Boston, 1986. [42] George Szpiro. Does the Proof Stack Up? Nature, 424:1213, Jul. 3, 2003. [43] H. S. Wall. Analytic Theory of Continued Fractions. Chelsea, New York, 1948.   Index Abel, Niels Henrik, 40 additive partitions, see partitions aeronautical engineering, 2 Archimedes, 33 Archimedes' inequality1 , 33 Au-Yeung, Enrico, 15 BBP-type formulas, 25 Bellard, Fabrice, 28 Benson's formula, 51 Berlinski, David, 1 Bernoulli numbers, 55 Berry, Michael2 , 31 Beukers, Frits, 34 biology, 2 Borwein, Peter, 23, 25 Broadhurst, David, 14, 17 Carath dory, Constantin, 43 eo Catalan, Eug ene Catalan numbers, 52 Catalan's constant, 54 Catalan, Eug` ene Catalan's constant, 20 Chaitin, Gregory, 59, 60 computer algebra systems, 2, 19 constant recognition, 19 continued fractions, 3538 numerical accuracy, 35 1 2  cosmology, 2 Coxeter, H. S. M. (Donald), 21, 21 4-D polytope, 22 kaleidoscope, 22 Crandall, Richard, 3, 56 Madelung's constant, 52 De Doelder, 15 Dedekind  -function, 45 Devlin, Keith, 44 Dijk paths, 52 economics, 2 Erd Paul, 46 os, Escher, Maurits C., 21 Euler, Leonhard Euler numbers, 89 Euler sums, 1516 quantum field theory, 17 Euler-Maclaurin summation formula, 5556 Euler-Zagier sums, see Euler sums Eureka (journal), 33 experimental mathematics, 2 Ferguson, Claire, 12 Ferguson, Helaman, 6, 1114 sculptures, 13, 1214 Ferguson, Samuel, 6 Fermat's Last Theorem, 4 Ferrer diagram, 44 Feynman, Richard, 53  In this Index,  denotes a figure. In this Index,  denotes a quote.  65   66 Feynman diagrams, 17 Fibonacci, Leonardo, 47 Fibonacci numbers, 52 Fibonacci sequence, 4750 fluid dynamics, 2 Gamma function, 42 G odel, Kurt, 1 Goldbach, Christian, 16 Gregory, James Gregory's series, 7, 9 Hadamard, Jacques, 5 Hales, Thomas, 6 Hamming, Richard, 7 Hardy, G. H., 5, 45 Hata, 34 Hilbert, David, 6 hypergeometric function calculation of  , 27 integer relation algorithm, 1011 LLL, 11 polynomials, 10 PSLQ, 10 integer relation algorithms, 50 Jacobi, Carl Jacobian theta functions, 46, 46 Julia sets, 59 Kanada, Yasumasa, 3 Kepler, Johanes, 6 Klein, James D., 41 Knuth, Donald, 26 Knuth, Donald E., 38 Lambert W function, 39 LLL, 11 logistic iteration, 9, 912  Index Bifurcation plot, 10 Lucas, Edouard, 48 Lucas numbers, 52 Lucas sequence, 48 MacMahon, Percy, 44 Madelung's constant, 52 Mahler, Kurt, 33 Mayer, Ernst, 3 Milnor, John, 5 Moore, Gordon Moore's Law, 2 Newton's method, 59 Newton, Isaac, 23 North, Joseph Roy, 7 Papadopoulos, Jason, 3 Parseval's equation, 15 partitions, 43 Percival, Colin, 3, 29 physics, 2 pi continued fraction, 32 estimates, 32 pictorial representations, 33  algorithms for calculating BBP algorithm, 2329 Biblical references, 30 calculations of, 3 formulas for calculating, 24, 27, 30 Plouffe, Simon, 8, 23, 25 Pochhammer function, 39 polylogarithms Euler sums, 16 PSLQ, 10, 12, 54 calculation of  , 27 Feynman diagrams, 17, 18 psychology, 2   Index quadrature, 53 error function quadrature, 5457 quantum physics, 2 Rademacher, Hans, 45 Ramanujan, Srinivasa, 45 Regiomontanus, Johann, 47 Riemann zeta function, 15, 39 Euler sums, 16 Feynman diagrams, 18 Sloane, Neil, 8 sociology, 2 Stirling, James Stirling's formula, 39 supernovas, 2 tangent numbers, 89 Taubes, Gary, 53 theta function Fibonacci sums, 4850 Turing, Alan, 1, 60 von Neumann, John, 1 Wiles, Andrew, 4  67"
GX049-00-14373160	"Profile Hidden Markov Model Analysis     [  Program Manual  |  User's Guide  |  Data Files  |  Databases  ]            Sensitive Database Searching and Identifying  Sequence Domains            Introduction         Profile analysis has long been  a useful tool in finding  and aligning distantly related sequences  and in  identifying known sequence domains in  new sequences.  Basically, a  profile is a description of  the  consensus of a multiple sequence  alignment.  It uses a  position-specific scoring system to capture  information about the degree of  conservation at various positions in  the multiple alignment.  This  makes it a much more  sensitive and specific method for  database searching than pairwise methods,  such as those used by   BLAST  or  FastA , that use  position-independent scoring.     Hidden Markov modeling, a technique  that has been used for  years in speech recognition, is  now being  applied to many types of  problems in molecular sequence analysis.   In particular, this technique  can  produce profiles that are an  improvement over traditionally constructed profiles.     Profile hidden Markov models (HMMs)  have several advantages over standard  profiles.  Profile  HMMs have a formal probabilistic  basis and have a consistant  theory behind gap and insertion  scores,  in contrast to standard profile  methods which use heuristic methods.   HMMs apply a statistical  method to estimate the true  frequency of a residue at  a given position in the  alignment from its  observed frequency while standard profiles  use the observed frequency itself  to assign the score for  that residue.  This means  that a profile HMM derived  from only 10 to 20  aligned sequences can be of  equivalent quality to a standard  profile created from 40 to  50 aligned sequences.  In  general,  producing good profile HMMs requires  less skill and manual intervention  than producing good  standard profiles.     The HMMER (pronounced  hammer ) package  developed by Sean Eddy of  Washington University in St.  Louis, Missouri, is a set  of programs that allow you  to create and manipulate profile  HMMs and  databases of profile HMMs (HmmerBuild,  HmmerConvert), perform sensitive searches of  sequence  and profile HMM databases, (HmmerSearch  and HmmerPfam) and create multiple  sequence  alignments efficiently (HmmerAlign).  In  collaboration with Dr. Eddy, GCG has  incorporated these  programs into the Wisconsin Package.        What is a Profile HMM?   - A Simplified Description         A profile HMM is a  linear state machine consisting of  a series of nodes, each  of which corresponds  roughly to a position (column)  in the alignment from which  it was built.  If  we ignore gaps, the  correspondence is exact -- the  profile HMM has a node  for each column in the  alignment, and each  node can exist in one  state, a match state.   (The word ""match"" here implies  that there is a position  in  the model for every position  in the sequence to be  aligned to the model.)       A profile HMM has several  types of probabilities associated with  it.  One type is  the  transition     probability  -- the probability of  transitioning from one state to  another.  In a simple  ungapped model,  the probability of a  transition from one match state  to the next match  state is 1.0 and the  path through the model is  strictly linear, moving from the  match state of node n  to  the match state of node  n+1.     There are also  emissions probabilities   associated with each match state,  based on the probability of  a  given residue existing at that  position in the alignment.   For example, for a fairly  well-conserved  column in a protein alignment,  the emissions probability for the  most common amino acid may  be  0.81, while for each of  the other 19 amino acids  it may be 0.01.    If you follow  a path through the model  to generate  a sequence consistent with the  model, the probability of any  sequence that is generated depends  on  the transition and emissions probabilities  at each node.     In order to model real  sequences, we also need to  consider the possibility that gaps  might occur when  a model is aligned to  a sequence.  Two types  of gaps may arise.   The first type occurs when  the  sequence contains a region that  is not present in the  model (an insertion in the  sequence).  The second  type occurs when there is  a region in the model  that is not present in  the sequence (a deletion in  the  sequence).  To handle these  cases, each node in the  profile HMM must now have  three states:  the  match state, an insert state,  and a delete state.   The model also needs more  types of transition  probabilities:  match->match, match->insert, match->delete, insert->match, etc.           Aligning a sequence to a  profile HMM is done by  a dynamic programming algorithm that  finds the  most probable path that the  sequence may take through the  model, using the transition and  emissions  probabilities to score each possible  path.     In general, if the sequence  is equivalent to the consensus  of the original alignment, the  path through  the model will pass from  match state to match state  in a linear fashion.   If the sequence contains a  deletion relative to the consensus,  the path passes through one  or more delete states before  transitioning to the next match  state; if the sequence contains  an insertion relative to the  consensus,  the path passes through an  insert state between two match  states.     For example, if a sequence  contains an insert that occurs  between nodes 5 and 6  of the model, the  path transitions from the node  5 match state to an  insert state.  It remains  in the insert state and  ""consumes"" residues in the sequence  until it reaches the residue  in the sequence that corresponds  to  node 6 in the model.   At this point the  path transitions from the insert  state to the node 6  match state.     Similarly, if the sequence contains  a deletion so that it  has no residues corresponding to  nodes 12  through 15 of the model,  the path transitions from the  node 11 match state into  a delete state, then  transitions through additional delete states  until it can transition to  the match state of node  16 of the  model.     Profile HMMs can be aligned  to a sequence either globally  (the whole profile HMM aligns  to the  sequence) or locally (only part  of the profile HMM need  be aligned with the sequence).   The alignment  type is actually part of  the model, so you must  specify whether the model is  to be global or local  at the  time the model is  built ,  not at the time the  model is used.  (See  HmmerBuild documentation for more  details.)        Most Common Uses for Profile  HMMs         Because a profile HMM can  serve as a representation of  a sequence family or sequence  domain, the  most common application is to  compare profile HMMs and sequences.   These types of comparisons  are  more likely to identify distant  homologs than sequence vs. sequence comparisons  used in most  database search programs.     For example, you can use  HmmerPfam to compare your sequence  to a database of profile  HMMs  representing known sequence families and  known sequence domains.  A  match to one of these  profile  HMMs can help you identify  your sequence and determine its  function.  The curated Pfam  (""Protein  families"") database contains a large  number of global profile HMMs  representing known protein  families, while the PfamFrag database  contains local profile HMMs for  these same families.     Similarly, you can create a  profile HMM representing a domain  or sequence family in which  you are  interested, then use this profile  HMM as a query to  search a sequence database with  HmmerSearch to  see if any other sequences  possess this domain.     Another use for profile HMMs  is to create a multiple  alignment of a large number  of sequences more  quickly than by using standard  methods.  HmmerAlign uses a  small seed alignment of representative  sequences to create a profile  HMM which is then used  as a template for aligning  the full set of  sequences.        Overview of the HMMER Programs         There are nine programs in  the GCG adaptation of the  HMMER package.  The following  five are used  to create and manipulate profile  HMMs:     HmmerBuild  -- creates a profile  HMM from a set of  pre-aligned sequences.  The profile  HMM can be  appended to a file containing  other profile HMMs in order  to create an HMM database  file.     HmmerCalibrate  -- calibrates an existing  profile HMM or profile HMM  database so that searches  performed with it will be  more sensitive.     HmmerConvert  -- converts a profile  HMM created by HmmerBuild into  other formats.     HmmerIndex  -- indexes a profile  HMM database so that profile  HMMs can be retrieved from  it easily  with HmmerFetch.     HmmerFetch  -- extracts a profile  HMM from an indexed profile  HMM database into a file.    The remaining four programs are  used for analyzing data:     HmmerSearch  -- searches a sequence  database with a profile HMM  query.     HmmerPfam  -- searches a profile  HMM database with a sequence  query.  The profile HMM  database  file may be one you  created as well as the  Pfam database created by Eddy  and collaborators.     HmmerAlign  -- efficiently creates a  large multiple alignment from a  small seed alignment and a  collection of unaligned sequences.     HmmerEmit  -- randomly generates sequences  that match a given profile  HMM.        Pfam Acknowledgement        Pfam - A database of  protein domain family alignments and  HMMs Copyright (C) 1996-2000 The  Pfam Consortium.        References          1.  Eddy, S.R., et al. (1996).   Hidden Markov Models.   Current  Opinion in Structural Biology ,      6 ; 361-365.     2.  Durbin, R., Eddy, S., Krogh,  A., and Mitchison, G. (1998).    Biological Sequence Analysis.       Probabilistic Models of Proteins and  Nucleic Acids , Cambridge University Press,  Cambridge, UK.     3.  Eddy, S.R. (1998).  Profile hidden  Markov models.   Bioinformatics ,  14 ; 755-763.                   Printed: February 5, 2001  15:21 (1162)         [  Program Manual  |  User's Guide  |  Data Files  |  Databases  ]       Technical Support:  support-us@accelrys.com  or  support-eu@accelrys.com      Copyright (c) 1982-2002 Accelrys Inc.  A subsidiary of Pharmacopeia, Inc.  All rights reserved.    Licenses and Trademarks Wisconsin Package is a trademark and GCG and the GCG logo are registered trademarks of Accelrys Inc.    All other product names mentioned in this documentation may be trademarks, and if so, are trademarks or registered trademarks of their respective holders and are used in this documentation for identification purposes only.       www.accelrys.com/bio"
GX255-61-7916521	"Date of revision: Saturday, July 21, 2001  Structural aspects of the fivefold quasicrystalline Al-Cu-Fe surface from STM and dynamical LEED studies  T. Caia, F. Shib, Z. Shena, M. Giererbc, A.I. Goldmand, M.J. Kramer,e C.J. Jenksa, T.A. Lograssoe, D.W. Delaneye, P.A. Thiela, and M.A. Van Hove bfg  a  Department of Chemistry and Ames Laboratory,  b  Materials Sciences Division, Lawrence Berkeley National Laboratory, University of California, Berkeley, CA 94720  c  Institut fr Kristallographie und Mineralogie der Universitt Mnchen, Theresienstr. 41, D-80333 Mnchen, Germany d  Department of Physics and Astronomy and Ames Laboratory,  e  Department of Materials Science and Engineering and Ames Laboratory, Iowa State University, Ames, IA 50011 f  Advanced Light Source, Lawrence Berkeley National Laboratory, University of California, Berkeley, CA 94720   Shi, Cai, et al. 2 g  Department of Physics, Univ. of California, Davis, CA 95616  Submitted to Surface Science, May 2001   Shi, Cai, et al. 3  Abstract We investigate the atomic structure of the fivefold surface of an icosahedral AlCu-Fe alloy, using scanning tunneling microscopy (STM) imaging and a special dynamical low energy-electron diffraction (LEED) method. STM indicates that the step heights adopt (primarily) two values in the ratio of tau, but the spatial distribution of these two values does not follow a Fibonacci sequence, thus breaking the ideal bulk-like quasicrystalline layer stacking order perpendicular to the surface. The appearance of screw dislocations in the STM images is another indication of imperfect quasicrystallinity. On the other hand, the LEED analysis, which was successfully applied to Al-Pd-Mn in a previous study, is equally successful for Al-Cu-Fe. Similar structural features are found for both materials, in particular for interlayer relaxations and surface terminations. Although there is no structural periodicity, there are clear atomic planes in the bulk of the quasicrystal, some of which can be grouped in recurring patterns. The surface tends to form between these grouped layers in both alloys. For Al-Cu-Fe, the step heights measured by STM are consistent with the thicknesses of the grouped layers favored in LEED. These results suggest that the fivefold Al-Cu-Fe surface exhibits a quasicrystalline layering structure, but with stacking defects.  Keywords: Theoretical methods, models & techniques: 1. Electron-solid interactions, scattering, diffraction   Shi, Cai, et al. 4  Experimental sample preparation and characterization methods: 2. Scanning Tunneling Microscopy (STM) 3. Electron-solid diffraction--Low Energy Electron Diffraction Phenomena: 4. Step formation and bunching Elemental and chemical identity: 5. Aluminum Compounds: 6. Alloys Surfaces: 7. Single crystal surfaces--Low index single crystal surfaces   Shi, Cai, et al. 5  1. Introduction The atomic structure at the surfaces of quasicrystals is a matter both of technological and fundamental interest. On the technological side, the highly-ordered but non-periodic bulk structure of these metallic alloys is tied to an unusual combination of physical properties. These properties have already led to some applications as coatings and composites, and may lead to more.[1, 2] On the fundamental side, a basic question is how the bulk structure of the three-dimensional quasicrystals (i.e., the icosahedral phases) responds to the two-dimensional truncation enforced by a surface. Another is how the bulk and surface structures relate to the macroscopic physical properties. In the present paper, we employ two techniques to gain insights into the atomic structure at the surface of one particular quasicrystal, icosahedral (i-) Al-Cu-Fe. The first technique used here is STM, which provides real-space information on some aspects of surface structure. The first report of an STM image of the 5f surface of this particular alloy, i-Al-Cu-Fe, was given by Becker et al., [3]. However, all subsequent work on surfaces of icosahedral materials was done with i-Al-Pd-Mn, and in these later studies a very detailed level of analysis emerged. Schaub, et al.[4-6] observed a stepterrace structure that displayed geometric characteristics expected for a bulk-terminated surface, namely, arrangements of features, both lateral and vertical, in Fibonacci sequences. Later, we provided an interpretation of the fine structure on the terraces in terms of the cluster structure of the bulk.[7] Very recently, Ledieu et al. have analyzed   Shi, Cai, et al. 6  the fine structure in terms of bulk tilings.[8] While these approaches and analyses differ, there is one main conclusion common to all of the three more recent studies (Schaub's, ours, and Ledieu's): The fine structure on the terraces of i-Al-Pd-Mn is consistent with bulk structural models. Hence, the horizontal structure, i.e. the structure within the surface plane, must be close or identical to that of the bulk. The step heights measured via STM present a different situation. While Schaub et al. reported only two values of step heights on i-Al-Pd-Mn, we found three; furthermore, we found the frequency of step heights as a function of height to be much different (qualitatively) than did Schaub et al., suggesting a significant difference in the layerstacking in the two studies. [4-7] Analysis of step height data are important for two reasons: First, they are a test of the vertical `perfection' of the quasicrystalline surface, i.e. of whether the layers are stacked in a bulk-like sequence; and second, the LEED I-V model predicts specific step heights by predicting separations between favored terminations. Hence, the step heights measured in STM can be cross-checked against the results of the LEED structure model. In this paper, we will report and analyze the step heights on 5f i-Al-Cu-Fe. In the second technique, we exploit the fact that surfaces of the icosahedral quasicrystals display sharp and dense LEED patterns.[5, 6, 9-16] Dynamical LEED analysis of experimental intensity-voltage (IV) curves can then be used to derive atomic structure, but for quasicrystals a special challenge comes from the non-periodic ordering of the atoms in the bulk. (A number of techniques, including LEED, have confirmed that the fivefold surfaces retain fivefold symmetry.[5-7, 9-20] Hence, it is reasonable to assume that a bulk-terminated surface is a good starting point in the structural analysis.)   Shi, Cai, et al. 7  It is impossible to define a unit cell as in periodic crystals; instead there is an infinite variety of local structures. Therefore an exact dynamical calculation of LEED IV curves for quasicrystals is not feasible, and certain approximations are required to make the analysis possible. Our previous study of the fivefold surface of i-Al-Pd-Mn [12, 21] showed that a successful dynamical analysis of a fivefold (5f) surface could be achieved. The main modification to the LEED theory was the ""average neighborhood approximation,"" according to which the scattering properties of atoms with similar neighborhoods were assumed to be identical. The success of the approximation was partly due to the fact that a few types of local structures repeat throughout a quasicrystal and thus dominate in the diffraction. For example, more than 50% of the atoms on the 5f surfaces of Al-Pd-Mn form pentagons. In addition, the structure of LEED IV curves primarily reflects local structure, mainly because of the short free mean path of the diffracting electrons. [22] It is reasonable to expect that the same analysis will work for i-Al-Cu-Fe. The bulk structures of i-Al-Cu-Fe and i-Al-Pd-Mn are very similar, with minor deviations due mainly to differences in composition: the changed composition fractions result primarily in substitution of chemical identities, and secondarily in a small number of added or deleted atoms in some sites. This paper presents the outcome of that analysis, for the 5f surface. The organization of the paper follows. First, experimental details, both of LEED and STM, are presented in Sec. 2. The theory of the dynamical LEED of quasicrystals is summarized in Sec. 3. In Sec. 4, the experimental STM data, including step height distributions, are presented. In Sec. 5, the dynamical LEED analysis on Al-Cu-Fe is   Shi, Cai, et al. 8  described. In Sec. 6, the results are compared with those for Al-Pd-Mn and conclusions are drawn.  2. Experimental Description. Samples were grown, characterized, and prepared as discussed in previous papers.[23, 24] The two samples used in STM were adjacent slices from the same single grain, whose composition was determined via inductively coupled plasma-atomic emission spectroscopy (ICP-AES) to be Al 61.5  Cu24.8Fe 63.4  13.7  . The composition of the sample 12.6  used in LEED was determined similarly to be Al  Cu24.0Fe  . In both the STM and  LEED experiments, we cleaned the sample in ultrahigh vacuum with cycles of ion etching and annealing. In the STM experiments, we used He ions, whereas in the LEED experiments, we used Ar ions. [13] Etching with Ar is known to shift the surface composition far away from the icosahedral region of the phase diagram in the Al-rich quasicrystals. [5, 11, 13, 14, 25-29] Helium was chosen in the STM experiments (done more recently than the LEED experiments) to reduce preferential etching of Al.[30] A fresh sample, introduced from air, was typically cleaned by sputtering for 30 minutes, followed by 30 minutes of annealing, starting at 450 K and stepping up by 50 K when significant carbon and oxygen were no longer detected by AES at a given temperature. The sample was heated to a maximum temperature of 875 K. STM and LEED experiments were performed in three separate ultrahigh vacuum chambers. The two STM chambers were each equipped with an Omicron STM, instrumentation for Auger electron spectroscopy (AES), a mass spectrometer, ion   Shi, Cai, et al. 9  sputtering gun, sample heating capability, and a manifold for introduction of selected pure gases by backfilling. A typical base pressure during STM measurements was 2 to 6 x 10 -11  Torr. The STM samples were each about 3x4 mm2 in area, and 1.5 mm thick.  Before each STM measurement, the sample was sputtered for 30 minutes with He (1.0 KeV beam voltage, 8-10 A from sample to ground with no bias), annealed at the stated temperature for one to two hours, and cooled down to room temperature. The typical tunneling current for the STM measurements was 0.3-0.5 nA, and the tunneling voltage was 1.0 V. In the course of measuring the step heights with STM, we used two different scanners. Comparison revealed that it was important to calibrate the piezoelectrics accurately. We used Ag(100) as the standard, with known atomic step heights of 2.04 . Without this in-house calibration, errors up to 40% would have resulted. The LEED chamber and measurements have been described previously.[13, 21]  3. Experimental STM Data. After annealing at temperatures lower than 825 K, STM reveals a rough morphology with cluster-like protrusions. These protrusions have different sizes, varying from 1 nm to 2.5 nm in diameter (Fig. 1a). Terraces start to appear at about 825 K, though still dotted by clusters (Fig. 1b). At higher annealing temperatures (850  875 K), a step-terrace morphology predominates (Fig. 1c, 1d). This cluster-to-terrace sequence is similar to the progression of structures that we, and others, have reported already for 5f[7, 31, 32] and 3f [33] surfaces of i-Al-Pd-Mn. Commonly, the well-annealed surfaces   Shi, Cai, et al. 10  exhibit apparent screw dislocations, as shown by the black arrow in Fig. 1e, and by the white arrow in Fig. 1f. It is also common that the large terraces contain broad but shallow pentagonal pits (black arrows in Fig. 1f). These two features--the screw dislocations, and the pentagonal pits--are distinctive, since we never observe screw dislocations and pentagonal pits on the 5f Al-Pd-Mn surfaces. The fine structure on the terraces after high temperature annealing is shown in Fig. 1g-h. It consists of many flower-like features that are approximately 18  in diameter, (most obvious in Fig. 1h) and of small black holes arranged in Fibonacci pentagrids (most obvious in Fig. 1g). The flowers are presumably the same as the ""daisies"" noted earlier by Becker, et al..[3] The origin of the fine structure on this surface will be discussed elsewhere. The corrugation in Fig. 1g is about 0.5  peak-to-peak. (Statistical analysis gives 0.25 for the root mean square, and 0.18 as the arithmetic mean.) The main point is that the corrugation on the terraces is much smaller than the step heights, and hence does not interfere significantly in the following analysis. We also analyzed the step heights carefully. Determination of individual step heights was problematic, for two reasons. First was the sloping background evident in Fig. 2, which could not be corrected satisfactorily by planing the entire image--either due to STM drift, or to a meandering, large-scale curvature of the surface. Second was the fact that the step height determined from a single line profile varied significantly, depending upon the exact point where the profile cut across the step. Hence, we devised a procedure to obtain a statistical average of heights measured along a continuous step, within localized regions of the image. For background correction, we used standard Omicron software to plane the original image. For individual step heights, we used the   Shi, Cai, et al. 11  Omicron software to construct histograms of pixel intensities in the immediate vicinity of each step. These histograms showed the frequency of z-values versus z within the local area bridging two neighboring terraces. This resulted in a histogram with two peaks, one corresponding to the lower terrace and the other to the upper terrace. We took the separation of the peaks as the step height across the two terraces. A typical histogram for a single step height measurement spanned roughly 300 nm2. An example of one such histogram is shown in Fig. 3, and corresponds to the pixel height distribution encompassed within the rectangular box of Fig. 1c. We avoided steps originating obviously from screw dislocations in the analysis of the step height distributions. Steps originating from screw dislocations predominantly displayed heights of 2.5 . The entire set of values of step heights is illustrated in the distribution of Fig. 4. We see three step heights: 2.5 , 4.0  and 6.2-6.6 . These values form consecutive ratios of 1.6 and 1.55-1.65, i.e. close to the golden mean, =1.618. The step height of 2.5  is found much less frequently than the other two values. For instance, in Fig. 4, we have 30 steps with height of 6.2 , compared to 5 with a height of 2.4 . Line profiles across series of terraces display sequences of step heights such as those illustrated in Fig. 2. Steps are labeled as low (L) and high (H), corresponding to heights of about 4  and 6 , respectively. If the surface were perfectly bulk-terminated, one would expect to see a Fibonacci sequence of L and H steps, in which sequences such as L-L-L would be forbidden.[34] The fact that we observe the forbidden L-L-L   Shi, Cai, et al. 12  sequences indicates that the surface cannot be perfectly bulk-terminated; instead, it appears that layers--or, more likely, groups of layers--are stacked imperfectly. The observation of a relatively high density of screw dislocations on this surface (on the order of 3 dislocations per 200  x 200 ) is also consistent with imperfections in the layer stacking sequence. In a regular periodic lattice, as one spirals around a screw dislocation, all layers line up properly again after each turn, preserving a simple sequence of equal step heights. But in a quasicrystal, because of the non-periodic spacings between layers, most layers do not line up correctly when spiraling around a screw dislocation, leading to mismatches and thus stacking errors. For example, if one propagates a step of height L around a screw dislocation (as in spiral growth), and if that step height is maintained, it will create a sequence of heights L-L-L-..., adding one height L at each turn. This simple example, if continued, leads to a periodic sequence. The exact relationship between screw dislocations and the non-Fibonacci step height sequences is not clear at this stage. Among other things, such a relationship must explain how the screw dislocations generate 2.5  step heights in their immediate vicinity, whereas the non-Fibonacci sequences include 4.0 and 6.2-6.6  steps (Fig. 2); for example, a 2.5  step may join another step and increase or decrease its height by 2.5  (Fig. 1e). It is nonetheless true that the screw dislocations and the non-Fibonacci step heights are both manifestations of imperfect quasicrystalline stacking, such as locally periodic stacking. Periodic sequences of interlayer spacings have in fact been observed by highresolution transmission electron microscopy (TEM) in defect areas of bulk i-Al-Cu-Fe samples. Similar periodic regions have been observed by TEM in the types of samples we   Shi, Cai, et al. 13  use, and are associated with strain fields. Strain fields arise because the samples are first prepared via liquid-assisted growth, then hot-isostatically-pressed to reduce porosity, and finally annealed again to reduce strain from the pressing. Strain fields become particularly abundant after the hot isostatic-pressing step, as shown by comparing the TEM images of Fig. 5a and 5b. Fig. 5a is a TEM image of an as-grown sample, whereas Fig. 5b shows a sample immediately after pressing. The strain fields are reduced considerably, although not eliminated, after the final annealing. Fig. 5c is a TEM image after the final treatment, showing a region that is locally perfect. We speculate that remnant strain fields from the hot-pressing procedure may play a role in the imperfections at our surfaces.  4. Sketch of dynamical LEED theory for quasicrystalline surfaces In spite of their non-periodicity, quasicrystals often produce LEED patterns with a set of well-defined spots, as shown in Fig. 6. The LEED patterns can be even sharper and clearer than for normal crystals; quasicrystals are thus structurally very different from amorphous materials. The sharp LEED pattern is evidence that quasicrystals are a class of ordered materials with particular long-range order and orientation symmetries; in fact they can be described with self-similar models (i.e. as structures that can be scaled up by constant factors to yield similar structures at different length scales). It has been proven in many studies that the Fourier transform of a quasicrystal structure is well ordered in reciprocal space, although again not periodic.[34] In LEED, the Fourier transform corresponds to single scattering theory. Just as with periodic crystals, multiple scattering does not change the LEED pattern in reciprocal lattice, but modifies the LEED spot   Shi, Cai, et al. 14  intensities. The LEED IV curves for these patterns have the same qualitative appearance as for a normal crystal, and are illustrated in Fig. 7 and Fig. 8, where diffraction spot intensity is plotted as a function of momentum transfer parallel to the surface, k||. As a starting point for the structural LEED analysis, we need a source of bulk atomic coordinates with icosahedral symmetry. Quasicrystals are most conveniently represented as a periodic bcc lattice in 6 dimensions. The ""atoms"" in that lattice are replaced by atomic hypersurfaces. Tricontahedral atomic hypersurfaces, instead of the usual spherical ones, are used in our models for Al-Cu-Fe and Al-Pd-Mn, in order to yield reasonable bond lengths. [35] The three-dimensional structure is generated as a particular projection from that six-dimensional bcc lattice into a 3-dimensional space, and the two-dimensional surface is obtained by terminating the three-dimensional lattice. The bulk structure of Al-Cu-Fe is determined by x-ray and neutron diffraction, which fix the atomic hypersurfaces. Once the 3-dimensional bulk structure of Al-Cu-Fe is determined, a 5f surface is obtained by rotating the bulk structure so that the surface plane is perpendicular to the 5f axis. The resulting atomic structure along the surface normal shows well-defined and separated atomic planes, with one, two or three chemical types of atoms in each plane. (We shall use the term ""plane"" for sets of coplanar atoms and ""layer"" for more general sets of atoms, including groups of atomic ""planes"", as in ""composite layers""). The interplanar spacings vary systematically according to Fibonacci sequences and the Golden mean. As a result of the non-periodicity, each plane is different. If we assume that the surface terminates along such atomic planes, there is, strictly speaking, an infinite number of inequivalent terminations. Within each atomic plane, although there is no   Shi, Cai, et al. 15  repeating unit cell, one finds repeating local 5f rings throughout the plane. The atomic density of different planes varies strongly in the range from 100 to 800 atoms in an area of 100  x 100 . The smallest size of the local rings varies with the atomic density. These planes are found to belong to subsets of planes with very similar compositions, atomic densities and geometry. Thus, the infinite variety of planes actually has a relatively small number of distinct types of structure, and we shall exploit this property. The major difficulty in the LEED analysis of quasicrystals comes from multiple scattering. In a single scattering LEED theory, which assumes that all electrons are scattered only once before they leave the surface, the outgoing wave amplitude from an atom depends only on the chemical identity and the position of the atom. In dynamical LEED, however, the outgoing wave amplitude from an atom is composed not only of that directly scattered wave, but also of waves that have undergone other scattering events within the surface. The latter events depend not only on the chemical identity and position of the scatterer, but also on the environment of the scatterer. In principle, the environment of every atom is different from that of every other atom, measured on an infinitely large scale. Therefore, the total wave amplitude of outgoing electrons from any given atom is different from that from all other atoms. In this sense, there are an infinite number of different atoms that have to be taken account in a dynamical LEED analysis of quasicrystals. This is fundamentally different from the dynamical LEED theory of periodic crystals, in which the number of different atoms is finite, due to the repeating unit cells. As a result, certain approximations have to be made in a realistic dynamical LEED analysis of quasicrystals. In this paper, we apply efficient approximations that   Shi, Cai, et al. 16  were tested in a previous LEED analysis of an i-Al-Pd-Mn surface;[21] these are sketched in the following. The basic idea is that the local environments of atoms tend to have only a few basic structural arrangements, as described above. Due to the limited mean free path of propagating electrons within the surface, the LEED IV curves are determined primarily by the scattering within such local environments. The different approximations depend on how we treat the similar local environments in a quasicrystal lattice. More specifically, the approximations in our theory take into account that the atoms are explicitly arranged in atomic planes parallel to the 5f surface in a bulk terminated quasicrystal. The atoms within a given atomic plane are typically evenly distributed in space. The atomic density measured per unit area is uniform within a plane, but fluctuates from plane to plane, in the range 0.01-0.13 atoms/2. The ""effective"" atomic scattering properties (including all multiple scattering events) are more alike within an atomic plane than between different atomic planes, as the average scattering properties of atoms in an atomic plane depend largely on the composition and the atomic density of that plane. Therefore, as an approximation, we assume that all atoms within a certain atomic plane are equivalent, but different from atoms in other planes. This approximation, in the single scattering theory, implies that the scattering properties of an atom are averaged over chemical identities. It leads to the average tmatrix approximation (ATA),[36] [37] [38] according to which the scattering matrices t of the individual atoms within one plane are replaced by an averaged scattering t-matrix:   Shi, Cai, et al. 17  t = c Al t  Al  + cCut  Cu  + c Fe t Fe ,  where Ci are the relative concentrations within the plane. Next, as an approximation in the multiple scattering LEED theory, the variable environments of the atoms in a particular plane are replaced by a fixed, simplified average geometry, referred to as ""average neighborhood approximation"" (ANA). More specifically, the final wave amplitude from an atom, with all multiple scattering events considered, is replaced by an averaged wave amplitude over all atoms in the plane. The ATA is applied before the wave amplitude is averaged. For more details, see Ref. [21]. With these approximations, we can perform the calculation with a relatively small number of atoms, equivalent to the number of atomic planes, with different planedependent scattering properties. If we take atomic planes as deep as 10  into account, we obtain about 12 planes, i.e. 12 atoms with different scattering properties. The incident beam is damped into the surface due to inelastic scattering events, so that the contributions of deeper atoms do not influence the IV curves significantly. The calculation of the averaged propagator matrix G which describes electron propagation within and between closely-spaced planes, is performed by averaging in an area of 100  x 100 . The averaged wave amplitude depends strongly on the atomic density of the plane. For a plane with high atomic density, the averaged amplitude is affected by multiple scattering events within the plane more than that between planes. Inversely, for a plane with low atomic density, it is affected more by the multiple scattering events between atomic planes than by that within the plane.   Shi, Cai, et al. 18  The ANA, in which all atoms in a particular atomic plane have the same averaged effective scattering properties, can be improved in several respects. First, it is possible to use the ANA without the ATA. In this case, a single coplanar atomic plane is divided into several mono-atomic planes such that each has only one chemical species. All atoms in one mono-atomic plane are then assumed to have the same scattered wave amplitude. The computation time increases because one has to deal with more mono-atomic planes, i.e. more atoms in the calculation. It has been demonstrated for Al-Pd-Mn that most IV curves are little affected by the use of the ATA: thus one can safely apply ATA and gain computational time. The ANA may be further improved if one divides an atomic plane into several subplanes in which the atoms in each subplane have exactly the same local environment within and out of the plane. For example, one can assume that the local environments of two atoms are similar only if the nearest-neighbor distances and the number of nearest-neighbors are equal. All the atoms are thus sorted into a few classes of atoms, each with the same local neighborhoods. Notice that the number of different atoms increases sharply if the size of the local neighborhood increases. In the LEED calculation for Al-Pd-Mn 5f surfaces, it was shown that the IV curves are also very similar under this approximation. This at least shows that the ANA is reliable for the 5f quasicrystal surfaces. However, the approximation was found to be less successful for the surfaces of Al-Pd-Mn with lower symmetry, such as the two- and three-fold surfaces. [39]  5. LEED structural analysis   Shi, Cai, et al. 19  Some of the experimental LEED IV curves are shown in Fig. 7 and Fig. 8. Normal incidence is established by optimizing the agreement between curves for different, but symmetry-equivalent, diffraction spots, as shown in Fig. 7. Each curve shown in Fig. 8 is a symmetry-averaged composite, normalized to approximately the same value. Different curves represent different sample treatments. We varied sample treatments in order to check the robustness of the IV data. In all, we reproduced the complete set of IV curves 9 times. We conclude that the IV curves are not sensitive to sample preparation conditions, within the range of sputtering conditions (15-30 minutes with Ar+) and annealing conditions (1-2 hours at 800-850 K) tested. This is similar to our previous findings for 5f i-Al-Pd-Mn, where the experimental IV curves were also found to be very robust. In the LEED analysis of the 5f Al-Cu-Fe surface, we first explored individual bulk terminations, with possible relaxations of the top few interplanar spacings. Later we also considered combinations of terminations. For the calculation, we used relativistic phase shifts of Al, Cu and Fe, which have also been used in the LEED analysis of the (110) surface of crystalline -Al1-x(CuFe)x with a similar composition. [40] Thermal effects were included through the usual Debye-Waller factor. The Pendry R- factor was used for comparison between theory and experiment. We thus started by analyzing a large set of individual terminations of the 5f AlCu-Fe surfaces, namely those that exist within a rectangular box with surface dimensions of 100  x 100  and a depth of 50 . Those terminations run through all the atomic planes along the surface normal in the entire depth of the box. For each termination, the top 12 atomic planes are chosen as a composite layer for LEED calculation. At first, in a   Shi, Cai, et al. 20  rough search, we only allowed the relaxation of the topmost interplanar spacing using a grid search, i.e. the R-factor was calculated on a grid of points in the range from -0.3 to 0.3 . The resulting R-factors for all the terminations that we examined are plotted in Fig. 9. It is seen that the R-factor varies from about 0.5 to 0.9 for different terminations. The R-factor drops if more planes are allowed to relax, but it is found that the change in R-factors is much smaller than the fluctuations between R-factors for different terminations. So the R-factors shown in Fig. 9 are reliable as a basis for further analysis. It is also interesting to find that the different terminations yield very similar optimized structural parameters. Next, we focused on the most promising surface terminations as given by the Rfactors shown in Fig. 9. We interpret the ""better"" terminations of Fig. 9 to be all present on the real surface, forming an array of terraces separated by steps of variable height. The favored step heights can be understood in terms of groups of closely-spaced planes. Even though there is no periodic repetition in the quasicrystalline bulk structure, certain groups of planes occur frequently (even if not identically). We recognize three main sets of grouped planes, each group containing 3, 5 or 9 atomic planes (they can be identified in Fig. 9, while the 5- and 9-plane cases are also illustrated in Figure 10; note that there sometimes exist additional planes of very low atomic density between these planes, resulting in 11-plane groups, for instance). Within each group, the distance between successive atomic planes is smaller than 0.78 . The different groups are mutually separated by at least a distance of 1.5-1.6 . The two large groups with 5 and 9 planes share some common features. Each group has an approximate central symmetry   Shi, Cai, et al. 21  around the middle plane. Both outer surfaces of such a group have two planes separated by only 0.48 ; these planes have the highest combined atomic density. It is found that the surface tends to form between these grouped planes, i.e. by splitting through the relatively large spacings of 1.5-1.6 , and thus exposing the closely-spaced pairs of planes. Since the outermost plane within these pairs has a high aluminum concentration and the other an Al:Cu ratio of about 50:50, these pairs are enriched in aluminum compared to the bulk average. Their combined surface atomic density is close to that of the densely-packed Al(111) crystalline surface. The step heights between terraces with such terminations are obtained by measuring the distances between two kinds of terminations, which are 3.99  for 5-plane steps (Fig. 10 shows one such step) and 6.47  for 9-plane steps; 3-plane steps give 2.40  heights. (Spacing relaxations, if identical for the terminations involved, do not change these distances). The next stage in our LEED analysis mixes different terminations. In a full LEED analysis of quasicrystals, one can explicitly take into account the coexistence of terraces with different types of terminations. Since they are all assumed to contribute to the LEED IV curves, one needs to mix their contributions. Assuming that the area of each terrace is large compared to the coherence width of the electron beam (which is justified by the STM observations in Section 3), then only the reflected intensities from different terraces need to be mixed, as opposed to reflected amplitudes. As shown both in the LEED analysis and in the STM data (Sec. 3), the frequencies of the three kinds of terminations are different. To reduce the number of fit parameters, we take into account that the terminations in 5-plane groups appear to have the better R-factors, and mix only this kind of termination. Since they have essentially the same structure perpendicular to the   Shi, Cai, et al. 22  surface, the number of fit-parameters can be reduced to a few, representing the number of interplanar spacings that one wishes to fit. We thus mixed the 6 terminations that yielded the best R-factors (indicated by the short arrows in Fig. 9) and gave them equal weights. We also allowed relaxation of the top 5 interplanar spacings. This was done by the linear LEED approximation.[41] In this very efficient approach, the wave fields are calculated for one reference and several trial structures. Then the Powell optimization scheme is applied to optimize the structure. The linear LEED approximation is valid for this material if the deviations between reference and trial structures are smaller than about 0.2  (which was always the case in our analysis). As a result, the R-factor could be reduced to 0.39, which we take as our optimum fit. The comparison between the experimental and theoretical IV curves for this best-fit structure is shown in Fig. 11, and the structure itself in Fig. 12 (again, this represents an average best-fit structure for the 5-plane terminations). The topmost interplanar spacing contracts from the bulk value by 0.100.08 , going from 0.48  to 0.38, while the second interplanar spacing expands, the third interplanar spacing contracts and the deeper spacings are close to bulk values, cf. Fig. 12. Since the third plane has a low atomic density, the error bar for the second and third interplanar spacings is large (comparable to the changes from the bulk values). We assumed the bulk composition for each layer, since the earlier work with AlPd-Mn showed little sensitivity of LEED to these compositions.[21] Thus, on average, the bulk composition of the outermost atomic plane is 90% Al and 10% Fe atoms, while the second atomic plane contains 45% Al, 45% Cu and 10% Fe atoms. The average   Shi, Cai, et al. 23  lateral density of the two top planes taken together is 0.14 atoms/2, a value very close to that of a single plane of Al(111), 0.141 atoms/2. This indicates a very densely packed surface double-layer. We notice that the optimized structure for a single termination is exactly the same as that for mixed terminations. This again indicates that the selected terminations are very similar in structure. From Fig. 9 it is to be noted that some 9- and 3-plane terminations give R-factors that are not much worse than those for 5-plane terminations. While the LEED analysis cannot be used to state that these terminations are also present on the surface or with what relative abundance, at least the LEED analysis is consistent with the presence of some 9- and 3-plane terminations on the surface: the LEED results are therefore also compatible with the STM data, which indicate the presence of several step heights. Finally, we note that other authors have suggested that three step heights may be a normal situation on 5fold surfaces of icosahedral quasicrystals. Fradkin [42, 43] has shown that three step heights are to be expected on supercooled quasicrystalline samples along the fivefold axis. The relative population of these three step heights depends upon the extent of supercooling, i.e. the extent of deviation from equilibrium. At the exact points where one population disappears, the ratio of the other two populations should be  = 1.61.... In Fig. 4, the 2.4  population is very small, and in the context of the Fradkin model, one might think that it is on the verge of disappearing. The ratio of the three populations is about 9%:37%:54%, i.e. the two largest populations are in the ratio of about 1.5. While this value is not exactly , it is close enough to be qualitatively in accord with the hypothesis of Fradkin.[42, 43]   Shi, Cai, et al. 24  Another prediction has been made recently by Gratias, et al., [44] who, on the basis of the specific structural model for i-Al-Cu-Fe, forecast that steps should adopt three values of heights with predictable frequencies. These heights (frequencies) are, relative to the longest step, L: L (31%); L/ (51%); and L/2 (19%), i.e. the two largest populations should be in the ratio of 1.65. Again, we see that our experimental value of 1.5 is only slightly lower than expected. Also, our experimental values of the step heights (2.5, 4.0, and 6.2-6.6 ) follow the predicted ratio of L, L/, and L/2 exactly.  6. Conclusions We have analyzed the atomic-scale structure of the 5f surface of an Al-Cu-Fe quasicrystal by STM imaging and a special dynamical LEED analysis. Basically, the surface is bulk-terminated with presumably a bulk-like layer-dependent composition. Our analysis finds that the surface tends to form between different groups of closely-spaced planes, i.e. by splitting the quasicrystal through the larger interplanar spacings that separate those groups. In particular, we find that those grouped planes can be sorted into three sets, with 3, 5 and 9 planes, respectively. The interplanar spacings are the same for the same set of grouped planes. The 5-plane and 9-plane groups are similar in that the top pair of planes have the highest combined atomic density, about 0.14 atoms/2--very close to that of the Al(111) surface--and are rich in Al. The sets of 3, 5, and 9 planes correspond to step heights of 2.47, 3.99, and 6.47 , respectively. The step height frequencies seen in STM hence favor the 9-plane steps, then the 5- plane steps and finally the 3-plane steps (Fig. 4). LEED does not measure step   Shi, Cai, et al. 25  heights directly: the R-factors favor 5-plane terminations, then 9- and 3-plane terminations. The LEED R-factors contain information both on the quality of fit of individual coexisting terminations, and on the frequency of occurrence of the different terminations; however, it is not clear how one could extract these two aspects separately from the data. Nonetheless, the fact that LEED and STM favor the same sets of step heights indicates that the two techniques are in overall agreement on the types of terminations present at the surface. The measured distribution is also roughly consistent with the predictions of Fradkin,[42, 43] and of Gratias, et al.[44] The sequence of step heights in STM (Fig. 2) suggests that there are defects in layer stacking, relative to a perfect bulk quasicrystalline termination. LEED shows that groups of planes are separated by distances of 1.4-1.6  with no atoms in between, as illustrated in Fig. 10. We speculate that ""mistakes"" in the quasicrystalline lattice, normal to the surface, occur between these groups of planes. In other words, we hypothesize that quasicrystalline order is maintained well within favored groups of planes, where atoms interconnect densely throughout the structure, but less well between groups of planes. Perhaps these mistakes occur as the surface regrows after ion bombardment. Or perhaps they are engendered by defects in the bulk structure incorporated during sample preparation (Fig. 5). In either case, this hypothesis provides some understanding for the observation that there is stronger evidence for defects perpendicular to the surface than for defects within, or very near to, the surface plane. Finally, it is interesting to compare the present results for 5f i-Al-Cu-Fe, and those reported previously for 5f i-Al-Pd-Mn. The main difference is that the i-Al-Cu-Fe surface contains screw dislocations and pentagonal pits, neither of which has been   Shi, Cai, et al. 26  observed on i-Al-Pd-Mn, in our laboratory and under analogous conditions. Our general impression is that the i-Al-Cu-Fe surface contains more defects, perhaps a remnant of the hot-pressing step which is a part of sample preparation, as discussed in Section 3. On the other hand, there are several similarities between the two types of surfaces. Both have been analyzed successfully by dynamical LEED theory. Similar R-factors are obtained for both materials. The preferred terminations, atomic densities, and chemical compositions are similar. As a result, the surface in both cases is aluminum-rich. The implied step heights are also very comparable. And the outermost interplanar spacing is contracted on both materials by 0.1 , leaving deeper spacings bulk-like within the error bars. Further discussion of geometric aspects of these surface structures can be found in ref. [21], given that the two materials are structurally so similar.  Acknowledgments This work was supported by the Director, Office of Science, Office of Basic Energy Sciences, Materials Sciences Division of the U.S. Department of Energy under Contract Nos. DE-AC03-76SF00098 and W-405-Eng-82.   Shi, Cai, et al. 27  References. 1. P. A. Thiel, J. M. Dubois, Materials Today 2 (1999) 3.  2.  Z. M. Stadnik, Physical Properties of Quasicrystals, M. Cardona, P.  Fulde, K. v. Klitzing, R. Merlin, H.-J. Queisser, H. Strmer (Series Ed.) Springer Series in Solid-State Sciences, Vol. 126, Springer-Verlag, Berlin, 1999.  3.  R. S. Becker, A. R. Kortan, F. A. Thiel, H. S. Chen, J. Vac. Sci. Technol.  B 9 (1991) 867.  4.  T. M. Schaub, D. E. Brgler, H.-J. Gntherodt, J. B. Suck, Phys. Rev.  Lett. 73 (1994) 1255.  5.  T. M. Schaub, D. E. Brgler, H.-J. Gntherodt, J. B. Suck, M. Audier,  Appl. Phys. A 61 (1995) 491.  6. (1994) 93.  T. M. Schaub, D. E. Brgler, H.-J. Gntherodt, J.-B. Suck, Z. Phys. B 96   Shi, Cai, et al. 28  7. (1999) 14688.  Z. Shen, C. Stoldt, C. Jenks, T. Lograsso, P. A. Thiel, Phys. Rev. B 60  8. J. Ledieu, R. McGrath, R.D. Diehl, T.A. Lograsso, D.W. Delaney, Z. Papadopolos, G. Kasner, Surface Sci. Lett. (2001) in press.  9.  T. M. Schaub, D. E. Brgler, H.-J. Gntherodt, J. B. Suck, M. Audier,  ""Fivefold symmetric LEED patterns measured on icosahdral Al68Pd23Mn9,"" in: C. Janot, R. Mosseri (Ed.) Proceedings of the 5th International Conference on Quasicrystals (ICQ5), Vol. World Scientific, Singapore, 1995, p. 132.  10.  X. Wu, S. W. Kycia, C. G. Olson, P. J. Benning, A. I. Goldman, D. W.  Lynch, Phys. Rev. Lett. 75 (1995) 4540.  11.  S.-L. Chang, W. B. Chin, C.-M. Zhang, C. J. Jenks, P. A. Thiel, Surf. Sci.  337 (1995) 135.  12.  M. Gierer, M. A. Van Hove, A. I. Goldman, Z. Shen, S.-L. Chang, C. J.  Jenks, C.-M. Zhang, P. A. Thiel, Phys. Rev. Lett. 78 (1997) 467.   Shi, Cai, et al. 29  13.  Z. Shen, P. J. Pinhero, T. A. Lograsso, D. W. Delaney, C. J. Jenks, P. A.  Thiel, Surface Sci. 385 (1997) L923.  14.  J. Ledieu, A. Munz, T. Parker, R. McGrath, R. D. Diehl, D. W. Delaney,  T. A. Lograsso, Surface Science 433-435 (1999) 666.  15.  G. Cappello, A. Dechelette, F. Schmithsen, J. Chevrier, F. Comin, A.  Stierle, V. Formoso, M. de Boissieu, T. Lograsso, C. Jenks, D. Delaney, ""Characterization and properties of the AlPdMn 5 surface,"" in: J. M. Dubois, P. A. Thiel, A. P. Tsai, K. Urban (Ed.) MRS Proceedings: Quasicrystals, Materials Research Society Symposium Proceedings Vol. 553, MRS, Boston, 1999, p. 243.  16.  Z. Shen, W. Raberg, M. Heinzig, C. J. Jenks, V. Fourne, M. A. V. Hove,  T. A. Lograsso, D. Delaney, T. Cai, P. C. Canfield, I. R. Fisher, A. I. Goldman, M. J. Kramer, P. A. Thiel, Surface Science 450 (2000) 1.  17.  M. Erbudak, H.-U. Nissen, E. Wetli, M. Hochstrasser, S. Ritsch, Phys.  Rev. Lett. 72 (1994) 3037.  18.  D. Naumovic, P. Aebi, L. Schlapbach, C. Beeli, ""A real-space and  chemically-sensitive study of the i-AlPdMn pentagonal surface,"" in: A. I. Goldman, D. J.   Shi, Cai, et al. 30  Sordelet, P. A. Thiel, J. M. Dubois (Ed.) New Horizons in Quasicrystals: Research and Applications, Vol. World Scientific, Singapore, 1997, p. 86.  19.  D. Naumovic, P. Aebi, L. Schlapbach, C. Beeli, T. A. Lograsso, D. W.  Delaney, ""Study of the 5-fold and 2-fold i-AlPdMn surfaces by full-hemispherical x-ray photoelectron diffraction,"" in: S. Takeuchi, T. Fujiwara (Ed.) Proceedings of the 6th International Conference on Quasicrystals (ICQ6), Vol. World Scientific, Singapore, 1998, p. 749.  20.  R. Bastasz, C. J. Jenks, T. A. Lograsso, A. R. Ross, P. A. Thiel, J. A.  Whaley, ""Low-Energy Ion Scattering Measurements from an Al-Pd-Mn Quasicrystal,"" in: E. Belin-Ferr, P. A. Thiel, K. Urban, A.-P. Tsai (Ed.) MRS Conference Proceedings: Quasicrystals, Vol. MRS, Warrendale, NJ, 2001,  21.  M. Gierer, M. A. Van Hove, A. I. Goldman, Z. Shen, S.-L. Chang, P. J.  Pinhero, C. J. Jenks, J. W. Anderegg, C.-M. Zhang, P. A. Thiel, Phys. Rev. B 57 (1998) 7628.  22.  D. K. Saldin, J. B. Pendry, M. A. Van Hove, G. A. Somorjai, Phys. Rev.  B31 (1985) 1216.   Shi, Cai, et al. 31  23.  C. J. Jenks, P. J. Pinhero, Z. Shen, T. A. Lograsso, D. W. Delaney, T. E.  Bloomer, S.-L. Chang, C.-M. Zhang, J. W. Anderegg, A. H. M. Z. Islam, A. I. Goldman, P. A. Thiel, ""Preparation of icosahedral AlPdMn and AlCuFe samples for LEED studies,"" in: S. Takeuchi, T. Fujiwara (Ed.) Proceedings of the 6th International Conference on Quasicrystals (ICQ6), Vol. World Scientific, Singapore, 1998, p. 741.  24.  C. J. Jenks, D. W. Delaney, T. E. Bloomer, S.-L. Chang, T. A.  Lograsso, Z. Shen, C.-M. Zhang, P. A. Thiel, Appl. Surf. Sci. 103 (1996) 485.  25. 95.  S.-L. Chang, J. W. Anderegg, P. A. Thiel, J. Non-cryst. Solids 195 (1996)  26. 891.  S. Suzuki, Y. Waseda, N. Tamura, K. Urban, Scripta Materialia 35 (1996)  27.  D. Rouxel, M. Gavatz, P. Pigeat, B. Weber, P. Plaindoux, ""Auger electron 25.5  microprobe analysis of surface of Al62Cu  Fe  12.5  quasicrystal. First steps of oxidation.,""  in: A. I. Goldman, D. J. Sordelet, P. A. Thiel, J. M. Dubois (Ed.) New Horizons in Quasicrystals: Research and Applications, Vol. World Scientific, Singapore, 1997, p. 173.   Shi, Cai, et al. 32  28. 302.  D. Naumovic, P. Aebi, C. Beeli, L. Schlapbach, Surf. Sci. 433-435 (1999)  29.  F. Schmithsen, G. Cappello, M. De Boissieu, M. Boudard, F. Comin, J.  Chevrier, Surface sci. 444 (2000) 113.  30.  C. J. Jenks, J. W. Burnett, D. W. Delaney, T. A. Lograsso, P. A. Thiel,  Applied Surface Science 157 (2000) 23.  31.  J. Ledieu, A. W. Munz, T. M. Parker, R. McGrath, R. D. Diehl, D. W.  Delaney, T. A. Lograsso, ""Clustered, terraced, and mixed surface phases of the Al70Pd21Mn9 quasicrystal,"" in: J. M. Dubois, P. A. Thiel, A.-P. Tsai, K. Urban (Ed.) MRS Proceedings: Quasicrystals, Materials Research Society Symposium Proceedings Vol. 553, Materials Research Society, Warrendale, Pennsylvania, 1999, p. 237.  32.  G. Cappello, Ph.D. Thesis, Universit Joseph Fourier--Grenoble I (2000).  33.  D. Rouxel, T. Cai, C. J. Jenks, T. Lograsso, A. Ross, P. A. Thiel, Surface  Sci. 461 (2000) L521.   Shi, Cai, et al. 33  34.  C. Janot, Quasicrystals: A Primer, C. J. Humphreys, P. B. Hirsch, N. F.  Mott, R. J. Brook (Series Ed.) Monographs on the Physics and Chemistry of Materials, Vol. 48, Clarendon Press, Oxford, 1992.  35.  A. Katz, D. Gratias, J. Non-Cryst. Solids 153-154 (1993) 187.  36.  Y. Gauthier, Surf. Rev. Lett. 3 (1996) 1663.  37.  F. Jona, K. O. Legg, H. D. Shih, D. W. Jepsen, P. M. Marcus, Phys. Rev.  Lett. 40 (1978) 1466.  38.  P. J. Rous, Surf. Sci. 244 (1991) L137.  39.  M. Gierer, F. Shi, Z. Shen, P. Thiel, C. Jenks, A. Goldman, M. A. Van  Hove, (1998) unpublished results.  40.  F. Shi, Z. Shen, D. W. Delaney, A. I. Goldman, C. J. Jenks, M. J. Kramer,  T. Lograsso, P. A. Thiel, M. A. Van Hove, Surface Science 411 (1998) 86.  41.  A. Wander, J. B. Pendry, M. A. Van Hove, Phys. Rev. B 46 (1992) 9847.   Shi, Cai, et al. 34  42.  M. A. Fradkin, ""Structure selection and generation of phasons at the  growing surface of quasicrystal,"" in: G. Chapuis, W. Paciorek (Ed.) Aperiodic Crystals, Vol. World Scientific, Singapore, 1995,  43.  M. A. Fradkin, JETP Letters 69 (1999) 570.  44.  D. Gratias, F. Puyraimond, M. Quiquandon, A. Katz, Phys. Rev. B 63 (2001)  024202.   Shi, Cai, et al. 35  Figure Captions. Fig. 1. Micrographs of the 5f Al-Cu-Fe surface, after different thermal treatments and at different magnifications. (a) T < 825 K, 30x30 nm2 (b) T  825 K, 50x50 nm2 (c) T > 825 K, 100x100 nm2 (d) T > 825 K, 200x200 nm2 (e) T > 825 K, 100 x 100 nm2 . Black arrow shows the origin of a screw dislocation. (f) T > 825 K, 100x100 nm2 . White arrow points to a screw dislocation; black arrows point to the pentagonal facets. (g) T  825 K, 50x50 nm2. (h) T  825 K, 15x15 nm2. The circle outlines a the flower. Fig. 2. Line profiles across consecutive terraces, from the data of Figs. 1c-d. The labels (i)-(iii) indicate the specific profiles in Figs. 1c-d. Steps are labelled as H, high (6.2-6.6 ) or L, low (4.0 ). Fig. 3. Histogram illustrating the method of step height measurement. The histogram shows the distribution of pixel heights, in the area encompassed by the rectangle in Fig. 1c. The two maxima correspond to the upper and lower terraces, respectively. Their separation is the step height. Fig. 4. Frequency of step heights. Steps originating at screw dislocations are not included in this distribution. The total number of observations is 278. The height 2.5A is the average of 23 data points, and the calculated standard deviation is 0.2. The height 4.0 is the average of 99 data points, with 0.4 standard deviation. The height 6.2 is from 142 data points, with 0.3 standard deviation. Fig. 5. TEM images of i-Al-Cu-Fe samples after different treatments. (a) Along the 2f axis, as grown. (b) Along the 2f axis, after hot isostatic pressing. Arrows point to periodic regions in the lattice; one such region is enclosed in a box to show it clearly. The   Shi, Cai, et al. 36  bands of periodic lattice align along the 5f directions. (c) Along the 5f direction, after pressing and annealing to 800C. The inset shows a selected area diffraction pattern. The lack of streaking between diffraction spots, and the high degree of perfection in the TEM image, indicate the absence or very low density of ""periodic"" defect regions. Contrast variations across the images in (a) and (c) are due to variations in foil thickness; in (b), the contrast variation is due to this and also to the strain fields associated with the periodic regions. Fig. 6. LEED pattern of the surface after annealing at 850 K for 1 hour. The incident beam energy is 70 eV. Fig. 7. LEED IV curves from the 5f surface of Al-Cu-Fe, taken after preparing the surface by sputtering at 15 minutes and annealing at 800 K for 1 hour, then cooling to 120 K for data acquisition. Diffraction spot intensity is plotted as a function of momentum transfer parallel to the surface, k||. Each box encloses four symmetryequivalent curves. At normal incidence, the curves should be identical. The values of k||, based upon x-ray diffraction data for the bulk, are: bottom two boxes-- 1.647 -1; middle two boxes--2.665 -1; top two boxes--4.312 -1. Fig. 8. LEED IV curves from the 5f surface of Al-Cu-Fe, showing the effect of different annealing temperatures on the IV curves. Diffraction spot intensity is plotted as a function of momentum transfer parallel to the surface, k||. Within each box, the lower and upper curves were measured after annealing for one hour to 800 and 850 K, respectively, then cooling to 120 K for data acquisition.   Shi, Cai, et al. 37  Fig. 9. R-factors for different terminations of bulk icosahedral Al-Cu-Fe, depending on the surface height z of the termination. (For each atomic plane shown as a vertical bar, the surface consists of this plane and all planes with higher z values; planes with lower z values are cut away.) The terminations giving the best R-factors are marked by arrows. In the lower part, the atomic planes are shown, at their respective depths z, as bars with thickness proportional to the atomic density in each layer. The colors of the bars suggest the chemical composition of each layer, according to the color codes shown below the figure. Fig. 10. Side view (parallel to the surface, which is at top) showing several atomic planes: the five upper ones form a 5-plane layer, the left part of which is cut away to suggest a step down to a 9-plane layer (the structure at the step edge is completely unknown). The colors identify chemical elements: Al red, Cu blue, and Fe green. The larger spacing between the thicker layers is believed to be the preferred place where a surface forms. Fig. 11. Comparison of the IV curves measured experimentally (solid lines) and generated from the best structural model (dashed lines). The experimental curves were measured after annealing at 800 K for 1 hour. The values of k||, based upon x-ray diffraction data for the bulk, are, for the bottom two boxes-- 1.647 -1; middle two boxes--2.665 -1; top two boxes--4.312 -1. Fig. 12. Most favored termination for the five-fold Al-Cu-Fe surface, from LEED. Individual atomic planes are shown as colored bars (color coded to suggest their   Shi, Cai, et al. 38  approximate chemical composition). The optimized and bulk interplanar spacings are shown (at right), as well as a possible step height (at left)."
GX255-34-5555531	"Title: Divine Ratios: A Study of the Fibonacci Sequence and Golden Ratio Link to Outcomes:  Problem Solving Students will demonstrate their ability to solve problems in mathematics, including problems with open-ended answers, problems which are solved in a cooperative atmosphere, and problems which are solved with the use of technology.  Communication Students will demonstrate their ability to communicate mathematically through oral and written analysis of group test results, representing and using numbers in a variety of equivalent forms.  Reasoning Students will reason mathematically by making conjectures, gathering evidence, and building arguments to support their conclusions. Students will draw on their knowledge of graphs and ratios to analyze their data and draw conclusions about correlations between groups of data. Students will plot ordered pairs to represent the relationship of the length to the width in selected rectangles. Students will select the appropriate metric unit, choose the tool to find measurements, and apply those measurements to interdisciplinary problems. Students will collect, organize, display, and interpret data through bar graphs and two dimensional graphs. They will then write an evaluative paragraph about the results. Students will write and solve simple proportions using the appropriate operation and a calculator. Students will apply number theory concepts, such as the Fibonacci Sequence and the Golden Ratio. Students will generalize a correlation between graphs of two different data sets.   Connections   Geometry  Measurement   Statistics   Arithmetic Operations  Number Relationships  Patterns/ Relationships Brief Overview:  This activity introduces the Fibonacci Sequence and relates the ratio of nth term to the previous term to a linear equation. They will compare the ratio of the length to the width in the selected rectangles to the ratios of n to n+1 (excluding the first three terms) found in the Fibonacci Sequence. Students then will correlate the ratio of the length of their upper body to the length of their lower body and the Golden Ratio.   Grade/Level: Grades 6 - 8 (General Math with extensions to Social Studies and Art) Duration/Length: This activity should take 3 or 4 days. The activities for the second day may take longer than anticipated. Prerequisite Knowledge: Students need to have a basic knowledge of :     ratio and proportions graphing metric measurement fractions, decimals and percents  Objectives:          Describe and list terms of the Fibonacci Sequence Generate subsequent values in a sequence Find length using metric measurement Express ratios as fractions and decimals Convert ratios to fractions, decimals, and percent Compare ratios in a variety of ways Construct two-dimensional graphs Communicate hypotheses and conclusions Justify conjectured correlations and conclusions  Materials/Resources/Printed Materials:         Calculator Pencil & Paper Meter Sticks Graph Paper Chart Paper for Tally Sheet Resource Material on Fibonacci Computer Software for Data Analysis (Optional) Index Cards  Development/Procedures: Day 1:  Introduce the historical figure Fibonacci.  Give the students the first five numbers of the Fibonacci Sequence (1,1,2,3,5,...) and have them hypothesize the next three numbers (8,13,21) in the sequence.  Have the students explain their methods for finding the next numbers.    Share the standard Fibonacci Sequence with the class. (Any number in the Fibonacce Sequence except for the first two is the sum of its two predecessors. Ex.:. 8 + 13 = 21, 13 + 21 = 34)  Have students list the ratio of nth term to previous term, starting with the second number in the sequence, (nth term:n-1 term) up to 233. (1:1) = 1 (2:1) = 2 (3:2) = 1.5 (5:3) = 1.666 (8:5) = 1.6 (13:8) = 1.62   Have the students graph ratios (n-1 term:nth term) up to 89 (if you have graphing calculators or computer software available, extend the graph to 233). SPECIAL NOTE : When you plot on a coordinate plane, the nth term MUST be the Y coordinate. Title: Fibonacci Sequence Plot.  Assign Worksheet #1 for homework (Survey Sheet, prerequisite to following day's work). Day 2:  Record students' survey results from Worksheet #1 on overhead or chart paper.  Total results from each individual rectangle. Note : Figures B and E have Golden Ratios. Note : Squares are a subset of rectangles (Figure F).  Use data to make a bar graph comparing popularity of the various rectangles from Worksheet #1 on overhead or chart paper (Computer Software - optional).  Discuss with students which rectangles received the greatest number of votes. Introduce the historical background on the ""Golden Ratio"" in architecture and art (""Golden Ratio"" is 2:1 +  5 or approx. 1:1.618). Examples: The Parthenon, Great Pyramids of Ghiza, and Last Supper painting by Salvador Dali.  Determine the ratio (shorter side:longer side) of each rectangle on Worksheet #1. Round to three decimal places (calculator is recommended).  Plot the ordered pairs representing the ratios just determined onto the Fibonacci Sequence Plot from Day 1. Analyze for correlations between the individual ratios (rectangle) and Fibonacci Sequence ratios. SPECIAL NOTE : When you plot your ordered pairs, the width MUST be the X-coordinate.  Assign homework to write a paragraph comparing the results of each individual's survey to the plot just completed. Then compare the class results to this plot. Day 3:  Share sampling of previous night's homework. Analyze results through class discussion.  Measure body parts as indicated on Worksheet #2.    Record A2 and B2 from Worksheet #2 on an index card.  Line students up by least to greatest using the A2 value.  Record all A2 and B2 values in order of line-up on a class chart.  Plot the ordered pairs from the chart on the Fibonacci Sequence Plot from Day 1.  Discuss in small group any correlation that they observed. Note: A line parallel to the Fibonacci Sequence Plot may be drawn to assist with analysis.  Write paragraph analyzing the correlation between the body ratios and the ""Golden Ratio"" as illustrated on the Fibonacci Sequence Plot.  Assign homework to complete chart on bottom of Worksheet #2 and complete the paragraph from the previous step. Evaluation: Worksheet #1 Total 15% Complete Survey Expressing Ratios Rectangle Survey Table Total 15% Measurement Activity Chart Total 15% Class Activity Total 15% Individual Activity 5% 5% 5% 10% 5% 15% 15% Criteria (complete/incomplete) (1% per correct response) (complete/incomplete) (complete/incomplete) (complete/incomplete) (complete/incomplete) (complete/incomplete)  Worksheet #2  Fibonacci Sequence Plot #1 Fibonacci Sequence Plot #2  Written Response #1 Total 20% Paragraph should include: 1) Summary of individual and class observations. 2) Observation of any correlations between expected ratio (approx. 1.618) and actual data. 3) Conclusion justified with data. Written Response #2 Total 20% Paragraph should include: 1) Summary of individual and class observations. 2) Observation of any correlations between expected ratio (approx. 1.618) and actual data. 3) Conclusion justified with data.   Extensions/Follow Up: Students will create a Collage of Human Figures and Golden Rectangles using the following procedure. Construct and cut out four different sized golden rectangles from one 9"" X 12"" piece of white construction paper. Trace a duplicate of each rectangle, using a different color of construction paper for each. Cut out the colored rectangles and set aside. On one side of each white rectangle, create a textured pattern using several different colored crayons. Turn each white rectangle over to the blank side and draw a human figure which is as tall as the rectangle's length while keeping in mind the proportions and golden rectangles discovered in the initial activity. Cut out the four human figures. Assemble the human figures and rectangles in an arrangement that is ""golden to their eyes"" and glue individual parts on an 8"" X 11"" black background paper. Mount the finished design on a 9"" X 12"" white background paper. Materials needed: White Construction paper (9"" X 12"") for each student, black construction paper (8"" X 11"") for each student, at least four different colors of construction paper, pencils, rulers, scissors and glue. Students will research a famous building from an ancient culture such as the Parthenon, the Great Pyramids of Ghiza, or the Great Temple of Karnak to find out what role the Golden Ratio plays in its architecture. Students will perform the activity titled ""Scaling Up the Human Body: Adding vs. Multiplying, Distortion vs. Proportion,"" from the book Equals Investigations: Growth Patterns. (ISBN 0-912511-57-5, published by Lawrence Hall of Science in 1994) Authors: Crissy Daniel Col. E. Brooke Lee Montgomery County Edgar W. Jones Sligo Middle School Montgomery County Marcia Rosenblum Col. E. Brooke Lee Montgomery County   Worksheet #1  Student Survey  Name:_______________________  A 1"" x 1.25""  B 1.13"" x 1.83""  2"" x 0.63"" C 0.25"" x 1.38""  D E 1.63"" x 2.63""  F  G  2.75"" x 0.88"" 0.69"" x 2.00"" H  0.75"" x 0.75""  Directions: Survey five different people. Ask the participant to identify the two rectangles that they find most pleasing to the eye. Record the results on a separate sheet of paper. After completing the survey, record the total for each rectangle on the table below. Rectangle Survey Table  Rectangle A __________ Rectangle B __________ Rectangle C __________ Rectangle D __________  Rectangle E __________ Rectangle F __________ Rectangle G __________ Rectangle H __________   Worksheet #2 ARE OUR BODIES GOLDEN ? Data Recording Sheet  Name: ____________________  Measure the following lengths to the nearest centimeter.  Data Set I A = Top of head to middle of throat B = Middle of throat to navel  ____________________________ ____________________________ ____________________________  A2 = A+B = Top of head to navel Data Set II C = Knee cap to floor D = Navel to knee cap  ____________________________ ____________________________ ____________________________  B2 = C+D = Navel to floor Complete the chart using data from above.  Ratio  Fraction  Decimal  Percent  A:B  A:A2  B:A2  C:D  C:B2  D:B2"
GX263-99-6685844	NTC QUARTERLY REPORT - PERIOD ENDING MARCH 31,1993  PAGE 33  phases randomly determined, 3)light slubs, and 4)light 4. Recruit a graduate student to work jointly on data acquisition and controls. and dark slubs. Of the along-end uniformities studied, the normally 5. Utilize equipment developed in year one to study dosing of chemicals and dyes. distributed one interfered with streak discernment most effectively. Even so this interference was effective only when the along-end variability was 5 or more times larger 6. To improve and validate theoretical dyeing and control models through data acquired from designed than the end-to-end variability. Random-phase sinusoidal experiments. variability produced severe chevroning, and failed to cover streaks at all, while simulated slubs produced effects which 7. Design a database management system to facilitate were similar to, albeit slightly less effective than, those technology transfer and user access to data by expressgenerated by normally distributed noise. ing results in terms of standard data structures. As is evident, there is a practically infinite list of pos- a. Explore joint efforts with researchers at the Swiss Fedsible types and magnitudes of along-end noise. We are eral Institute of Technology to use supercomputers in continuing to look for good ideas in this regard, because studying the dyeing process. this type of approach is conceivably commercially viable, SUMMARY: assuming an effective solution can be found. At this time, five faculty and six students are working Real-Time Data Acquisition, Theoretical on the project. The faculty are K. Beck, R. McGregor, W. Modeling, And Adaptive Control Of Batch Jasper, G. Lee, and B. Smith. The students are G. Berkstresser, IV, M. Arora, M. Lefeber, J. Lu, M. Kashif, and J. Dyeing Processes Peterson. Also, Bill Hunter (Adjunct Professor of Textile File: S92ClO Chemistry) of Ciba, and other interested industrial parties are collaborating. During this reporting period, activities PI(s): Brent Smith focused mainly on collection and analysis of data, equipQUARTERLY REPORT ENDING: March 3 1,1993 ment fabrication and upgrading. Within the main project, there are essentially three subprojects: dye process modelREPORT COMPILED BY: Brent Smith ling; real-time data acquisition and analysis; and control OBJECTIVES: systems development. Work progresses well in all areas, and poster and plenary session presentations of all progress Longterm Goals were made at the NTC national meeting in Auburn, AL. 1. To develop novel state-of-the-art methods for real-time Dyeing experiments were continued to validate and sensing of dye concentration upgrade certain aspects of the models. These models are 2. To utilize those methods for the generation of a database for improving and testing theoretical models of the dyeing process To adaptively control the dyeing process through the use of advanced control algorithms and/or neural networks To investigate dye and chemical dosing as means of controlling the dyeing process, minimizing effluent discharge, and reusing dyebaths proving very useful in predicting the ultimate state the dyeing system (thus the ultimate color obtained) from state variables. This information is being integrated into the control protocols to give real-time estimates of final dye shade, thus facilitating real-time multi-channel adaptive control. The non-dimensional equilibrium model for the dyeing of ionic fibers by ionic dyes was extended to include polrionic dyes, and mixtures of dyes. Data obtained from the literature were fitted to the model. Very good least-squares non-linear fits to the data were obtained. Experimental work is also being done on the control of the dyeing of nylon by acid dyes. The dyeing will be controlled by dosing the acid dye into the dyebath. Kinetic models will be developed and will be used to design the control strategies for the dyeing process. Flow injection analysis (FIA) data from pilot plant dyeings done at Ciba in Greensboro using three different calibration models showed that FIA peak measurements are linear in the range of 0 to 2 absorbance units (AU), with excellent agreement between FIA peak and extended backslope calibration sets. There was good agreement between  3.  4.  Year Two Goals 1. To further develop flow injection analysis (FIA) and investigate a planar waveguide as tools for measuring dye concentrations in mixtures. Enhance database development by installing equip ment for sampling multiple dyebaths:. Close the loop in the current lab dyeing system so that the process is computer controlled.  2. 3.   NTC QUARTERLY REPORT - PERIOD ENDING MARCH 31,1993  PAGE 34  concentrations generated from the FIA data and color predictions based on reflectance measurements on the dyed goods. Several dyeings were done, with similar results. This work will be confirmed by running the same formulas on our own in-house laboratory dyeing equipment. A new piston-type pump is being evaluated as a constant flow FIA carrier pump. A problem which has not yet has been solved is to develop a more satisfactory dynamic mixer. As presently configured, the volume of the commercial chamber is 320=, however, a chamber volume of about 100 m is needed, in order obtain a comparable level of precision. Further design modifications are being carried out. FIA curves for 16 dye combinations have been determined and are being analyzed to give a firm direction to use in optimizing system parameters, once the mixer configuration is established. The FIA system was modified so that its utility in analyzing disperse dyes can be determined. Further progress was made for the research of fuzzy logic control of dyeing processes. Based on previous work, some modified control schemes have been developed, such as self-learning scheme for control rules, and self-tuning scheme of fuzzy calibration. All these schemes were tested through computer simulation. Also, the work to combine fuzzy logic and optimization to control complex multi-input-multi-output (MIMO) systems has been started. Considerable progress has been made in the analysis of dye mixtures, and especially in identifying appropriate strategies (eg. matrix mathematics, neural networks) to handle dye/dye and dye/salt interactions. Some of this work will be presented at a regional American Chemical Society Meeting in April.  Design, test, and operate an integrated environment which enables fundamental changes in design, manufacturing and business operations. 1. Establish the architecture of a CIM network to provide a generic design for industry. These architectures will be demonstrated from the Textile Design Laboratory, the Management systems Laboratory and the Model Manufacturing Facility @&IF). Continue establishing CIM network in limited number of processing areas. Define philosophies and strategies for the FIA complex which will be supported by appropriate monitoring, control and analysis tools.. Create the ability to integrate the FTA activities by defining and developing information transmission and retrieval systems for activities within and among textile companies. Develop systematic decision making techniques for evaluating investments for controlling and managing operations in CIM technologies.  2.  3.  4.  SUMMARY: The CIM project teams continue to seek ways to integrate and leverage the work of the various projects into a whole consistent with the overall project objectives. The display board developed for the Auburn meeting to show the integration of the projects is an example. The overall vision of the project is becoming clearer, especially as it relates to system simulation work by other research teams and technology transfer to industry. We are planning a major week-long workshop for late June to which forty CIM and FTA experts worldwide are invited. Their purpose is to reshape the view of CIM technology and its capability to transform the competitive capability of the domestic fiber-textile-apparel complex. 1. INFORMATION INTEGRATION TECHNOLOGY:  CIM In The FTA Industrial Complex File: S92Cll  PRINCIPAL INVESTIGATOR: Perry Grady (PG) amd Sa, Winchester (SW) - Team Leaders; D. R. Buchanan (DB), North Carolina State; J. Cuculo (JC), North Carolina State; Alan Donaldson (AD), North Carolina State; G. L. Hodge (GH) , North Carolina State; W. Jasper (WJ), North Carolina State; T. Little (TL), North Carolina State; G. Mock (GM), North Carolina State; M. Mohamed (MM), North Carolina State; J. P. Rust (JR), North Carolina State; A. Seyam (AS), North Carolina State; G. Smith (GS), North Carolina State; Paul Tucher (PT), North Carolina State QUARTERLY REPORT ENDING: March 3 1,1993 REPORT COMPILED BY: SAM WINCHESTER OBJECTIVE:  Overall Design of CIM System - The development of an overall CIM system prototype continues using a centralized data base for all process areas. Strategies for information flow within and among process areas are being assessed. Interest from industry continues to increase indicating the significance of this work to textile company operations. Computer Integration of Short-Staple Spinning Enterprise - A practical method for the use of neural networks in analysis and control applications was begun with emphasis on the quantity and form of data required as well as the capabilities and training methodologies for neural network architectures. We established an understanding of the theoretical significance of trained neural network weights and biases in model building. Methods are being
GX004-68-9122379	"Profile Hidden Markov Model Analysis     [  Program Manual  |  User's Guide  |  Data Files  |  Databases  ]            Sensitive Database Searching and Identifying  Sequence Domains            Introduction         Profile analysis has long been  a useful tool in finding  and aligning distantly related sequences  and in  identifying known sequence domains in  new sequences.  Basically, a  profile is a description of  the  consensus of a multiple sequence  alignment.  It uses a  position-specific scoring system to capture  information about the degree of  conservation at various positions in  the multiple alignment.  This  makes it a much more  sensitive and specific method for  database searching than pairwise methods,  such as those used by   BLAST  or  FastA , that use  position-independent scoring.     Hidden Markov modeling, a technique  that has been used for  years in speech recognition, is  now being  applied to many types of  problems in molecular sequence analysis.   In particular, this technique  can  produce profiles that are an  improvement over traditionally constructed profiles.     Profile hidden Markov models (HMMs)  have several advantages over standard  profiles.  Profile  HMMs have a formal probabilistic  basis and have a consistant  theory behind gap and insertion  scores,  in contrast to standard profile  methods which use heuristic methods.   HMMs apply a statistical  method to estimate the true  frequency of a residue at  a given position in the  alignment from its  observed frequency while standard profiles  use the observed frequency itself  to assign the score for  that residue.  This means  that a profile HMM derived  from only 10 to 20  aligned sequences can be of  equivalent quality to a standard  profile created from 40 to  50 aligned sequences.  In  general,  producing good profile HMMs requires  less skill and manual intervention  than producing good  standard profiles.     The HMMER (pronounced  hammer ) package  developed by Sean Eddy of  Washington University in St.  Louis, Missouri, is a set  of programs that allow you  to create and manipulate profile  HMMs and  databases of profile HMMs (HmmerBuild,  HmmerConvert), perform sensitive searches of  sequence  and profile HMM databases, (HmmerSearch  and HmmerPfam) and create multiple  sequence  alignments efficiently (HmmerAlign).  In  collaboration with Dr. Eddy, GCG has  incorporated these  programs into the Wisconsin Package.        What is a Profile HMM?   - A Simplified Description         A profile HMM is a  linear state machine consisting of  a series of nodes, each  of which corresponds  roughly to a position (column)  in the alignment from which  it was built.  If  we ignore gaps, the  correspondence is exact -- the  profile HMM has a node  for each column in the  alignment, and each  node can exist in one  state, a match state.   (The word ""match"" here implies  that there is a position  in  the model for every position  in the sequence to be  aligned to the model.)       A profile HMM has several  types of probabilities associated with  it.  One type is  the  transition     probability  -- the probability of  transitioning from one state to  another.  In a simple  ungapped model,  the probability of a  transition from one match state  to the next match  state is 1.0 and the  path through the model is  strictly linear, moving from the  match state of node n  to  the match state of node  n+1.     There are also  emissions probabilities   associated with each match state,  based on the probability of  a  given residue existing at that  position in the alignment.   For example, for a fairly  well-conserved  column in a protein alignment,  the emissions probability for the  most common amino acid may  be  0.81, while for each of  the other 19 amino acids  it may be 0.01.    If you follow  a path through the model  to generate  a sequence consistent with the  model, the probability of any  sequence that is generated depends  on  the transition and emissions probabilities  at each node.     In order to model real  sequences, we also need to  consider the possibility that gaps  might occur when  a model is aligned to  a sequence.  Two types  of gaps may arise.   The first type occurs when  the  sequence contains a region that  is not present in the  model (an insertion in the  sequence).  The second  type occurs when there is  a region in the model  that is not present in  the sequence (a deletion in  the  sequence).  To handle these  cases, each node in the  profile HMM must now have  three states:  the  match state, an insert state,  and a delete state.   The model also needs more  types of transition  probabilities:  match->match, match->insert, match->delete, insert->match, etc.           Aligning a sequence to a  profile HMM is done by  a dynamic programming algorithm that  finds the  most probable path that the  sequence may take through the  model, using the transition and  emissions  probabilities to score each possible  path.     In general, if the sequence  is equivalent to the consensus  of the original alignment, the  path through  the model will pass from  match state to match state  in a linear fashion.   If the sequence contains a  deletion relative to the consensus,  the path passes through one  or more delete states before  transitioning to the next match  state; if the sequence contains  an insertion relative to the  consensus,  the path passes through an  insert state between two match  states.     For example, if a sequence  contains an insert that occurs  between nodes 5 and 6  of the model, the  path transitions from the node  5 match state to an  insert state.  It remains  in the insert state and  ""consumes"" residues in the sequence  until it reaches the residue  in the sequence that corresponds  to  node 6 in the model.   At this point the  path transitions from the insert  state to the node 6  match state.     Similarly, if the sequence contains  a deletion so that it  has no residues corresponding to  nodes 12  through 15 of the model,  the path transitions from the  node 11 match state into  a delete state, then  transitions through additional delete states  until it can transition to  the match state of node  16 of the  model.     Profile HMMs can be aligned  to a sequence either globally  (the whole profile HMM aligns  to the  sequence) or locally (only part  of the profile HMM need  be aligned with the sequence).   The alignment  type is actually part of  the model, so you must  specify whether the model is  to be global or local  at the  time the model is  built ,  not at the time the  model is used.  (See  HmmerBuild documentation for more  details.)        Most Common Uses for Profile  HMMs         Because a profile HMM can  serve as a representation of  a sequence family or sequence  domain, the  most common application is to  compare profile HMMs and sequences.   These types of comparisons  are  more likely to identify distant  homologs than sequence vs. sequence comparisons  used in most  database search programs.     For example, you can use  HmmerPfam to compare your sequence  to a database of profile  HMMs  representing known sequence families and  known sequence domains.  A  match to one of these  profile  HMMs can help you identify  your sequence and determine its  function.  The curated Pfam  (""Protein  families"") database contains a large  number of global profile HMMs  representing known protein  families, while the PfamFrag database  contains local profile HMMs for  these same families.     Similarly, you can create a  profile HMM representing a domain  or sequence family in which  you are  interested, then use this profile  HMM as a query to  search a sequence database with  HmmerSearch to  see if any other sequences  possess this domain.     Another use for profile HMMs  is to create a multiple  alignment of a large number  of sequences more  quickly than by using standard  methods.  HmmerAlign uses a  small seed alignment of representative  sequences to create a profile  HMM which is then used  as a template for aligning  the full set of  sequences.        Overview of the HMMER Programs         There are nine programs in  the GCG adaptation of the  HMMER package.  The following  five are used  to create and manipulate profile  HMMs:     HmmerBuild  -- creates a profile  HMM from a set of  pre-aligned sequences.  The profile  HMM can be  appended to a file containing  other profile HMMs in order  to create an HMM database  file.     HmmerCalibrate  -- calibrates an existing  profile HMM or profile HMM  database so that searches  performed with it will be  more sensitive.     HmmerConvert  -- converts a profile  HMM created by HmmerBuild into  other formats.     HmmerIndex  -- indexes a profile  HMM database so that profile  HMMs can be retrieved from  it easily  with HmmerFetch.     HmmerFetch  -- extracts a profile  HMM from an indexed profile  HMM database into a file.    The remaining four programs are  used for analyzing data:     HmmerSearch  -- searches a sequence  database with a profile HMM  query.     HmmerPfam  -- searches a profile  HMM database with a sequence  query.  The profile HMM  database  file may be one you  created as well as the  Pfam database created by Eddy  and collaborators.     HmmerAlign  -- efficiently creates a  large multiple alignment from a  small seed alignment and a  collection of unaligned sequences.     HmmerEmit  -- randomly generates sequences  that match a given profile  HMM.        Pfam Acknowledgement        Pfam - A database of  protein domain family alignments and  HMMs Copyright (C) 1996-2000 The  Pfam Consortium.        References          1.  Eddy, S.R., et al. (1996).   Hidden Markov Models.   Current  Opinion in Structural Biology ,      6 ; 361-365.     2.  Durbin, R., Eddy, S., Krogh,  A., and Mitchison, G. (1998).    Biological Sequence Analysis.       Probabilistic Models of Proteins and  Nucleic Acids , Cambridge University Press,  Cambridge, UK.     3.  Eddy, S.R. (1998).  Profile hidden  Markov models.   Bioinformatics ,  14 ; 755-763.                   Printed: February 5, 2001  15:21 (1162)         [  Program Manual  |  User's Guide  |  Data Files  |  Databases  ]       Documentation Comments:  doc-comments@gcg.com  Technical Support:  help@gcg.com      Copyright (c) 1982-2001  Genetics Computer Group  Inc.  A subsidiary of Pharmacopeia, Inc.  All rights reserved.    Licenses and Trademarks Wisconsin Package is a trademark of  Genetics Computer Group , Inc.  GCG and the GCG logo are registered trademarks of  Genetics Computer Group , Inc.    All other product names mentioned in this documentation may be trademarks, and if so, are trademarks or registered trademarks of their respective holders and are used in this documentation for identification purposes only.       www.gcg.com"
GX005-26-11849146	Baum Welch algorithm    (algorithm)       Definition:  An algorithm to find  hidden Markov model  parameters A, B, and    with the maximum likelihood of generating the given symbol sequence in the observation vector.       See also   Viterbi algorithm .       Note: Contributed by Arvind <uk_arvind@mail.utexas.edu> May 2002.      Author:  PEB   More information    See this  lesson on Hidden Markov Models  for more explanation.       Go to the  Dictionary of Algorithms and Data Structures  home page.       If you have suggestions, corrections, or comments, please get in touch with  Paul E. Black   (paul.black@nist.gov).      Entry modified Fri Aug  2 10:21:30 2002.  HTML page formatted Thu Apr 17 10:48:18 2003.      This page's URL is  http://www.nist.gov/dads/HTML/baumWelch.html
GX261-11-3967086	"1 Strategies and Tools for Whole Genome Alignments Olivier Couronne1,3, Alexander Poliakov1, Nicolas Bray1, Tigran Ishkhanov1, Dmitriy Ryaboy1, Edward Rubin1, Lior Pachter2,3, Inna Dubchak 1 1  Lawrence Berkeley National Laboratory, Berkeley, CA 94720, USA; 2Department of  Mathematics, University of California at Berkeley, Berkeley, CA 94720, USA.  Abstract The availability of the assembled mouse genome makes possible, for the first time, an alignment and comparison of two large vertebrate genomes. We have investigated different strategies of alignment for the subsequent analysis of conservation of genomes that are effective for different quality assemblies. These strategies were applied to the comparison of the working draft of the human genome with the Mouse Genome Sequencing Consortium assembly, as well as other intermediate mouse assemblies. Our methods are fast and the resulting alignments exhibit a high degree of sensitivity, covering more than 90% of known coding exons in the human genome. We have obtained such coverage while preserving specificity. With a view towards the end user, we have developed a suite of tools and websites for automatically aligning, and subsequently browsing and working with whole genome comparisons. We describe the use of these tools to identify conserved non-coding regions between the human and mouse genomes, some of which have not been identified by other methods.  1. Introduction The expectation behind the sequencing of the mouse genome is to gain a deeper understanding of the human genome through comparative analysis. Comparative genomic studies of selected regions have already resulted in interesting biological discoveries; from many examples we mention here the discovery of new genes (Pennacchio et al. 2001; Dehal et al. 2001) and the identification of conserved noncoding sequences with regulatory functions (Hardison et al. 1997; Oeltjen et al. 1997; Hardison et al. 2000;   2 Loots et al. 2000; Krivan and Wasserman 2001). These comparative genomic studies have been based on sequence alignments and have been successful because the evolutionary distance between mouse and man appears to be small enough so that genes and other functional elements have been conserved both in sequence (Batzoglou et al. 2000; Hardison et al. 1997) and function (Huxley 1997). On the other hand, sufficient time has elapsed so that non-functional sequence has diverged enough to yield a good ""signal to noise"" ratio. Alignments of whole genomes have already been undertaken for complete genomic sequences of various bacterial species (Tatusov et al. 1997; Delcher et al. 1999; Florea et al. 2000) where the problem w as feasible due to the small genomic size of these organisms (up to 4Mb). The recently published Fugu genome (Aparicio et al. 2002) has been aligned to the human genome using BLAST program, but the complexity of the problem was mitigated by the small size of the Fugu genome and its relatively simple repeat structure. A similar local alignment approach has been applied to the mouse genome by the Blastz group (Schwartz et al. 2002). Aligning large vertebrate genomes that are structurally complex poses a variety of problems not encountered on smaller scales. They are rich in repetitive elements (~50% in the human genome, I.H.G.S., 2001, Venter et al. 2001) and contain multiple segmental duplications (the human genome seems likely to contain about 5% segmental duplication, with most of this sequence in large blocks greater than 10 kb, Bailey et al. 2002). The sizes of the sequences is perhaps the biggest hurdle, since many alignment algorithms were designed for comparing single proteins and are extremely inefficient when processing large genomic intervals (Miller, 2001). The complexity of vertebrate genomes also increases the difficulty of identifying true orthologous DNA segments in alignments. Taking into account that there are sometimes near perfect matches between paralogous DNA regions it is necessary to statistically assess the identification of the most likely orthologous DNA segments, while minimizing the rate of misaligned regions. In this paper we describe our strategies and results for the human and mouse genomes. We have integrated both local and global alignment programs , and our study provides   3 the first quantitative analysis of how such strategies perform in tandem. The resulting implementation allows rapid and specific whole genome alignments and comparisons. The ultimate goal of genome alignment is to facilitate biological discovery, and with this in mind we have also integrated in the computational system a variety of browsing and analysis tools. We present visualization tools for browsing the alignments, as well as a web server that allows users to align their own sequences against completed genome assemblies.  2. Algorithms Finding the orthologous regions between two species computationally is a non-trivial task that has never been explored on a whole genome scale for vertebrate genomes. Local alignment tools find a lot of high scoring matching segments, in particular the orthologous segments, but in addition they identify many paralogous relationships, or even false positives alignments resulting from simple sequence repeats and other sequence artifacts (Chen et al., 2001). BLAST was successfully utilized in the study of Gibbs and coauthors (Chen et al., 2001) where high-quality rat WGS reads (covering 7.5% of the rat genome) were compared with the GoldenPath human genome assembly. The authors of the study investigated statistical significance of BLAST search results and parameters, but they did not focus on finding `true' orthologs and were mostly interested in higher sensitivity of alignment and completeness of coverage of coding exons. When compared with the human assembly more than 47.3% of all aligned reads produced between 2 and 12 hits (which correspond to medium represented elements), and 7.6% had more than 12 hits (likely containing repetitive elements). Unlike local alignment, global alignment methods require aligned features to be conserved in both order and orientation, and are therefore appropriate for aligning orthologous regions in the domain where this assumption applies. But whole genome rearrangements, duplications, inversions, and other evolutionary events restrict the resolution at which the order and orientation assumption of global alignment applies. In   4 the case of the human and mouse genomes, it appears that this assumption applies, on average, to regions less than 8 megabases in length (Mural et al. 2002). Our strategy is to use a fast local alignment method to find anchors on the base genome to identify regions of possible homology for a query sequence. The key element is then to be able to post-process these anchors in order to delimit a region of homology where the order and orientation seems conserved. These regions are then globally aligned. In the work presented here we used BLAT (Kent 2002) to find anchors and AVID (version 2.0, Bray et al. 2002) to generate global alignments (see Figure 1 for an overview of the pipeline and how they were combined). BLAT was designed for cDNA/DNA alignment and first used in Intronerator (Kent and Zahler, 2000). BLAT is not optimized for crossspecies alignments (Kent, 2002), but we chose this program because our tests demonstrated that it performed very well as an anchoring tool in our computational scheme. It is also about 500 times faster than other existing alignment tools. Heuristic for selecting candidate regions for global aligning ( post-processing of anchors). For each sequence, BLAT matches are sorted by score, and regions of possible homology are selected around the strongest matches which serve as anchors. All BLAT hits at most L bases apart are grouped together (here L is the length of the region being aligned,). For groups smaller than L/4, the regions were then extended out by min(50k, L/2-G) where G was the length of the group. For groups with G greater than L/4, the regions were extended out by min(50k,L/4). The groups obtained are compared and the ones with less than 30% of the score of the best group are rejected at this stage (see Figure 2). Various parameters for these heuristics were explored in order to obtain a method that would work for different size of sequences. This heuristic may identify multiple regions of possible homology of different size and score in the base genome. These regions are proposed as candidates to the alignment program. The score obtained by the global alignment is used to make the decision about which alignments to report or to reject. Strategies for different types of analyzed sequence . Different sequencing strategies, coupled with the various assembly methods used to build contigs and scaffolds, result in genomic sequence at different stages of completion and of different quality. There is a   5 significant number of BAC-size finished contigs particularly suitable for higher quality comparative analysis (Mardis et al. 2002), while whole genome shotgun generated assemblies result in shorter contigs and scaffolds. We developed different strategies for aligning sequences at different stages of completion by taking into account all available information, such as the scaffold or the map information, when available. Table 1 summarizes the computational schemes we developed for different types of sequences. In the case of finished BACs or individual sequences submitted by the user through the pipeline interface, no other information is available and we use a `contig' scheme where mapping is based solely on the found anchors followed by the global alignment stage with its scoring. When we align an assembly with the scaffold information available, anchors obtained for different contigs in a scaffold are analyzed together to select candidate positions . We map the whole scaffold, but have the flexibility to reorient and reorder each of the contigs in the scaffold at the alignment stage if necessary. The algorithm also allows us to break the scaffold by selecting more than one candidate region, so that some of the contigs can be aligned to a different place. These last two features allow our alignment to be tolerant to scaffold assembly errors. For an advanced assembly scaffold information is very reliable. In MGSCv3 the quality of assembly was high enough that contigs and scaffolds were mapped to the mouse chromosomes (Waterson et al. 2002). For such cases we chopped the mouse chromosomes into large sections before aligning them. The chromosomes were chopped around the contig boundaries in order not to split them. We tested different sizes and found that fragments averaging 250kbp in length give us the best sensitivity. 3. Results. Here we present the results of alignment of the Mouse Genome Sequencing Consortium assembly MGSCv3 with the June 2002 Human Genome freeze (NCBI build 30). Alignments on this freeze as well as the December 2001 freeze are available at http://pipeline.lbl.gov. The human genome sequence was soft-masked, so that repeats were not considered at the anchoring level, although the global alignments generated at later stages do extend into repeats.   6  Sensitivity. For the final alignment we calculated the level of coverage on known coding and non-coding functional features of the human genome (Table 2). The alignments were scored according to the procedure described in the paper on the mouse genome (Waterson et al. 2002). Three different evolutionary models were selected for scoring the alignments. For coding regions, a high stringency and high penalty for indels was chosen. In order to assess performance on less conserved regulatory regions we also applied less stringent filters. The overall coverage was computed, as well as the coverage of the RefSeq exons, upstream (100, 200 and 500bp) and downstream (200bp) regions, and UTR. About 2% of aligned base pairs of the human genome were covered by more than one mouse sequence fragment. Figure 3 shows an example of a chromosome 3 location where several copies of the mouse pseudogene of Laminin B receptor (LAMR1) from different chromosomes was aligned (laminin B receptor has multiple pseudogenes, http://www.ncbi.nlm.nih.gov/LocusLink/). Our alignment showed more than one million regions conserved at higher than 70% conservation over 100bp level. These features cover about 217 million base pairs. Only 61.6% of them are covered by at least one base pair of a BLAT hit. This means that about two fifths of the conserved features are found only at the global alignment stage. This result is critical because it proves that a local aligner such as BLAT set up with parameters for which its sensitivity is not optimal, but its speed is, can nevertheless be used as an anchoring system because the global alignment retrieves a lot of additional conserved regions outside the anchors (Figure 4). The amount of conserved non-coding sequence was extraordinarily high. At least 5.82% of the bases in the genome are conserved at the 70%/100bp threshold but do not overlap annotated RefSeq, mRNA, Genscan predictions or ESTs. Our scheme has the flexibility to align a query sequence to multiple regions in the genome. Among the conserved features (70% over 100bp) 6.6% of the total conserved sequences, came from secondary hits. These conserved regions may arise from genomic rearrangements or duplications.   7 Specificity. Measuring the specificity, ie how much alignments are correct, is considerably more difficult than measuring the sensitivity. To test the specificity of our method, we aligned a ""random"" mouse genome obtained by reversing (not complementing) the mouse sequences (as proposed by Arian Smit, MGSC communication). Figure 5 presents the ratio of the number of nucleotides on each human chromosome covered by alignments of the random mouse sequence and the number of nucleotides covered by the real one for each chromosome. Alignments were filtered out at different thresholds. For most of the chromosomes, this ratio is below 0.0005, meaning that less than 0.05% of the mouse versus human alignments can be accounted for by random sequence alignments even at low thresholds. This number is higher for certain chromosomes, especially short ones, partly because of numerical instability caused by the very small amount of alignment obtained on these chromosomes. Another test to estimate specificity is to measure the total coverage of the human chromosome 20 by alignments of sequences from all mouse chromosomes with the exception of chromosome 2. The human chromosome 20 being entirely syntenic with the mouse chromosome 2, we should expect to have, for example, only a few percentage of non syntenic coverage coming from pseudogenes. We found a coverage of only 5.6% for exons, with the tight filter, and 0.43% for upstream 100, with the medium filter (Table 3). It is interesting to note that most of these are covered more than once. An interesting example is the case of the Apolipoprotein(a) region. The expressed gene is confined to a subset of primates, as most mammal lack apo(a) (only hedgehogs produce an apo(a)-like protein) (Lawn et al. 1997). Figure 6 shows the coverage in this region by the mouse sequence utilizing two methods: Blastz (Schwartz et al. 2002) and the method presented here. Our method is the only one to predict that apoa(a) has no homology in the mouse, as it had been shown experimentally. This example is interesting because it uniquely demonstrates the importance of specificity.  We set up a database of conserved elements obtained by three different groups using different methods of genome alignment (local and global) and the same conservation cutoff (available at http://pipeline.lbl.gov/cgi-bin/cnc). The most interesting result of   8 comparing the three different sets of conserved non-coding sequences is that the sets overlap by no more than 80%. This suggests that a combination of strategies and methods could lead to a better overall whole genome alignment; this is similar to the situation that has been observed in gene finding (Rogic et al. 2000). An analysis of conservation was performed and every conserved region was classified as coding, noncoding, intronic, repetitive element, or UTR based on annotations associated with the human genome assembly. The alignments of the human and mouse sequences have revealed a significant number of conserved coding and non-coding elements. In addition to deciphering the coding component of the genome, the discovery of conserved noncoding sequences (CNCs) for their potential role in gene regulation is of particular interest. The identification of all such regions is complicated by the high level of conservation between as yet un-annotated coding regions (which can be viewed as non-coding false positives) and the variation in underlying mutation rates throughout the genome. As described in the mouse genome paper (Waterson et al., 2002), we believe the annotation of the genome is not missing vast numbers of genes, which suggests that most of the CNC bases identified do not code for proteins. Conserved sequences for the whole genome were calculated by identifying all regions at least 100bp long conserved at greater than 70% identity. In many cases this scheme has allowed for retrieving important regulatory sequences (Loots et al. 2000; Henkel et al. 1992). Alternatively, more sophisticated methods for retrieving ""significant"" conserved non-coding regions can be selected by the user, such as regions identified by scoring with evolutionary model based matrices (Waterson et al., 2002). 4.1 Implementation. 4.1 Database and software. The pipeline was built on a MySQL database platform selected for its compa tibility with major sources of annotation data like Ensembl (Hubbard et al. 2002) and the UCSC Genome Browser (Kent et al. 2002). The tables contain all the input sequences (either format, draft or finished), and all the data generated by the pipeline, repeats regions, anchors, alignments and regions of high conservation (both coding and non coding). The pipeline software consists of a combination of Perl, C and Java programs. It includes a   9 scheduler that gets control data from the database, builds a queue of jobs, and dispatches them to the computation nodes of the cluster for execution, and the main program that processes individual sequences. A Perl library acts as an interface between the database and the above programs. The use of a separate library allows the programs to function independently of the database schema. The library also improves on the standard Perl MySQL database interface package by providing auto-reconnect functionality and improved error handling. One of the main features of the pipeline is its modular design which allows us to be relatively independent of the specific choice of integrated programs; with slight modifications of input and output scripts, other alignment and visualization tools can substitute the ones mentioned above. The code source is available at http://pipeline.lbl.gov/downloads.shtml. 4.2 Performance. The whole alignment of the mouse and the human genomes presented here took 17 hours on a cluster of sixteen 2.2GHz Pentium IV CPUs (20 CPU days). For comparison, the Blastz alignment took an order of magnitude longer time (Waterson et al. 2002, Supp. Mat.). Our generated alignments represent 7.5GB of data, stored in binary format in a MySQL database and are available for download in AXT format at http://pipeline.lbl.gov/downloads.shtml. 4.3 Data presentation. Two schemes of data presentation on the whole genome scale are available to the user  the VISTA Genome Browser and the Text browser, both synchronized with the pipeline database. They can be accessed at the gateway Web site http://pipeline.lbl.gov. VISTA Genome Browser is a Java applet for interactively visualizing results of comparative sequence analysis in a VISTA format on the scale of whole chromosomes. It has a number of options, such as zoom, extraction of a region to be displayed, userdefined parameters for conservation level, and options for selecting sequence elements to study (Figure 7). VISTA Genome Browser is realized as a dynamic web-interface synchronized with the central MySQL.   10 The Text browser is the most direct front end to the central MySQL database. It allows a user to examine detailed information about each mouse sequence aligned to the selected region on the human genome. For each aligned region exact location of alignments on the human and mouse genomes, the sequences, alignments, coordinates of conserved regions, and a lot of other information are easily retrieved. The pipeline annotation of conserved regions is DAS compatible (Distributed Annotation System, Dowell et al. 2001; Fumoto et al. 2002) and can be viewed through the Ensembl browser at http://www.ensembl.org (step-by-step instructions for viewing the data are available at http://pipeline.lbl.gov/das.shtml).  4.3 Web-based server to align and compare user-submitted sequences with a base genome. As described above, we developed alignment methods for sequences of different quality and length against the whole genome assembly. Our computational system is open for user queries through a web interface accessible from http://pipeline.lbl.gov. Comparative analysis can be done against the base human or mouse genomes. This server accepts sequences in either finished or draft format. Contigs in draft sequences are ordered and oriented according to their alignment with the base genome (Figure 7). The server also accepts GENBANK accession number and connects automatically to GENBANK to retrieve the sequence. The user is provided with detailed results of the comparative analysis, including the alignments, VISTA pictures and the ability to interactively navigate the Vista Genome Browser. A typical query sequence of up to a few hundred kilobases is processed in seconds. Based on current usage (5000 requests/month) we have determined that the average query size is 150kb.  5. Discussion. As we have pointed out in this paper, an alignment of the whole human and mouse genomes represents both an algorithmic challenge, and yet holds the promise of significant biological understanding. We expect that the methodology for aligning the   11 human and mouse genomes will change over time, eventually leading to a ""true"" alignment of the genomes which correctly identifies orthologous relationships between genes and nucleotides, and in which parologous genes, and duplications within genomes are correctly handled. It seems to us that a dramatic improvement in the alignment of the human and mouse genomes will be possible with more genomes available. Significant initial results from the alignment of the human and mouse genomes are that coding regions are highly conserved as expected, but an additional large portion of the genome (roughly as much sequence as is coding) is highly conserved with unknown function. This conserved sequence is arguably not coding and cannot all be explained by neutral evolution (Waterston et al, 2002). It is interesting to note that comparisons of three species (Dubchak et al. 2000) show that many human-mouse conserved regions are also present in the dog, suggesting that they may indeed be functional. Unfortunately, current methods for predicting transcription factor binding sites and other regulatory elements are not accurate enough to classify the conserved regions (Fickett and Wasserman 2000). Our studies of alignment efficiency with respect to different contig sizes should be useful for dynamic alignment tools that rapidly align query sequences to genomes, and for devising strategies for combined local and global alignment. It is important to note that we specifically designed our method in such a way that anchor selection is a standalone module, so that different methods can be used without difficulty. For example, it is possible that other whole genome local alignment methods such as PatternHunter (Ma et al, 2002) or Blastz (http://bio.cse.psu.edu/) could also be very effective at selecting anchors. We plan to test different local and global programs, and novel methods for combining them to optimize performance and accuracy of our comparative analysis scheme. Unfortunately, it still remains an open problem to devise accurate criteria for judging the accuracy of an alignment. The sensitivity is not that difficult to measure (one can, for example, check to see how many exons were aligned), but the specificity (a measure of how much incorrect alignment there is), is considerably harder to estimate as we have discussed.   12 The alignments described in this paper are currently being used by a number of projects, including a comparative-based annotation of genes in the human and mouse genome (Pachter et al. 2002) and a study of the rearrangement history of the genomes (P. Pevzner, unpublished). These projects will in turn lead to better alignments, and eventually, in conjunction with more genomes, a more complete understanding of genome evolution. Acknowledgements. We thank the Mouse Genome Sequencing Consortium for the possibility to work with the mouse genome during the sequencing phases and in the subsequent analysis phase. The analysis group, comprising many individuals and teams from around the world, was particularly helpful not only in providing crucial suggestions and advice as the project unfolded, but also in contributing many independent ideas. Special thanks go to Jim Kent who coordinated the alignment efforts of the mouse sequencing consortium analysis group and designed the filtering methods for calculating alignment coverage. Thanks also to the Penn State Group (Laura Elnitsky, Ross Hardison, Webb Miller, Scott Schwartz and others) and the Pattern Hunter Group (Ming Li, Mike Zody and others) who developed different alignment strategies with which we compared throughout. We thank Ivan Ovcharenko for initiating the project and developing the prototype. We also thank Serafim Batzoglou for his help with generating simulated reads and assemblies for our test sets. The project was partially supported by a Program for Genomic Applications (PGA) grant from the National Heart Lung and Blood Institute.  Footnotes. 3  Corresponding authors  E-MAIL ocouronne@lbl.gov; Tel (510)486-6030; Fax (510)486-5717 lpachter@math.berkeley.edu; Tel (510)642-2028; Fax (510)642-8204   13 References: Altschul, S.F., Gish, W., Miller, W., Myers, E.W., and Lipman, D.J. 1990. Basic local alignment search tool. J. Mol. Biol. 215: 403-410. Aparicio, S., Chapman, J., Stupka, E., Putnam, N., Chia , J.M., Dehal, P., Christoffels , A., Rash, S., Hoon , S., Smit , A., et al. 2002. Whole-genome shotgun assembly and analysis of the genome of Fugu rubripes. Science 297: 1301-1310. Bailey, J.A. et al. 2002. Recent Segemental Duplication in the Human Genome. 2002. Science 297: 1003-1007. Batzoglou, S., Jaffe, D.B., Stanley, K., Butler, J., Gnerre, S., Mauceli, E., Berger, B., Mesirov, J.P., and Lander, E.S. 2002. ARACHNE: a whole-genome shotgun assembler. Genome Res. 12: 177-189. Batzoglou, S., Pachter, L., Mesirov, J.P., Berger, B., Lander, E.S. 2000. Human and mouse gene structure: comparative analysis and application to exon prediction. Genome Res. 10: 950-958. Bedell, J.A., Korf, I., and Gish, W. 2000. MaskerAid: a performance enhancement to RepeatMasker. Bioinformatics 16: 1040-1041 Bray, N., Dubchak, I., and Pachter, L. AVID: A global alignment program for large genomic sequences. Submitted. Chen, R., Bouck, J.B., Weinstock, G.M. and Gibbs, R.A. 2001. Comparing Vertebrate Whole-Genome Shotgun Reads to the Human Genome . Genome Res. 11: 1807-1816 Dehal, P., Predki, P., Olsen, A.S., Kobayashi, A., Folta, P., Lucas, S., Land, M., Terry, A., Ecale Zhou, C.L., Rash, S., et al. 2001. Human chromosome 19 and related regions in mouse: conservative and lineage-specific evolution. Science. 293: 104-111 Delcher, A.L., Kasif, S., Fleischmann, R.D., Peterson, J., White, O., and Salzberg SL. 1999. Alignment of whole genomes. Nucleic Acids Res. 27: 2369-2376. Dowell, R.D., Jokerst, R.M., Day, A., Eddy, S.R., and Stein, L. 2001. The Distributed Annotation System. BMC Bioinformatics. 2: 7   14 Dubchak, I., Brudno, M., Pachter, L.S., Loots,G.G., Mayor,C., Rubin, E.M., and Frazer, K.A.. 2000. Active conservation of noncoding sequences revealed by 3-way species comparisons. Genome Research 10: 1304-1306. Fickett, J.W. and Wasserman, W.W. 2000. Discovery and modeling of transcriptional regulatory regions. Curr. Opinion Biotechnol. 11: 19-24 Florea, L., Riemer ,C., Schwartz, S., Zhang, Z., Stajonovic, N., Miller, W., and McClelland, M. 2000. Web-based visualization tools for bacterial genome alignments. Nucelic Acids Res. 28: 3486-3496. Fumoto, M., Miyazaki, S., and Sugawara, H. Genome Information Broker (GIB): data retrieval and comparative analysis system for completed microbial genomes and more. Nucleic Acids Res. 2002 30: 66-68. Hardison, R.C., Oeltjen, J., and Miller, W. 1997. Long human-mouse sequence alignments reveal novel regulatory elements: a reason to sequence the mouse genome. Genome Res. 7:959-66. Hardison RC. 2000. Conserved noncoding sequences are reliable guides to regulatory elements. Trends Genet. 16: 369-372. Hattori, M., Fujiyama, A., Taylor, T.D., Watanabe, H., Yada, T., Park, H.S., Toyoda, A., Ishii K, Totoki Y, Choi DK, et al. 2000. The DNA sequence of human chromosome 21. Nature 405: 311-319. Henkel, G., Weiss, D.L., McCoy, R., Deloughery , T., Tara, D., and Brown, M.A. 1992. A DNase I-hypersensitive site in the second intron of the murine IL-4 gene defines a mast cell-specific enhancer. Immunology 149: 3239-3246. Hubbard, T., Barker, D., Birney, E., Cameron G., Chen, Y., Clark, L., Cox, T., Cuff, J., Curwen, V., Down, T., et al. 2002. The Ensembl genome database project. Nucleic Acids Res. 30: 38-41. Huxley, C. 1997. Mammalian artificial chromosomes and chromosome transgenics. TIG 13: 345-347.   15 International Human Genome Sequencing (I.H.G.S.) Consortium. 2001. Initial sequencing and analysis of the human genome. International Human Genome Sequencing Consortium. Nature 409: 860-921 Kent, J. 2002. BLAT - The BLAST-Like Alignment Tool. Genome Res. 12: 656-664 Kent,W.J, Sugnet, C.W. Furey,T.S. , Roskin,K.M., Pringle,T.H. Alan M. Zahler, A.M., Haussler, D. 2002. The Human Genome Browser at UCSC. Genome Res. 6: 996-1006. Kent W.J., and Zahler, A.M. 2000. The intronerator: exploring introns and alternative splicing in Caenorhabditis elegans. Nucleic Acids Res. 28: 91-93. Krivan, W. and Wasserman, W.W. 2001. A predictive model for regulatory sequences directing liver-specific transcription. Genome Res 11: 1559-1566. Lawn, R.M., Schwartz, K., Patthy, L. Convergent evolution of apolipoprotein(a) in primates and hedgehog. Proc. Natl. Acad. Sci, 94: 11992-11997. Loots, G.G., Locksley, R.M., Blankespoor, C.M., Wang, Z.E., Miller, W., Rubin, E.M., and Frazer, K.A. 2000. Identification of a coordinate regulator of cytokines 4, 13, and 5 by cross-species sequence comparisons. Science 288: 136-140. Ma, B., Tromp, J., and Li, M. 2002. PatternHunter: faster and more sensitive homology search. Bioinformatics 18: 440-445. Mardis, E., McPherson, J., Martienssen, R., Wilson, R.K., and McCombie, W.R. 2002. What is finished, and why does it matter. Genome Res. 12: 669-671. Mayor, C., Brudno, M., Schwartz, J.R., Poliakov, A., Rubin, E.M., Frazer, K.A., Pachter, L., and Dubchak, I. 2000. VISTA: Visualizing global DNA sequence alignments of arbitrary length. Bioinformatics 16: 1046-1047. Miller W. 2001. Comparison of genomic DNA sequences: solved and unsolved problems. Bioinformatics 17: 391-397 Mural, R. et al. A comparison of whole genome derived mouse chromosome 16 and the human genome. 2002. Science 296: 1661-1671   16 Oeltjen, J.C., Malley, T.M., Muzny, D.M., Miller, W., Gibbs, R.A., and Belmont, J.W. 1997. Large-scale comparative sequence analysis of the human and murine Bruton's tyrosine kinase loci reveals conserved regulatory domains. Genome Res. 7: 315-329. Pachter, L. , Alexandersson, M., and Cawley, S. 2002. Applications of Generalized Pair Hidden Markov Models to Alignment and Gene Finding Problems. J. Comp. Biol. 9: 389-399 Pennacchio, L.A., Olivier, M., Hubacek, J.A., Cohen, J.C., Cox, D.R., Fruchart , J.C., Krauss, R.M., and Rubin, E.M. 2001. An apolipoprotein influencing triglycerides in humans and mice revealed by comparative sequencing. Science 294: 169-173. Rogic, S., Ouellette F. and Mackworth A.K. 2002 Improving gene recognition accuracy by combining predictions from two gene-finding program. Bioinformatics 2002 18: 1034-1045. Smith, T.F. and Waterman, M.S. 1981. Identification of common molecular subsequences. J. Mol. Biol. 147: 195-197 Schwartz, S., Kent, W.J., Smit, A., Zhang, Z., Baertsch, R., Hardison, R., Haussler, D., Miller, W. Human-mouse alignment with Blastz (submitted) Tatusov, R.L., Koonin, E.V., and Lipman, D.J. 1997. A genomic perspective on protein families. Science 278: 631-637. Venter, J.C. et al. 2001. The sequence of the human genome. Science 291: 1304-1351. Waterston et al. 2002. Initial sequencing and comparative analysis of the mouse genome. Nature, in press.  Website references.  http://pipeline.lbl.gov, Comparative analysis pipeline gateway at Lawrence Berkeley National laboratory.   17 http://pipeline.lbl.gov/cgi-bin/cnc, Database of conserved sequences from LBNL, PSU, and UCSC. http://pipeline.lbl.gov/tradeoff/, Results of the study on specificity and sensitivity of different anchoring techniques. http://bio.math.berkeley.edu/avid/ Avid website http://repeatmasker.genome.washington.edu/cgi-bin/RepeatMasker, RepeatMasker. http://www.tigr.org/tdb/tgi/software/, TIGR`s standalone low complexity (""dust"") filter http://genome.ucsc.edu/, UCSC web site from which human genome assemblies used in the study where downloaded from   18  Figure 1. General scheme of the pipeline. The pipeline processes individual contigs, supercontigs or long fragments of assemblies. Figure 2. Heuristic for selecting anchors to determinate candidates regions for global alignment. Figure 3. location at chr3:38787874-38793594 on the Human Genome, June 2002 (hg12/ncbi30) where LAMR1 gene is covered by the alignments of sequences from different Mouse chromosomes. Figure 4. The global alignment of the mouse finished sequence NT_002570 against the region found by BLAT anchors revealed conserved coding and non coding elements not found by the BLAT program. The anchoring scheme is sensitive enough to provide the global alignment with the correct homology candidate. The location found for this Mouse finished contig on the Human genome, June 2002 (hg12/ncbi30) is chr20:4297459042993423. Figure 5. The ratio of the number of nucleotides on each human chromosome covered by alignments of the random mouse sequence and the number of nucleotides covered by the real mouse sequence for each chromosome. Threshold definition is described in the alignment section of Waterson et al, 2002. Figure 6. Apolipoprotein(a) region. The expressed gene is confined to a subset of primates, as most mammal lack apo(a) (only hedgehogs produce an apo(a)-like protein) (Lawn et al. 1997). This figure shows the coverage in this region by the mouse sequence utilizing Blastz (Schwartz et al. 2002) and the method presented here. Our method is the only one to predict that apoa(a) has no homology in the mouse, as it had been shown experimentally. Figure 7. results of a on-line submission of a draft unannotated platypus sequence to the genome alignment web server. The gene has been correctly identified. It is interesting to note the general lack of conservation in non-coding regions, except for a few highly conserved islands. The submission was done directly with the GENBANK accession number AC130185 and was completed in less than 30 seconds.   19  Table 1: Alignment strategies for different types of assemblies.  Method Contigs Scaffold  Scheme of alignment Individual contigs contigs can be reoriented and reordered  Examples Finished BACs Arachne October 2001 Phusion November 2001 Celera chromosome 16 MGSC v3  Chopped pieces  mouse chromosomes are chopped in 250 kb and aligned to the Human Genome   20 Table2. percentage of bases pairs covered for known coding and non-coding functional features of the human genome (see text for details).  Overall coverage Feature Coverage Exons UTR Upstream 500 Upstream 200 Upstream 100 Downstream 200  matrix loose threshold=2500 22.15%  matrix medium threshold=2500 7.26%  matrix loose threshold=3400 4.48%  90.93% 72.21% 56.08% 65.94% 70.83% 53.42%  88.19% 34.43% 23.35% 33.01% 38.94% 17.62%  85.76% 23.96% 15.19% 22.61% 27.38% 10.85%   21  Table 3. specificity test: coverage on human chromosome 20 only by all the mouse chromosomes except chromosome 2 (see text for details).  Overall coverage Features Coverage exons UTR upstream 500 upstream 200 upstream 100 downstream 200  matrix loose threshold=2500 0.49%  matrix medium threshold=2500 0.29%  matrix tight threshold=3400 0.22%  5.57% 3.85% 0.10% 0.24% 0.46% 1.59%  5.36% 2.71% 0.09% 0.22% 0.43% 0.91%  5.06% 1.84% 0.08% 0.19% 0.35% 0.23%"
GX090-15-4802477	"Next   Previous   Contents     2. Traditional Artificial Intelligence         Traditional AI is based around the ideas of logic, rule systems, linguistics, and the concept of rationality.  At its roots are programming languages such as Lisp and Prolog. Expert systems are the largest successful example of this paradigm.  An expert system consists of a detailed knowledge base and a complex rule system to utilize it.  Such systems have been used for such things as medical diagnosis support and credit checking systems.      2.1 AI class/code libraries        These are libraries of code or classes for use in programming within the artificial intelligence field.  They are not meant as stand alone applications, but rather as tools for building your own applications.              ACL2     Web site:   www.telent.net/cliki/ACL2     ACL2 (A Computational Logic for Applicative Common Lisp) is a theorem prover for industrial applications. It is both a mathematical logic and a system of tools for constructing proofs in the logic.  ACL2 works with GCL (GNU Common Lisp).          AI Search II     WEB site:   www.bell-labs.com/topic/books/ooai-book/     Submitted by:   Peter M. Bouthoorn   Basically, the library offers the programmer a set of search algorithms that may be used to solve all kind of different problems. The idea is that when developing problem solving software the programmer should be able to concentrate on the representation of the problem to be solved and should not need to bother with the implementation of the search algorithm that will be used to actually conduct the search. This idea has been realized by the implementation of a set of search classes that may be incorporated in other software through  C++ 's features of derivation and inheritance.  The following search algorithms have been implemented:        depth-first tree and graph search.   breadth-first tree and graph search.   uniform-cost tree and graph search.   best-first search.   bidirectional depth-first tree and graph search.   bidirectional breadth-first tree and graph search.   AND/OR depth tree search.   AND/OR breadth tree search.       This library has a corresponding book, ""  Object-Oriented Artificial Instelligence, Using C++ "".          Chess In Lisp (CIL)     FTP site:   chess.onenet.net/pub/chess/uploads/projects/       The CIL (Chess In Lisp) foundation is a Common Lisp implementaion of all the core functions needed for development of chess applications.  The main purpose of the CIL project is to get AI researchers interested in using Lisp to work in the chess domain.            DAI     Web site:   starship.python.net/crew/gandalf/DNET/AI/     A library for the Python programming language that provides an object oriented interface to the CLIPS expert system tool. It  includes an interface to COOL (CLIPS Object Oriented Language) that allows:    Investigate COOL classes   Create and manipulate with COOL instances   Manipulate with COOL message-handler's   Manipulate with Modules           HTK     Web site:   htk.eng.cam.ac.uk     The Hidden Markov Model Toolkit (HTK) is a portable toolkit for building and manipulating hidden Markov models.  HTK consists of a set of library modules and tools available in C source form. The tools provide sophisticated facilities for speech analysis, HMM training, testing and results analysis. The software supports HMMs using both continuous density mixture Gaussians and discrete distributions and can be used to build complex HMM systems.  The HTK release contains extensive documentation and examples.        LK     Web site:   www.cs.utoronto.ca/~neto/research/lk/     LK is an implementation of the Lin-Kernighan heuristic for the Traveling Salesman Problem and for the minimum weight perfect matching problem. It is tuned for 2-d geometric instances, and has been applied to certain instances with up to a million cities. Also included are instance generators and Perl scripts for munging TSPLIB instances.   This implementation introduces ``efficient cluster compensation'', an experimental algorithmic technique intended to make the Lin-Kernighan heuristic more robust in the face of clustered data.          Nyquist     Web site:   www.cs.cmu.edu/afs/cs.cmu.edu/project/music/web/music.html       The Computer Music Project at CMU is developing computer music and interactive performance technology to enhance human musical experience and creativity. This interdisciplinary effort draws on Music Theory, Cognitive Science, Artificial Intelligence and Machine Learning, Human Computer Interaction, Real-Time Systems, Computer Graphics and Animation, Multimedia, Programming Languages, and Signal Processing. A paradigmatic example of these interdisciplinary efforts is the creation of interactive performances that couple human musical improvisation with intelligent computer agents in real-time.          PDKB     Web site:   lynx.eaze.net/~pdkb/web/   SourceForge site:   sourceforge.net/project/pdkb     Public Domain Knowledge Bank (PDKB) is an Artificial Intelligence Knowledge Bank of common sense rules and facts. It is based on the Cyc Upper Ontology and the MELD language.          Python Fuzzy Logic Module     FTP site:   ftp://ftp.csh.rit.edu/pub/members/retrev/     A simple python module for fuzzy logic. The file is 'fuz.tar.gz' in this directory. The author plans to also write a simple genetic algorithm and a neural net library as well. Check the 00_index file in this directory for release info.        QUANT1     Web site:   linux.irk.ru/projects/QUANT/     QUANT/1 stands for type QUANTifier. It aims to be an alternative to Prolog-like (Resulutional-like) systems. Main features include a lack of necessity for eliminating Quantifiers, scolemisation, ease of comprehension, large scale formulae operation, acceptance of nonHorn formulaes, and Iterative deeping. The actual library implemented in this project is called ATPPCF (Automatic Theorem Prover in calculus of Positively Constructed Formulae).  ATPPCF will be a library (inference engine) and an extension of the Predicate Calculus Language as a new logical language. The library will be incorporable in another software such as TCL, Python, Perl. The engine's primary inference method will be the ""search of inference in language of Positively Constructed Formulas (PCFs)"" (a subset of Predicate Calculus well translated in both directions). The language will be used as scripting language to the engine. But there will be possibility to replace it with extensions languages of main software.          Screamer     Web site:   www.cis.upenn.edu/~screamer-tools/home.html     Screamer is an extension of Common Lisp that adds support for nondeterministic programming. Screamer consists of two levels. The basic nondeterministic level adds support for backtracking and undoable side effects.  On top of this nondeterministic substrate, Screamer provides a comprehensive constraint programming language in which one can formulate and solve mixed systems of numeric and symbolic constraints. Together, these two levels augment Common Lisp with practically all of the functionality of both Prolog and constraint logic programming languages such as CHiP and CLP(R). Furthermore, Screamer is fully integrated with Common Lisp. Screamer programs can coexist and interoperate with other extensions to Common Lisp such as CLOS, CLIM and Iterate.          ThoughtTreasure     Web site:   www.signiform.com/tt/htm/tt.htm     ThoughtTreasure is a project to create a database of commonsense rules for use in any application. It consists of a database of a little over 100K rules and a C API to integrate it with your applications. Python, Perl, Java and TCL wrappers are already available.                    2.2 AI software kits, applications, etc.            These are various applications, software kits, etc. meant for research in the field of artificial intelligence. Their ease of use will vary, as they were designed to meet some particular research interest more than as an easy to use commercial package.          ASA - Adaptive Simulated Annealing     Web site:   www.ingber.com/#ASA-CODE   FTP site:   ftp.ingber.com/       ASA (Adaptive Simulated Annealing) is a powerful global optimization C-code algorithm especially useful for nonlinear and/or stochastic systems.    ASA is developed to statistically find the best global fit of a nonlinear non-convex cost-function over a D-dimensional space. This algorithm permits an annealing schedule for 'temperature' T decreasing exponentially in annealing-time k, T = T_0 exp(-c k^1/D). The introduction of re-annealing also permits adaptation to changing sensitivities in the multi-dimensional parameter-space. This annealing schedule is faster than fast Cauchy annealing, where T = T_0/k, and much faster than Boltzmann annealing, where T = T_0/ln k.            Babylon     FTP site:   ftp.gmd.de/gmd/ai-research/Software/Babylon/     BABYLON is a modular, configurable, hybrid environment for developing expert systems. Its features include objects, rules with forward and backward chaining, logic (Prolog) and constraints. BABYLON is implemented and embedded in Common Lisp.        cfengine     Web site:   www.iu.hioslo.no/cfengine/     Cfengine, or the configuration engine is a very high level language for building expert systems which administrate and configure large computer networks. Cfengine uses the idea of classes and a primitive form of intelligence to define and automate the configuration of large systems in the most economical way possible. Cfengine is design to be a part of computer immune systems.        CLEARS     Web site:   www.coli.uni-sb.de/~clears/     The CLEARS system is an interactive graphical environment for computational semantics. The tool allows exploration and comparison of different semantic formalisms, and their interaction with syntax. This enables the user to get an idea of the range of possibilities of semantic construction, and also where there is real convergence between theories.        CLIG     Web site:   www.ags.uni-sb.de/~konrad/clig.html     CLIG is an interactive, extendible grapher for visualizing linguistic data structures like trees, feature structures, Discourse Representation Structures (DRS), logical formulas etc. All of these can be freely mixed and embedded into each other. The grapher has been designed both to be stand-alone and to be used as an add-on for linguistic applications which display their output in a graphical manner.        CLIPS     Web site:   www.jsc.nasa.gov/~clips/CLIPS.html   FTP site:   cs.cmu.edu/afs/cs.cmu.edu/project/ai-repository/ai/areas/expert/systems/clips     CLIPS is a productive development and delivery expert system tool which provides a complete environment for the construction of rule and/or object based expert systems.      CLIPS provides a cohesive tool for handling a wide variety of knowledge with support for three different programming paradigms: rule-based, object-oriented and procedural.  Rule-based programming allows knowledge to be represented as heuristics, or ""rules of thumb,"" which specify a set of actions to be performed for a given situation. Object-oriented programming allows complex systems to be modeled as modular components (which can be easily reused to model other systems or to create new components).  The procedural programming capabilities provided by CLIPS are similar to capabilities found in languages such as C, Pascal, Ada, and LISP.        EMA-XPS - A Hybrid Graphic Expert System Shell     Web site:   wmwap1.math.uni-wuppertal.de:80/EMA-XPS/       EMA-XPS is a hybrid graphic expert system shell based on the ASCII-oriented shell Babylon 2.3 of the German National Research Center for Computer Sciences (GMD). In addition to Babylon's AI-power (object oriented data representation, forward and backward chained rules - collectible into sets, horn clauses, and constraint networks) a graphic interface based on the X11 Window System and the OSF/Motif Widget Library has been provided.        FOOL & FOX     FTP site:   ntia.its.bldrdoc.gov/pub/fuzzy/prog/       FOOL stands for the Fuzzy Organizer OLdenburg. It is a result from a project at the University of Oldenburg. FOOL is a graphical user interface to develop fuzzy rulebases.  FOOL will help you to invent and maintain a database that specifies the behavior of a fuzzy-controller or something like that.    FOX is a small but powerful fuzzy engine which reads this database, reads some input values and calculates the new control value.        FUF and SURGE     Web site:   www.dfki.de/lt/registry/generation/fuf.html   FTP site:   ftp.cs.columbia.edu/pub/fuf/     FUF is an extended implementation of the formalism of functional unification grammars (FUGs) introduced by Martin Kay specialized to the task of natural language generation. It adds the following features to the base formalism:    Types and inheritance.    Extended control facilities (goal freezing, intelligent backtracking).    Modular syntax.     These extensions allow the development of large grammars which can be processed efficiently and can be maintained and understood more easily.  SURGE is a large syntactic realization grammar of English written in FUF. SURGE is developed to serve as a black box syntactic generation component in a larger generation system that encapsulates a rich knowledge of English syntax. SURGE can also be used as a platform for exploration of grammar writing with a generation perspective.        The Grammar Workbench     Web site:   www.cs.kun.nl/agfl/GWB.html             The Grammar Workbench, or GWB for short, is an environment for the comfortable development of Affix Grammars in the AGFL-formalism. Its purposes are:     to allow the user to input, inspect and modify a grammar;    to perform consistency checks on the grammar;    to compute grammar properties;    to generate example sentences;    to assist in performing grammar transformations.            GSM Suite     Web site:   www.slip.net/~andrewm/gsm/     The GSM Suite is a set of programs for using Finite State Machines in a graphical fashion. The suite consists of programs that edit, compile, and print state machines. Included in the suite is an editor program, gsmedit, a compiler, gsm2cc, that produces a C++ implementation of a state machine, a PostScript generator, gsm2ps, and two other minor programs. GSM is licensed under the GNU Public License and so is free for your use under the terms of that license.        Illuminator     Web site:   documents.cfar.umd.edu/resources/source/illuminator.html     Illuminator is a toolset for developing OCR and Image Understanding applications.  Illuminator has two major parts: a library for representing, storing and retrieving OCR information, heretofore called dafslib, and an X-Windows ""DAFS"" file viewer, called illum. Illuminator and DAFS lib were designed to supplant existing OCR formats and become a standard in the industry. They particularly are extensible to handle more than just English.  The features of this release:     5 magnification levels for images   flagged characters and words   unicode support -- American, British, French, German,  Greek, Italian, MICR, Norwegian, Russian, Spanish, Swedish,  keyboards    reads DAFS, TIFF's, PDA's (image only)   save to DAFS, ASCII/UTF or Unicode   Entity Viewer - shows properties, character choices,  bounding boxes image fragment for a selected entity, change  type, change content, hierarchy mode           Isabelle     Web site:   isabelle.in.tum.de     Isabelle is a popular generic theorem prover developed at Cambridge University and TU Munich. Existing logics like Isabelle/HOL provide a theorem proving environment ready to use for sizable applications. Isabelle may also serve as framework for rapid prototyping of deductive systems. It comes with a large library including Isabelle/HOL (classical higher-order logic), Isabelle/HOLCF (Scott's Logic for Computable Functions with HOL), Isabelle/FOL (classical and intuitionistic first-order logic), and Isabelle/ZF (Zermelo-Fraenkel set theory on top of FOL).        Jess, the Java Expert System Shell     Web site:   herzberg.ca.sandia.gov/jess/     Jess is a clone of the popular CLIPS expert system shell written entirely in Java. With Jess, you can conveniently give your applets the ability to 'reason'. Jess is compatible with all versions of Java starting with version 1.0.2. Jess implements the following constructs from CLIPS: defrules, deffunctions, defglobals, deffacts, and deftemplates.          learn     FTP site:   sunsite.unc.edu/pub/Linux/apps/cai/     Learn is a vocable learning program with memory model.         LISA     Web site:   lisa.sourceforge.net     LISA (Lisp-based Intelligent Software Agents) is a production-rule system heavily influenced by JESS (Java Expert System Shell). It has at its core a reasoning engine based on the Rete pattern matching algorithm. LISA also provides the ability to reason over ordinary CLOS objects.        NICOLE     Web site:   nicole.sourceforge.net     NICOLE (Nearly Intelligent Computer Operated Language Examiner) is a theory or experiment that if a computer is given enough combinations of how words, phrases and sentences are related to one another, it could talk back to you. It is an attempt to simulate a conversation by learning how words are related to other words. A human communicates with NICOLE via the keyboard and NICOLE responds back with its own sentences which are automatically generated, based on what NICOLE has stored in it's database. Each new sentence that has been typed in, and NICOLE doesn't know about, is included into NICOLE's database, thus extending the knowledge base of NICOLE.        Otter: An Automated Deduction System     Web site:   www-unix.mcs.anl.gov/AR/otter/     Our current automated deduction system  Otter is designed to prove theorems stated in first-order logic with equality.  Otter's inference rules are based on resolution and paramodulation, and it includes facilities for term rewriting, term orderings, Knuth-Bendix completion, weighting, and strategies for directing and restricting searches for proofs.   Otter can also be used as a symbolic calculator and has an embedded equational programming system.        PVS     Web site:   pvs.csl.sri.com/     PVS is a verification system: that is, a specification language integrated with support tools and a theorem prover. It is intended to capture the state-of-the-art in mechanized formal methods and to be sufficiently rugged that it can be used for significant applications. PVS is a research prototype: it evolves and improves as we develop or apply new capabilities, and as the stress of real use exposes new requirements.          RIPPER     Web site:   www.research.att.com/~wcohen/ripperd.html       Ripper is a system for fast effective rule induction. Given a set of data, Ripper will learn a set of rules that will predict the  patterns in the data. Ripper is written in ASCI C and comes with documentation and some sample problems.          SNePS     Web site:   www.cs.buffalo.edu/pub/sneps/WWW/   FTP site:   ftp.cs.buffalo.edu/pub/sneps/     The long-term goal of The SNePS Research Group is the design and construction of a natural-language-using computerized cognitive agent, and carrying out the research in artificial intelligence, computational linguistics, and cognitive science necessary for that endeavor. The three-part focus of the group is on knowledge representation, reasoning, and natural-language understanding and generation. The group is widely known for its development of the SNePS knowledge representation/reasoning system, and Cassie, its computerized cognitive agent.              Soar     Web site:   bigfoot.eecs.umich.edu/~soar/   FTP site:   cs.cmu.edu/afs/cs/project/soar/public/Soar6/       Soar has been developed to be a general cognitive architecture. We intend ultimately to enable the Soar architecture to:    work on the full range of tasks expected of an intelligent agent, from highly routine to extremely difficult, open-ended problems   represent and use appropriate forms of knowledge, such as procedural, declarative, episodic, and possibly iconic   employ the full range of problem solving methods    interact with the outside world and    learn about all aspects of the tasks and its performance on them.      In other words, our intention is for Soar to support all the capabilities required of a general intelligent agent. http://wwwis.cs.utwente.nl:8080/ tcm/index.html        TCM     Web site:   wwwis.cs.utwente.nl:8080/~tcm/index.html   FTP site:   ftp.cs.vu.nl/pub/tcm/       TCM (Toolkit for Conceptual Modeling) is our suite of graphical editors. TCM contains graphical editors for Entity-Relationship diagrams, Class-Relationship diagrams, Data and Event Flow diagrams, State Transition diagrams, Jackson Process Structure diagrams and System Network diagrams, Function Refinement trees and various table editors, such as a Function-Entity table editor and a Function Decomposition table editor.  TCM is easy to use and performs numerous consistency checks, some of them immediately, some of them upon request.          WEKA     Web site:   lucy.cs.waikato.ac.nz/~ml/       WEKA (Waikato Environment for Knowledge Analysis) is an state-of-the-art facility for applying machine learning techniques to practical problems. It is a comprehensive software ""workbench"" that allows people to analyse real-world data. It integrates different machine learning tools within a common framework and a uniform user interface. It is designed to support a ""simplicity-first"" methodology, which allows users to experiment interactively with simple machine learning tools before looking for more complex solutions.              Next   Previous   Contents"
GX261-74-10407099	"Do cument Structure Analysis Algorithms: A Literature Survey Song Maoa , Azriel Rosenfelda , and Tapas Kanungob Center for Automation Research  University of Maryland, College Park, MD 20742  Email: maosong,ar@cfar.umd.edu  IBM Almaden Research Center  650 Harry Road, San Jose, CA 95120  Email: kanungo@almaden.ibm.com  ABSTRACT Document structure analysis can be regarded as a syntactic analysis problem. The order and containment relations among the physical or logical components of a document page can be described by an ordered tree structure and can be modeled by a tree grammar which describes the page at the component level in terms of regions or blocks. This paper provides a detailed survey of past work on document structure analysis algorithms and summarize the limitations of past approaches. In particular, we survey past work on document physical layout representations and algorithms, document logical structure representations and algorithms, and performance evaluation of document structure analysis algorithms. In the last section, we summarize this work and point out its limitations. b a  1. INTRODUCTION Electronic documents have many advantages over paper documents, including compact and lossless storage, easy maintainance, efficient retrieval and fast transmission. As a result, there has been extensive research on converting paper-based documents into electronic documents. One of the ma jor advantages of electronic documents is that an electronic document can have an explicit structure; it can be partitioned into a hierarchy of physical components, such as pages, columns, paragraphs, textlines, words, tables, figures, halftones, etc.; a hierarchy of logical components, such as (for example) titles, authors, affiliations, abstracts, sections, etc.; or both. This structural information can be very useful in indexing and retrieving the information contained in the document. Document understanding modules, such as Optical Character Recognition (OCR) and graphics recognition modules, can also be selectively applied to the structural components of document images. Physical layout and logical structure analysis of document images is a crucial stage in a document image analysis system. Numerous algorithms have been proposed to analyze the physical layout and logical structure of document images in many different domains. Previous surveys14 of these algorithms have been given in relatively smaller scale, or are not current and do not categorize the surveyed algorithms in detail. In this paper, we provide a detailed survey of these algorithms in the following three aspect: document physical layout representation and analysis algorithms, document logical structure representation and analysis algorithms, and performance evaluation. Song Mao is now with the Communications Engineering Branch, U. S. National Library of Medicine, Bethesda, Maryland.   2. DOCUMENT PHYSICAL LAYOUT REPRESENTATIONS AND ANALYSIS ALGORITHMS Document physical layout can be represented in various forms, independently of or jointly with document logical structure. Document style parameters have been used to represent document physical layout in.59 These style parameters typically correspond to sizes of and gaps between document ob jects such as characters, words, lines or zones. While this representation method provides useful information, it does not fully reflect the spatial relations among document physical components. Document physical layout can be more fully represented by trees that are derived from a set of rules, as in.1012 Such a representation describes the spatial relations, in many cases hierarchical, among document physical components. A disadvantage of rule-based representations is that the rules can become rather arbitrary. Representations based on formal grammars have the advantage that the type of grammar (in the Chomsky hierarchy) limits the types of productions that can be used, and hence constrains the rules that the language can satisfy. Systems that use grammars to describe hierarchical document physical layout are described at the end of this section. In grammar-based algorithms, a document is usually regarded as a sequence, i.e. a string, of features of physical components. Document image physical layout analysis algorithms can be categorized into three classes: top-down ap proaches, bottom-up approaches and hybrid approaches. Top-down algorithms start from the whole document image and iteratively split it into smaller ranges. The splitting procedure stops when some criterion is met and the ranges obtained at that stage constitute the final segmentation results. Bottom-up algorithms start from document image pixels, and cluster the pixels into connected components such as characters which are then clustered into words, lines or zones. Hybrid algorithms can be regarded as a mix of the above two approaches. The Docstrum algorithm of O'Gorman,13 the Voronoi-diagram-based algorithm of Kise et al.,14 the run-length smearing algorithm of Wahl et al.,15 the segmentation algorithm of Jain and Yu,3 and the text string separation algorithm of Fletcher and Kasturi16 are typical bottom-up algorithms. The X - Y -cut-based algorithm of Nagy et al.17 and the shape-directed-covers-based algorithm of Baird et al.18 are top-down algorithms. Pavlidis and Zhou19 proposed a hybrid algorithm using a split-and-merge strategy. Surveys of page segmentation algorithms can be found in O'Gorman and Kasturi20 and Jain and Yu.3 A recent workshop21 was devoted to addressing issues related to physical layout analysis. Most of the algorithms mentioned above do not create hierarchical descriptions or allow users to specify document structure information. Furthermore, they do not provide methods of estimating algorithm parameters from groundtruth data. A rigorous empirical comparison of five document physical layout analysis using the PSET software package22 can be found in Mao and Kanungo.23 Liang et al.24 propose a performance metric for evaluating document structure extraction algorithms. They describe a method for finding the optimal tuning parameters of their algorithm. They evaluated several document layout analysis algorithms on 1600 images from the UW-III dataset. An OCR zoning evaluation method based on string matching is proposed by25 and a yearly conference26 is devoted to the evaluation of OCR accuracy of various OCR algorithms. Yanikoglu and Vincent27 describe an environment (called Pink Pather) for ground-truthing and benchmarking document page segmentation. They use a bitmap-level region-based metric. A few researchers have developed document physical layout analysis algorithms that make use of grammatical methods. Kopec and Chou28 describe an algorithm for segmenting a column of text that is modeled using a stochastic regular grammar. However, their algorithm assumes that it is given templates for the symbols in the language; this is not always the case, for example if we must analyze document pages in a previously unknown language. The algorithm also assumes that the page is segmented into columns by some other procedure, and it does not provide any estimation procedure for the model parameters. Tokuyasu and Chou29 recently proposed a communication theory approach to page segmentation. They used regular grammars to describe the structure of document page images in terms of axis-parallel rectangles obtained by subdividing the image vertically and horizontally, and they used a Turbo decoding approach to estimate the 2D image from the observations. However, they provided very limited experimental verification of their approach.   Krishnamoorthy et al.30 describe a hierarchical document page segmentation algorithm that constructs a tree in which each node represents an axis-parallel rectangle. Users can specify grammars for individual blocks. However, in the presence of noise their parsing algorithm can fail, and no method of parameter estimation is provided. No ob jective function is minimized; thus the analysis is not optimal. Spitz31 described a system for style-directed recognition. While the user can specify the style interactively, the algorithm itself is a rule-based system.  3. DOCUMENT LOGICAL STRUCTURE REPRESENTATIONS AND ANALYSIS ALGORITHMS Document logical structure can be represented by logical labels of document physical components. These logical labels usually are derived from a set of rules.5, 8, 9, 32, 33 In this representation method, there is no description of semantic relations among logical components. To reflect these relations, document logical structures are represented by trees that are derived either from a set of rules6, 1012, 34 or from formal grammars.30, 3537 The document is regarded as a sentence which can be either a string of logical labels or a string of observed features of document physical components. The grammatical rules used in most algorithms for either physical layout analysis or logical structure analysis30, 35, 36 are deterministic. It is difficult for deterministic parsing methods to remove ambiguity in the parsing results; for the same input, multiple parse trees can be generated. In some applications, the input sentence is probabilistic (for example, derived from a preceding physical layout analysis), some inputs are not accurate, and they often have errors. Deterministic parsing cannot handle any of these situations. Tateisi and Itoh37 augmented the grammars by a set of cost attributes and were able to select a parsing result that had least cost. Tsujimoto and Asada10 represented document physical layout and logical structure as trees. They posed document understanding as the transformation of a physical tree into a logical one using a set of generic transformation rules and a virtual field separator technique. The physical tree is constructed using block dominating rules. The blocks in the tree are classified into head and body using rules related to the physical properties of the block. Once the logical tree is obtained, logical labels are assigned to the blocks using another set of rules. The logical labels include title, abstract, sub-title, paragraph, header, footer, page number, and caption. To effectively use the information carried by field separators and frames, a virtual field separator technique, in which separators and frames are considered as virtual physical blocks in the physical layout tree, is used for tree transformation without increasing the number of transformation rules. They tested their algorithm on 106 pages from various sources and reported a 94/106 logical structure recognition accuracy. Errors were due to inaccurate physical segmentation, insufficient transformation rules, and the fact that some pages did not have hierarchical physical and logical structures. Yamashita et al.11 proposed a model-based method for logical structure analysis. The model is a treestructured layout model which defines the minimum necessary information about the geometrical arrangement of document ob jects. Specifically, the model describes each document ob ject's logical label, tree level, separator location, minimum and maximum numbers of constituent character strings, as well as its successor's orientation. The physical segments are character strings, lines, and picture elements, and they are segmented using extracted horizontal and vertical separators. The picture elements are removed. Logical labels are then assigned to character strings consistently with the layout model using a relaxation method. If contradictory labeling occurs, all related labels are deleted. If two or more labels are assigned to a character string, a confidence value is computed for all possible labeling paths and the path with the highest confidence value is retained. Seventyseven Japanese patent application front pages were used for testing the algorithm, and fifty-nine of them were correctly labeled. Errors were due to incorrect recognition of the page number as part of the body, skew, blots, and connected character strings. Kreich et al.5 described an experimental environment called SODA (System for Office Document Analysis) for model-based document analysis. They first used a bottom-up approach to group connected components into text blocks, then found lines within each text block and words within each line. OCR and graphics recognition were performed on the document segments. The domain knowledge and metaknowledge, the physical layout and logical structure knowledge were stored in a knowledge base. Document ob jects were matched to the layout   and logical information in the knowledge base. A generalized Hamming metric was used in the matching process to calculate a confidence measure. A match was considered successful if its confidence measure was greater than a threshold. The experimental result on one letter was displayed. Otherwise, no quantitative performance data were reported. Fisher12 presented a rule-based system for recognizing the physical layout and logical structure of a document image without prior information about the document's format or content. The system automatically extracts the general physical layout of the document and transforms it into a logical structure. Three types of rules are used in the system: location cues, format cues, and textual cues. The system can reconstruct paragraphs broken during formatting, determine the read order of text blocks, and express its results in a document markup language. Text and nontext regions are assumed to be already identified. First, words are grouped into paragraphs or columns. Then text column boundaries and locations are identified. The physical layout and logical structure are determined using the appropriate rules. The algorithm's performance was highly dependent on the accuracy of the text/nontext segmentation process. The analysis results were expressed in Maker Interchange Format (MIF). No experimental results were given. Derrien-Peden34 proposed a frame-based system for analyzing document physical layout and logical struc ture. The document layout structure is obtained in three steps. First document columns and text blocks are obtained by recursively performing an X - Y cut. Then lines are extracted using special rules. Finally, physical zones are obtained by analyzing their topographical features. The reading order is obtained by a depth-first search of the layout structure. Logical structure recognition is conducted in two steps: 1) paragraphs with the same features are grouped into classes, 2) logical labels are assigned to each class using a set of general layout rules. A knowledge base containing both the physical layout and logical structure models is used during the analysis procedure. No experimental results were reported for this algorithm. Ingold and Armangil35 proposed a document logical structure recognition method using a formal description of each document class that includes composition rules and presentation rules. The composition rules define the generic logical structure, and the presentation rules define the physical characteristics of the logical entities to be recognized. The composition rules are formally represented by Extended Backus-Naur Form grammars. The document description completely defines an analysis graph whose vertices are labeled with the classes of entities to be recognized. The successor of an entity is the logical label of the entity that will be evaluated after the current entity. Alternatives specify possible replacements for the current entity. Document analysis is achieved by finding a path through such a graph under the constraint that the typographic attributes of an entity on the path must match those of the corresponding document ob ject. The authors assumed that the physical zones were already segmented out and OCR was performed on the logical ob jects. No experimental results were reported. Brugger et al.38 described a document logical structure model based on a statistical representation of patterns in a document class, i.e. on generalized N -grams. In an N -gram model, only the previous N - 1 words can affect and can be used to estimate the probability of the current word in a sentence. The tree structure of document logical components is represented by the probabilities of local tree node patterns similar to N -grams. The logical tree is constructed from physical entities in conformity with the given model. There can be multiple valid trees, but only the tree with the best conformity with the model is selected. The model can be learned from samples. The physical segments were assumed to be available. Five memo pages were used in the experiments; one of them was used for training the model and the remaining four for testing the model. Conway36 used page grammars and page parsing techniques to recognize document logical structure from physical layout. The physical layout is described by a set of grammar rules, each of which is a string of compo nents specified by a neighbor relationship. Possible neighbor relationships include above, left-of, over, left-side, and close-to, so that the layout is two dimensional. Context-free string grammars are used to describe log ical structure. Both grammars are deterministic. The physical layout grammar has attached constraints to incorporate information such as font size, style, alignment and indentation. The physical segmentation is performed independently of logical structure recognition using a run length smoothing algorithm. No quantitative experimental results were reported.   Krishnamoorthy et al.30 proposed a document logical structure recognition method that recursively applies grammars to horizontal and vertical pro jection profiles of the page. The parsing process is divided into four stages. In the first stage, the lengths of runs of zeros or ones in the thresholded pro jection profiles are thresholded into atoms. In the second stage, the atoms are grouped into molecules. In the third stage, logical labels are assigned to the molecules. In the fourth stage, contiguous entities of the same type are merged. The results of the segmentation and logical labeling processes are saved in a labeled X - Y tree. This method transforms a twodimensional segmentation and labeling problem into a one-dimensional segmentation and labeling problem in an X - Y tree. The authors did not distinguish between physical layout and logical structure. Their algorithm was trained on twenty-one IBM journal pages, and was tested on twelve IBM/PAMI pages. The algorithm performance was reported in terms of percentage of labeled area and missed labels. Saitoh et al.7 presented a system for document segmentation, text area classification and ordering. This system is independent of the shapes of the physical blocks and is robust to document skew. Connected com ponents are first extracted and classified. The connected components are then merged into lines which are merged into zones. The extracted zones are classified into body, caption, header and footer. A tree structure is generated from the classified zones using text area influence ranges. The order of the text is obtained by preorder traversal of the tree. The experimental dataset included 131 Japanese and English documents which were scanned with skew. The size of the final dataset was 393 images. The authors used three criteria to evaluate their segmentation and classification results, and three other criteria to evaluate their text ordering results. Tateisi and Itoh37 posed document logical structure analysis as a stochastic syntactic analysis problem. The document is modeled as a string of text lines and graphic ob jects. The text lines and graphic ob jects are segmented and classified in a preprocessing step, and the string is parsed using a stochastic regular grammar with attributes. Characters within text lines are recognized and their font sizes are determined. Each grammatical rule is associated with a cost. The parser retains possible parsing results in order of their total cost. The algorithm was tested on seventy pages of Japanese text taken from books and magazines. The authors reported an 86% average markup accuracy on manuals and an 82% average markup accuracy on technical papers for the parsing result with the least cost. When the parsing result with the second least cost was used, the average markup accuracy for the technical journals increased to 89%. Niyogi and Srihari6 presented a system called DeLoS for document logical structure derivation. In this system, a computational model is developed based on a rule-based control structure as well as a hierarchical multi-level knowledge representation scheme. In this scheme, knowledge about the physical layouts and logical structures of various types of documents is encoded into a knowledge base. The system included three levels of rules: Knowledge rules, control rules, and strategy rules. The control rules control the application of knowledge rules and the strategy rules determine the usage of control rules. A document image is first segmented using a bottom-up algorithm. The segmented blocks are then classified. Finally, the classified blocks are input into the DeLoS system and a logical tree structure is derived. The DeLoS system was tested on 44 newspaper pages. The performance results were reported in terms of block classification accuracy, block grouping accuracy, and read-order extraction accuracy. Summers33 described an algorithm for automatic derivation of logical document structure from generic physical layout. The algorithm is divided into segmentation of text into zones and classification of these zones into logical components. The document logical structure is obtained by computing a distance measure between a physical segment and predefined prototypes. For each logical label, a set of prototypes is specified. The prototypes include contours, context, successor, height, symbols, and children. The algorithm was tested on 196 pages from computer science technical reports. The input was the segmented text blocks. The labeling result of each text segment was characterized as correct, overgeneralized, or incorrect. Two metrics, precise accuracy and generalized accuracy, were used to evaluate the performance. Accuracies above 85% were reported. Dengel and Dubiel39 described a system (DAVOS) that is capable of both learning and extracting document logical structure. DAVOS is a concept formation system that learns document structure concepts by detecting distinct attribute values in document ob jects. The structural concepts are represented by relation patterns defined by a cut-and-label language. A GTree (Geometric Tree) is used to represent the concept language.   Unsupervised decision tree based learning techniques are used to build the GTree. Two learning techniques were compared, a bottom-up approach and a top-down approach (DAVOS). The authors used forty letters to train both systems. They then used the learned GTrees to classify another set of forty unknown letters. The evaluation results were reported in terms of precision, recall, and F value metrics. The DAVOS system outperformed the bottom-up system. Lin et al.8 proposed a method of analyzing the logical structure of book pages using contents page infor mation. The contents page of a book contains a concise and accurate logical structure description of the whole book. Text lines are first extracted from the contents page, and OCR is then performed for each text line. The structures of the page number, head, foot, headline, chart and main text of the text page are analyzed and matched with information obtained from the contents page. The algorithm was tested on 235 pages. The experimental results were reported in terms of two labeling errors and the logical labeling identification rate. Ishitani9 proposed a document logical structure analysis system based on emergent computation. The system includes five interacting modules: typography analysis, ob ject recognition, ob ject segmentation, ob ject grouping, and ob ject modification. The interaction results in an adaptive system configuration which provides robust document analysis. The document image is first segmented into text lines, which are then classified into different types using special rules. The classified text lines are then grouped and classified into logical components using heuristic rules. The document ob jects that are incorrectly segmented can be modified by checking for logical consistency among ob jects. Modified ob jects are sent to other modules and new ob jects are created by module interactions. Since new logical structures are created, interactive computation among modules is induced by feedback between levels. This system was tested on 150 documents taken from various sources. The author reported a 96.3% average rate of correct logical ob ject extraction. Srihari et al.40 proposed a information-theory-based method for automatic address interpretation in postal address fields of mail pieces. Shannon's entropy theory is used to characterize address components and their interaction. Interested logical components are city name, state abbreviation, ZIP code, ZIP+4 add-on, primary number, street name, building/firm name, et al. Experimental results are shown on a US postal address directory. Other postal address analysis methods include.41 Kim et al.32 proposed a rule-based automated labeling module in MARS system (Medical Article Record System) to extract bibliographic records for the MEDLINE database. They derived rules from the results of a page layout analysis of medical journals and features extracted from OCR output. Therefore, both geometric and non-geometric features of journals are used in their labeling process. This system was tested on more than 11,000 articles in over 1,000 biomedical journals. The author reported a labeling accuracy that exceeds 96%.  4. ALGORITHM PERFORMANCE EVALUATION The performance evaluation of an algorithm should address the following aspects: performance metric, exper imental dataset, groundtruth specification, performance results, error analysis, and comparative evaluation. In this section, we survey document structure analysis algorithms with respect to these aspects. Document structure analysis of a particular type such as table recognition and their performance evaluation have been described in.42, 43 A meaningful and computable metric is necessary for quantitatively evaluating the performance of any algorithm. It is a function of the given dataset, the groundtruth and the algorithm parameters. A performance metric is typically not unique, and researchers can select particular performance metrics to study particular aspects of the evaluated algorithms. Krishnamoorthy et al.30 proposed a metric based on the percentage of area labeled and missed labels. Saitoh et al.7 used three criteria to show the results of their algorithm, based on three proposed ways of using their experimental results. Niyogi and Srihari6 reported their results using three metrics: block classification, block grouping, and read-order accuracy. Lin et al.8 used two types of labeling errors and an identification rate to report the experimental results of their algorithm. A common aspect of these metrics is their lack of formal definitions; verbal descriptions are used instead.   Yamashita et al.11 described a cost function based metric for selecting the result with the least cost. Kreich et al.5 used a generalized Hamming metric to compute a confidence measure for matches between a document physical layout and logical structure knowledge base and a document ob ject. Summers33 defined precise and generalized accuracy metrics and reported the performance of his algorithm using these metrics. Dengel and Dubiel39 used recall, precision and F value to evaluate the performance of their algorithm. These metrics are relatively formally defined and hence have less ambiguity in their interpretations. In,9, 10, 37 experimental results were reported, but no clear definition of the performance metrics used was given. In,12, 3436, 38 no quantitative experimental results were reported, and hence it is hard to assess the performance of the algorithms. Evaluation based on large-scale experimental datasets is crucial for ob jectively evaluating the performance of algorithms and assessing the state of the art. The groundtruth of a given dataset is necessary for scoring experimental results using that dataset. Some authors tested their algorithms on relatively large datasets. In,710, 33 more than 100 document images were used, and in,6, 11, 30, 37, 39 tens of document images were used. Other authors,5, 12, 38 however, tested their algorithms on very small datasets. In,3436 no dataset was specified. None of the authors clearly specified the groundtruth of the datasets used for testing their algorithms. Performance results and error analysis (if any) can be found in the descriptions of the individual algorithms. Comparative performance evaluations are necessary for comparing the performance of algorithms on some common ground and identifying state-of-the-art techniques. However, for most algorithms, there is a lack of comparative evaluation. Dengel and Dubiel39 performed a comparative evaluation of the bottom-up and top-down versions of his algorithm through learning and testing procedures. In Table 1, we summarize the experiments and performance evaluations that have been performed for various logical structure analysis algorithms.  5. SUMMARY AND LIMITATIONS OF THE SURVEYED ALGORITHMS Table 2 summarizes the surveyed algorithms in terms of key idea, physical layout representation, logical structure representation, logical labels, output representation, and application domain. As pointed out in Section 2 and Section 3, most of the past work on document structure analysis has been limited in one or more respects: 1. Much of the work has not been based on formal models for document pages. The use of formal models has several important advantages: (a) In a formal model framework, one can use a model that has an appropriate level of complexity for a given class of documents. (b) Once a model has been chosen for a given document class, examples of the class can be used to estimate model parameters. (c) Formal models can be used for both analysis and synthesis of documents. A model can be validated by using it to synthesize document page images that can be compared to real page images of the given class. The model can also be used to generate synthetic page image data which can be used in controlled experiments. 2. Much of the work on logical structure analysis of documents assumes that physical layout analysis has already been performed. 3. Most of the work makes use of deterministic models. Such models fail in the presence of noise or ambiguity. 4. In some of the work, quantitative performance evaluation issues have been neglected. While most document structure analysis algorithms are based explicitly or implicitly on document models, relatively few of them have provided formal definitions of these models. This has made it difficult to characterize the relation between the models and the performance of the algorithms. Furthermore, the parameter values in the algorithms have usually been manually selected.   Table 1. This table summarizes experiments involving various logical structure analysis algorithms in terms of ex perimental dataset, performance metric, groundtruth specification, performance results, error analysis and comparative evaluation. Note: N/S means not specified. Authors Tsujimoto and Asada10 Yamashita et al.11 Kreich et al.5 Fisher12 Derrien-Peden 34  Year 1990 1991  1991 1991 1991 1991  Experimental Dataset 106 pages from various sources 77 Japanese patent application front pages one page one page none none five memo pages -- one for training, four for testing. none 21 IBM journal pages for training, 12 IBM/PAMI pages for testing 393 Japanese/ English pages for testing 70 Japanese pages from books/magazines 44 newspaper pages 196 pages from technical reports with corrected segmentation 40 letters for learning, 40 letters for testing. 235 book pages  Performance Metric N/S cost function confidence measure N/S N/S N/S N/S  Groundtruth Specification N/S N/S  Performance Results 94/106 accuracy 59/77 accuracy N/S N/S N/S N/S N/S  Error Analysis yes yes  Comparative Evaluation none none  N/S N/S N/S N/S N/S  no no no no no  none none none none none  Ingold and Armangil35 Brugger et al.38 Conway36  1993 1993  N/S  N/S  N/S  no  none  Krishnamoorthy et al.30 Saitoh et al.7 Tateisi and Itoh37  1993  % area labeled, missed labels six criteria based on result usage N/S  N/S  1993  N/S  1994  N/S  reported for each of 12 IBM journal and IEEE PAMI pages results reported based on three criteria 87% and 82% logical labeling accuracy for manuals and technical papers etc. reported for each, of 32 newspaper pages and read order accuracy 85.5% logical labeling accuracy  no  none  yes  none  yes  none  Niyogi and Srihari6  1995  Summers  33  1995  block classification, block grouping, read order accuracy Precise and generalized accuracy  N/S  yes  none  N/S  no  none  Dengel and Dubiel39 Lin et al.8 Ishitani9 Srihari et al. Kim et al. 40  1996  recall, precision, F value two types of errors, identification rate N/S N/S labeling  N/S  reported for 40 letters reported for 235 pages 96.3% logical ob ject extraction accuracy ZIP code, city name state, stree name 96.7% labeling accuracy  no  yes  1997  N/S  yes  none  1999 1999 2001  32  150 pages from various sources US postal address directory over 11,000 pages from over 1,000 biomedical journals  N/S N/S N/S  no no yes  none none none  Document physical and logical structures vary greatly in complexity. If we could characterize the complexity of the document images in a given dataset, we could use appropriate analysis techniques. Existing document structure analysis algorithms have not addressed this issue; it too could be addressed if formal models were used. The use of generative document models would enable us to simulate document images and perform controlled experiments to evaluate algorithms and study their breakdown points. Deterministic models often cannot handle noise or ambiguity. Document pages are usually noisy due to printing, handling, photocopying, scanning, and faxing processes, and this can lead to ambiguous or false results. Document physical structure analysis procedures also have performance uncertainties and so may provide uncertain input to the logical structure analysis process. Stochastic models, represented by stochastic grammars and related parsing techniques,44 could be used to address these problems. The input to the parser could be regarded as probabilistic to reflect uncertainty due to erroneous physical layout analysis results and document noise. Physical layout and logical structure analysis algorithms based on stochastic language models   Table 2. In this table, state-of-the-art document logical structure analysis algorithms are analyzed in terms of key idea, physical layout representation, logical structure representation, output representation, logical labels and application domain. Authors Tsujimoto and Asada10 Yamashita et al.11 Year 1990 Key Idea mapping a physical tree to a logical one top-down layout model and relaxation labeling knowledge based analysis rule-based Physical Layout Representation block dominating rules, tree tree Logical Structure Representation tree Output Representation not mentioned Logical Labels title, abstract, sub-title, paragraph, header, footer page number, caption title, author, affiliation, body column, block sender, date, reference section heading, figure, figure caption, page heading, page footings title, list, paragraph abstract title, paragraph, section, chapter not mentioned title, heading, paragraph, figure title, author, abstract body, caption, header footer headings, paragraph, list item title, story, sub-story, photo, caption, graph paragraph, heading, list item sender, recipient, date logo, sub ject, footer body-text headline, content, figure, table, page number, head-foot headline, header, footer note, caption, program, formula, title, list title, author affiliation, abstract Application Domain various documents patent applications not mentioned not mentioned not mentioned not mentioned memo pages not mentioned journal pages various documents not mentioned newspaper pages technical reports letters  1991  tree  ODA  Kreich et al.5 Fisher12  1991  document style parameters rules, tree  logical labels rules, labeling  not given MIF  Derrien-Peden  34  1991 1991  Ingold and Armangil35 Brugger et al.38  1993  Conway36 Krishnamoorthy et al.30 Saitoh et al.7 Tateisi and Itoh 37  1993 1993 1993 1994  frame and macrotypographical based rule based, physical zones available N -gram model, physical zones available page grammar page parsing, block grammar text area influence rules stochastic grammars, physical zones available rule-based, knowledge-based logical prototype, matching, physical zones available logical structure learning, physical zones available OCR and rule based emergent computation, rule based OCR and rule based  tree none  rules, labeling EBNF grammars, presentation rules tree  MML not mentioned  none  not mentioned  page grammars block grammar, tree document style parameters none  context-free string grammar block grammar, tree tree grammar rules  SGML not mentioned not mentioned not mentioned not mentioned not mentioned not mentioned not mentioned not mentioned database tables  Niyogi and Srihari6 Summers 33  1995 1995  rules none  rules, tree logical prototypes  Dengel and Dubiel39  1996  none  GTree  Lin et al.8  1997  document style parameters document style parameters zones  logical labels  book pages various documents biomedical journals  Ishitani9  1999  logical labels  Kim et al.32  2001  logical labels  have been recently proposed in.45, dictionaries based on learning.  46  Doermann et al.47 proposed a method for lexicon acquisition from bilingual  A soundly designed experimental methodology should include: a meaningful and computable performance metric, large datasets with well-defined groundtruth, a training procedure and a testing procedure, a thorough error analysis and, finally, comparisons with other state-of-the-art algorithms. As described in Section 2.2.3, very few algorithms have used such complete experimental designs.  ACKNOWLEDGMENTS This research was funded in part by the Department of Defense under Contract MDA 9049-6C-1250, Lockheed Martin under Contract 9802167270, the Defense Advanced Research Pro jects Agency under Contract N660010028910, and the National Science Foundation under Grant IIS9987944.  REFERENCES 1. R. M. Haralick, ""Document image understanding: Geometric and logical layout,"" in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, pp. 385390, (Seattle, WA), June 1994.   2. G. Nagy, ""Twenty years of document image analysis in pami,"" IEEE Transactions on Pattern Analysis and Machine Intel ligence 22, pp. 3862, 2000. 3. A. K. Jain and B. Yu, ""Document representation and its application to page decomposition,"" IEEE Transactions on Pattern Analysis and Machine Intel ligence 20, pp. 294308, 1998. 4. H. Fujisawa, Y. Nakano, and K. Kurino, ""Segmentation methods for character recognition: from segmentation to document structure analysis,"" in Proceeding of the IEEE, vol. 80, pp. 10791092, 1992. 5. J. Kreich, A. Luhn, and G. Maderlechner, ""An experimental environment for model based document analysis,"" in Proceedings of International Conference on Document Analysis and Recognition, pp. 5058, (Saint-Malo, France), September 1991. 6. D. Niyogi and S. N. Srihari, ""Knowledge-based derivation of document logical structure,"" in Proceedings of International Conference on Document Analysis and Recognition, pp. 472475, (Montreal, Canada), August 1995. 7. T. Saitoh, M. Tachikawa, and T. Yamaai, ""Document image segmentation and text area ordering,"" in Proceedings of International Conference on Document Analysis and Recognition, pp. 323329, (Tsukuba Science City, Japan), October 1993. 8. C. C. Lin, Y. Niwa, and S. Narita, ""Logical structure analysis of book document images using contents information,"" in Proceedings of International Conference on Document Analysis and Recognition, pp. 10481054, (Ulm, Germany), August 1997. 9. Y. Ishitani, ""Logical structure analysis of document images based on emergent computation,"" in Proceedings of International Conference on Document Analysis and Recognition, pp. 189192, (Bangalore, India), September 1999. 10. S. Tsujimoto and H. Asada, ""Understanding multi-articled documents,"" in Proceedings of International Conference on Pattern Recognition, pp. 551556, (Atlantic City, NJ), June 1990. 11. A. Yamashita, T. Amano, I. Takahashi, and K. Toyokawa, ""A model based layout understanding method for the document recognition system,"" in Proceedings of International Conference on Document Analysis and Recognition, pp. 130138, (Saint-Malo, France), September 1991. 12. J. L. Fisher, ""Logical structure descriptions of segmented document images,"" in Proceedings of International Con ference on Document Analysis and Recognition, pp. 302310, (Saint-Malo, France), September 1991. 13. L. O'Gorman, ""The document spectrum for page layout analysis,"" IEEE Transactions on Pattern Analysis and Machine Intel ligence 15, pp. 11621173, 1993. 14. K. Kise, A. Sato, and M. Iwata, ""Segmentation of page images using the area Voronoi diagram,"" Computer Vision and Image Understanding 70, pp. 370382, 1998. 15. F. Wahl, K. Wong, and R. Casey, ""Block segmentation and text extraction in mixed text/image documents,"" Graphical Models and Image Processing 20, pp. 375390, 1982. 16. L. A. Fletcher and R. Kasturi, ""A robust algorithm for text string separation from mixed text/graphics images,"" IEEE Transactions on Pattern Analysis and Machine Intel ligence 10, pp. 910918, 1988. 17. G. Nagy, S. Seth, and M. Viswanathan, ""A prototype document image analysis system for technical journals,"" Computer 25, pp. 1022, 1992. 18. H. S. Baird, S. E. Jones, and S. J. Fortune, ""Image segmentation by shape-directed covers,"" in Proceedings of International Conference on Pattern Recognition, pp. 820825, (Atlantic City, NJ), June 1990. 19. T. Pavlidis and J. Zhou, ""Page segmentation and classification,"" Graphical Models and Image Processing 54, pp. 484 496, 1992. 20. L. O'Gorman and R. Kasturi, Document Image Analysis, IEEE Computer Society Press, Los Alamitos, CA, 1995. 21. T. Breuel and M. Worring, eds., Document Layout Interpretation and its Applications, (Bangalore, India), September 1999. 22. S. Mao and T. Kanungo, ""Software architecture of PSET: A page segmentation evaluation toolkit,"" International Journal on Document Analysis and Recognition 4, pp. 205217, 2002. 23. S. Mao and T. Kanungo, ""Empirical performance evaluation methodology and its application to page segmentation algorithms,"" IEEE Transactions on Pattern Analysis and Machine Intel ligence 23, pp. 242256, 2001. 24. J. S. Liang, I. T. Phillips, and R. M. Haralick, ""Performance evaluation of document structure extraction algo rithms,"" Computer Vision and Image Understanding 84, pp. 144159, 2001. 25. J. Kanai, S. V. Rice, T. A. Nartker, and G. Nagy, ""Automated evaluation of OCR zoning,"" IEEE Transactions on Pattern Analysis and Machine Intel ligence 17, pp. 8690, 1995. 26. S. V. Rice, F. R. Jenkins, and T. A. Nartker, ""The fifth annual test of OCR accuracy,"" Tech. Rep. TR-96-01, University of Nevada, Las Vegas, NV, 1996. 27. B. A. Yanikoglu and L. Vincent, ""Pink pather: A complete environment for ground-truthing and benchmarking document page segmentation,"" Pattern Recognition 31, pp. 1191204, 1998. 28. G. E. Kopec and P. A. Chou, ""Document image decoding using Markov source models,"" IEEE Transactions on Pattern Analysis and Machine Intel ligence 16, pp. 602617, 1994. 29. T. A. Tokuyasu and P. A. Chou, ""Turbo recognition: a statistical approach to layout analysis,"" in Proceedings of SPIE Conference on Document Recognition and Retrieval, (San Jose, CA), January 2001.   30. M. Krishnamoorthy, G. Nagy, S. Seth, and M. Viswanathan, ""Syntactic segmentation and labeling of digitized pages from technical journals,"" IEEE Transactions on Pattern Analysis and Machine Intel ligence 15, pp. 737747, 1993. 31. A. L. Spitz, ""Style-directed document segmentation,"" in Proceedings of 2001 Symposium on Document Image Un derstanding Technology, (Baltimore, MD), April 2001. 32. J. Kim, D. X. Le, and G. R. Thoma, ""Automated labeling in document images,"" in Proceedings of SPIE Conference on Document Recognition and Retrieval VIII, pp. 111122, (San Jose, CA), January 2001. 33. K. Summers, ""Near-wordless document structure classification,"" in Proceedings of International Conference on Document Analysis and Recognition, pp. 462465, (Montreal, Canada), August 1995. 34. D. Derrien-Peden, ""Frame-based system for macro-typographical structure analysis in scientific papers,"" in Pro ceedings of International Conference on Document Analysis and Recognition, pp. 311319, (Saint-Malo, France), September 1991. 35. R. Ingold and D. Armangil, ""A top-down document analysis method for logical structure recognition,"" in Proceedings of International Conference on Document Analysis and Recognition, pp. 4149, (Saint-Malo, France), September 1991. 36. A. Conway, ""Page grammars and page parsing: A syntatic approach to document layout recognition,"" in Proceedings of International Conference on Document Analysis and Recognition, pp. 761764, (Tsukuba Science City, Japan), October 1993. 37. Y. Tateisi and N. Itoh, ""Using stochastic syntactic analysis for extracting a logical structure from a document image,"" in Proceedings of International Conference on Pattern Recognition, pp. 391394, (Jerusalem, Israel), October 1994. 38. R. Brugger, A. Zramdini, and R. Ingold, ""Modeling documents for structure recognition using generalized n-gram,"" in Proceedings of International Conference on Document Analysis and Recognition, pp. 5660, (Ulm, Germany), August 1997. 39. A. Dengel and F. Dubiel, ""Computer understanding of document structure,"" International Journal of Imaging Systems and Technology 7, pp. 271278, 1996. 40. S. N. Srihari, W. Yang, and V. Govindara ju, ""Information theoretic analysis of postal address fields for automatic address interpretation,"" in Proceedings of International Conference on Document Analysis and Recognition, pp. 309 312, (Bangalore, India), September 1999. 41. P. G. Mulgaonkar, ""Automatic detection of address blocks on irregular mail pieces,"" in Proceedings of IEEE Con ference on Computer Vision and Pattern Recognition, pp. 672674, (Miami, Fl), June 1986. 42. J. Hu, R. Kashi, D. P. Lopresti, and G. T. Wilfong, ""Evaluating the performance of table processing algorithms,"" International Journal on Document Analysis and Recognition 4, pp. 140153, 2001. 43. M. Hurst, ""Layout and language: An efficient algorithm for detecting text blocks based on spatial and linguistic evidence,"" in Proceedings of SPIE Conference on Document Recognition, pp. 5667, (San Jose, CA), January 2001. 44. D. Jurafsky and J. H. Martin, Speech and Language Processing, Prentice Hall, Upper Saddle River, NJ, 2000. 45. S. Mao and T. Kanungo, ""Stochastic language models for automatic acquisition of lexicons from printed bilingual dictionaries,"" in Document Layout Interpretation and Its Applications, (Seattle, WA), September 2001. 46. T. Kanungo and S. Mao, ""Stochastic language model for style-directed physical layout analysis of document images,"" IEEE Transactions on Image Processing . To appear. 47. D. S. Doermann, H. Ma, B. Karagol-Ayan, and D. W. Oard, ""Translation lexicon acquisition from bilingual dic tionaries,"" in Proceedings of SPIE Conference on Document Recognition and Retrieval VIII, pp. 3748, (San Jose, CA), January 2002."
GX198-64-1927384	"Next   Previous   Contents     2. Traditional Artificial Intelligence         Traditional AI is based around the ideas of logic, rule systems, linguistics, and the concept of rationality.  At its roots are programming languages such as Lisp and Prolog. Expert systems are the largest successful example of this paradigm.  An expert system consists of a detailed knowledge base and a complex rule system to utilize it.  Such systems have been used for such things as medical diagnosis support and credit checking systems.      2.1 AI class/code libraries        These are libraries of code or classes for use in programming within the artificial intelligence field.  They are not meant as stand alone applications, but rather as tools for building your own applications.                AI Search     FTP site:   ftp.icce.rug.nl/pub/peter/     Submitted by:   Peter M. Bouthoorn   Basically, the library offers the programmer a set of search algorithms that may be used to solve all kind of different problems. The idea is that when developing problem solving software the programmer should be able to concentrate on the representation of the problem to be solved and should not need to bother with the implementation of the search algorithm that will be used to actually conduct the search. This idea has been realized by the implementation of a set of search classes that may be incorporated in other software through  C++ 's features of derivation and inheritance.  The following search algorithms have been implemented:    - depth-first tree and graph search. - breadth-first tree and graph search. - uniform-cost tree and graph search. - best-first search. - bidirectional depth-first tree and graph search. - bidirectional breadth-first tree and graph search. - AND/OR depth tree search. - AND/OR breadth tree search.    Peter plans to release a new version of the library soon, which will also be featured in a book about C++ and AI to appear this year.          Chess In Lisp (CIL)     FTP site:   chess.onenet.net/pub/chess/uploads/projects/       The CIL (Chess In Lisp) foundation is a Common Lisp implementaion of all the core functions needed for development of chess applications.  The main purpose of the CIL project is to get AI researchers interested in using Lisp to work in the chess domain.            DAI     Web site:   starship.skyport.net/crew/gandalf/DNET/AI       A library for the Python programming language that provides an object oriented interface to the CLIPS expert system tool. It  includes an interface to COOL (CLIPS Object Oriented Language) that allows:    Investigate COOL classes   Create and manipulate with COOL instances   Manipulate with COOL message-handler's   Manipulate with Modules                 Nyquist     Web site:   www.cs.cmu.edu/afs/cs.cmu.edu/project/music/web/music.html       The Computer Music Project at CMU is developing computer music and interactive performance technology to enhance human musical experience and creativity. This interdisciplinary effort draws on Music Theory, Cognitive Science, Artificial Intelligence and Machine Learning, Human Computer Interaction, Real-Time Systems, Computer Graphics and Animation, Multimedia, Programming Languages, and Signal Processing. A paradigmatic example of these interdisciplinary efforts is the creation of interactive performances that couple human musical improvisation with intelligent computer agents in real-time.          Screamer     Web site:   www.cis.upenn.edu/~screamer-tools/home.html       Screamer is an extension of Common Lisp that adds support for nondeterministic programming. Screamer consists of two levels. The basic nondeterministic level adds support for backtracking and undoable side effects.  On top of this nondeterministic substrate, Screamer provides a comprehensive constraint programming language in which one can formulate and solve mixed systems of numeric and symbolic constraints. Together, these two levels augment Common Lisp with practically all of the functionality of both Prolog and constraint logic programming languages such as CHiP and CLP(R). Furthermore, Screamer is fully integrated with Common Lisp. Screamer programs can coexist and interoperate with other extensions to Common Lisp such as CLOS, CLIM and Iterate.                  2.2 AI software kits, applications, etc.            These are various applications, software kits, etc. meant for research in the field of artificial intelligence. Their ease of use will vary, as they were designed to meet some particular research interest more than as an easy to use commercial package.          ASA - Adaptive Simulated Annealing     Web site:   www.ingber.com/#ASA-CODE   FTP site:   ftp.ingber.com/       ASA (Adaptive Simulated Annealing) is a powerful global optimization C-code algorithm especially useful for nonlinear and/or stochastic systems.    ASA is developed to statistically find the best global fit of a nonlinear non-convex cost-function over a D-dimensional space. This algorithm permits an annealing schedule for 'temperature' T decreasing exponentially in annealing-time k, T = T_0 exp(-c k^1/D). The introduction of re-annealing also permits adaptation to changing sensitivities in the multi-dimensional parameter-space. This annealing schedule is faster than fast Cauchy annealing, where T = T_0/k, and much faster than Boltzmann annealing, where T = T_0/ln k.              Babylon     FTP site:   ftp.gmd.de/gmd/ai-research/Software/Babylon/       BABYLON is a modular, configurable, hybrid environment for developing expert systems. Its features include objects, rules with forward and backward chaining, logic (Prolog) and constraints. BABYLON is implemented and embedded in Common Lisp.            CLEARS     Web site:   www.coli.uni-sb.de/~clears/       The CLEARS system is an interactive graphical environment for computational semantics. The tool allows exploration and comparison of different semantic formalisms, and their interaction with syntax. This enables the user to get an idea of the range of possibilities of semantic construction, and also where there is real convergence between theories.        CLIG     Web site:   www.ags.uni-sb.de/~konrad/clig.html       CLIG is an interactive, extendible grapher for visualizing linguistic data structures like trees, feature structures, Discourse Representation Structures (DRS), logical formulas etc. All of these can be freely mixed and embedded into each other. The grapher has been designed both to be stand-alone and to be used as an add-on for linguistic applications which display their output in a graphical manner.        CLIPS     Web site:   www.jsc.nasa.gov/~clips/CLIPS.html   FTP site:   cs.cmu.edu/afs/cs.cmu.edu/project/ai-repository/ai/areas/expert/systems/clips         CLIPS is a productive development and delivery expert system tool which provides a complete environment for the construction of rule and/or object based expert systems.      CLIPS provides a cohesive tool for handling a wide variety of knowledge with support for three different programming paradigms: rule-based, object-oriented and procedural.  Rule-based programming allows knowledge to be represented as heuristics, or ""rules of thumb,"" which specify a set of actions to be performed for a given situation. Object-oriented programming allows complex systems to be modeled as modular components (which can be easily reused to model other systems or to create new components).  The procedural programming capabilities provided by CLIPS are similar to capabilities found in languages such as C, Pascal, Ada, and LISP.        EMA-XPS - A Hybrid Graphic Expert System Shell     Web site:   wmwap1.math.uni-wuppertal.de:80/EMA-XPS/       EMA-XPS is a hybrid graphic expert system shell based on the ASCII-oriented shell Babylon 2.3 of the German National Research Center for Computer Sciences (GMD). In addition to Babylon's AI-power (object oriented data representation, forward and backward chained rules - collectible into sets, horn clauses, and constraint networks) a graphic interface based on the X11 Window System and the OSF/Motif Widget Library has been provided.            FOOL & FOX     FTP site:   ntia.its.bldrdoc.gov/pub/fuzzy/prog/       FOOL stands for the Fuzzy Organizer OLdenburg. It is a result from a project at the University of Oldenburg. FOOL is a graphical user interface to develop fuzzy rulebases.  FOOL will help you to invent and maintain a database that specifies the behavior of a fuzzy-controller or something like that.    FOX is a small but powerful fuzzy engine which reads this database, reads some input values and calculates the new control value.        FUF and SURGE     Web site:   www.dfki.de/lt/registry/generation/fuf.html   FTP site:   ftp.cs.columbia.edu/pub/fuf/     FUF is an extended implementation of the formalism of functional unification grammars (FUGs) introduced by Martin Kay specialized to the task of natural language generation. It adds the following features to the base formalism:    Types and inheritance.    Extended control facilities (goal freezing, intelligent backtracking).    Modular syntax.     These extensions allow the development of large grammars which can be processed efficiently and can be maintained and understood more easily.  SURGE is a large syntactic realization grammar of English written in FUF. SURGE is developed to serve as a black box syntactic generation component in a larger generation system that encapsulates a rich knowledge of English syntax. SURGE can also be used as a platform for exploration of grammar writing with a generation perspective.        The Grammar Workbench     Web site:   www.cs.kun.nl/agfl/GWB.html             The Grammar Workbench, or GWB for short, is an environment for the comfortable development of Affix Grammars in the AGFL-formalism. Its purposes are:     to allow the user to input, inspect and modify a grammar;    to perform consistency checks on the grammar;    to compute grammar properties;    to generate example sentences;    to assist in performing grammar transformations.            GSM Suite     Web site:   www.slip.net/~andrewm/gsm/       The GSM Suite is a set of programs for using Finite State Machines in a graphical fashion. The suite consists of programs that edit, compile, and print state machines. Included in the suite is an editor program, gsmedit, a compiler, gsm2cc, that produces a C++ implementation of a state machine, a PostScript generator, gsm2ps, and two other minor programs. GSM is licensed under the GNU Public License and so is free for your use under the terms of that license.        Illuminator     Web site:   documents.cfar.umd.edu/resources/source/illuminator.html       Illuminator is a toolset for developing OCR and Image Understanding applications.  Illuminator has two major parts: a library for representing, storing and retrieving OCR information, heretofore called dafslib, and an X-Windows ""DAFS"" file viewer, called illum. Illuminator and DAFS lib were designed to supplant existing OCR formats and become a standard in the industry. They particularly are extensible to handle more than just English.  The features of this release:     5 magnification levels for images   flagged characters and words   unicode support -- American, British, French, German, Greek, Italian, MICR, Norwegian, Russian, Spanish, Swedish, keyboards    reads DAFS, TIFF's, PDA's (image only)   save to DAFS, ASCII/UTF or Unicode   Entity Viewer - shows properties, character choices, bounding boxes image fragment for a selected entity, change type, change content, hierarchy mode             Jess, the Java Expert System Shell     Web site:   herzberg.ca.sandia.gov/jess/       Jess is a clone of the popular CLIPS expert system shell written entirely in Java. With Jess, you can conveniently give your applets the ability to 'reason'. Jess is compatible with all versions of Java starting with version 1.0.2. Jess implements the following constructs from CLIPS: defrules, deffunctions, defglobals, deffacts, and deftemplates.            learn     FTP site:   sunsite.unc.edu/pub/Linux/apps/cai/       Learn is a vocable learning program with memory model.           Otter: An Automated Deduction System     Web site:   www-unix.mcs.anl.gov/AR/otter/       Our current automated deduction system  Otter is designed to prove theorems stated in first-order logic with equality.  Otter's inference rules are based on resolution and paramodulation, and it includes facilities for term rewriting, term orderings, Knuth-Bendix completion, weighting, and strategies for directing and restricting searches for proofs.   Otter can also be used as a symbolic calculator and has an embedded equational programming system.            PVS     Web site:   pvs.csl.sri.com/     PVS is a verification system: that is, a specification language integrated with support tools and a theorem prover. It is intended to capture the state-of-the-art in mechanized formal methods and to be sufficiently rugged that it can be used for significant applications. PVS is a research prototype: it evolves and improves as we develop or apply new capabilities, and as the stress of real use exposes new requirements.          RIPPER     Web site:   www.research.att.com/~wcohen/ripperd.html       Ripper is a system for fast effective rule induction. Given a set of data, Ripper will learn a set of rules that will predict the  patterns in the data. Ripper is written in ASCI C and comes with documentation and some sample problems.          SNePS     Web site:   www.cs.buffalo.edu/pub/sneps/WWW/   FTP site:   ftp.cs.buffalo.edu/pub/sneps/     The long-term goal of The SNePS Research Group is the design and construction of a natural-language-using computerized cognitive agent, and carrying out the research in artificial intelligence, computational linguistics, and cognitive science necessary for that endeavor. The three-part focus of the group is on knowledge representation, reasoning, and natural-language understanding and generation. The group is widely known for its development of the SNePS knowledge representation/reasoning system, and Cassie, its computerized cognitive agent.              Soar     Web site:   bigfoot.eecs.umich.edu/~soar/   FTP site:   cs.cmu.edu/afs/cs/project/soar/public/Soar6/       Soar has been developed to be a general cognitive architecture. We intend ultimately to enable the Soar architecture to:    work on the full range of tasks expected of an intelligent agent, from highly routine to extremely difficult, open-ended problems   represent and use appropriate forms of knowledge, such as procedural, declarative, episodic, and possibly iconic   employ the full range of problem solving methods    interact with the outside world and    learn about all aspects of the tasks and its performance on them.      In other words, our intention is for Soar to support all the capabilities required of a general intelligent agent. http://wwwis.cs.utwente.nl:8080/ tcm/index.html        TCM     Web site:   wwwis.cs.utwente.nl:8080/~tcm/index.html   FTP site:   ftp.cs.vu.nl/pub/tcm/       TCM (Toolkit for Conceptual Modeling) is our suite of graphical editors. TCM contains graphical editors for Entity-Relationship diagrams, Class-Relationship diagrams, Data and Event Flow diagrams, State Transition diagrams, Jackson Process Structure diagrams and System Network diagrams, Function Refinement trees and various table editors, such as a Function-Entity table editor and a Function Decomposition table editor.  TCM is easy to use and performs numerous consistency checks, some of them immediately, some of them upon request.          WEKA     Web site:   lucy.cs.waikato.ac.nz/~ml/       WEKA (Waikato Environment for Knowledge Analysis) is an state-of-the-art facility for applying machine learning techniques to practical problems. It is a comprehensive software ""workbench"" that allows people to analyse real-world data. It integrates different machine learning tools within a common framework and a uniform user interface. It is designed to support a ""simplicity-first"" methodology, which allows users to experiment interactively with simple machine learning tools before looking for more complex solutions.              Next   Previous   Contents"
GX222-33-9976890	"Next   Previous   Contents     2. Traditional Artificial Intelligence         Traditional AI is based around the ideas of logic, rule systems, linguistics, and the concept of rationality.  At its roots are programming languages such as Lisp and Prolog. Expert systems are the largest successful example of this paradigm.  An expert system consists of a detailed knowledge base and a complex rule system to utilize it.  Such systems have been used for such things as medical diagnosis support and credit checking systems.      2.1 AI class/code libraries        These are libraries of code or classes for use in programming within the artificial intelligence field.  They are not meant as stand alone applications, but rather as tools for building your own applications.              ACL2     Web site:   www.telent.net/cliki/ACL2     ACL2 (A Computational Logic for Applicative Common Lisp) is a theorem prover for industrial applications. It is both a mathematical logic and a system of tools for constructing proofs in the logic.  ACL2 works with GCL (GNU Common Lisp).          AI Search II     WEB site:   www.bell-labs.com/topic/books/ooai-book/     Submitted by:   Peter M. Bouthoorn   Basically, the library offers the programmer a set of search algorithms that may be used to solve all kind of different problems. The idea is that when developing problem solving software the programmer should be able to concentrate on the representation of the problem to be solved and should not need to bother with the implementation of the search algorithm that will be used to actually conduct the search. This idea has been realized by the implementation of a set of search classes that may be incorporated in other software through  C++ 's features of derivation and inheritance.  The following search algorithms have been implemented:    - depth-first tree and graph search. - breadth-first tree and graph search. - uniform-cost tree and graph search. - best-first search. - bidirectional depth-first tree and graph search. - bidirectional breadth-first tree and graph search. - AND/OR depth tree search. - AND/OR breadth tree search.    This library has a corresponding book, ""  Object-Oriented Artificial Instelligence, Using C++ "".          Chess In Lisp (CIL)     FTP site:   chess.onenet.net/pub/chess/uploads/projects/       The CIL (Chess In Lisp) foundation is a Common Lisp implementaion of all the core functions needed for development of chess applications.  The main purpose of the CIL project is to get AI researchers interested in using Lisp to work in the chess domain.            DAI     Web site:   starship.python.net/crew/gandalf/DNET/AI/     A library for the Python programming language that provides an object oriented interface to the CLIPS expert system tool. It  includes an interface to COOL (CLIPS Object Oriented Language) that allows:    Investigate COOL classes   Create and manipulate with COOL instances   Manipulate with COOL message-handler's   Manipulate with Modules           LK     Web site:   www.cs.utoronto.ca/~neto/research/lk/     LK is an implementation of the Lin-Kernighan heuristic for the Traveling Salesman Problem and for the minimum weight perfect matching problem. It is tuned for 2-d geometric instances, and has been applied to certain instances with up to a million cities. Also included are instance generators and Perl scripts for munging TSPLIB instances.   This implementation introduces ``efficient cluster compensation'', an experimental algorithmic technique intended to make the Lin-Kernighan heuristic more robust in the face of clustered data.          Nyquist     Web site:   www.cs.cmu.edu/afs/cs.cmu.edu/project/music/web/music.html       The Computer Music Project at CMU is developing computer music and interactive performance technology to enhance human musical experience and creativity. This interdisciplinary effort draws on Music Theory, Cognitive Science, Artificial Intelligence and Machine Learning, Human Computer Interaction, Real-Time Systems, Computer Graphics and Animation, Multimedia, Programming Languages, and Signal Processing. A paradigmatic example of these interdisciplinary efforts is the creation of interactive performances that couple human musical improvisation with intelligent computer agents in real-time.          PDKB     Web site:   lynx.eaze.net/~pdkb/web/   SourceForge site:   sourceforge.net/project/pdkb     Public Domain Knowledge Bank (PDKB) is an Artificial Intelligence Knowledge Bank of common sense rules and facts. It is based on the Cyc Upper Ontology and the MELD language.          Python Fuzzy Logic Module     FTP site:   ftp://ftp.csh.rit.edu/pub/members/retrev/     A simple python module for fuzzy logic. The file is 'fuz.tar.gz' in this directory. The author plans to also write a simple genetic algorithm and a neural net library as well. Check the 00_index file in this directory for release info.        QUANT1     Web site:   linux.irk.ru/projects/QUANT/     QUANT/1 stands for type QUANTifier. It aims to be an alternative to Prolog-like (Resulutional-like) systems. Main features include a lack of necessity for eliminating Quantifiers, scolemisation, ease of comprehension, large scale formulae operation, acceptance of nonHorn formulaes, and Iterative deeping. The actual library implemented in this project is called ATPPCF (Automatic Theorem Prover in calculus of Positively Constructed Formulae).  ATPPCF will be a library (inference engine) and an extension of the Predicate Calculus Language as a new logical language. The library will be incorporable in another software such as TCL, Python, Perl. The engine's primary inference method will be the ""search of inference in language of Positively Constructed Formulas (PCFs)"" (a subset of Predicate Calculus well translated in both directions). The language will be used as scripting language to the engine. But there will be possibility to replace it with extensions languages of main software.          Screamer     Web site:   www.cis.upenn.edu/~screamer-tools/home.html     Screamer is an extension of Common Lisp that adds support for nondeterministic programming. Screamer consists of two levels. The basic nondeterministic level adds support for backtracking and undoable side effects.  On top of this nondeterministic substrate, Screamer provides a comprehensive constraint programming language in which one can formulate and solve mixed systems of numeric and symbolic constraints. Together, these two levels augment Common Lisp with practically all of the functionality of both Prolog and constraint logic programming languages such as CHiP and CLP(R). Furthermore, Screamer is fully integrated with Common Lisp. Screamer programs can coexist and interoperate with other extensions to Common Lisp such as CLOS, CLIM and Iterate.          ThoughtTreasure     Web site:   www.signiform.com/tt/htm/tt.htm     ThoughtTreasure is a project to create a database of commonsense rules for use in any application. It consists of a database of a little over 100K rules and a C API to integrate it with your applications. Python, Perl, Java and TCL wrappers are already available.                    2.2 AI software kits, applications, etc.            These are various applications, software kits, etc. meant for research in the field of artificial intelligence. Their ease of use will vary, as they were designed to meet some particular research interest more than as an easy to use commercial package.          ASA - Adaptive Simulated Annealing     Web site:   www.ingber.com/#ASA-CODE   FTP site:   ftp.ingber.com/       ASA (Adaptive Simulated Annealing) is a powerful global optimization C-code algorithm especially useful for nonlinear and/or stochastic systems.    ASA is developed to statistically find the best global fit of a nonlinear non-convex cost-function over a D-dimensional space. This algorithm permits an annealing schedule for 'temperature' T decreasing exponentially in annealing-time k, T = T_0 exp(-c k^1/D). The introduction of re-annealing also permits adaptation to changing sensitivities in the multi-dimensional parameter-space. This annealing schedule is faster than fast Cauchy annealing, where T = T_0/k, and much faster than Boltzmann annealing, where T = T_0/ln k.            Babylon     FTP site:   ftp.gmd.de/gmd/ai-research/Software/Babylon/     BABYLON is a modular, configurable, hybrid environment for developing expert systems. Its features include objects, rules with forward and backward chaining, logic (Prolog) and constraints. BABYLON is implemented and embedded in Common Lisp.        cfengine     Web site:   www.iu.hioslo.no/cfengine/     Cfengine, or the configuration engine is a very high level language for building expert systems which administrate and configure large computer networks. Cfengine uses the idea of classes and a primitive form of intelligence to define and automate the configuration of large systems in the most economical way possible. Cfengine is design to be a part of computer immune systems.        CLEARS     Web site:   www.coli.uni-sb.de/~clears/     The CLEARS system is an interactive graphical environment for computational semantics. The tool allows exploration and comparison of different semantic formalisms, and their interaction with syntax. This enables the user to get an idea of the range of possibilities of semantic construction, and also where there is real convergence between theories.        CLIG     Web site:   www.ags.uni-sb.de/~konrad/clig.html     CLIG is an interactive, extendible grapher for visualizing linguistic data structures like trees, feature structures, Discourse Representation Structures (DRS), logical formulas etc. All of these can be freely mixed and embedded into each other. The grapher has been designed both to be stand-alone and to be used as an add-on for linguistic applications which display their output in a graphical manner.        CLIPS     Web site:   www.jsc.nasa.gov/~clips/CLIPS.html   FTP site:   cs.cmu.edu/afs/cs.cmu.edu/project/ai-repository/ai/areas/expert/systems/clips     CLIPS is a productive development and delivery expert system tool which provides a complete environment for the construction of rule and/or object based expert systems.      CLIPS provides a cohesive tool for handling a wide variety of knowledge with support for three different programming paradigms: rule-based, object-oriented and procedural.  Rule-based programming allows knowledge to be represented as heuristics, or ""rules of thumb,"" which specify a set of actions to be performed for a given situation. Object-oriented programming allows complex systems to be modeled as modular components (which can be easily reused to model other systems or to create new components).  The procedural programming capabilities provided by CLIPS are similar to capabilities found in languages such as C, Pascal, Ada, and LISP.        EMA-XPS - A Hybrid Graphic Expert System Shell     Web site:   wmwap1.math.uni-wuppertal.de:80/EMA-XPS/       EMA-XPS is a hybrid graphic expert system shell based on the ASCII-oriented shell Babylon 2.3 of the German National Research Center for Computer Sciences (GMD). In addition to Babylon's AI-power (object oriented data representation, forward and backward chained rules - collectible into sets, horn clauses, and constraint networks) a graphic interface based on the X11 Window System and the OSF/Motif Widget Library has been provided.        FOOL & FOX     FTP site:   ntia.its.bldrdoc.gov/pub/fuzzy/prog/       FOOL stands for the Fuzzy Organizer OLdenburg. It is a result from a project at the University of Oldenburg. FOOL is a graphical user interface to develop fuzzy rulebases.  FOOL will help you to invent and maintain a database that specifies the behavior of a fuzzy-controller or something like that.    FOX is a small but powerful fuzzy engine which reads this database, reads some input values and calculates the new control value.        FUF and SURGE     Web site:   www.dfki.de/lt/registry/generation/fuf.html   FTP site:   ftp.cs.columbia.edu/pub/fuf/     FUF is an extended implementation of the formalism of functional unification grammars (FUGs) introduced by Martin Kay specialized to the task of natural language generation. It adds the following features to the base formalism:    Types and inheritance.    Extended control facilities (goal freezing, intelligent backtracking).    Modular syntax.     These extensions allow the development of large grammars which can be processed efficiently and can be maintained and understood more easily.  SURGE is a large syntactic realization grammar of English written in FUF. SURGE is developed to serve as a black box syntactic generation component in a larger generation system that encapsulates a rich knowledge of English syntax. SURGE can also be used as a platform for exploration of grammar writing with a generation perspective.        The Grammar Workbench     Web site:   www.cs.kun.nl/agfl/GWB.html             The Grammar Workbench, or GWB for short, is an environment for the comfortable development of Affix Grammars in the AGFL-formalism. Its purposes are:     to allow the user to input, inspect and modify a grammar;    to perform consistency checks on the grammar;    to compute grammar properties;    to generate example sentences;    to assist in performing grammar transformations.            GSM Suite     Web site:   www.slip.net/~andrewm/gsm/     The GSM Suite is a set of programs for using Finite State Machines in a graphical fashion. The suite consists of programs that edit, compile, and print state machines. Included in the suite is an editor program, gsmedit, a compiler, gsm2cc, that produces a C++ implementation of a state machine, a PostScript generator, gsm2ps, and two other minor programs. GSM is licensed under the GNU Public License and so is free for your use under the terms of that license.        Illuminator     Web site:   documents.cfar.umd.edu/resources/source/illuminator.html     Illuminator is a toolset for developing OCR and Image Understanding applications.  Illuminator has two major parts: a library for representing, storing and retrieving OCR information, heretofore called dafslib, and an X-Windows ""DAFS"" file viewer, called illum. Illuminator and DAFS lib were designed to supplant existing OCR formats and become a standard in the industry. They particularly are extensible to handle more than just English.  The features of this release:     5 magnification levels for images   flagged characters and words   unicode support -- American, British, French, German,  Greek, Italian, MICR, Norwegian, Russian, Spanish, Swedish,  keyboards    reads DAFS, TIFF's, PDA's (image only)   save to DAFS, ASCII/UTF or Unicode   Entity Viewer - shows properties, character choices,  bounding boxes image fragment for a selected entity, change  type, change content, hierarchy mode           Isabelle     Web site:   isabelle.in.tum.de     Isabelle is a popular generic theorem prover developed at Cambridge University and TU Munich. Existing logics like Isabelle/HOL provide a theorem proving environment ready to use for sizable applications. Isabelle may also serve as framework for rapid prototyping of deductive systems. It comes with a large library including Isabelle/HOL (classical higher-order logic), Isabelle/HOLCF (Scott's Logic for Computable Functions with HOL), Isabelle/FOL (classical and intuitionistic first-order logic), and Isabelle/ZF (Zermelo-Fraenkel set theory on top of FOL).        Jess, the Java Expert System Shell     Web site:   herzberg.ca.sandia.gov/jess/     Jess is a clone of the popular CLIPS expert system shell written entirely in Java. With Jess, you can conveniently give your applets the ability to 'reason'. Jess is compatible with all versions of Java starting with version 1.0.2. Jess implements the following constructs from CLIPS: defrules, deffunctions, defglobals, deffacts, and deftemplates.          learn     FTP site:   sunsite.unc.edu/pub/Linux/apps/cai/     Learn is a vocable learning program with memory model.         NICOLE     Web site:   nicole.sourceforge.net     NICOLE (Nearly Intelligent Computer Operated Language Examiner) is a theory or experiment that if a computer is given enough combinations of how words, phrases and sentences are related to one another, it could talk back to you. It is an attempt to simulate a conversation by learning how words are related to other words. A human communicates with NICOLE via the keyboard and NICOLE responds back with its own sentences which are automatically generated, based on what NICOLE has stored in it's database. Each new sentence that has been typed in, and NICOLE doesn't know about, is included into NICOLE's database, thus extending the knowledge base of NICOLE.          Otter: An Automated Deduction System     Web site:   www-unix.mcs.anl.gov/AR/otter/     Our current automated deduction system  Otter is designed to prove theorems stated in first-order logic with equality.  Otter's inference rules are based on resolution and paramodulation, and it includes facilities for term rewriting, term orderings, Knuth-Bendix completion, weighting, and strategies for directing and restricting searches for proofs.   Otter can also be used as a symbolic calculator and has an embedded equational programming system.        NICOLE     Web site:   nicole.sourceforge.net     It is an attempt to simulate a conversation by learning how words are related to other words. A Human communicates with NICOLE via the keyboard and NICOLE responds back with its own sentences which are automatically generated, based on what NICOLE has stored in it's database.  Each new sentence that has been typed in, and NICOLE doesn't know about it, it is included into NICOLE's database, thus extending the knowledge base of NICOLE.        PVS     Web site:   pvs.csl.sri.com/     PVS is a verification system: that is, a specification language integrated with support tools and a theorem prover. It is intended to capture the state-of-the-art in mechanized formal methods and to be sufficiently rugged that it can be used for significant applications. PVS is a research prototype: it evolves and improves as we develop or apply new capabilities, and as the stress of real use exposes new requirements.          RIPPER     Web site:   www.research.att.com/~wcohen/ripperd.html       Ripper is a system for fast effective rule induction. Given a set of data, Ripper will learn a set of rules that will predict the  patterns in the data. Ripper is written in ASCI C and comes with documentation and some sample problems.          SNePS     Web site:   www.cs.buffalo.edu/pub/sneps/WWW/   FTP site:   ftp.cs.buffalo.edu/pub/sneps/     The long-term goal of The SNePS Research Group is the design and construction of a natural-language-using computerized cognitive agent, and carrying out the research in artificial intelligence, computational linguistics, and cognitive science necessary for that endeavor. The three-part focus of the group is on knowledge representation, reasoning, and natural-language understanding and generation. The group is widely known for its development of the SNePS knowledge representation/reasoning system, and Cassie, its computerized cognitive agent.              Soar     Web site:   bigfoot.eecs.umich.edu/~soar/   FTP site:   cs.cmu.edu/afs/cs/project/soar/public/Soar6/       Soar has been developed to be a general cognitive architecture. We intend ultimately to enable the Soar architecture to:    work on the full range of tasks expected of an intelligent agent, from highly routine to extremely difficult, open-ended problems   represent and use appropriate forms of knowledge, such as procedural, declarative, episodic, and possibly iconic   employ the full range of problem solving methods    interact with the outside world and    learn about all aspects of the tasks and its performance on them.      In other words, our intention is for Soar to support all the capabilities required of a general intelligent agent. http://wwwis.cs.utwente.nl:8080/ tcm/index.html        TCM     Web site:   wwwis.cs.utwente.nl:8080/~tcm/index.html   FTP site:   ftp.cs.vu.nl/pub/tcm/       TCM (Toolkit for Conceptual Modeling) is our suite of graphical editors. TCM contains graphical editors for Entity-Relationship diagrams, Class-Relationship diagrams, Data and Event Flow diagrams, State Transition diagrams, Jackson Process Structure diagrams and System Network diagrams, Function Refinement trees and various table editors, such as a Function-Entity table editor and a Function Decomposition table editor.  TCM is easy to use and performs numerous consistency checks, some of them immediately, some of them upon request.          WEKA     Web site:   lucy.cs.waikato.ac.nz/~ml/       WEKA (Waikato Environment for Knowledge Analysis) is an state-of-the-art facility for applying machine learning techniques to practical problems. It is a comprehensive software ""workbench"" that allows people to analyse real-world data. It integrates different machine learning tools within a common framework and a uniform user interface. It is designed to support a ""simplicity-first"" methodology, which allows users to experiment interactively with simple machine learning tools before looking for more complex solutions.              Next   Previous   Contents"
GX229-45-15798690	"Linux AI & Alife HOWTO   Linux AI & Alife HOWTO  Table of Contents Linux AI & Alife HOWTO................................................................................................................................1 by John Eikenberry..................................................................................................................................1 1.Introduction...........................................................................................................................................1 2.Traditional Artificial Intelligence.........................................................................................................1 3.Connectionism......................................................................................................................................1 4.Evolutionary Computing .......................................................................................................................1 5.Alife & Complex Systems....................................................................................................................2 6.Autonomous Agents.............................................................................................................................2 7.Programming languages ........................................................................................................................2 1. Introduction..........................................................................................................................................2 1.1 Purpose ...............................................................................................................................................2 1.2 Where to find this software................................................................................................................2 1.3 Updates and comments......................................................................................................................3 2. Traditional Artificial Intelligence........................................................................................................3 2.1 AI class/code libraries........................................................................................................................3 2.2 AI software kits, applications, etc......................................................................................................5 3. Connectionism...................................................................................................................................13 3.1 Connectionist class/code libraries....................................................................................................13 3.2 Connectionist software kits/applications.........................................................................................18 4. Evolutionary Computing ....................................................................................................................24 4.1 EC class/code libraries.....................................................................................................................25 4.2 EC software kits/applications..........................................................................................................31 5. Alife & Complex Systems.................................................................................................................33 5.1 Alife & CS class/code libraries........................................................................................................34 5.2 Alife & CS software kits, applications, etc......................................................................................35 6. Autonomous Agents..........................................................................................................................40 7. Programming languages .....................................................................................................................49  i   Linux AI & Alife HOWTO by John Eikenberry Last modified: Jan 2000  This howto mainly contains information about, and links to, various AI related software libraries, applications, etc. that work on the Linux platform. All of it is (at least) free for personal use. The new master page for this document is http://zhar.net/gnu-linux/howto/  1.Introduction  1.1 Purpose  1.2 Where to find this software  1.3 Updates and comments  2.Traditional Artificial Intelligence  2.1 AI class/code libraries  2.2 AI software kits, applications, etc.  3.Connectionism  3.1 Connectionist class/code libraries  3.2 Connectionist software kits/applications  4.Evolutionary Computing  4.1 EC class/code libraries  4.2 EC software kits/applications  Linux AI & Alife HOWTO  1   Linux AI & Alife HOWTO  5.Alife & Complex Systems  5.1 Alife & CS class/code libraries  5.2 Alife & CS software kits, applications, etc.  6.Autonomous Agents 7.Programming languages Next Previous Contents Next Previous Contents  1. Introduction  1.1 Purpose The Linux OS has evolved from its origins in hackerdom to a full blown UNIX, capable of rivaling any commercial UNIX. It now provides an inexpensive base to build a great workstation. It has shed its hardware dependencies, having been ported to DEC Alphas, Sparcs, PowerPCs, with others on the way. This potential speed boost along with its networking support will make it great for workstation clusters. As a workstation it allows for all sorts of research and development, including artificial intelligence and artificial life.  The purpose of this Mini-Howto is to provide a source to find out about various software packages, code libraries, and anything else that will help someone get started working with (and find resources for) artificial intelligence and artificial life. All done with Linux specifically in mind.  1.2 Where to find this software All this software should be available via the net (ftp || http). The links to where to find it will be provided in the description of each package. There will also be plenty of software not covered on these pages (which is usually platform independent) located on one of the resources listed on the links section of the Master Site (given above).  5.Alife & Complex Systems  2   Linux AI & Alife HOWTO  1.3 Updates and comments If you find any mistakes, know of updates to one of the items below, or have problems compiling and of the applications, please mail me at: jae@NOSPAM-zhar.net and I'll see what I can do.  If you know of any AI/Alife applications, class libraries, etc. Please email me about them. Include your name, ftp and/or http sites where they can be found, plus a brief overview/commentary on the software (this info would make things a lot easier on me... but don't feel obligated ;).  I know that keeping this list up to date and expanding it will take quite a bit of work. So please be patient (I do have other projects). I hope you will find this document helpful.  Next Previous ContentsNextPreviousContents  2. Traditional Artificial Intelligence Traditional its roots are example of to utilize it. systems. AI is based around the ideas of logic, rule systems, linguistics, and the concept of rationality. At programming languages such as Lisp and Prolog. Expert systems are the largest successful this paradigm. An expert system consists of a detailed knowledge base and a complex rule system Such systems have been used for such things as medical diagnosis support and credit checking  2.1 AI class/code libraries These are libraries of code or classes for use in programming within the artificial intelligence field. They are not meant as stand alone applications, but rather as tools for building your own applications.  AI Search   FTP site: ftp.icce.rug.nl/pub/peter/ Submitted by: Peter M. Bouthoorn 1.3 Updates and comments 3   Linux AI & Alife HOWTO Basically, the library offers the programmer a set of search algorithms that may be used to solve all kind of different problems. The idea is that when developing problem solving software the programmer should be able to concentrate on the representation of the problem to be solved and should not need to bother with the implementation of the search algorithm that will be used to actually conduct the search. This idea has been realized by the implementation of a set of search classes that may be incorporated in other software through C++'s features of derivation and inheritance. The following search algorithms have been implemented:  - depth-first tree and graph search. - breadth-first tree and graph search. - uniform-cost tree and graph search. - best-first search. - bidirectional depth-first tree and graph search. - bidirectional breadth-first tree and graph search. - AND/OR depth tree search. - AND/OR breadth tree search.  Peter plans to release a new version of the library soon, which will also be featured in a book about C++ and AI to appear this year.  Chess In Lisp (CIL)   FTP site: chess.onenet.net/pub/chess/uploads/projects/  The CIL (Chess In Lisp) foundation is a Common Lisp implementaion of all the core functions needed for development of chess applications. The main purpose of the CIL project is to get AI researchers interested in using Lisp to work in the chess domain.  DAI   Web site: starship.skyport.net/crew/gandalf/DNET/AI  A library for the Python programming language that provides an object oriented interface to the CLIPS expert system tool. It includes an interface to COOL (CLIPS Object Oriented Language) that allows:     Investigate COOL classes Create and manipulate with COOL instances Manipulate with COOL message-handler's Manipulate with Modules  1.3 Updates and comments  4   Linux AI & Alife HOWTO Nyquist   Web site: www.cs.cmu.edu/afs/cs.cmu.edu/project/music/web/music.html  The Computer Music Project at CMU is developing computer music and interactive performance technology to enhance human musical experience and creativity. This interdisciplinary effort draws on Music Theory, Cognitive Science, Artificial Intelligence and Machine Learning, Human Computer Interaction, Real-Time Systems, Computer Graphics and Animation, Multimedia, Programming Languages, and Signal Processing. A paradigmatic example of these interdisciplinary efforts is the creation of interactive performances that couple human musical improvisation with intelligent computer agents in real-time.  Screamer   Web site: www.cis.upenn.edu/~screamer-tools/home.html  Screamer is an extension of Common Lisp that adds support for nondeterministic programming. Screamer consists of two levels. The basic nondeterministic level adds support for backtracking and undoable side effects. On top of this nondeterministic substrate, Screamer provides a comprehensive constraint programming language in which one can formulate and solve mixed systems of numeric and symbolic constraints. Together, these two levels augment Common Lisp with practically all of the functionality of both Prolog and constraint logic programming languages such as CHiP and CLP(R). Furthermore, Screamer is fully integrated with Common Lisp. Screamer programs can coexist and interoperate with other extensions to Common Lisp such as CLOS, CLIM and Iterate.  2.2 AI software kits, applications, etc. These are various applications, software kits, etc. meant for research in the field of artificial intelligence. Their ease of use will vary, as they were designed to meet some particular research interest more than as an easy to use commercial package.  2.2 AI software kits, applications, etc.  5   Linux AI & Alife HOWTO ASA - Adaptive Simulated Annealing   Web site: www.ingber.com/#ASA-CODE  FTP site: ftp.ingber.com/  ASA (Adaptive Simulated Annealing) is a powerful global optimization C-code algorithm especially useful for nonlinear and/or stochastic systems.  ASA is developed to statistically find the best global fit of a nonlinear non-convex cost-function over a D-dimensional space. This algorithm permits an annealing schedule for 'temperature' T decreasing exponentially in annealing-time k, T = T_0 exp(-c k^1/D). The introduction of re-annealing also permits adaptation to changing sensitivities in the multi-dimensional parameter-space. This annealing schedule is faster than fast Cauchy annealing, where T = T_0/k, and much faster than Boltzmann annealing, where T = T_0/ln k.  Babylon   FTP site: ftp.gmd.de/gmd/ai-research/Software/Babylon/  BABYLON is a modular, configurable, hybrid environment for developing expert systems. Its features include objects, rules with forward and backward chaining, logic (Prolog) and constraints. BABYLON is implemented and embedded in Common Lisp.  CLEARS   Web site: www.coli.uni-sb.de/~clears/  The CLEARS system is an interactive graphical environment for computational semantics. The tool allows exploration and comparison of different semantic formalisms, and their interaction with syntax. This enables the user to get an idea of the range of possibilities of semantic construction, and also where there is real convergence between theories.  2.2 AI software kits, applications, etc.  6   Linux AI & Alife HOWTO CLIG   Web site: www.ags.uni-sb.de/~konrad/clig.html  CLIG is an interactive, extendible grapher for visualizing linguistic data structures like trees, feature structures, Discourse Representation Structures (DRS), logical formulas etc. All of these can be freely mixed and embedded into each other. The grapher has been designed both to be stand-alone and to be used as an add-on for linguistic applications which display their output in a graphical manner.  CLIPS   Web site: www.jsc.nasa.gov/~clips/CLIPS.html  FTP site: cs.cmu.edu/afs/cs.cmu.edu/project/ai-repository/ai/areas/expert/systems/clips  CLIPS is a productive development and delivery expert system tool which provides a complete environment for the construction of rule and/or object based expert systems.  CLIPS provides a cohesive tool for handling a wide variety of knowledge with support for three different programming paradigms: rule-based, object-oriented and procedural. Rule-based programming allows knowledge to be represented as heuristics, or ""rules of thumb,"" which specify a set of actions to be performed for a given situation. Object-oriented programming allows complex systems to be modeled as modular components (which can be easily reused to model other systems or to create new components). The procedural programming capabilities provided by CLIPS are similar to capabilities found in languages such as C, Pascal, Ada, and LISP.  EMA-XPS - A Hybrid Graphic Expert System Shell   Web site: wmwap1.math.uni-wuppertal.de:80/EMA-XPS/  EMA-XPS is a hybrid graphic expert system shell based on the ASCII-oriented shell Babylon 2.3 of the German National Research Center for Computer Sciences (GMD). In addition to Babylon's AI-power (object oriented data representation, forward and backward chained rules - collectible into sets, horn clauses, and constraint networks) a graphic interface based on the X11 Window System and the OSF/Motif Widget Library has been provided.  2.2 AI software kits, applications, etc.  7   Linux AI & Alife HOWTO FOOL & FOX   FTP site: ntia.its.bldrdoc.gov/pub/fuzzy/prog/  FOOL stands for the Fuzzy Organizer OLdenburg. It is a result from a project at the University of Oldenburg. FOOL is a graphical user interface to develop fuzzy rulebases. FOOL will help you to invent and maintain a database that specifies the behavior of a fuzzy-controller or something like that.  FOX is a small but powerful fuzzy engine which reads this database, reads some input values and calculates the new control value.  FUF and SURGE   Web site: www.dfki.de/lt/registry/generation/fuf.html  FTP site: ftp.cs.columbia.edu/pub/fuf/ FUF is an extended implementation of the formalism of functional unification grammars (FUGs) introduced by Martin Kay specialized to the task of natural language generation. It adds the following features to the base formalism:  Types and inheritance.  Extended control facilities (goal freezing, intelligent backtracking).  Modular syntax. These extensions allow the development of large grammars which can be processed efficiently and can be maintained and understood more easily. SURGE is a large syntactic realization grammar of English written in FUF. SURGE is developed to serve as a black box syntactic generation component in a larger generation system that encapsulates a rich knowledge of English syntax. SURGE can also be used as a platform for exploration of grammar writing with a generation perspective.  The Grammar Workbench   Web site: www.cs.kun.nl/agfl/GWB.html  The Grammar Workbench, or GWB for short, is an environment for the comfortable development of Affix Grammars in the AGFL-formalism. Its purposes are:  2.2 AI software kits, applications, etc.  8   Linux AI & Alife HOWTO      to to to to to allow the user to input, inspect and modify a grammar; perform consistency checks on the grammar; compute grammar properties; generate example sentences; assist in performing grammar transformations.  GSM Suite   Web site: www.slip.net/~andrewm/gsm/  The GSM Suite is a set of programs for using Finite State Machines in a graphical fashion. The suite consists of programs that edit, compile, and print state machines. Included in the suite is an editor program, gsmedit, a compiler, gsm2cc, that produces a C++ implementation of a state machine, a PostScript generator, gsm2ps, and two other minor programs. GSM is licensed under the GNU Public License and so is free for your use under the terms of that license.  Illuminator   Web site: documents.cfar.umd.edu/resources/source/illuminator.html  Illuminator is a toolset for developing OCR and Image Understanding applications. Illuminator has two major parts: a library for representing, storing and retrieving OCR information, heretofore called dafslib, and an X-Windows ""DAFS"" file viewer, called illum. Illuminator and DAFS lib were designed to supplant existing OCR formats and become a standard in the industry. They particularly are extensible to handle more than just English. The features of this release:  5 magnification levels for images  flagged characters and words  unicode support -- American, British, French, German, Greek, Italian, MICR, Norwegian, Russian, Spanish, Swedish, keyboards  reads DAFS, TIFF's, PDA's (image only)  save to DAFS, ASCII/UTF or Unicode  Entity Viewer - shows properties, character choices, bounding boxes image fragment for a selected entity, change type, change content, hierarchy mode  Jess, the Java Expert System Shell 2.2 AI software kits, applications, etc. 9   Linux AI & Alife HOWTO  Web site: herzberg.ca.sandia.gov/jess/  Jess is a clone of the popular CLIPS expert system shell written entirely in Java. With Jess, you can conveniently give your applets the ability to 'reason'. Jess is compatible with all versions of Java starting with version 1.0.2. Jess implements the following constructs from CLIPS: defrules, deffunctions, defglobals, deffacts, and deftemplates.  learn   FTP site: sunsite.unc.edu/pub/Linux/apps/cai/  Learn is a vocable learning program with memory model.  Otter: An Automated Deduction System   Web site: www-unix.mcs.anl.gov/AR/otter/  Our current automated deduction system Otter is designed to prove theorems stated in first-order logic with equality. Otter's inference rules are based on resolution and paramodulation, and it includes facilities for term rewriting, term orderings, Knuth-Bendix completion, weighting, and strategies for directing and restricting searches for proofs. Otter can also be used as a symbolic calculator and has an embedded equational programming system.  PVS   Web site: pvs.csl.sri.com/ PVS is a verification system: that is, a specification language integrated with support tools and a theorem prover. It is intended to capture the state-of-the-art in mechanized formal methods and to be sufficiently rugged that it can be used for significant applications. PVS is a research prototype: it evolves and improves as we develop or apply new capabilities, and as the stress of real use exposes new requirements. 2.2 AI software kits, applications, etc. 10   Linux AI & Alife HOWTO RIPPER   Web site: www.research.att.com/~wcohen/ripperd.html  Ripper is a system for fast effective rule induction. Given a set of data, Ripper will learn a set of rules that will predict the patterns in the data. Ripper is written in ASCI C and comes with documentation and some sample problems.  SNePS   Web site: www.cs.buffalo.edu/pub/sneps/WWW/  FTP site: ftp.cs.buffalo.edu/pub/sneps/ The long-term goal of The SNePS Research Group is the design and construction of a natural-language-using computerized cognitive agent, and carrying out the research in artificial intelligence, computational linguistics, and cognitive science necessary for that endeavor. The three-part focus of the group is on knowledge representation, reasoning, and natural-language understanding and generation. The group is widely known for its development of the SNePS knowledge representation/reasoning system, and Cassie, its computerized cognitive agent.  Soar   Web site: bigfoot.eecs.umich.edu/~soar/  FTP site: cs.cmu.edu/afs/cs/project/soar/public/Soar6/  Soar has been developed to be a general cognitive architecture. We intend ultimately to enable the Soar architecture to:  work on the full range of tasks expected of an intelligent agent, from highly routine to extremely difficult, open-ended problems  represent and use appropriate forms of knowledge, such as procedural, declarative, episodic, and possibly iconic  employ the full range of problem solving methods  interact with the outside world and  learn about all aspects of the tasks and its performance on them. 2.2 AI software kits, applications, etc. 11   Linux AI & Alife HOWTO In other words, our intention is for Soar to support all the capabilities required of a general intelligent agent. http://wwwis.cs.utwente.nl:8080/ tcm/index.html  TCM   Web site: wwwis.cs.utwente.nl:8080/~tcm/index.html  FTP site: ftp.cs.vu.nl/pub/tcm/  TCM (Toolkit for Conceptual Modeling) is our suite of graphical editors. TCM contains graphical editors for Entity-Relationship diagrams, Class-Relationship diagrams, Data and Event Flow diagrams, State Transition diagrams, Jackson Process Structure diagrams and System Network diagrams, Function Refinement trees and various table editors, such as a Function-Entity table editor and a Function Decomposition table editor. TCM is easy to use and performs numerous consistency checks, some of them immediately, some of them upon request.  WEKA   Web site: lucy.cs.waikato.ac.nz/~ml/  WEKA (Waikato Environment for Knowledge Analysis) is an state-of-the-art facility for applying machine learning techniques to practical problems. It is a comprehensive software ""workbench"" that allows people to analyse real-world data. It integrates different machine learning tools within a common framework and a uniform user interface. It is designed to support a ""simplicity-first"" methodology, which allows users to experiment interactively with simple machine learning tools before looking for more complex solutions.  NextPreviousContentsNextPreviousContents  2.2 AI software kits, applications, etc.  12   Linux AI & Alife HOWTO  3. Connectionism Connectionism is a technical term for a group of related techniques. These techniques include areas such as Artificial Neural Networks, Semantic Networks and a few other similar ideas. My present focus is on neural networks (though I am looking for resources on the other techniques). Neural networks are programs designed to simulate the workings of the brain. They consist of a network of small mathematical-based nodes, which work together to form patterns of information. They have tremendous potential and currently seem to be having a great deal of success with image processing and robot control.  3.1 Connectionist class/code libraries These are libraries of code or classes for use in programming within the Connectionist field. They are not meant as stand alone applications, but rather as tools for building your own applications.  ANSI-C Neural Networks   Web site: www.geocities.com/CapeCanaveral/1624/  This site contains ANSC-C source code for 8 types of neural nets, including:         Adaline Network Backpropagation Hopfield Model (BAM) Bidirectional Associative Memory Boltzmann Machine Counterpropagation (SOM) Self-Organizing Map (ART1) Adaptive Resonance Theory  They were designed to help turn the theory of a particular network model into the design for a simulator implementation , and to help with embeding an actual application into a particular network model.  BELIEF  3. Connectionism  13   Linux AI & Alife HOWTO  Web site: www.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/reasonng/probabl/belief/  BELIEF is a Common Lisp implementation of the Dempster and Kong fusion and propagation algorithm for Graphical Belief Function Models and the Lauritzen and Spiegelhalter algorithm for Graphical Probabilistic Models. It includes code for manipulating graphical belief models such as Bayes Nets and Relevance Diagrams (a subset of Influence Diagrams) using both belief functions and probabilities as basic representations of uncertainty. It uses the Shenoy and Shafer version of the algorithm, so one of its unique features is that it supports both probability distributions and belief functions. It also has limited support for second order models (probability distributions on parameters).  CONICAL   Web site: strout.net/conical/ CONICAL is a C++ class library for building simulations common in computational neuroscience. Currently its focus is on compartmental modeling, with capabilities similar to GENESIS and NEURON. A model neuron is built out of compartments, usually with a cylindrical shape. When small enough, these open-ended cylinders can approximate nearly any geometry. Future classes may support reaction-diffusion kinetics and more. A key feature of CONICAL is its cross-platform compatibility; it has been fully co-developed and tested under Unix, DOS, and Mac OS.  IDEAL   Web site: www.rpal.rockwell.com/ideal.html  IDEAL is a test bed for work in influence diagrams and Bayesian networks. It contains various inference algorithms for belief networks and evaluation algorithms for influence diagrams. It contains facilities for creating and editing influence diagrams and belief networks.  IDEAL is written in pure Common Lisp and so it will run in Common Lisp on any platform. The emphasis in writing IDEAL has been on code clarity and providing high level programming abstractions. It thus is very suitable for experimental implementations which need or extend belief network technology.  At the highest level, IDEAL can be used as a subroutine library which provides belief network 3. Connectionism 14   Linux AI & Alife HOWTO inference and influence diagram evaluation as a package. The code is documented in a detailed manual and so it is also possible to work at a lower level on extensions of belief network methods.  IDEAL comes with an optional graphic interface written in CLIM. If your Common Lisp also has CLIM, you can run the graphic interface.  Matrix Class   FTP site: ftp.cs.ucla.edu/pub/  A simple, fast, efficient C++ Matrix class designed for scientists and engineers. The Matrix class is well suited for applications with complex math algorithms. As an demonstration of the Matrix class, it was used to implement the backward error propagation algorithm for a multi-layer feed-forward artificial neural network.  nunu   Web site: ruby.ddiworld.com/jreed/web/software/nn.html  nunu is a multi-layered, scriptable, back-propagation neural network. It is build to be used for intensive computation problems scripted in shell scripts. It is written in C++ using the STL. nn is based on material from the ""Introduction to the Theory of Neural Computation"" by John Hertz, Anders Krogh, and Richard G. Palmer, chapter 6.  Pulcinella   Web site: iridia.ulb.ac.be/pulcinella/Welcome.html  Pulcinella is written in CommonLisp, and appears as a library of Lisp functions for creating, modifying and evaluating valuation systems. Alternatively, the user can choose to interact with Pulcinella via a graphical interface (only available in Allegro CL). Pulcinella provides primitives to build and evaluate uncertainty models according to several uncertainty calculi, including probability theory, possibility theory, and Dempster-Shafer's theory of belief functions; and the possibility theory by Zadeh, Dubois and Prade's. A User's Manual is available on request. 3. Connectionism 15   Linux AI & Alife HOWTO S-ElimBel   Web site (???): www.spaces.uci.edu/thiery/elimbel/  S-ElimBel is an algorithm that computes the belief in a Bayesian network, implemented in MIT-Scheme. This algorithm has the particularity of being rather easy to understand. Moreover, one can apply it to any kind of Bayesian network - it being singly connected or muliply connected. It is, however, less powerful than the standard algorithm of belief propagation. Indeed, the computation has to be reconducted entirely for each new evidence added to the network. Also, one needs to run the algorithm as many times as one has nodes for which the belief is wanted.  Software for Flexible Bayesian Modeling   Web site: www.cs.utoronto.ca/~radford/fbm.software.html  This software implements flexible Bayesian models for regression and classification applications that are based on multilayer perceptron neural networks or on Gaussian processes. The implementation uses Markov chain Monte Carlo methods. Software modules that support Markov chain sampling are included in the distribution, and may be useful in other applications.  Spiderweb2   Web site: www.cs.nyu.edu/~klap7794/spiderweb2.html  A C++ artificial neual net library. Spiderweb2 is a complete rewrite of the original Spiderweb library, it has grown into a much more flexible and object-oriented system. The biggest change is that each neuron object is responsible for its own activations and updates, with the network providing only the scheduling aspect. This is a very powerful change, and it allows easy modification and experimentation with various network architectures and neuron types.  Symbolic Probabilistic Inference (SPI)  3. Connectionism  16   Linux AI & Alife HOWTO  FTP site: ftp.engr.orst.edu/pub/dambrosi/spi/  Paper (ijar-94.ps): ftp.engr.orst.edu/pub/dambrosi/  Contains Common Lisp function libraries to implement SPI type baysean nets. Documentation is very limited. Features:  Probabilities, Local Expression Language Utilities, Explanation, Dynamic Models, and a TCL/TK based GUI.  TresBel   FTP site: iridia.ulb.ac.be/pub/hongxu/software/  Libraries containing (Allegro) Common Lisp code for Belief Functions (aka. Dempster-Shafer evidential reasoning) as a representation of uncertainty. Very little documentation. Has a limited GUI.  Various (C++) Neural Networks   Web site: www.dontveter.com/nnsoft/nnsoft.html  Example neural net codes from the book, The Pattern Recognition Basics of AI. These are simple example codes of these various neural nets. They work well as a good starting point for simple experimentation and for learning what the code is like behind the simulators. The types of networks available on this site are: (implemented in C++)           The Backprop Package The Nearest Neighbor Algorithms The Interactive Activation Algorithm The Hopfield and Boltzman machine Algorithms The Linear Pattern Classifier ART I Bi-Directional Associative Memory The Feedforward Counter-Propagation Network  3. Connectionism  17   Linux AI & Alife HOWTO  3.2 Connectionist software kits/applications These are various applications, software kits, etc. meant for research in the field of Connectionism. Their ease of use will vary, as they were designed to meet some particular research interest more than as an easy to use commercial package.  Aspirin - MIGRAINES  (am6.tar.Z on ftp site)  FTP site: sunsite.unc.edu/pub/academic/computer-science/neural-networks/programs/Aspirin/  The software that we are releasing now is for creating, and evaluating, feed-forward networks such as those used with the backpropagation learning algorithm. The software is aimed both at the expert programmer/neural network researcher who may wish to tailor significant portions of the system to his/her precise needs, as well as at casual users who will wish to use the system with an absolute minimum of effort.  DDLab   Web site: www.santafe.edu/~wuensch/ddlab.html  FTP site: ftp.santafe.edu/pub/wuensch/ DDLab is an interactive graphics program for research into the dynamics of finite binary networks, relevant to the study of complexity, emergent phenomena, neural networks, and aspects of theoretical biology such as gene regulatory networks. A network can be set up with any architecture between regular CA (1d or 2d) and ""random Boolean networks"" (networks with arbitrary connections and heterogeneous rules). The network may also have heterogeneous neighborhood sizes.  GENESIS   Web site: www.bbb.caltech.edu/GENESIS/  FTP site: genesis.bbb.caltech.edu/pub/genesis/ GENESIS (short for GEneral NEural SImulation System) is a general purpose simulation platform 3.2 Connectionist software kits/applications 18   Linux AI & Alife HOWTO which was developed to support the simulation of neural systems ranging from complex models of single neurons to simulations of large networks made up of more abstract neuronal components. GENESIS has provided the basis for laboratory courses in neural simulation at both Caltech and the Marine Biological Laboratory in Woods Hole, MA, as well as several other institutions. Most current GENESIS applications involve realistic simulations of biological neural systems. Although the software can also model more abstract networks, other simulators are more suitable for backpropagation and similar connectionist modeling.  JavaBayes   Web site: www.cs.cmu.edu/People/javabayes/index.html/  The JavaBayes system is a set of tools, containing a graphical editor, a core inference engine and a parser. JavaBayes can produce:     the marginal distribution for any variable in a network. the expectations for univariate functions (for example, expected value for variables). configurations with maximum a posteriori probability. configurations with maximum a posteriori expectation for univariate functions.  Jbpe   Web site: cs.felk.cvut.cz/~koutnij/studium/jbpe.html  Jbpe is a back-propagation neural network editor/simulator. Features     Standart back-propagation networks creation. Saving network as a text file, which can be edited and loaded back. Saving/loading binary file Learning from a text file (with structure specified below), number of learning periods / desired network energy can be specified as a criterion.  Network recall  Neural Network Generator 3.2 Connectionist software kits/applications 19   Linux AI & Alife HOWTO  Web site: www.idsia.ch/~rafal/research.html  FTP site: >ftp.idsia.ch/pub/rafal  The Neural Network Generator is a genetic algorithm for the topological optimization of feedforward neural networks. It implements the Semantic Changing Genetic Algorithm and the Unit-Cluster Model. The Semantic Changing Genetic Algorithm is an extended genetic algorithm that allows fast dynamic adaptation of the genetic coding through population analysis. The Unit-Cluster Model is an approach to the construction of modular feedforward networks with a ''backbone'' structure.  NOTE: To compile this on Linux requires one change in the Makefiles. You will need to change '-ltermlib' to '-ltermcap'.  Neureka ANS (nn/xnn)   Web site: www.bgif.no/neureka/  FTP site: ftp.ii.uib.no/pub/neureka/  nn is a high-level neural network specification language. The current version is best suited for feed-forward nets, but recurrent models can and have been implemented, e.g. Hopfield nets, Jordan/Elman nets, etc. In nn, it is easy to change network dynamics. The nn compiler can generate C code or executable programs (so there must be a C compiler available), with a powerful command line interface (but everything may also be controlled via the graphical interface, xnn). It is possible for the user to write C routines that can be called from inside the nn specification, and to use the nn specification as a function that is called from a C program. Please note that no programming is necessary in order to use the network models that come with the system (`netpack').  xnn is a graphical front end to networks generated by the nn compiler, and to the compiler itself. The xnn graphical interface is intuitive and easy to use for beginners, yet powerful, with many possibilities for visualizing network data.  NOTE: You have to run the install program that comes with this to get the license key installed. It gets put (by default) in /usr/lib. If you (like myself) want to install the package somewhere other than in the /usr directory structure (the install program gives you this option) you will have to set up some environmental variables (NNLIBDIR & NNINCLUDEDIR are required). You can read about these (and a few other optional variables) in appendix A of the documentation (pg 113).  NEURON  3.2 Connectionist software kits/applications  20   Linux AI & Alife HOWTO  Web site: www.neuron.yale.edu/neuron.html  FTP site: ftp.neuron.yale.edu/neuron/unix/ NEURON is an extensible nerve modeling and simulation program. It allows you to create complex nerve models by connecting multiple one-dimensional sections together to form arbitrary cell morphologies, and allows you to insert multiple membrane properties into these sections (including channels, synapses, ionic concentrations, and counters). The interface was designed to present the neural modeler with a intuitive environment and hide the details of the numerical methods used in the simulation.  PDP++   Web site: www.cnbc.cmu.edu/PDP++/  FTP site (US): cnbc.cmu.edu/pub/pdp++/  FTP site (Europe): unix.hensa.ac.uk/mirrors/pdp++/  As the field of Connectionist modeling has grown, so has the need for a comprehensive simulation environment for the development and testing of Connectionist models. Our goal in developing PDP++ has been to integrate several powerful software development and user interface tools into a general purpose simulation environment that is both user friendly and user extensible. The simulator is built in the C++ programming language, and incorporates a state of the art script interpreter with the full expressive power of C++. The graphical user interface is built with the Interviews toolkit, and allows full access to the data structures and processing modules out of which the simulator is built. We have constructed several useful graphical modules for easy interaction with the structure and the contents of neural networks, and we've made it possible to change and adapt many things. At the programming level, we have set things up in such a way as to make user extensions as painless as possible. The programmer creates new C++ objects, which might be new kinds of units or new kinds of processes; once compiled and linked into the simulator, these new objects can then be accessed and used like any other.  RNS   Web site: www.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/neural/systems/rns/ RNS (Recurrent Network Simulator) is a simulator for recurrent neural networks. Regular neural networks are also supported. The program uses a derivative of the back-propagation algorithm, but also includes other (not that well tested) algorithms. Features include  3.2 Connectionist software kits/applications  21   Linux AI & Alife HOWTO      freely choosable connections, no restrictions besides memory or CPU constraints delayed links for recurrent networks fixed values or thresholds can be specified for weights (recurrent) back-propagation, Hebb, differential Hebb, simulated annealing and more patterns can be specified with bits, floats, characters, numbers, and random bit patterns with Hamming distances can be chosen for you  user definable error functions  output results can be used without modification as input  Simple Neural Net (in Python)   Web site: starship.python.net/crew/amk/unmaintained/  Simple neural network code, which implements a class for 3-level networks (input, hidden, and output layers). The only learning rule implemented is simple backpropagation. No documentation (or even comments) at all, because this is simply code that I use to experiment with. Includes modules containing sample datasets from Carl G. Looney's NN book. Requires the Numeric extensions.  SCNN   Web site: apx00.physik.uni-frankfurt.de/e_ag_rt/SCNN/  SCNN is an universal simulating system for Cellular Neural Networks (CNN). CNN are analog processing neural networks with regular and local interconnections, governed by a set of nonlinear ordinary differential equations. Due to their local connectivity, CNN are realized as VLSI chips, which operates at very high speed.  Semantic Networks in Python   Web site: strout.net/info/coding/python/ai/index.html  The semnet.py module defines several simple classes for building and using semantic networks. A semantic network is a way of representing knowledge, and it enables the program to do simple 3.2 Connectionist software kits/applications 22   Linux AI & Alife HOWTO reasoning with very little effort on the part of the programmer.  The following classes are defined:  Entity: This class represents a noun; it is something about which you can store facts.  Relation: A Relation is a type of relationship which special relation, ""IS_A"", is predefined because it has inheritance).  Fact: A Fact is an assertion that a relationship exists which can be related to other things, and may exist between two entities. One special meaning (a sort of logical between two entities.  With these three object types, you can very quickly define knowledge about a set of objects, and query them for logical conclusions.  SNNS   Web site: www.informatik.uni-stuttgart.de/ipvr/bv/projekte/snns/  FTP site: ftp.informatik.uni-stuttgart.de/pub/SNNS/ Stuttgart Neural Net Simulator (version 4.1). An awesome neural net simulator. Better than any commercial simulator I've seen. The simulator kernel is written in C (it's fast!). It supports over 20 different network architectures, has 2D and 3D X-based graphical representations, the 2D GUI has an integrated network editor, and can generate a separate NN program in C. SNNS is very powerful, though a bit difficult to learn at first. To help with this it comes with example networks and tutorials for many of the architectures. ENZO, a supplementary system allows you to evolve your networks with genetic algorithms.  There is a debian package of SNNS available. So just get it (and use alien to convert it to RPM if you need to).  SPRLIB/ANNLIB   Web site: www.ph.tn.tudelft.nl/~sprlib/  SPRLIB (Statistical Pattern Recognition Library) was developed to support the easy construction and simulation of pattern classifiers. It consist of a library of functions (written in C) that can be called from your own program. Most of the well-known classifiers are present (k-nn, Fisher, Parzen, ....), 3.2 Connectionist software kits/applications 23   Linux AI & Alife HOWTO as well as error estimation and dataset generation routines.  ANNLIB (Artificial Neural Networks Library) is a neural network simulation library based on the data architecture laid down by SPRLIB. The library contains numerous functions for creating, training and testing feed-forward networks. Training algorithms include back-propagation, pseudo-Newton, Levenberg-Marquardt, conjugate gradient descent, BFGS.... Furthermore, it is possible - due to the datastructures' general applicability - to build Kohonen maps and other more exotic network architectures using the same data types.  TOOLDIAG   Web site: www.inf.ufes.br/~thomas/www/home/tooldiag.html  FTP site: ftp.inf.ufes.br/pub/tooldiag/ TOOLDIAG is a collection of methods for statistical pattern recognition. The main area of application is classification. The application area is limited to multidimensional continuous features, without any missing values. No symbolic features (attributes) are allowed. The program in implemented in the 'C' programming language and was tested in several computing environments.  NextPreviousContentsNextPreviousContents  4. Evolutionary Computing Evolutionary computing is actually a broad term for a vast array of programming techniques, including genetic algorithms, complex adaptive systems, evolutionary programming, etc. The main thrust of all these techniques is the idea of evolution. The idea that a program can be written that will evolve toward a certain goal. This goal can be anything from solving some engineering problem to winning a game.  4. Evolutionary Computing  24   Linux AI & Alife HOWTO  4.1 EC class/code libraries These are libraries of code or classes for use in programming within the evolutionary computation field. They are not meant as stand alone applications, but rather as tools for building your own applications.  daga   Web site: GARAGe.cps.msu.edu/software/software-index.html  daga is an experimental release of a 2-level genetic algorithm compatible with the GALOPPS GA software. It is a meta-GA which dynamically evolves a population of GAs to solve a problem presented to the lower-level GAs. When multiple GAs (with different operators, parameter settings, etc.) are simultaneously applied to the same problem, the ones showing better performance have a higher probability of surviving and ""breeding"" to the next macro-generation (i.e., spawning new ""daughter""-GAs with characteristics inherited from the parental GA or GAs. In this way, we try to encourage good problem-solving strategies to spread to the whole population of GAs.  FORTRAN GA   Web site: www.staff.uiuc.edu/~carroll/ga.html  This program is a FORTRAN version of a genetic algorithm driver. This code initializes a random sample of individuals with different parameters to be optimized using the genetic algorithm approach, i.e. evolution via survival of the fittest. The selection scheme used is tournament selection with a shuffling technique for choosing random pairs for mating. The routine includes binary coding for the individuals, jump mutation, creep mutation, and the option for single-point or uniform crossover. Niching (sharing) and an option for the number of children per pair of parents has been added. More recently, an option for the use of a micro-GA has been added.  GAGS   Web site: kal-el.ugr.es/gags.html  FTP site: kal-el.ugr.es/GAGS/ 4.1 EC class/code libraries 25   Linux AI & Alife HOWTO Genetic Algorithm application generator and class library written mainly in C++. As a class library, and among other thing, GAGS includes:  A chromosome hierarchy with variable length chromosomes. Genetic operators: 2-point crossover, uniform crossover, bit-flip mutation, transposition (gene interchange between 2 parts of the chromosome), and variable-length operators: duplication, elimination, and random addition.  Population level operators include steady state, roulette wheel and tournament selection.  Gnuplot wrapper: turns gnuplot into a iostreams-like class.  Easy sample file loading and configuration file parsing. As an application generator (written in PERL), you only need to supply it with an ANSI-C or C++ fitness function, and it creates a C++ program that uses the above library to 90% capacity, compiles it, and runs it, saving results and presenting fitness thru gnuplot.  GAlib: Matthew's Genetic Algorithms Library   Web Site: lancet.mit.edu/ga/  FTP site: lancet.mit.edu/pub/ga/  Register GAlib at: lancet.mit.edu/ga/Register.html  GAlib contains a set of C++ genetic algorithm objects. The library includes tools for using genetic algorithms to do optimization in any C++ program using any representation and genetic operators. The documentation includes an extensive overview of how to implement a genetic algorithm as well as examples illustrating customizations to the GAlib classes.  GALOPPS   Web site: GARAGe.cps.msu.edu/software/software-index.html  FTP site: garage.cps.msu.edu/pub/GA/galopps/  GALOPPS is a flexible, generic GA, in 'C'. It was based upon Goldberg's Simple Genetic Algorithm (SGA) architecture, in order to make it easier for users to learn to use and extend.  GALOPPS extends the SGA capabilities several fold:  (optional) A new Graphical User Interface, based on TCL/TK, for Unix users, allowing easy running of GALOPPS 3.2 (single or multiple subpopulations) on one or more processors. GUI writes/reads ""standard"" GALOPPS input and master files, and displays graphical output 4.1 EC class/code libraries 26   Linux AI & Alife HOWTO (during or after run) of user-selected variables. 5 selection methods: roulette wheel, stochastic remainder sampling, tournament selection, stochastic universal sampling, linear-ranking-then-SUS. Random or superuniform initialization of ""ordinary"" (non-permutation) binary or non-binary chromosomes; random initialization of permutation-based chromosomes; or user-supplied initialization of arbitrary types of chromosomes. Binary or non-binary alphabetic fields on value-based chromosomes, including different user-definable field sizes. 3 crossovers for value-based representations: 1-pt, 2-pt, and uniform, all of which operate at field boundaries if a non-binary alphabet is used. 4 crossovers for order-based reps: PMX, order-based, uniform order-based, and cycle. 4 mutations: fast bitwise, multiple-field, swap and random sublist scramble. Fitness scaling: linear scaling, Boltzmann scaling, sigma truncation, window scaling, ranking. Plus a whole lot more....           GAS   Web site: starship.skyport.net/crew/gandalf  FTP site: ftp.coe.uga.edu/users/jae/ai  GAS means ""Genetic Algorithms Stuff"". GAS is freeware. Purpose of GAS is to explore and exploit artificial evolutions. Primary implementation language of GAS is Python. The GAS software package is meant to be a Python framework for applying genetic algorithms. It contains an example application where it is tried to breed Python program strings. This special problem falls into the category of Genetic Programming (GP), and/or Automatic Programming. Nevertheless, GAS tries to be useful for other applications of Genetic Algorithms as well.  GECO   FTP site: ftp://ftp.aic.nrl.navy.mil/pub/galist/src/  GECO (Genetic Evolution through Combination of Objects), an extendible object-oriented tool-box for constructing genetic algorithms (in Lisp). It provides a set of extensible classes and methods designed for generality. Some simple examples are also provided to illustrate the intended use.  4.1 EC class/code libraries  27   Linux AI & Alife HOWTO GPdata   FTP site: ftp.cs.bham.ac.uk/pub/authors/W.B.Langdon/gp-code/  Documentation (GPdata-icga-95.ps): cs.ucl.ac.uk/genetic/papers/  GPdata-3.0.tar.gz (C++) contains a version of Andy Singleton's GP-Quick version 2.1 which has been extensively altered to support:       Indexed memory operation (cf. teller) multi tree programs Adfs parameter changes without recompilation populations partitioned into demes (A version of) pareto fitness  This ftp site also contains a small C++ program (ntrees.cc) to calculate the number of different there are of a given length and given function and terminal set.  gpjpp Genetic Programming in Java   [Dead Link] Web site: http://www.turbopower.com/~kimk/gpjpp.asp  Anyone who knows where to find gpjpp, please let me know.  gpjpp is a Java package I wrote for doing research in genetic programming. It is a port of the gpc++ kernel written by Adam Fraser and Thomas Weinbrenner. Included in the package are four of Koza's standard examples: the artificial ant, the hopping lawnmower, symbolic regression, and the boolean multiplexer. Here is a partial list of its features:              graphic output of expression trees efficient diversity checking Koza's greedy over-selection option for large populations extensible GPRun class that encapsulates most details of a genetic programming test more robust and efficient streaming code, with automatic checkpoint and restart built into the GPRun class an explicit complexity limit that can be set on each GP additional configuration variables to allow more testing without recompilation support for automatically defined functions (ADFs) tournament and fitness proportionate selection demetic grouping optional steady state population subtree crossover swap and shrink mutation 28  4.1 EC class/code libraries   Linux AI & Alife HOWTO GP Kernel   Web site (???): www.emk.e-technik.th-darmstadt.de/~thomasw/gp.html The GP kernel is a C++ class library that can be used to apply genetic programming techniques to all kinds of problems. The library defines a class hierarchy. An integral component is the ability to produce automatically defined functions as found in Koza's ""Genetic Programming II"". Technical documentation (postscript format) is included. There is also a short introduction into genetic programming.  Functionality includes; Automatically defined functions (ADFs), tournament and fitness proportionate selection, demetic grouping, optional steady state genetic programming kernel, subtree crossover, swap and shrink mutation, a way of changing every parameter of the system without recompilation, capacity for multiple populations, loading and saving of populations and genetic programs, standard random number generator, internal parameter checks.  lil-gp   Web site: GARAGe.cps.msu.edu/software/software-index.html#lilgp  FTP site: garage.cps.msu.edu/pub/GA/lilgp/ patched lil-gp *   Web site: www.cs.umd.edu/users/seanl/gp/  lil-gp is a generic 'C' genetic programming tool. It was written with a number of goals in mind: speed, ease of use and support for a number of options including:  Generic 'C' program that runs on UNIX workstations  Support for multiple population experiments, using arbitrary and user settable topologies for exchange, for a single processor (i.e., you can do multiple population gp experiments on your PC).  lil-gp manipulates trees of function pointers which are allocated in single, large memory blocks for speed and to avoid swapping. * The patched lil-gp kernel is strongly-typed, with modifications on multithreading, coevolution, and other tweaks and features.  4.1 EC class/code libraries  29   Linux AI & Alife HOWTO PGAPack  Parallel Genetic Algorithm Library  Web site: www.mcs.anl.gov/~levine/PGAPACK/  FTP site: ftp.mcs.anl.gov/pub/pgapack/  PGAPack is a general-purpose, data-structure-neutral, parallel genetic algorithm library. It is intended to provide most capabilities desired in a genetic algorithm library, in an integrated, seamless, and portable manner. Key features are in PGAPack V1.0 include:             Callable from Fortran or C. Runs on uniprocessors, parallel computers, and workstation networks. Binary-, integer-, real-, and character-valued native data types. Full extensibility to support custom operators and new data types. Easy-to-use interface for novice and application users. Multiple levels of access for expert users. Parameterized population replacement. Multiple crossover, mutation, and selection operators. Easy integration of hill-climbing heuristics. Extensive debugging facilities. Large set of example problems. Detailed users guide.  PIPE   Web site: www.idsia.ch/~rafal/research.html  FTP site: ftp.idsia.ch/pub/rafal Probabilistic Incremental Program Evolution (PIPE) is a novel technique for automatic program synthesis. The software is written in C. It  is easy to install (comes with an automatic installation tool).  is easy to use: setting up PIPE_V1.0 for different problems requires a minimal amount of programming. User-written, application- independent program parts can easily be reused.  is efficient: PIPE_V1.0 has been tuned to speed up performance.  is portable: comes with source code (optimized for SunOS 5.5.1).  is extensively documented(!) and contains three example applications.  supports statistical evaluations: it facilitates running multiple experiments and collecting results in output files.  includes testing tool for testing generalization of evolved programs.  supports floating point and integer arithmetic.  has extensive output features.  For lil-gp users: Problems set up for lil-gp 1.0 can be easily ported to PIPE_v1.0. The 4.1 EC class/code libraries 30   Linux AI & Alife HOWTO testing tool can also be used to process programs evolved by lil-gp 1.0.  Sugal   Web site: www.trajan-software.demon.co.uk/sugal.htm Sugal [soo-gall] is the SUnderland Genetic ALgorithm system. The aim of Sugal is to support research and implementation in Genetic Algorithms on a common software platform. As such, Sugal supports a large number of variants of Genetic Algorithms, and has extensive features to support customization and extension.  4.2 EC software kits/applications These are various applications, software kits, etc. meant for research in the field of evolutionary computing. Their ease of use will vary, as they were designed to meet some particular research interest more than as an easy to use commercial package.  ADATE   Web site: www-ia.hiof.no/~rolando/adate_intro.html  ADATE (Automatic Design of Algorithms Through Evolution) is a system for automatic programming i.e., inductive inference of algorithms, which may be the best way to develop artificial and general intelligence.  The ADATE system can automatically generate non-trivial and novel algorithms. Algorithms are generated through large scale combinatorial search that employs sophisticated program transformations and heuristics. The ADATE system is particularly good at synthesizing symbolic, functional programs and has several unique qualities.  4.2 EC software kits/applications  31   Linux AI & Alife HOWTO esep & xesep   Web site(esep): www.iit.edu/~linjinl/esep.html  Web site(xesep): www.iit.edu/~linjinl/xesep.html  This is a new scheduler, called Evolution Scheduler, based on Genetic Algorithms and Evolutionary Programming. It lives with original Linux priority scheduler.This means you don't have to reboot to change the scheduling policy. You may simply use the manager program esep to switch between them at any time, and esep itself is an all-in-one for scheduling status, commands, and administration. We didn't intend to remove the original priority scheduler; instead, at least, esep provides you with another choice to use a more intelligent scheduler, which carries out natural competition in an easy and effective way.  Xesep is a graphical user interface to the esep (Evolution Scheduling and Evolving Processes). It's intended to show users how to start, play, and feel the Evolution Scheduling and Evolving Processes, including sub-programs to display system status, evolving process status, queue status, and evolution scheduling status periodically in as small as one mini-second.  Corewar VM   Web site: www.jedi.claranet.fr/  This is a virtual machine written in Java (so it is a virtual machine for another virtual machine !) for a Corewar game.  FSM-Evolver   Web site (???): pages.prodigy.net/czarneckid  A Java (jdk-v1.0.2+) code library that is used to evolve finite state machines. The problem included in the package is the Artificial Ant problem. You should be able to compile the .java files and then run: java ArtificialAnt.  GPsys  4.2 EC software kits/applications  32   Linux AI & Alife HOWTO  Web site: www.cs.ucl.ac.uk/staff/A.Qureshi/gpsys.html  GPsys (pronounced gipsys) is a Java (requires Java 1.1 or later) based Genetic Programming system developed by Adil Qureshi. The software includes documentation, source and executables.  Feature Summary:  Steady State engine  ADF support  Strongly Typed 1. supports generic functions and terminals 2. has many built-in primitives 3. includes indexed memory  Save/Load feature 1. can save/load current generation to/from a file 2. data stored in GZIP compression format to minimise disk requirements 3. uses serialisable objects for efficiency  Fully Documented  Example Problems 1. Lawnmower (including GUI viewer) 2. Symbolic Regression Totally Parameterised Fully Object Oriented and Extensible High Performance Memory Efficient       NextPreviousContentsNextPreviousContents  5. Alife & Complex Systems Alife takes yet another approach to exploring the mysteries of intelligence. It has many aspects similar to EC and Connectionism, but takes these ideas and gives them a meta-level twist. Alife emphasizes the development of intelligence through emergent behavior of complex adaptive systems. Alife stresses the social or group based aspects of intelligence. It seeks to understand life and survival. By studying the behaviors of groups of 'beings' Alife seeks to discover the way intelligence or higher order activity emerges from seemingly simple individuals. Cellular Automata and Conway's Game of Life are probably the most 5. Alife & Complex Systems 33   Linux AI & Alife HOWTO commonly known applications of this field. Complex Systems (abbreviated CS) are very similar to alife in the way the are approached, just more general in definition (ie. alife is a type of complex system). Usually complex system software takes the form of a simulator.  5.1 Alife & CS class/code libraries These are libraries of code or classes for use in programming within the artificial life field. They are not meant as stand alone applications, but rather as tools for building your own applications.  CASE   Web site: www.iu.hioslo.no/~cell/  FTP site: ftp.iu.hioslo.no/pub/ CASE (Cellular Automaton Simulation Environment) is a C++ toolkit for visualizing discrete models in two dimensions: so-called cellular automata. The aim of this project is to create an integrated framework for creating generalized cellular automata using the best, standardized technology of the day.  John von Neumann Universal Constructor   Web site: alife.santafe.edu/alife/software/jvn.html  FTP site: alife.santafe.edu/pub/SOFTWARE/jvn/  The universal constructor of John von Neumann is an extension of the logical concept of universal computing machine. In the cellular environment proposed by von Neumann both computing and constructive universality can be achieved. Von Neumann proved that in his cellular lattice both a Turing machine and a machine capable of producing any other cell assembly, when fed with a suitable program, can be embedded. He called the latter machine a ''universal constructor'' and showed that, when provided with a program containing its own description, this is capable of self-reproducing.  5.1 Alife & CS class/code libraries  34   Linux AI & Alife HOWTO Swarm   Web site: www.santafe.edu/projects/swarm  FTP site: ftp.santafe.edu/pub/swarm  The swarm Alife simulation kit. Swarm is a simulation environment which facilitates development and experimentation with simulations involving a large number of agents behaving and interacting within a dynamic environment. It consists of a collection of classes and libraries written in Objective-C and allows great flexibility in creating simulations and analyzing their results. It comes with three demos and good documentation.  Swarm 1.0 is out. It requires libtclobjc and BLT 2.1 (both available at the swarm site).  5.2 Alife & CS software kits, applications, etc. These are various applications, software kits, etc. meant for research in the field of artificial life. Their ease of use will vary, as they were designed to meet some particular research interest more than as an easy to use commercial package.  BugsX   FTP site: ftp.de.uu.net/pub/research/ci/Alife/packages/bugsx/  Display and evolve biomorphs. It is a program which draws the biomorphs based on parametric plots of Fourier sine and cosine series and let's you play with them using the genetic algorithm.  The Cellular Automata Simulation System   Web site: www.cs.runet.edu/~dana/ca/cellular.html 5.2 Alife & CS software kits, applications, etc. 35   Linux AI & Alife HOWTO The system consists of a compiler for the Cellang cellular automata programming language, along with the corresponding documentation, viewer, and various tools. Cellang has been undergoing refinement for the last several years (1991-1995), with corresponding upgrades to the compiler. Postscript versions of the tutorial and language reference manual are available for those wanting more detailed information. The most important distinguishing features of Cellang, include support for:  any number of dimensions;  compile time specification of each dimensions size; cell neighborhoods of any size (though bounded at compile time) and shape;  positional and time dependent neighborhoods;  associating multiple values (fields), including arrays, with each cell;  associating a potentially unbounded number of mobile agents [ Agents are mobile entities based on a mechanism of the same name in the Creatures system, developed by Ian Stephenson (ian@ohm.york.ac.uk).] with each cell; and  local interactions only, since it is impossible to construct automata that contain any global control or references to global variables.  dblife & dblifelib   FTP site: ftp.cc.gatech.edu/ac121/linux/games/amusements/life/  dblife: Sources for a fancy Game of Life program for X11 (and curses). It is not meant to be incredibly fast (use xlife for that:-). But it IS meant to allow the easy editing and viewing of Life objects and has some powerful features. The related dblifelib package is a library of Life objects to use with the program.  dblifelib: This is a library of interesting Life objects, including oscillators, spaceships, puffers, and other weird things. The related dblife package contains a Life program which can read the objects in the Library.  Drone   Web site: pscs.physics.lsa.umich.edu/Software/Drone/  Drone is a tool for automatically running batch jobs of a simulation program. It allows sweeps over arbitrary sets of parameters, as well as multiple runs for each parameter set, with a separate random seed for each run. The runs may be executed either on a single computer or over the Internet on a set 5.2 Alife & CS software kits, applications, etc. 36   Linux AI & Alife HOWTO of remote hosts. Drone is written in Expect (an extension to the Tcl scripting language) and runs under Unix. It was originally designed for use with the Swarm agent-based simulation framework, but Drone can be used with any simulation program that reads parameters from the command line or from an input file.  EcoLab   Web site: parallel.acsu.unsw.edu.au/rks/ecolab.html  EcoLab is a system that implements an abstract ecology model. It is written as a set of Tcl/Tk commands so that the model parameters can easily be changed on the fly by means of editing a script. The model itself is written in C++.  Game Of Life (GOL)   Web site: www.arrakeen.demon.co.uk/downloads.html  FTP site: metalab.unc.edu/pub/Linux/science/ai/life  GOL is a simulator for conway's game of life (a simple cellular automata), and other simple rule sets. The emphasis here is on speed and scale, in other words you can setup large and fast simulations.  LEE   Web site: dollar.biz.uiowa.edu/~fil/LEE/  FTP site: dollar.biz.uiowa.edu/pub/fil/LEE/  LEE (Latent Energy Environments) is both an Alife model and a software tool to be used for simulations within the framework of that model. We hope that LEE will help understand a broad range of issues in theoretical, behavioral, and evolutionary biology. The LEE tool described here consists of approximately 7,000 lines of C code and runs in both Unix and Macintosh platforms.  5.2 Alife & CS software kits, applications, etc.  37   Linux AI & Alife HOWTO Net-Life & ZooLife   FTP site: ftp.coe.uga.edu/users/jae/alife/ *(netlife-2.0.tar.gz contains both Net-Life and ZooLife)  Net-Life is a simulation of artificial-life, with neural ""brains"" generated via slightly random techniques. Net-Life uses artificial neural nets and evolutionary algorithms to breed artificial organisms that are similar to single cell organisms. Net-life uses asexual reproduction of its fittest individuals with a chance of mutation after each round to eventually evolve successful life-forms.  ZooLife is a simulation of artificial-life. ZooLife uses probabilistic methods and evolutionary algorithms to breed artificial organisms that are similar to plant/animal zoo organisms. ZooLife uses asexual reproduction with a chance of mutation.  POSES++   Web site: www.tu-chemnitz.de/ftp-home/pub/Local/simulation/poses++/www/index.html The POSES++ software tool supports the development and simulation of models. Regarding the simulation technique models are suitable reproductions of real or planned systems for their simulative investigation.  In all industrial sectors or branches POSES++ can model and simulate any arbitrary system which is based on a discrete and discontinuous behaviour. Also continuous systems can mostly be handled like discrete systems e.g., by quantity discretion and batch processing.  Primordial Soup   Web site: alife.santafe.edu/alife/software/psoup.html  Primordial Soup is an artificial life program. Organisms in the form of computer software loops live in a shared memory space (the ""soup"") and self-reproduce. The organisms mutate and evolve, behaving in accordance with the principles of Darwinian evolution.  5.2 Alife & CS software kits, applications, etc.  38   Linux AI & Alife HOWTO The program may be started with one or more organisms seeding the soup. Alternatively, the system may be started ""sterile"", with no organisms present. Spontaneous generation of self-reproducing organisms has been observed after runs as short as 15 minutes.  Tierra       Web site: www.hip.atr.co.jp/~ray/tierra/tierra.html FTP site: alife.santafe.edu/pub/SOFTWARE/Tierra/ Alternate FTP site: ftp.cc.gatech.edu/ac121/linux/science/biology/  Tierra's written in the C programming language. This source code creates a virtual computer and its operating system, whose architecture has been designed in such a way that the executable machine codes are evolvable. This means that the machine code can be mutated (by flipping bits at random) or recombined (by swapping segments of code between algorithms), and the resulting code remains functional enough of the time for natural (or presumably artificial) selection to be able to improve the code over time.  TIN   FTP site: ftp.coe.uga.edu/users/jae/alife/  This program simulates primitive life-forms, equipped with some basic instincts and abilities, in a 2D environment consisting of cells. By mutation new generations can prove their success, and thus passing on ""good family values"".  The brain of a TIN can be seen as a collection of processes, each representing drives or impulses to behave a certain way, depending on the state/perception of the environment ( e.g. presence of food, walls, neighbors, scent traces) These behavior process currently are : eating, moving, mating, relaxing, tracing others, gathering food and killing. The process with the highest impulse value takes control, or in other words: the tin will act according to its most urgent need.  XLIFE  5.2 Alife & CS software kits, applications, etc.  39   Linux AI & Alife HOWTO  FTP site: ftp.cc.gatech.edu/ac121/linux/games/amusements/life/  This program will evolve patterns for John Horton Conway's game of Life. It will also handle general cellular automata with the orthogonal neighborhood and up to 8 states (it's possible to recompile for more states, but very expensive in memory). Transition rules and sample patterns are provided for the 8-state automaton of E. F. Codd, the Wireworld automaton, and a whole class of `Prisoner's Dilemma' games.  Xtoys   Web site: penguin.phy.bnl.gov/www/xtoys.html xtoys contains a set of cellular automata simulators for X windows. Programs included are:       xising --- a two dimensional Ising model simulator, xpotts --- the two dimensional Potts model, xautomalab --- a totalistic cellular automaton simulator, xsand --- for the Bak, Tang, Wiesenfeld sandpile model, xwaves --- demonstrates three different wave equations, schrodinger --- play with the Scrodinger equation in an adjustable potential.  NextPreviousContentsNextPreviousContents  6. Autonomous Agents Also known as intelligent software agents or just agents, this area of AI research deals with simple applications of small programs that aid the user in his/her work. They can be mobile (able to stop their execution on one machine and resume it on another) or static (live in one machine). They are usually specific to the task (and therefore fairly simple) and meant to help the user much as an assistant would. The most popular (ie. widely known) use of this type of application to date are the web robots that many of the indexing engines (eg. webcrawler) use.  AgentK 6. Autonomous Agents 40   Linux AI & Alife HOWTO  FTP site: ftp.csd.abdn.ac.uk/pub/wdavies/agentk  This package synthesizes two well-known agent paradigms: Agent-Oriented Programming, Shoham (1990), and the Knowledge Query & Manipulation Language, Finin (1993). The initial implementation of AOP, Agent-0, is a simple language for specifying agent behaviour. KQML provides a standard language for inter-agent communication. Our integration (which we have called Agent-K) demonstrates that Agent-0 and KQML are highly compatible. Agent-K provides the possibility of inter-operable (or open) software agents, that can communicate via KQML and which are programmed using the AOP approach.  Agent   FTP site: www.cpan.org/modules/by-category/23_Miscellaneous_Modules/Agent/  The Agent is a prototype for an Information Agent system. It is both platform and language independent, as it stores contained information in simple packed strings. It can be packed and shipped across any network with any format, as it freezes itself in its current state.  D'Agent (was AGENT TCL)   Web site: agent.cs.dartmouth.edu/software/agent2.0/  FTP site: ftp.cs.dartmouth.edu/pub/agents/  A transportable agent is a program that can migrate from machine to machine in a heterogeneous network. The program chooses when and where to migrate. It can suspend its execution at an arbitrary point, transport to another machine and resume execution on the new machine. For example, an agent carrying a mail message migrates first to a router and then to the recipient's mailbox. The agent can perform arbitrarily complex processing at each machine in order to ensure that the message reaches the intended recipient.  Aglets Workbench   Web site: www.trl.ibm.co.jp/aglets/ 6. Autonomous Agents 41   Linux AI & Alife HOWTO An aglet is a Java object that can move from one host on the Internet to another. That is, an aglet that executes on one host can suddenly halt execution, dispatch to a remote host, and resume execution there. When the aglet moves, it takes along its program code as well as its state (data). A built-in security mechanism makes it safe for a computer to host untrusted aglets. The Java Aglet API (J-AAPI) is a proposed public standard for interfacing aglets and their environment. J-AAPI contains methods for initializing an aglet, message handling, and dispatching, retracting, deactivating/activating, cloning, and disposing of the aglet. J-AAPI is simple, flexible, and stable. Application developers can write platform-independent aglets and expect them to run on any host that supports J-AAPI.  Ara   Web site: www.uni-kl.de/AG-Nehmer/Projekte/Ara/index_e.html  Ara is a platform for the portable and secure execution of mobile agents in heterogeneous networks. Mobile agents in this sense are programs with the ability to change their host machine during execution while preserving their internal state. This enables them to handle interactions locally which otherwise had to be performed remotely. Ara's specific aim in comparison to similar platforms is to provide full mobile agent functionality while retaining as much as possible of established programming models and languages.  JAFMAS   Web site: www.ececs.uc.edu/~abaker/JAFMAS  JAFMAS provides a framework to guide the coherent development of multiagent systems along with a set of classes for agent deployment in Java. The framework is intended to help beginning and expert developers structure their ideas into concrete agent applications. It directs development from a speech-act perspective and supports multicast and directed communication, KQML or other speech-act performatives and analysis of multiagent system coherency and consistency.  Only four of the provided Java classes must be extended for any application. Provided examples of the N-Queens and Supply Chain Integration use only 567 and 1276 lines of additional code respectively for implementation.  6. Autonomous Agents  42   Linux AI & Alife HOWTO JATLite   Web site: java.stanford.edu/java_agent/html/  JATLite is providing a set of java packages which makes easy to build multi-agent systems using Java. JATLite provides only light-weight, small set of packages so that the developers can handle all the packages with little efforts. For flexibility JATLite provides four different layers from abstract to Router implementation. A user can access any layer we are providing. Each layer has a different set of assumptions. The user can choose an appropriate layer according to the assumptions on the layer and user's application. The introduction page contains JATLite features and the set of assumptions for each layer.  Java(tm) Agent Template   Web site: cdr.stanford.edu/ABE/JavaAgent.html  The JAT provides a fully functional template, written entirely in the Java language, for constructing software agents which communicate peer-to-peer with a community of other agents distributed over the Internet. Although portions of the code which define each agent are portable, JAT agents are not migratory but rather have a static existence on a single host. This behavior is in contrast to many other ""agent"" technologies. (However, using the Java RMI, JAT agents could dynamically migrate to a foreign host via an agent resident on that host). Currently, all agent messages use KQML as a top-level protocol or message wrapper. The JAT includes functionality for dynamically exchanging ""Resources"", which can include Java classes (e.g. new languages and interpreters, remote services, etc.), data files and information inlined into the KQML messages.  Java-To-Go   Web site: ptolemy.eecs.berkeley.edu/dgm/javatools/java-to-go/  Java-To-Go is an experimental infrastructure that assists in the development and experimentation of mobile agents and agent-based applications for itinerative computing (itinerative computing: the set of applications that requires site-to-site computations. The main emphasis here is on a easy-to-setup environment that promotes quick experimentation on mobile agents.  6. Autonomous Agents  43   Linux AI & Alife HOWTO Kafka   Web site: www.fujitsu.co.jp/hypertext/free/kafka/  Kafka is yet another agent library designed for constructing multi-agent based distributed applications. Kafka is a flexible, extendable, and easy-to-use java class library for programmers who are familiar with distributed programming. It is based on Java's RMI and has the following added features:  Runtime Reflection: Agents can modify their behaviour (program codes) at runtime. The behaviour of the agent is represented by an abstract class Action. It is useful for remote maintenance or installation services.  Remote Evaluation: Agents can receive and evaluate program codes (classes) with or without the serialized object. Remote evaluation is a fundamental function of a mobile agent and is thought to be a push model of service delivery.  Distributed Name Service: Agents have any number of logical names that don't contain the host name. These names can be managed by the distributed directories.  Customizable security policy: a very flexible, customizable, 3-layered security model is implemented in Kafka.  100% Java and RMI compatible: Kafka is written completely in Java. Agent is a Java RMI server object itself. So, agents can directly communicate with other RMI objects.  Khepera Simulator   Web site: diwww.epfl.ch/lami/team/michel/khep-sim/  Khepera Simulator is a public domain software package written by Olivier MICHEL during the preparation of his Ph.D. thesis, at the Laboratoire I3S, URA 1376 of CNRS and University of Nice-Sophia Antipolis, France. It allows to write your own controller for the mobile robot Khepera using C or C++ languages, to test them in a simulated environment and features a nice colorful X11 graphical interface. Moreover, if you own a Khepera robot, it can drive the real robot using the same control algorithm. It is mainly oriented toward to researchers studying autonomous agents.  Mole   Web site: www.informatik.uni-stuttgart.de/ipvr/vs/projekte/mole.html 6. Autonomous Agents 44   Linux AI & Alife HOWTO Mole is an agent system supporting mobile agents programmed in Java. Mole's agents consist of a cluster of objects, which have no references to the outside, and as a whole work on tasks given by the user or another agent. They have the ability to roam a network of ""locations"" autonomously. These ""locations"" are an abstraction of real, existing nodes in the underlying network. They can use location-specific resources by communicating with dedicated agents representing these services. Agents are able to use services provided by other agents and to provide services as well.  Odyssey   Web site: www.genmagic.com/agents/  Odyssey is General Magic's initial implementation of mobile agents in 100% pure Java. The Odyssey class libraries enable you to develop your own mobile agent applications. Use mobile agents to access data, make decisions and notify users. Your agent-enabled applications may also take full advantage of the Java platform and use other third party libraries, for example, to access remote CORBA objects or to access relational databases using JDBC. To see how it's done, take a look at the sample applications included as part of the Odyssey download.  Penguin!   FTP site: www.perl.org/CPAN/modules/by-category/23_Miscellaneous_Modules/Penguin/FSG/  Penguin is a Perl 5 module. It provides you with a set of functions which allow you to:  send encrypted, digitally signed Perl code to a remote machine to be executed.  receive code and, depending on who signed it, execute it in an arbitrarily secure, limited compartment. The combination of these functions enable direct Perl coding of algorithms to handle safe internet commerce, mobile information-gathering agents, ""live content"" web browser helper apps, distributed load-balanced computation, remote software update, distance machine administration, content-based information propagation, Internet-wide shared-data applications, network application builders, and so on.  SimRobot  6. Autonomous Agents  45   Linux AI & Alife HOWTO  Web site: www.informatik.uni-bremen.de/~simrobot/  FTP site: ftp.uni-bremen.de/pub/ZKW/INFORM/simrobot/  SimRobot is a program for simulation of sensor based robots in a 3D environment. It is written in C++, runs under UNIX and X11 and needs the graphics toolkit XView.              Simulation of robot kinematics Hierarchically built scene definition via a simple definition language Various sensors built in: camera, facette eye, distance measurement, light sensor, etc. Objects defined as polyeders Emitter abstractly defined; can be interpreted e.g. as light or sound Camera images computed according to the raytracing or Z-buffer algorithms known from computer graphics Specific sensor/motor software interface for communicating with the simulation Texture mapping onto the object surfaces: bitmaps in various formats Comprehensive visualization of the scene: wire frame w/o hidden lines, sensor and actor values Interactive as well as batch driven control of the agents and operation in the environment Collision detection Extendability with user defined object types Possible socket communication to e.g. the Khoros image processing software  TclRobots   FTP site: ftp.neosoft.com/pub/tcl/sorted/games/tclrobots-2.0/  Redhat Patch: ftp.coe.uga.edu/users/jae/ai/tclrobots-redhat.patch  RPMs (search at): http://rufus.w3.org/  TclRobots is a programming game, similar to 'Core War'. To play TclRobots, you must write a Tcl program that controls a robot. The robot's mission is to survive a battle with other robots. Two, three, or four robots compete during a battle, each running different programs (or possibly the same program in different robots.) Each robot is equipped with a scanner, cannon, drive mechanism. A single match continues until one robot is left running. Robots may compete individually, or combine in a team oriented battle. A tournament can be run with any number of robot programs, each robot playing every other in a round-robin fashion, one-on-one. A battle simulator is available to help debug robot programs.  The TclRobots program provides a physical environment, imposing certain game parameters to which all robots must adhere. TclRobots also provides a view on a battle, and a controlling user interface. TclRobots requirements: a wish interpreter built from Tcl 7.4 and Tk 4.0.  6. Autonomous Agents  46   Linux AI & Alife HOWTO The Tocoma Project   Web site: www.tacoma.cs.uit.no/  An agent is a process that may migrate through a computer network in order to satisfy requests made by clients. Agents are an attractive way to describe network-wide computations.  The TACOMA project focuses on operating system support for agents and how agents can be used to solve problems traditionally addressed by operating systems. We have implemented a series of prototype systems to support agents.  TACOMA Version 1.2 is based on UNIX and TCP. The system supports agents written in C, Tcl/Tk, Perl, Python, and Scheme (Elk). It is implemented in C. This TACOMA version has been in public domain since April 1996.  We are currently focusing on heterogeneity, fault-tolerance, security and management issues. Also, several TACOMA applications are under construction. We implemented StormCast 4.0, a wide-area network weather monitoring system accessible over the internet, using TACOMA and Java. We are now in the process of evaluating this application, and plan to build a new StormCast version to be completed by June 1997.  Virtual Secretary Project (ViSe)  (Tcl/Tk)  Web site: www.cs.uit.no/DOS/Virt_Sec  The motivation of the Virtual Secretary project is to construct user-model-based intelligent software agents, which could in most cases replace human for secretarial tasks, based on modern mobile computing and computer network. The project includes two different phases: the first phase (ViSe1) focuses on information filtering and process migration, its goal is to create a secure environment for software agents using the concept of user models; the second phase (ViSe2) concentrates on agents' intelligent and efficient cooperation in a distributed environment, its goal is to construct cooperative agents for achieving high intelligence. (Implemented in Tcl/TclX/Tix/Tk)  6. Autonomous Agents  47   Linux AI & Alife HOWTO VWORLD   Web site: zhar.net/gnu-linux/projects/vworld/  Vworld is a simulated environment for research with autonomous agents written in prolog. It is currently in something of an beta stage. It works well with SWI-prolog, but should work with Quitnus-prolog with only a few changes. It is being designed to serve as an educational tool for class projects dealing with prolog and autonomous agents. It comes with three demo worlds or environments, along with sample agents for them. There are two versions now. One written for SWI-prolog and one written for LPA-prolog. Documentation is roughly done (with a student/professor framework in mind), and a graphical interface is planned.  WebMate   Web site: www.cs.cmu.edu/~softagents/webmate/  WebMate is a personal agent for World-Wide Web browsing and searching. It accompanies you when you travel on the internet and provides you what you want. Features include:  Searching enhancement, including parallel search, searching keywords refinement using our relevant keywords extraction technology, relevant feedback, etc.  Browsing assistant, including learning your current interesting, recommending you new URLs according to your profile and selected resources, monitoring bookmarks of Netscape or IE, sending the current browsing page to your friends, etc.  Offline browsing, including downloading the following pages from the current page for offline browsing.  Filtering HTTP header, including recording http header and all the transactions between your browser and WWW servers, etc.  Checking the HTML page to find the errors or dead links, etc.  Programming in Java, independent of operating system, runing in multi-thread.  NextPreviousContents Next PreviousContents  6. Autonomous Agents  48   Linux AI & Alife HOWTO  7. Programming languages While any programming language can be used for artificial intelligence/life research, these are programming languages which are used extensively for, if not specifically made for, artificial intelligence programming.  Allegro CL   Web site: www.franz.com Franz Inc's free linux version of their lisp development environment. You can download it or they will mail you a CD free (you don't even have to pay for shipping). It is generally considered to be one of the better lisp platforms.  B-Prolog   Web site: www.sci.brooklyn.cuny.edu/~zhou/bprolog.html  Web site: www.cad.mse.kyutech.ac.jp/people/zhou/bprolog.html B-Prolog is a compact and complete CLP system that runs Prolog and CLP(FD) programs. An emulator-based system, B-Prolog has a performance comparable with SICStus-Prolog.  In addition to Edinburgh-style programs, B-Prolog accepts canonical-form programs that can be compiled into more compact and faster code than standard Prolog programs.  B-Prolog includes an interpreter and provides an interactive interface through which users can consult, list, compile, load, debug and run programs. The command editor facilitates reuse old commands.  B-Prolog provides a bi-directional interface with C and Java.> resources in C and Java such as Graphics and sockets, and also makes it possible for a Prolog program to be embadded in a C and Java applications.  B-Prolog supports most of the built-ins in ISO Prolog.  B-Prolog supports the delaying (co-routining) mechanism, which can be used to implement concurrency, test-and-generate search algorithms, and most importantly constraint propagation algorithms.  B-Prolog has an efficient constraint compiler for constraints> over finite-domains and Booleans.  B-Prolog supports the tabling mechanism, which has proven effective for applications including parsing, problem solving, theorem proving, and deductive databases.  7. Programming languages  49   Linux AI & Alife HOWTO ECoLisp   Web site (???): www.di.unipi.it/~attardi/software.html  ECoLisp (Embeddable Common Lisp) is an implementation of Common Lisp designed for being embeddable into C based applications. ECL uses standard C calling conventions for Lisp compiled functions, which allows C programs to easily call Lisp functions and viceversa. No foreign function interface is required: data can be exchanged between C and Lisp with no need for conversion. ECL is based on a Common Runtime Support (CRS) which provides basic facilities for memory managment, dynamic loading and dumping of binary images, support for multiple threads of execution. The CRS is built into a library that can be linked with the code of the application. ECL is modular: main modules are the program development tools (top level, debugger, trace, stepper), the compiler, and CLOS. A native implementation of CLOS is available in ECL: one can configure ECL with or without CLOS. A runtime version of ECL can be built with just the modules which are required by the application. The ECL compiler compiles from Lisp to C, and then invokes the GCC compiler to produce binaries.  Gdel   Web page: www.cs.bris.ac.uk/~bowers/goedel.html Gdel is a declarative, general-purpose programming language in the family of logic programming languages. It is a strongly typed language, the type system being based on many-sorted logic with parametric polymorphism. It has a module system. Gdel supports infinite precision integers, infinite precision rationals, and also floating-point numbers. It can solve constraints over finite domains of integers and also linear rational constraints. It supports processing of finite sets. It also has a flexible computation rule and a pruning operator which generalizes the commit of the concurrent logic programming languages. Considerable emphasis is placed on Gdel's meta- logical facilities which provide significant support for meta-programs that do analysis, transformation, compilation, verification, debugging, and so on.  LIFE   Web page: www.isg.sfu.ca/life LIFE (Logic, Inheritance, Functions, and Equations) is an experimental programming language proposing to integrate three orthogonal programming paradigms proven useful for symbolic 7. Programming languages 50   Linux AI & Alife HOWTO computation. From the programmer's standpoint, it may be perceived as a language taking after logic programming, functional programming, and object-oriented programming. From a formal perspective, it may be seen as an instance (or rather, a composition of three instances) of a Constraint Logic Programming scheme due to Hoehfeld and Smolka refining that of Jaffar and Lassez.  CLisp (Lisp)   FTP site: sunsite.unc.edu/pub/Linux/devel/lang/lisp/ CLISP is a Common Lisp implementation by Bruno Haible and Michael Stoll. It mostly supports the Lisp described by Common LISP: The Language (2nd edition) and the ANSI Common Lisp standard. CLISP includes an interpreter, a byte-compiler, a large subset of CLOS (Object-Oriented Lisp) , a foreign language interface and, for some machines, a screen editor.  The user interface language (English, German, French) is chosen at run time. Major packages that run in CLISP include CLX & Garnet. CLISP needs only 2 MB of memory.  CMU Common Lisp   Web page: www.mv.com/users/pw/lisp/index.html  FTP site: sunsite.unc.edu/pub/Linux/devel/lang/lisp/  CMU Common Lisp is a public domain ""industrial strength"" Common Lisp programming environment. Many of the X3j13 changes have been incorporated into CMU CL. Wherever possible, this has been done so as to transparently allow the use of either CLtL1 or proposed ANSI CL. Probably the new features most interesting to users are SETF functions, LOOP and the WITH-COMPILATION-UNIT macro.  GCL (Lisp)   FTP site: sunsite.unc.edu/pub/Linux/devel/lang/lisp/ 7. Programming languages 51   Linux AI & Alife HOWTO GNU Common Lisp (GCL) has a compiler and interpreter for Common Lisp. It used to be known as Kyoto Common Lisp. It is very portable and extremely efficient on a wide class of applications. It compares favorably in performance with commercial Lisps on several large theorem-prover and symbolic algebra systems. It supports the CLtL1 specification but is moving towards the proposed ANSI definition. GCL compiles to C and then uses the native optimizing C compilers (e.g., GCC). A function with a fixed number of args and one value turns into a C function of the same number of args, returning one value, so GCL is maximally efficient on such calls. It has a conservative garbage collector which allows great freedom for the C compiler to put Lisp values in arbitrary registers.  It has a source level Lisp debugger for interpreted code, with display of source code in an Emacs window. Its profiling tools (based on the C profiling tools) count function calls and the time spent in each function.  GNU Prolog   Web site: pauillac.inria.fr/~diaz/gnu-prolog/  Web site: www.gnu.org/software/prolog/prolog.html  GNU Prolog is a free Prolog compiler with constraint solving over finite domains developed by Daniel Diaz. GNU Prolog accepts Prolog+constraint programs and produces native binaries (like gcc does from a C source). The obtained executable is then stand-alone. The size of this executable can be quite small since GNU Prolog can avoid to link the code of most unused built-in predicates. The performances of GNU Prolog are very encouraging (comparable to commercial systems). Beside the native-code compilation, GNU Prolog offers a classical interactive interpreter (top-level) with a debugger. The Prolog part conforms to the ISO standard for Prolog with many extensions very useful in practice (global variables, OS interface, sockets,...). GNU Prolog also includes an efficient constraint solver over Finite Domains (FD). This opens contraint logic pogramming to the user combining the power of constraint programming to the declarativity of logic programming.  Mercury   Web page: www.cs.mu.oz.au/research/mercury/ 7. Programming languages 52   Linux AI & Alife HOWTO Mercury is a new, purely declarative logic programming language. Like Prolog and other existing logic programming languages, it is a very high-level language that allows programmers to concentrate on the problem rather than the low-level details such as memory management. Unlike Prolog, which is oriented towards exploratory programming, Mercury is designed for the construction of large, reliable, efficient software systems by teams of programmers. As a consequence, programming in Mercury has a different flavor than programming in Prolog.  Mozart   Web page: www.mozart-oz.org/  The Mozart system provides state-of-the-art support in two areas: open distributed computing and constraint-based inference. Mozart implements Oz, a concurrent object-oriented language with dataflow synchronization. Oz combines concurrent and distributed programming with logical constraint-based inference, making it a unique choice for developing multi-agent systems. Mozart is an ideal platform for both general-purpose distributed applications as well as for hard problems requiring sophisticated optimization and inferencing abilities. We have developed applications in scheduling and time-tabling, in placement and configuration, in natural language and knowledge representation, multi-agent systems and sophisticated collaborative tools.  BinProlog   FTP site(source): clement.info.umoncton.ca/BinProlog  BinProlog is a fast and compact Prolog compiler, based on the transformation of Prolog to binary clauses. The compilation technique is similar to the Continuation Passing Style transformation used in some ML implementations. BinProlog 5.00 is also probably the first Prolog system featuring dynamic recompilation of asserted predicates (a technique similar to the one used in some object oriented languages like SELF 4.0), and a very efficient segment preserving copying heap garbage collector.  BinProlog is no longer maintained. It has turned into a commercial app. :(  SWI Prolog  7. Programming languages  53   Linux AI & Alife HOWTO  Web page: www.swi.psy.uva.nl/projects/SWI-Prolog/  FTP site: swi.psy.uva.nl/pub/SWI-Prolog/  SWI is a free version of prolog in the Edinburgh Prolog family (thus making it very similar to Quintus and many other versions). With: a large library of built in predicates, a module system, garbage collection, a two-way interface with the C language, plus many other features. It is meant as a educational language, so it's compiled code isn't the fastest. Although it similarity to Quintus allows for easy porting.  XPCE is freely available in binary form for the Linux version of SWI-prolog. XPCE is an object oriented X-windows GUI development package/environment.  Kali Scheme   Web site: www.neci.nj.nec.com/PLS/Kali.html  Kali Scheme is a distributed implementation of Scheme that permits efficient transmission of higher-order objects such as closures and continuations. The integration of distributed communication facilities within a higher-order programming language engenders a number of new abstractions and paradigms for distributed computing. Among these are user-specified load-balancing and migration policies for threads, incrementally-linked distributed computations, agents, and parameterized client-server applications. Kali Scheme supports concurrency and communication using first-class procedures and continuations. It integrates procedures and continuations into a message-based distributed framework that allows any Scheme object (including code vectors) to be sent and received in a message.  RScheme   Web site: www.rosette.com/~donovan/rs/rscheme.html  FTP site: ftp.rosette.com/pub/rscheme  RScheme is an object-oriented, extended version of the Scheme dialect of Lisp. RScheme is freely redistributable, and offers reasonable performance despite being extraordinarily portable. RScheme can be compiled to C, and the C can then compiled with a normal C compiler to generate machine code. By default, however, RScheme compiles to bytecodes which are interpreted by a (runtime) virtual machine. This ensures that compilation is fast and keeps code size down. In general, we recommend using the (default) bytecode code generation system, and only compiling your time-critical code to machine code. This allows a nice adjustment of space/time tradeoffs. (see web 7. Programming languages 54   Linux AI & Alife HOWTO site for details)  Scheme 48   Web site: www.neci.nj.nec.com/homepages/kelsey/  Scheme 48 is a Scheme implementation based on a virtual machine architecture. Scheme 48 is designed to be straightforward, flexible, reliable, and fast. It should be easily portable to 32-bit byte-addressed machines that have POSIX and ANSI C support. In addition to the usual Scheme built-in procedures and a development environment, library software includes support for hygienic macros (as described in the Revised^4 Scheme report), multitasking, records, exception handling, hash tables, arrays, weak pointers, and FORMAT. Scheme 48 implements and exploits an experimental module system loosely derived from Standard ML and Scheme Xerox. The development environment supports interactive changes to modules and interfaces.  SCM (Scheme)   Web site: www-swiss.ai.mit.edu/~jaffer/SCM.html  FTP site: swiss-ftp.ai.mit.edu:/archive/scm/  SCM conforms to the Revised^4 Report on the Algorithmic Language Scheme and the IEEE P1178 specification. Scm is written in C. It uses the following utilities (all available at the ftp site).  SLIB (Standard Scheme Library) is a portable Scheme library which is intended to provide compatibility and utility functions for all standard Scheme implementations, including SCM, Chez, Elk, Gambit, MacScheme, MITScheme, scheme->C, Scheme48, T3.1, and VSCM, and is available as the file slib2c0.tar.gz. Written by Aubrey Jaffer.  JACAL is a symbolic math system written in Scheme, and is available as the file jacal1a7.tar.gz.  Interfaces to standard libraries including REGEX string regular expression matching and the CURSES screen management package.  Available add-on packages including an interactive debugger, database, X-window graphics, BGI graphics, Motif, and Open-Windows packages.  A compiler (HOBBIT, available separately) and dynamic linking of compiled modules.  7. Programming languages  55   Linux AI & Alife HOWTO Shift   Web site: www.path.berkeley.edu/shift/ Shift is a programming language for describing dynamic networks of hybrid automata. Such systems consist of components which can be created, interconnected and destroyed as the system evolves. Components exhibit hybrid behavior, consisting of continuous-time phases separated by discrete-event transitions. Components may evolve independently, or they may interact through their inputs, outputs and exported events. The interaction network itself may evolve.  Next PreviousContents  7. Programming languages  56"
GX003-50-3807114	"Design Criteria        This section discusses basic graphic design features such as titles, frames, X- and Y-axes scales, data point plotting, data line differentiation, and data line labeling as well as shading, horizontal and vertical grid lines, avoidance of data line overlap, information messages, and color. During the discussion, the basic decision rule on whether and how to use a feature is the one outlined in the Introduction; i.e., does the design support the message? If a feature does not provide information on the data, there is no advantage in using it and it may reduce clarity. Good graphs have a minimum of design and a maximum of data.     There are three sample graphs in this chapter. Figure 1 illustrates the zero-base and the use of break marks on the Y-axis. Figure 2 shows why ""period data"" (i.e., averages) should be plotted in the middle of the X-axis interval instead of at its end. Finally, Figure 3 illustrates the advantage of plotting daily rates instead of aggregates for monthly data.     Title    Every graph needs a clearly worded and concise title that answers three questions:         data illustrated ( what )        geographic area represented ( where )        and date of data ( when ).          In graphs, unlike as in data tables, the unit of measurement is not in the title because it is displayed on the axis, usually on the Y-axis.      There are variations and exceptions to these rules. In statistical maps, because there is no axis, the unit of measurement is in the title. Also, in publications devoted exclusively to a particular geographic locale, the title of the publication that appears on the same page is sufficient to answer the question "" where ."" For a full discussion of correct figure (and table) title construction in Energy Information Administration (EIA) products, see the   EIA Publishing Style Guide .     Frame Dimensions    Graphics tend towards the horizontal rather than the vertical. The primary reason is that ""horizontally stretched time series are more accessible to the eye.  [3]   A pleasing shape is called the ""golden rectangle"" of frame design in which the ratio of the longer axis to the shorter axis is 1.6 to 1. For example, if a graph's horizontal axis is 8 inches long, its vertical axis is 5 inches long. The graphical perception research has found ""a mild preference for proportions near the golden rectangle."" Given that the ""golden rectangle"" is a tendency and a preference, Tufte concludes:         ""If the nature of the data suggests the shape of the graphic, then follow that     suggestion.""      ""Otherwise, move toward horizontal graphics about fifty percent wider than     tall.""  [4]         Authors have the option to choose the frame dimensions that best illustrate the structure of the data. In this report, the ""golden rectangle"" is illustrated in Figures 1, 4, 5, and 9 -- all long time series graphs.     The DOS version of Harvard 3.0 had a ratio of 1.3 to 1. The Windows 3.0 version ratio is 1.7 to 1, close to the ""golden rectangle."" In Microsoft, the user can change the frames dimensions to whatever his or her purpose is.     Scale      In a time graph, the horizontal scale (X-axis) always represents time. Demarcations of time intervals must be proportional to the length of the time represented. There are exceptions, such as quarters of a year, where differences in the number of days are so small relative to the total that they are usually ignored. Monthly data need special treatment, which is described in the section on plotting for time series.     The vertical scale (Y-axis) displays the units of measurement, an average or a quantity, of the data. A time plot shows one measurement for each time unit on the horizontal axis. Thus, the vertical axis represents either:        A rate that has been determined as the total for one time period divided by the number     of smaller time units in the time period, or      A quantity measured at one specific instant during the time interval; for example,     stocks at the end of the month.       Logically, there is only one vertical scale because the purpose of the scale is to provide the background against which the data shown in the graph are judged. Multiple vertical scales may be used only if the scales are linearly related to one another; i.e., Btu and joules, gallons and liters, short tons and metric tons. Using multiple vertical scales of linearly unrelated variables implies an empirical relationship that may not exist. This subject is covered in detail in the chapters on line graphs and on scatter diagrams.     In recent years, many of EIA's statistical graphs presented data in both U.S. customary  and  metric units. Figure 19, ""A Histogram With U.S. Customary and Metric Scales,"" is an example of such a graph.     The logarithmic scale can be used when it is important to understand percent change or multiplicative factors, and the intended audience will not be puzzled by such a scale. If the Y-axis in a plot is logarithmic and the X-axis is not, the graph is called a semi-logarithmic graph. Software packages (including Harvard Graphics) generally compute logarithms to the base 10, but logarithms to other bases, such as base 2, can also be used. The logarithmic scale can be used to improve the resolution of highly skewed data.     Authors also have the option to insert the Y-axis scale on the right side of the graph frame. For example, in a line graph that presents a long time series, the most recent trends may be of greater interest than those from earlier periods. In this case, the author may decide that the reader would be helped by placing the Y-axis on the right. Figure 4 (Line Graphs Chapter) and Figure 18 (Vertical/Horizontal Bars, Pie/Dot Charts, 3-D Features Chapter) have the Y-axis scale on the right side to illustrate this point.     Scale Intervals and Tick Marks  In designating scale values, 5 or multiples of 5 are frequently used in preference to multiples such as 3, 6, 7, 9, 11, and so on. If the values of the vertical scale run into thousands, millions, or larger numbers, the zeros are commonly dropped and the scale label will read, for example, ""Thousand Short Tons.""     Scale divisions for both axes are indicated by major tick marks. Major tick marks usually are outside the axes and are labeled with the scale values. In certain scales, the use of minor tick marks is a good idea, particularly on a Y-axis with a wide scale range (i.e., 0 to 5,000 with intervals of 1,000). Minor tick marks, though not labeled, assist the user to read the data more quickly and accurately. The minor tick mark divisor should be a number that the reader can instantaneously divide into the major tick mark number. For example, if the scale interval is 1,000, the minor tick divisor can be 2, 4, 5, 10, or 20, with minor tick mark intervals of 500, 250, 200, 100, or 50, respectively. If the reader has to think about the division (i.e., 6 into 1,000), then the minor tick marks are counterproductive.     Scale Labeling   For clarity, in addition to the units designated on the scale, both the horizontal and vertical axes need explanatory labels for the variables represented. The unit of measure is customarily also specified in the explanatory label. The exception is for the horizontal axis. If the horizontal axis scale displays months or years, the horizontal axis label (i.e., ""Month"" or ""Year"") is not needed. The months and/or years on the scale are self-explanatory.     Months or year labels, such as Jan., Feb., Mar., or 90, 91, 92, are logically placed in the interval between the tick marks, but numerical values, such as 5, 10, 15, are placed on the tick marks. The rationale is that a period of time is called, for example, ""May,"" while only one point on a continuous scale is the precise amount or volume ""X."" Also, in long time series where there is not enough space between the tick marks to label every data point, the labels need not be on each interval. Instead, they may be placed in some regular pattern such as every 3 months (quarterly) or every 5 years. Single letters (i.e., ""J,"" ""M,"") to note months on the horizontal scale can be confusing. There are three ""Js,"" two ""Ms,"" and two ""As"" in a single 12-month period.     The Y-axis label normally is placed parallel to and reading inward to the Y-axis because the Y-axis label refers to the Y-axis scale, not the top frame line where the default setting is in some software packages (such as in Harvard Graphics). There are different conventions for placing this label to avoid having to read it sideways. Some graph producers put the units in the graph title; others put it on the top frame line. The generally accepted, and most widely used, practice is to place it parallel to the scale to which it refers. In situations where there are multiple graphs on a page and all the graphs have the same scale, the Y-axis label can be placed in the title subheading.     It is a good idea to avoid abbreviations and acronyms, but if they must be used because of space limitations, they need to conform to the EIA Standard ""Codes, Abbreviations, and Acronyms.""     The Zero-Base  As a rule, the vertical scale starts at zero. Otherwise, the relative importance of changes in levels is hard to assess and comparison is difficult, or an insignificant change can be made to look like a major change.     The graphs in Figure 1 (below) present the same data, once without a break mark (left figure) and once with a break mark (right figure). The figure on the left shows an invariant data line with a lot of ""white space"" below it. This is not the best design to communicate the message. The figure on the right communicates the message and eliminates the ""white space."" Break marks are a visual warning to readers that there is a discontinuity in the Y-axis scale and that the data should be read accordingly.     Figure 1. Examples of the Zero-Base and the Use of Break Marks                                      Break marks are also useful if the purpose of the graph is to display the fine details of differences between two lines. Then, that portion of the scales needs to be presented.     There are instances where the zero-base is not necessary on the vertical axis. For example, in line graphs showing relative quantities, the natural basis (such as 100 for percentage indices) often is drawn in the middle of the vertical axis. Figures 6 (""Line Graphs"" chapter) and 13 (""Measuring from the Baseline"" chapter) are examples of this. Also, the zero-base is irrelevant when a logarithmic scale is used. Finally, the Y-axis is neither broken in cumulative line graphs nor, with rare exceptions as described in the chapter on vertical/horizontal bars, pie/dot charts, and three-dimensional features, in bar charts. The reason is that the area depicted is proportional to the quantity displayed.       Plotting Data for Time Series      Midpoint or Endpoint  When data on the X-axis of a line graph are being plotted, the points are located either directly above the tick marks or above the space between the tick marks, depending on the type of data presented. Data points for averages over the time interval, such as ""average production per day"" or ""average price for the month"" are ""period"" data (rates). Period data also include all quantities that are tabulated as the ""total"" for a month. Period data are plotted in the middle of the time interval. Although the point is plotted in the middle of the interval, the connecting line extends over the entire interval. This means that the line connecting the first and second point plotted needs to be extended to the beginning for the first interval. Similarly, the extension for the last point is to the end of the interval. This refinement is usually not necessary as it does not affect perception, but is needed if the area between the lines is shaded. Data points measured at one point in time, such as ""stocks at end of month,"" are ""point"" data. These data are plotted at the specific time of measurement. Some graphics packages plot data (and division or value labels) on the tick marks by default rather than between tick marks, there are methods or options in most of these packages to override this default.     Tables 2 and 3 and Figures 2 and 3 illustrate the difference between ""period"" data and ""point"" data. Suppose production of a commodity on January 1 was 100 units and for the next 3 months, production was increased by 1 unit per day for each day. Thus, we have:     Table 2. Hypothetical Data With Increase of One Unit per Day                     Month       Days in Month       Midmonth Day       Range in Daily Production       Total for Month       Monthly Average Units Per Day               January       31       16.0       100-130       3,565       115.0               February       28       14.5       131-158       4,046       144.5               March       31       16.0       159-189       5,394       174.0               April       30       15.5       190-219       6,135       204.5               Total       120                                            In Figure 2, below, the points plotted at mid-month (series 1) correctly fall on the daily line. Points plotted at the end of the month (series 2) are 15 units below the line for 31-day months, 14.5 units below the line for a 28-day month; so, the line for average production per day is approximately 15 units too low.     Figure 2. Illustration of the Mid-Point Problem - Hypothetical Data         Now suppose that for April, instead of monthly data, we have weekly data. Assuming that April 1st is the start of the week, we have:     Table 3. April Weekly Data Derived from Table 2                    Month/Week       Mid-Weekday       Range in Daily Production       Total for Week       Weekly Average Units per Day               April 1-7       3.5       190-196       1351       193               April 8-14       10.5       197-203       1400       200               April 15-21       17.5       204-210       1449       207               April 22-28               211-217       1498       214                                                               Last 2 Days                                               April 29-30       ---       218-219       437       ---                                       6135                    If the weekly averages are plotted mid-week, they will continue to fall on the series 1 line. If they are plotted at the end of the week, they will fall on a line that is 3.5 units below the correct line, series 1. Now, if we were to observe only series 2, the dotted line rises sharply in the first week in April when we change from monthly to weekly data.     When data are plotted at two periodicities, such as weekly and monthly, it is always necessary to use mid-interval plots to avoid the appearance of a false change in level when the periodicity changes. It is always preferable to use mid-interval plots for period data but, in practice, the error introduced by using the default option at the beginning or end of the interval is hard to detect when the time gradients (interval size) are graphically small. Thus, when 12 or more periods are presented, and all periods are the same lenght, it is  not essential  to plot at mid-interval.     Shading    Time series plots are essentially rates, and each value plotted denotes the rate that applies during a time period. The use of shading below the line is somewhat misleading as it implies that the area shaded is proportional to the rate during the referenced time period. Examination of an irregular graph shows this is not true. For example, if a low value has higher values on either side, then the lines will ascend from each side of the plotted low point; so, the shaded area will be slightly too large.     There is also a problem in determining where the shading should start and end horizontally. For values plotted at the end of the time periods, the connecting line will start at the end of the first period. Thus, no shading need be used for the first period when the area below the line is shaded. Similarly, for mid-interval plots, only half (n-1) of the first period will be shaded unless the line is extended to the beginning of the interval so that the shaded area is proportional to the quantity depicted. A similar procedure is needed at the end of the series. This is a reasonable solution where shading is needed to accentuate features of the data. Figure 4 (Line Graphs section) is an example of this.     Shading also can be used to highlight the differences between two data series (for example, surplus or deficit), or the range of a variable (the standard error if the data are uncertain). Shading is not necessary for highlighting the frame of a graph or for background color in it. It is occasionally needed for a balanced effect if the rest of the page has a heavy load of printing, such as multiple captions in heavy type.     Unless there is a need for shading, the practice of just using a line to connect the data points is preferable. Yet, if shading is used, it should be used consistently and in the same color for each commodity in time line graphs throughout a publication. (Color and shading are discussed in more detail further into this section.)     Monthly Total Dips in February  The observant reader will notice that the intervals on the horizontal scale in Figure 2 differ, February being narrower than the other months. This refinement was necessary to ensure that the series 1 plotted points fell exactly on the hypothetical straight line representing the daily rate of production. In practice, months are usually plotted as if each is of equal length. This slight inaccuracy is not perceptible and does not affect the perception of the pattern of average production per day.     When total production per month is plotted, the 3-day difference between February and adjacent months creates a loss of 3 days' production, about one-tenth of the monthly total. The resulting 10-percent dip in February is perceptible and sends a false message that there has been a change in the prevailing pattern. These dips cannot be eliminated by changing the width of the interval for February. This false message is the reason for plotting monthly data as a daily rate, i.e., barrels per day produced.     Figure 3 illustrates this point. The total monthly generation graph on the left shows a dip in February. Yet, when the data are converted to daily rates in the graph on the right, the dip is eliminated. (The right side Y-axis in the right-hand graph expresses the data as a yearly rate.)                Figure 3.       U.S. Generation of Electricity, Month Totals (Left) and     Daily (and Yearly) Rates (Right), January-June 1993                                     The producers of line graphs on a time scale should keep in mind that what is being plotted is a  rate , such as a quantity per day or per year, where day and year are fixed constant increments of time. Quarterly intervals are not precisely equal, being 90 (91 in leap years), 91, 92, and 92 days, but the relative differences between lengths of quarters are small and the effect on production is usually not perceptible. This is not the case with months.     For many years, users of petroleum data have expressed all quantities produced or consumed as barrels per day. It is not customary to compute a daily rate for other fuels such as coal or electricity. Another solution, if plotting monthly data for these fuels, is to adjust the quantities to a 30-day month. The February total, thus, would be multiplied by 30/28 and the January, March, May, July, August, October, and December totals, by 30/31. The title or footnotes of the graph would state that monthly totals are adjusted to give the rate for a 30-day month.     Changing to either a daily rate or adjusting to a 30-day month is only necessary when plotting monthly period data. None of these difficulties occur with ""point"" data, such as stocks. These data measure the level at a specific point in time, usually the last day of the time period covered by other questions in the survey. They are correctly plotted at the point in time where they are measured.     Differentiating Lines      Line Patterns and Line Symbols  If there is more than one line in a graph, each line usually needs to be identified by a unique line pattern, unique line symbol, or color. For example, lines may be represented by a solid line, dashed line, dot-and-dash line, or some other pattern. Line symbols may be squares, circles, diamonds, triangles, etc. The symbols are placed where the data points are plotted, in the middle of the time period for ""period data"" and at the end of the interval for ""point data."" If symbols are used, the connecting lines should be solid. Line patterns and line symbols need not be combined. One or the other is sufficient to differentiate the data lines in a data series. See Figure 2 for an example of a data line with small squares and a dotted line with circles.     In cases where data lines are separated by a lot of space and do not come near each other, each line can have a solid (or some other) line pattern and a line label to identify the curve. This will reproduce clearly (when printed in black and white or when photocopied), not lead to confusion, and eliminate the need for a legend.     It is a service to the reader to use the same pattern (symbol or color) throughout a publication. If, for example, coal is always depicted as a solid line and crude oil is shown as a dotted line, the reader does not need to refer repeatedly to legends. Such consistency is desirable, if feasible, and not preempted for other reasons. As illustrated in the next chapter, line patterns are preferable to line symbols. Line symbols use more space than line patterns and are more distracting.     If a graph has too many lines, it may become confusing and lose its visual impact. It is not possible to state a hard-and-fast rule for the optimum number of lines in a graph as it depends on the amount of overlap. When overlap is not a problem, four lines, usually, is the optimal number that can be read clearly, but there can be graphs when more than four lines can be clearly read. Figure 18 (chapter on Vertical Bars, Horizontal Bars, Pie and Dot Charts, and 3-Dimensional Features) illustrates this. Figure 5 (Line Graphs chapter) also illustrates a convenient method to summarize information from multiple lines, presenting the range of all values with a line of primary interest (i.e., the mean or median) superimposed. The chapter on Measuring from the Baseline discusses other methods to eliminate overlap.     Real and Nominal Dollars  In some instances, where justifiable in economic theory and practice (i.e., refiner acquisition costs for crude oil), it is a good idea to present data (lines) in both real and nominal dollars. In the last several years, EIA publications have increasingly published financial data in both real and nominal dollars.     Grid Lines      Horizontal grid lines are often not necessary. When needed to guide the eye, they should be kept to a minimum, evenly spaced, and comparatively light in contrast to the data lines. If grid lines are not light, they may overpower or ""hide"" the data lines and, thus, obscure or distort the data presentation.     Vertical grid lines, however, are often helpful. The following are situations in which it is recommended that they be used:        To separate calendar years in a monthly data series to reinforce the perception of     seasonal patterns      To separate historical data from projections      To separate a change in the data series (i.e., the method of sampling was changed).      Internal Labeling and Messages      Line Labels and Legends    To facilitate interpretation, each line needs a short, simple, self-explanatory label. Labels can be located either directly next to the lines or listed in the legend with the line pattern, line symbol, or color. Direct labeling is much preferred to the use of legends. With legends, readers have to look back and forth between the data lines and the legend, while at the same time decoding the information, instead of just reading and analyzing the data. This annoys readers and decreases their comprehension of the graph. Legends also take up space that could be used to present more data. If a legend is used and the space is available, placing the legend inside the graph reduces the amount of searching the reader must do.     Placement    Placement of line labels depends on the amount of overlap. If the line graph is uncluttered, the line label or message can be placed parallel to or near the line or point which the label or message is describing. If the data lines are close together or overlap, but there is enough space within the   graph for line labels (as opposed to a line legend), then arrows need to be used to connect the line label (or message) to the data line. The design of the arrow is simple: a straight, narrow shaft with a small arrowhead. Arrows should not actually touch the curve nor should they cross each other.     Messages  If they do not clutter the graph, short informational messages (i.e., ""Iraq Invaded Kuwait"" in a graph on crude oil production or prices) are very useful and can be added to the graph. Messages may provide additional context to the data, emphasize a particular point, and/or explain ambiguous or possibly hidden features in the graph.     Messages are particularly useful in explaining the data (and even the graph format) in graphs that may be easily comprehensible by some readers because they are familiar with the subject matter, but are not as well understood by other readers because they are not familiar with the subject matter. Authors are advised to keep in mind this distinction when constructing graphs.     In some graphs, informational messages are distinguished from the line labels by enclosing them in a box. This, though, is not a recommended practice. Boxes take up space and are essentially ornamental.     Color      Color statistical graphs have become technologically, economically, and aesthetically more feasible to produce and, hence, more commonplace in publications because of advances in hardware (such as increased resolution in laser and other printers) and software (palettes with greater selection of colors). In addition, when graphs are presented on the World Wide Web, the use of colors is a free feature, where the main restraint would be to not use too many colors, so that an image will load fairly quickly into a browser window. While the choice of when to use colors and what colors to use is basically artwork, there are some general rules to observe.     The purposes of color are to differentiate and to encourage comparison. In this sense, color is another variable. These purposes can be achieved by using colors in consistent thematic patterns throughout each product. For example, to represent a low to high scale ina statistical map or grouped bar chart, use light to dark hues of the same color. This will be easy to remember and give readers ""a sense of natural visual sequence.""   [5]         In  Envisioning Information , Tufte also suggests that authors not use pure, bright, or very strong colors ""unrelieved over large areas adjacent to each other."" This creates ""noise"" that distorts, blurs, or covers the visual impact of lines, bars, strata, map contours, etc., which represent the data. Pure, bright, and very strong colors should be used sparingly in statistical graphs. Further, using light bright colors against a white background produces a ""1 + 1 = 3 effect,"" a form of noise where the colors are at ""visual war"" with the information in the data display.   [6]   Use softer colors for the background and display.     The following are additional guidelines on the use of color advocated by Tufte:        Use only colors that enhance the data presentation. Do not use color for graph frames,     large unnecessary symbols, or titles, for example.      Be careful in the selection of colors. The wrong colors can distort the data and lead to     incorrect perceptions. For example, big differences in color should not be used to     represent small quantitative changes. The color scale should be proportional to the data     scale.    [7]       Experiment with color combinations. Use colors or hues that are distinguishable without     one color overpowering the other(s). Tufte recommends using the ""smallest effective     difference."" The ""visual move"" from one hue (or color) to the next is as     small as possible but is still distinctive and clear. This allows for more differences     and, hence, data to be displayed.    [8]        Test the colors you select throughout the composing, printing, and publishing process.     The color you select from the software color palette that you see on your computer screen     may not come out as the same color on a color printer, or the color in the paper-printed     report, or the color in a Web browser.      Ask coworkers if they can distinguish between the colors or hues that are being used in     the construction of, for example, a grouped bar chart or statistical map. When in doubt,     use soft colors.      Visual perception of color varies among readers. Some are unable to distinguish among     colors in different parts of the spectrum. For them, too much reliance on color is a     disacvantage.      Hardware and software should provide sufficient resolution so that the edges of color     components in a graph are not fuzzy (called the ""jaggies"").      Assume that readers will photocopy graphs and that reproduction could cause problems.     Thus, it is advisable to photocopy graphs you produce that will be published to see if     photocopying produces a drop-off in quality.      Click    here  to return to front of report.  Click  here  to return to Home Page."
GX004-28-6758231	Frequency Analysis of Data on Telerobotic Tasks  Spectral signatures could lend insight into design and performance.   NASAUs Jet Propulsion Laboratory, Pasadena, California   Data on forces and torques measured in experiments with remote manipulators can be processed into spectral signatures via a special frequency-analysis procedure. The spectral signatures complement other measures used to evaluate performances of telerobotic systems and human operators. In particular, spectral signatures can contribute to verification of some assumptions made in designing manipulator arms and control subsystems and can be used as feedback by operators engaged in real-time monitoring of telerobotic tasks. Spectral signatures also provide useful indications of flows of power between manipulators and their environments: this is important in that it is highly desir able to minimize both task-execution times and the magnitudes of forces and torques employed in tasks, thus minimizing such flows of power. The telerobotic experiments involve various tasks and subtasks in which the operator commands the remote manipulator to move a peg to a marked location on a board, tap the peg on the mark, move the peg to a hole in the board, insert the peg in the hole, release the peg, regrasp and extract the peg, return to and tap the mark with the peg, then move the peg to a final position. The start-and- stop motions in these subtasks give rise to sudden changes in torques and forces on the manipulator, so that traditional spectral analysis is almost useless in analyzing the resulting force and torque data. In the special frequency-analysis procedure, which was developed specifically for this application, segmentation algorithms are used to extract data on homogeneous subtasks from different experiments. By so doing, the algorithms synthesize artificial streams of data, such that all data in a given stream pertain to a given subtaskQ for example, move, grasp, or insert. The segmentation algorithms include a Viterbi-decoder algorithm based on an underlying hidden Markov model of the peg-in-hole task and its subtasks. A hidden Markov model is a mathematical model of a Markov process that cannot be observed directly. Each state of the Markov model is associated with a probability density that manifests itself in the experimental observations. In most cases, the probability densities overlap, and the problem of recognizing the sequence of states that generates a given sequence of measurement data involves the combination of information on probabilities of transitions between states with state-observation densities and with the specific sequence of observations. The figure presents an example of segmentation of force data by use of the hidden Markov model plus intervention by the operator to establish the intermediate RtapS states that correspond to the tapping subtasks. Following segmentation, the data that pertain to the taps are eliminated, and the remaining data on each subtask are filtered by use of a Hanning window over time. Filtering is necessary to compress the data at the tails of the subtasks an d reduce their effects on the spectra. The data on homogeneous tasks are then combined into a single file, and this file is processed with a fast Fourier trans form to obtain the desired spectral signature (see Figure 2).    More details can be found in:   P. Fiorini and A. Giancaspro, RA Procedure for the Frequency Analysis of Telerobotics Tasks Data,S IEEE/RSJ International Conference on Intelligent Robots & Systems (IROSU92), Raleigh, NC, July 7-10, 1992, pp. 873-880.     Point of Contact:  Antonio Giancaspro,  Paolo Fiorini  Mail Stop 198-219  Jet Propulsion Laboratory  4800 Oak Grove Drive  Pasadena, CA 91109  818-354-9061   Paolo.Fiorini@jpl.nasa.gov                                Telerobotics Program page     Please  email the site webmaster   with any comments, criticisms or corrections for this page.  Maintained by:  Dave Lavery  Last updated: May 10, 1996
GX027-69-16591888	"Security and Privacy Notice                                                                                        Research Initiatives: Artificial-Intelligence-Based Nonlinear Controllers for Fossil Power Plants and the Chemical Industry                                                                                                                        AI-Based Nonlinear Controllers for Fossil Power Plants and the Chemical Industry                                                                                                                 Overview    The overall objective of this research is to explore and demonstrate the utility of artificial intelligence (AI) technologies for developing effective controllers for dynamical industrial processes.  Research results obtained  with AI technologies, in the form of artificial neural networks (ANNs) and fuzzy logic, have shown the potential for creating nonlinear multivariable controllers for complex industrial processes, without explicitly representing  the underlying physical principles that govern process behavior.  In addition, this approach allows for ease in handling state-space constraints, and it does not require linear approximations of the system performance, a requirement of most classical approaches that often distorts and fails to truly represent the real problem.  The methods being investigated are expected to have very broad applicability because they permit the development of computer-based controllers even in cases where the process is poorly understood or far too complex to be modeled from first principles.    The research initiative includes the development of generalized, inductive, input/output data-driven  algorithms for creating models of nonlinear dynamical processes, the development of AI-based suboptimal and optimal control algorithms, and the comparison of these algorithms with standard PI and PID controllers, as well as with classical optimal control methods to establish their efficacy.  To date, ANNs have served as the  nonlinear models of the process, and either ANNs or fuzzy logic methods have been used for the nonlinear process controllers.    High-fidelity ANN models of the process to be controlled are developed first by ""training"" them to accurately map process inputs onto process outputs on the basis of measured process data.  With successful training, the ANN process model can accurately reproduce the measured process dynamics, even with minimal understanding of first principles.  That model is then used for developing and tuning the AI-based controller--again by ""training"" the controller to manipulate the model inputs in a way that yields the desired model outputs.  For real-world control, after the controller has been properly tuned, the process model is replaced by the actual process.  The schematic below illustrates one of many possible neural network representations for closed-loop control of discrete-time dynamical processes.  This representation has two basic components: a neutral network controller and a neural network model of the process.  Given the desired process setpoint of target  T  and an arbitrarily selected initial process state  y 1 , the controller provides a sequence of control actions  u 1 , u 2 , ... , u k , ... , u K-1  that drives the dynamical system, represented by an ANN model of the process from  y 1  to  y K = T .  This representation offers the advantage that training of the neural controller requires only the provision of the desired process setpoint  T , rather than the (generally unknown) control law  u k .  Training of the neural controller is achieved by minimizing the difference between  T  and  y K .    This general approach is being tested for industrial-scale plant control in an application that has the goal of limiting emissions from operating fossil power plants.  The  RA Division has two ongoing projects in this area, for the purpose of developing controllers that improve coal power plant efficiency while reducing environmental impacts, such as the emissions of oxides of nitrogen (NO x ) and carbon monoxide (CO).  The objective of the first project, funded internally through ANL's Laboratory Directed Research and Development Program, was to reduce NO x  emissions through improved control of furnace tilt and air injection.  Initial proof-of-principle results 1-3  are encouraging regarding the feasibility of using ANNs following the representation shown above for both modeling the dynamic formation of NO x  emissions in the furnace and for closed-loop control of NO x  emissions with the routine industrial control variables.  Further research on this project will develop control systems as illustrated in the schematic,  except that the control actions will be  provided by fuzzy-logic-based controllers instead of neural controllers.  This will allow intercomparisons of the efficacy of the two AI control technologies.  Further efforts will extend the scope of the research to  development of  optimal  control algorithms, where the controllers will be required to reach the given process setpoint while at the same time optimizing a prespecified objective function related to operating cost.    In a second project, conducted in collaboration with a small consulting company and funded by a U.S. Department of Energy grant under the Small Business Technology Transfer Program (Phase I), the goal is to optimize the injection of natural gas in coal-fired power plants. 4   When injected into the upper region of the plant's furnace, natural gas will ""re-burn"" the NO x   in the flue gas, converting it to atmospheric nitrogen and thereby reducing NO x  emissions.  Natural gas is more expensive than coal; thus, the objective of this project is to develop controllers that achieve NO x  reduction while at the same time optimizing the gas injection amplitude and spatial distribution  to reduce average gas consumption and limit CO generation.    References       J. Reifman, J. E. Vitela, E. E. Feldman, and T. Y. C. Wei, ""Recurrent Neural Networks for NO x  Prediction in Fossil Plants,""  Proceedings of the Computer Simulation Multiconference , New Orleans, Louisiana, April 8-11, 1996.    J. Reifman and E. E. Feldman, ""Identification and Control of NO x  Emissions in Fossil Plants,""  Proceedings of the Computer Simulation Multiconference , Atlanta, Georgia, April 6-10, 1997.    J. Reifman and E. E. Feldman, ""Identification and Control of NO x  Emissions Using Neural Networks,"" accepted for publication in the journal of the  Air & Waste Management Association , May 1997.    J. Reifman, E. E. Feldman, T. Y. C. Wei, R. W. Glickert, J. M. Pratapas, and K. E. Wanninger, ""An Intelligent Emissions Controller for Fuel Lean Gas Reburn,"" to be submitted to the POWER-GEN International Conference, Orlando, Florida, December 9-11, 1998.       Links      Publications                                                         New RAE Division                Reactor Analysis Division at Argonne National Laboratory - http://web.ra.anl.gov/"
GX039-62-1229545	THE USE OF ARTIFICIAL INTELLIGENCE TO IMPROVE THE NUMERICAL OPTIMIZATION OF COMPLEX ENGINEERING DESIGNS. Mark A. Schwabacher, Rutgers University, New Brunswick, NJ, USA. Current address: NIST, Building 304, Room 12, Gaithersburg, MD, 20899, (301) 975-4574,  mark.schwabacher@nist.gov       Gradient-based numerical optimization of complex engineering designs promises to produce better designs rapidly. However, such methods generally assume that the objective function and constraint functions are continuous, smooth, and defined everywhere. Unfortunately, realistic simulators tend to violate these assumptions. We present several artificial intelligence-based techniques for improving the numerical optimization of complex engineering designs in the presence of such pathologies in the simulators. We have tested the resulting system in several realistic engineering domains, and have found that using our techniques can greatly decrease the cost of design space search, and can also increase the quality of the resulting designs.
GX019-16-10452782	"Disclaimer/Privacy                                                                         Holmes                     Sequence homology search tools    on the world wide web         Ian Holmes    Berkeley Drosophila Genome Project, Berkeley, CA    email: ihh@fruitfly.org         Introduction     Sequence homology search tools may be divided into four groups, illustrated in Figure 1.         1.  Pairwise searches  ( e.g. , BLAST, Smith-Waterman) These programs compare a single query sequence against each sequence in a (large) database and report significant similarities         2.  Profile searches  ( e.g. , HMMER) These programs, if supplied with several examples of a family of sequences, will attempt to construct a profile of this family, then search a database for sequences that fit the profile         3.  Automated searches  ( e.g. , PSI-BLAST) These programs first seek out close relatives of a single query sequence, then use these close relatives to build a profile. In other words, they combine the tasks performed by pairwise and profile search tools         4.  Protein family databases  ( e.g. , PFAM, PROSITE, BLOCKS) Here, a single query sequence is compared to entries in a database of profiles, each representing a distinct protein family. This approach shares many of the benefits of profile searches, without the duplicated effort of finding members of an already well-characterised family         Each method has its advantages. Automated tools take a lot of the pain out of homology searching, but may return spurious results. Protein family databases bring greater reliability at little extra cost but may miss some homologies. Profile searches are best for investigators who have the time to carefully curate their query alignment. Pairwise searches arguably remain the most transparent (and fastest) of methods.    This article briefly discusses the main examples of each type of search program, outlining relevant issues and giving links to websites where available. Slightly more detail is given for more recent tools such as PSI-BLAST, where commentary is less readily available. Unless otherwise indicated, all programs may be used to search for both DNA-to-DNA and protein-to-protein homologies. At the end of the article, a short section outlining promising developments in automated profiling is included.    This is not intended to be a complete tutorial on sequence homology searching; for that, see  e.g. , [1] for an introduction or [2] for a more technical treatment. It should also be noted that links can go out of date quickly; often a judiciously worded web search is the easiest way to find the service required.    Note regarding algorithms and implementations . Often, the methodology that a program uses (the underlying algorithm) is shared by more than one program. Recommending one among several implementations of an algorithm can be controversial but it has been attempted here in the interests of clarity.             Figure 1 . The four groups of homology search tools.    Pairwise searches     In a pairwise search, a query sequence is compared to a database sequence, yielding a score that indicates the likelihood of homology. This comparison is repeated for every sequence in the database and high-scoring hits are reported. This basic procedure is shared by all pairwise searches. The various tools available differ most noticeably in speed and sensitivity ( e.g. , whether, and how often, insertions or deletions [""indels""] in one of the sequences will cause the comparison to fail).         A default scoring scheme is usually provided; this may be overridden by the ""power user"". Most often this scoring scheme consists of a substitution matrix, which specifies the likelihoods of all possible point mutations. Other aspects of the scoring scheme can be a gap penalty, specifying the cost of deletions, and a score threshold for reporting hits.           The fastest and most popular pairwise search tool is BLAST. The most sensitive is the Smith-Waterman algorithm, of which the most common implementation is SSEARCH.         BLAST.  The BLAST program works on the principle that regions of homology are likely to contain strongly conserved, indel-resistant segments. These areas of strong conservation show up as ungapped blocks in an alignment. Recent versions [3] are also capable of producing gapped alignments, but they still build these gapped alignments up from ungapped segments. This means that BLAST is most likely to fail to detect a homologous sequence where indels are scattered liberally and regularly throughout: in other words, highly divergent sequences may be missed.    Searching for ungapped matches is much faster than searching for gapped matches. BLAST speeds things up even more by looking initially for matches to individual words in the query sequence. This makes BLAST by far the quickest search program on the web today.    Public interfaces to BLAST can be found on the NCBI and EBI websites:    http://www.ncbi.nlm.nih.gov/BLAST/    http://www2.ebi.ac.uk/    A list of further BLAST servers can be found at:    http://www.sdsc.edu/ResTools/biotools/biotools1.html         BLAST databases.  The above BLAST servers search query sequences against comprehensive databases such as GENBANK, SWISSPROT, TREMBL or a non-redundant combination of these. In fact, this is true of most search interfaces to be found on the web. Numerous local databases, including the HIV database and many genome sequencing projects, also offer BLAST interfaces to their sequences. It is less common to find such databases offering interfaces to the other kinds of search algorithm described below.    Low-complexity sequence filters.  A lot of theoretical work has gone into assigning statistical significance to scores produced by sequence alignment programs. BLAST is the best such studied system. Unfortunately, most statistics can be skewed by repetitive or low-information sequences such as DNA microsatellites or protein coiled-coils. The problem of low-information sequences is so severe that it is unwise to run any search tool without filtering it for low-complexity segments. Programs such as SEG and DUST will handle this. The BLAST server on the NCBI website (see above) is set up to use these filters automatically and most mirror sites will have filtering as a selectable option, if not the default.    A wide selection of tools for predicting and masking features such as coiled-coil sequences can be found at:    http://www.expasy.ch/tools/    The Smith-Waterman algorithm . The Smith-Waterman algorithm performs an exhaustive search of all possible gapped alignments between a pair of sequences, given a particular set of scoring parameters. At first this task may appear gargantuan, but the algorithm employs a dynamic programming technique that keeps the search time within manageable bounds. Nonetheless, Smith-Waterman searches do take longer than BLAST searches (as may be expected, since they're more exhaustive).    The most widely web-accessible implementation of Smith-Waterman is SSEARCH [4]. The gain in sensitivity of Smith-Waterman has also prompted some manufacturers ( e.g. , Paracel, Compugen, Time Logic) to produce hardware accelerators for dynamic programming and some of these boxes also have web front-ends.    Links to web-accessible Smith-Waterman implementations can be found at the following sites, among others:    http://www.expasy.ch/tools/ similarity    http://www2.ebi.ac.uk/    http://www.sdsc.edu/ResTools/biotools/biotools1.html    Smith-Waterman with pre-filtering : FASTA, SCANPS. Somewhere between BLAST and Smith-Waterman in the speed/sensitivity trade-off lie a family of algorithms that initially pre-screen the database for putative hit regions using fast heuristic rules (like BLAST), then focus in on those regions with a full dynamic programming search (like SSEARCH).      Examples of these programs are FASTA [5] and SCANPS; the most commonly encountered of these is FASTA.    Wise2.  Programs in the Wise2 suite can compare DNA and protein sequences to one another directly, automatically translating the DNA. For example, the GeneWise program can do a full Smith-Waterman comparison between an amino acid sequence and a stretch of unspliced, untranslated genomic DNA, reporting homologies in spite of intron-exon structure and sequencing errors (including frameshifts). The Wise2 tools are relatively slow to run, although the package includes ""HalfWise"" programs that employ BLAST as a pre-filter to speed up gene prediction. The Wise2 suite is written using the dynamic programming language Dynamite [6].    There is a form-based interface to some of the programs on the Wise2 web site:    http://www.sanger.ac.uk/Software/Wise2/    Bayesian alignment algorithms . Also worth mentioning are Bayesian probabilistic alignment methods. These handle distant homologies by averaging over all possible evolutionary relationships between a pair of sequences, rather than just picking the most likely one. Notable Bayesian approaches include Lawrence et al's Bayes aligner [7] and Bucher and Hofmann's PSW (probabilistic Smith-Waterman) algorithm [8].    The Bayes aligner is particularly well suited to sequences suspected to contain several conserved ungapped blocks, such as transmembrane proteins. Though not currently accessible through web interfaces, it may be downloaded from:    http://www.wadsworth.org/resnres/bioinfo/software.html    The PSW algorithm is implemented in the Wise2 package:    http://www.sanger.ac.uk/Software/Wise2/    Profile searches     Profile searches are considerably more sensitive than simple pairwise searches as they make use of position-specific substitution matrices (and sometimes position-specific gap penalties too). Unfortunately they are also even slower than Smith-Waterman; it is possible to use hardware accelerators to speed them up, but interfaces to accelerated profile searches are rarely found on the web.    In this section, tools to work with probabilistic profiles (the most successful type of profile) are described.    Hidden Markov models.  The majority of profiling tools currently available make use of hidden Markov models (HMMs). The solid basis of HMMs in Bayesian machine learning theory has helped this field considerably. Chief amongst HMM tools is the HMMER package [9]; predating HMMER, but less widely used, is SAM [10].    To train an HMM profile from a set of sequences, it is generally required that the sequences be aligned. Version 1 of HMMER attempted to do this alignment itself, but good multiple sequence alignment is a non-trivial challenge and so this feature has been dropped from HMMER version 2. The most popular of the multiple alignment packages available is CLUSTAL [11]. A recent benchmark of multiple alignment programs may be found in [12].    A comprehensive list of links to HMM-related software can be found on the HMMER web site:    http://hmmer.wustl.edu/    SAM may be accessed via a web interface:    http://www.cse.ucsc.edu/research/compbio/HMM-apps/HMM-applications.html    The previously mentioned Wise2 package is capable of comparing protein HMMs to DNA, as well as protein sequences.    The Wise2 web site is located at:    http://www.sanger.ac.uk/Software/Wise2/    Stochastic context-free grammars . A generalisation of hidden Markov models, capable of modelling the nested correlations between base pairs that are characteristic of RNA structures, are ""stochastic context-free grammars"" or SCFGs [2]. While their increased modelling power makes them     potentially more sensitive tools, SCFGs are considerably trickier to use than HMMs and demand more computational resources. Software for training SCFGs and using them to search sequence databases can be downloaded from    http://www.genetics.wustl.edu/eddy/software/    Automated searches    PSI-BLAST . A major improvement on the original release of BLAST, PSI-BLAST automates the process of profile construction and database searching. Given a query sequence, PSI-BLAST will search a sequence database for close relatives of the query, then construct a profile using these relatives and search the database again using the profile. This process can be repeated several times.    While it has been possible for some time to construct a fully automated search-and-profiling tool from the modular components described above, PSI-BLAST is the first such integrated system to have gained wide appeal. Since its release, PSI-BLAST has proved a highly popular and useful tool, due perhaps to its ease of use as well as its increased speed and sensitivity compared to BLAST.    For reference, the successive steps performed in a single PSI-BLAST iteration are summarised below:    Candidate match ""seeds"" are picked from the database using an algorithm similar to BLAST (but incorporating a two-word-hit rule that is more stringent than BLAST's single-hit rule, thereby saving time later). Seeds are extended by exploring only high-scoring cells in the dynamic programming matrix. An appropriate score cutoff for these high-scoring alignment extensions is found by regression. The parameters were estimated in simulations performed by the program authors. Sufficiently high-scoring pairwise alignments between query and database sequences are combined to give a multiple alignment with no gaps in the query. Virtually identical sequences in this multiple alignment are thrown out. Closely related sequences in the multiple alignment are downweighted, using a sequence weighting scheme that works best for small families. A substitution/deletion profile is created using a pseudocount method to incorporate prior knowledge into sparse datasets. This profile is used as a query and the whole process begins again, performing a pre-specified number of iterations before terminating. enumerate    Although the relative contribution of each step to the speed and sensitivity gains of PSI-BLAST are not entirely clear from the published data, it seems that step 1 yields a significant speed gain over BLAST, steps 2-3 improve on the limited sensitivity of BLAST's ungapped scoring scheme and steps 4-8 are profiling steps that focus the next iteration of the search on probable family members.    An obvious problem with PSI-BLAST is the very same problem that would be encountered were a profile being constructed manually: if a chance similarity is mistakenly included in the profile training set at an early stage, the error may become ""fixed"" if the next iteration of the search algorithm picks up relatives of the imposter sequence rather than members of the query family. This potential amplification of false positives is an inherent feature of any iterative search algorithm; the best way to guard against it is to manually check that the sequences reported by the program appear relevant when compared to the query, not just to one another.    The NCBI has a web interface to PSI-BLAST with an attractive results display (Figure 2):    http://www.ncbi.nlm.nih.gov/cgi-bin/BLAST/nph-psi    Arguably, there is considerable room for improvement on PSI-BLAST. The previously-described tools specialising in multiple alignment and profile training are much better at these jobs than the corresponding parts of PSI-BLAST. This is particularly evident in the way that gaps are handled. Packages such as MEME [13] and work on probabilistic models of protein evolution [14] suggest that more consistent approaches to integrated systems can be found. At the time of writing, however, PSI-BLAST is the best (perhaps the only) freely available reliable automated system for homologous sequence discovery and its popularity signals a clear challenge for computational biology to shift up a gear.    MEME.  MEME is a program for identifying motifs from a training set of unaligned sequences [13]. Particularly suited to finding short motifs such as nucleotide binding sites, it proceeds by a ""greedy""           Figure 2.  The NCBI web interface to PSI-BLAST.    strategy: beginning with a hidden Markov model seeded on a single subsequence from the training set, it iteratively refines this model until no more improvements to the overall score can be found. This algorithm is a reasonably fast and highly effective way of locating repeated motifs. MEME is web-accessible:    http://meme.sdsc.edu/meme/website/    Gibbs sampling . The Gibbs sampling algorithm for multiple sequence alignment has become a popular tool for discovery of short ungapped motifs [15]. The algorithm performs a ""random walk"" through the space of multiple alignments, weighted by the likelihood of those alignments (so that, over long enough times, high-scoring alignments should be visited proportionally more often than low-scoring ones).    Gibbs samplers are theoretically less prone to getting ""stuck"" than greedy algorithms like MEME, as they are capable of making choices that are unfavourable in the short term. The downside of this is that convergence is unpredictable. The algorithm can be VERY slow to run. Perhaps for this reason it is hard to find interfaces to Gibbs samplers on the web; however, one can download software from the following URL:    http://stl.wustl.edu/~ecr/GIBBS/       Protein family databases     There are a number of efforts underway to attempt to organise the protein database into clusters, corresponding to motifs that are conserved throughout nature. Searching these databases rather than the ""raw"" protein databases brings several advantages:              the search is more sensitive, as the query sequence is    compared to a profile of each family, domain or cluster rather    than individual members of the cluster alone. This also makes the    search process faster;          the results of the search are more succinct than a typical    ""raw"" database search;          and the database may have links to other online resources,    such as structure or functional annotation. itemize           The main drawback is that the protein family database may be inaccurate or incomplete    Unfortunately there is no good competitive evaluation of the various protein family databases accessible on the web. It is probably best to try more than one; most of them now contain cross-references to one another anyway, as well as links to structure and literature databases.    There follows a brief description of the most widely used databases.    Pfam . Pfam is a database of strictly non-overlapping protein domains [16]. Each domain entry consists of a seed alignment from which an HMM profile has been trained. Pfam is the only protein family database to be built using HMM-profiles from the very beginning and its development is inextricably linked with that of the HMMER package.    In addition to the curated database (Pfam-A) there exists an automatically generated database of putative families (Pfam-B), from which more families are ""upgraded"" into Pfam-A with each release.    Pfam can be accessed at the following URLs:    http://www.cgr.ki.se/Pfam/    http://www.sanger.ac.uk/Software/Pfam/    http://pfam.wustl.edu/    PROSITE.  PROSITE is a collection of protein families and domains with comprehensive and thorough annotation that includes links to scientific literature and even email addresses of contactable experts [17]. The profiles for each domain are manually constructed and less quantitatively flexible than the statistical HMM methods used by Pfam.    PROSITE is located at:    http://www.expasy.ch/prosite/    PRINTS.  PRINTS is a well-annotated database of protein fingerprints, each of which may be compounded of several distinct motifs [18]. This approach differs from those described above in that a single motif may be used by multiple fingerprints. This is intended to reflect the observation that structural patterns may be decomposed into smaller elements; thus the curators of PRINTS suggest it to be a useful tool for recognising larger structural patterns.    PRINTS is located at:    http://www.biochem.ucl.ac.uk/bsm/dbbrowser/PRINTS/PRINTS.html    ProDom.  The ProDom database is automatically generated from a clustering of the protein sequence database which currently uses the PSI-BLAST algorithm [19]. This automatic generation procedure makes ProDom less tidy than some of the other databases, but its coverage should be more complete.    http://www.toulouse.inra.fr/prodom/doc/prodom.html    InterPro.  The above four databases (Pfam, PROSITE, PRINTS and ProDom) have been gathered together as the composite database InterPro. Sequences can be queried simultaneously against all four databases and the results displayed in parallel. InterPro is searchable online at the following URL:    http://www.ebi.ac.uk/interpro/    Blocks.  Blocks is a database of ungapped multiple alignments [20]. The exclusion of gaps has two immediate implications: firstly, some motifs present in the other databases will be missing from Blocks;     and secondly, those motifs that are present will tend to be more strongly conserved.    The Blocks database may be searched at the URL below. The search engine can also search ungapped subsets of all the other protein family databases mentioned above.    http://blocks.fhcrc.org/    SMART.  The SMART database, like Pfam, contains HMM profiles of protein families [21]. Although it covers a smaller fraction of the protein universe than Pfam (it contains around 400 families as opposed to Pfam's 2290), it compensates by providing significantly enriched structural, functional and phyletic annotation for each domain. SMART may be queried online at:    http://smart.embl-heidelberg.de/    Developments in integrated profiling     Multiple alignment, phylogenetic tree construction and sequence profiling are all attempts to handle the observed statistical nature of relationships between protein sequences. For maximum sensitivity and precision, one would ideally like to be able to model all these aspects of protein evolution using a single integrated tool.    PSI-BLAST may be viewed as one attempt at this. However, as has been mentioned, it makes little use of statistical modelling theory. More promising are probabilistic models of sequence evolution emanating from the phylogenetics camp [22, 23, 24, 14, 25] where debates on flavours of statistical method constitute a familiar theme.    The common theme of these methods is to define a scoring function for a set of sequences aligned to a profile and related by a phylogenetic tree. The profile, alignment and tree that optimise this (probabilistic) scoring function are then to be simultaneously optimised. By taking phylogenetic correlations into account, these methods are theoretically capable of detecting signals that phylogenetically naive programs (read: most of the other algorithms described in this article) will miss. It is the large number of parameters and unknowns in this problem that favour approaches with a solid mathematical foundation such as probabilistic modeling.    Perhaps the simplest of these models is the links model [22], in some ways the probabilistic multiple-alignment analogue of the Smith-Waterman algorithm. This model treats residue insertion and deletions as independent events that occur at a constant rate along the length of the sequence, using a birth-death process familiar from probability theory. The links model has been implemented as a multiple alignment algorithm [26] which may feasibly be extended to a non-homogenous profiling system by concatenating multiple links models.    For profiles without gaps, the RIND program is an interesting and effective way of modelling site-to-site heterogeneity in conservation patterns and substitution rates for sets of pre-aligned protein sequences [24]. RIND attempts to fit the best substitution matrix to each separate column of an alignment, using a variant of the EM algorithm from statistical theory that allows all the entries in the rate matrices to vary.    In contrast, ""tree HMMs"" [23, 14] permit only a finite number of rate matrices for each column, but are otherwise similar to hidden Markov models in that they allow for large deletions (but not insertions) relative to the profile. Tree HMMs are also capable of aligning sequences de novo using methods akin to Gibbs sampling. (Such methods tend to be less effective than dedicated multiple alignment programs such as CLUSTAL [11] but are, arguably, easier for independent parties to improve on.) A fusion of tree HMMs with the links model would allow insertions, bringing tree HMMs closer to the profile HMMs used for database searching by HMMER and SAM, but this is speculation.    Probabilistic methods have many advantages over more heuristic approaches. They are well-defined, facilitating both collaborative and competitive development by multiple groups. They are also capable of sophisticated refinement and great sensitivity, as has been demonstrated in the case of profile HMMs. However they require considerably more effort to develop than heuristic algorithms that can be bolted together relatively quickly. It may be hoped that the success of HMMs in modelling patterns in sequences will encourage this effort to be spent in developing further probabilistic models in sequence analysis and indeed bioinformatics in general.      Acknowledgments     This article has benefited from conversations with Bill Bruno, Ewan Birney and Roger Sayle. The author is in receipt of the Fulbright-Zeneca 1998-1999 Fellowship for Research in Bioinformatics and has also received support from Los Alamos National Laboratory.    References    [1] A. Baxevanis and B. F. Francis Ouellette, editors.  Bioinformatics: A Practical Guide to the Analysis of Genes and Proteins . John Wiley Sons, Inc., 1998.    [2] R. Durbin, S. Eddy, A. Krogh, and G. Mitchison.  Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids . Cambridge University Press, Cambridge, UK, 1998.    [3] S. F. Altschul, T. L. Madden, A. A. Schaffer, J. Zhang, Z. Zhang, W. Miller, and D. J. Lipman. Gapped BLAST and PSI-BLAST: a new generation of protein database search programs.  Nucleic Acids Research ,  25 :3389&endash;3402, 1997.    [4] W. R. Pearson. Effective protein sequence comparison.  Methods in Enzymology ,  266 :227&endash;258, 1996.    [5] W. R. Pearson and D. J. Lipman. Improved tools for biological sequence comparison.  Proceedings of the National Academy of Sciences of the USA ,  4 :2444&endash;2448, 1988.    [6] E. Birney and R. Durbin. Dynamite: a flexible code generating language for dynamic programming methods used in sequence comparison. In T. Gaasterland, P. Karp, K. Karplus, C. Ouzounis, C.Sander, and A. Valencia, editors, Proceedings of the Fifth International Conference on Intelligent Systems for Molecular Biology, pages 56&endash;64, Menlo Park, CA, 1997. AAAI Press.    [7] J. Zhu, J. S. Liu, and C. E. Lawrence. Bayesian adaptive sequence alignment algorithms.  Bioinformatics ,  14 :25&endash;39, 1998.    [8] P. Bucher and K. Hofmann. A sequence similarity search algorithm based on a probabilistic interpretation of an alignment scoring system. In D. J. States, P. Agarwal, T. Gaasterland, L. Hunter, and R. F. Smith, editors, Proceedings of the Fourth International Conference on Intelligent Systems for Molecular Biology, pages 44-51,Menlo Park, CA, 1996. AAAI Press.    [9] S. R. Eddy. Hidden Markov models.  Current Opinion in Structural Biology ,  6 :361&endash;365, 1996.    [10] A. Krogh, M. Brown, I. S. Mian, K. Sj lander, and D. Haussler. Hidden Markov models in computational biology: applications to protein modeling.  Journal of Molecular Biology ,  235 :1501&endash;1531, Feb.1994.    [11] J. D. Thompson, D. G. Higgins, and T. J. Gibson. CLUSTAL W : improving the sensitivity of progressive multiple sequence alignment through sequence weighting, position specific gap penalties and weight matrix choice.  Nucleic Acids Research ,  22 :4673&endash;4680, 1994.    [12] J. D. Thompson, F. Plewniak, and O. Poch. A comprehensive comparison of multiple sequence alignment programs.  Nucleic Acids Research ,  27 (13):2682&endash;2690, 1999.    [13] W. N. Grundy, T. L. Bailey, C. P. Elkan, and Michael E. Baker. Meta-MEME: Motif-based hidden M arkov models of protein families.  Computer Applications in the Biosciences ,  13 :397&endash;406, 1997.    [14] G. J. Mitchison. A probabilistic treatment of phylogeny and sequence alignment.  Journal of Molecular Evolution ,  49 (1):11&endash;22,1999.    [15] C. E. Lawrence, S. F. Altschul, M. S. Boguski, J. S. Liu, A. F. Neuwald, and J. C. Wootton. Detecting subtle sequence signals: a Gibbs sampling strategy for multiple alignment.  Science ,  262 (5131):208&endash;214, 1993.    [16] A. Bateman, E. Birney, R. Durbin, S. R. Eddy, R. D. Finn, and E.L. Sonnhammer. Pfam 3.1: 1313 multiple alignments match the majority of proteins.  Nucleic Acids Research ,  27 (1):260&endash;262, 1999.      [17] K. Hofmann, P. Bucher, L. Falquet, and A. Bairoch. The PROSITE database, its status in 1999.  Nucleic Acids Research ,  27 (1):215&endash;219, 1999.    [18] T. K. Attwood, D. R. Flower, A. P. Lewis, J. E. Mabey, S. R. Morgan, P. Scordis, J. Selley, and W. Wright. PRINTS prepares for the new millennium.  Nucleic Acids Research ,  27 (1):220&endash;225, 1999.    [19] F. Corpet, J. Gouzy, and D. Kahn. Recent improvements of the Pro Dom database of protein domain families.  Nucleic Acids Research ,  27 (1):263&endash;267, 1999.    [20] J. G. Henikoff, S. Henikoff, and S. Pietrokovski. New feature sof the Blocks Database servers.  Nucleic Acids Research ,  27 (1):226&endash;228, 1999.    [21] C.P. Ponting, J.Schultz, F.Milpetz, andP.Bork. SMART: identification and annotation of domains from signalling and extracellular protein sequences.  Nucleic Acids Research , ( 27 ):229&endash;232, 1999.    [22] J. L. Thorne, H. Kishino, and J. Felsenstein. Inching toward reality: an improved likelihood model of sequence evolution.  Journal of Molecular Evolution ,  34 :3&endash;16, 1992.    [23] G. J. Mitchison and R. Durbin. Tree-based maximal likelihood substitution matrices and hidden Markov models.  Journal of Molecular Evolution ,  41 :1139&endash;1151, 1995.    [24] W. J. Bruno. Modelling residue usage in aligned protein sequences via maximum likelihood.  Molecular Biology and Evolution , 13 (10):1368&endash;1374, 1996.    [25] A.L.Halpern and W.J.Bruno. Evolutionary distances forprotein-coding sequences: modeling site-specific residue frequencies.  Molecular Biology and Evolution ,  15 (7):910&endash;917, 1998.    [26] I. Holmes and W.J.Bruno. Evolutionary HMMs: a Bayesian approach to multiple alignment. Submitted.                            Questions or comments? Contact us at  seq-info@t10.lanl.gov                                                                                                                                Operated by the  University of California               for the US  Department of Energy                Copyright © 2001 UC  |               Disclaimer/Privacy"
GX026-22-9831674	Experiments with a Gaussian Merging-Splitting Algorithm for HMM Training for Speech Recognition   Ananth Sankar    Speech Technology and Research Laboratory   SRI International  333 Ravenswood Avenue  Menlo Park, CA 94025     Abstract:   It is well known that the expectation-maximization (EM) algorithm, commonly used to estimate hidden Markov model (HMM) parameters for speech recognition, is sensitive to the initial model parameter values, making appropriate parameter initialization important. We investigate the use of iterative Gaussian splitting and EM training to initialize the desired number of Gaussians per HMM state (or state cluster). We then study merging of Gaussians which contain little training data as an approach to robust parameter estimation. Finally Gaussian merging and splitting is combined to form the Gaussian Merging-Splitting (GMS) algorithm.  Detailed experimental studies show that Gaussian splitting gives similar performance to our previous training algorithm, even though the two algorithms give very different parameter values. The robust parameter estimation from Gaussian merging results in better performance than our old algorithm for speaker-independent models that have a large number of parameters relative to the amount of training data. In addition, in one experiment, speaker adaptation gave a 7% relative improvement in word error rate over the SI models when the SI models were trained with Gaussian splitting alone, as compared to a 15.5% improvement when the SI models were trained with both splitting and merging, even though the unadapted SI models gave similar performance. Finally, experiments with the GMS algorithm show that, for a given number of Gaussian parameters, better performance is achieved by reducing the number of HMM state clusters and increasing the number of Gaussians per state cluster.    Introduction    Most conventional automatic speech recognition (ASR) systems are based on    context-dependent (CD) phone-based hidden Markov models (HMMs) that use Gaussian    mixture models (GMMs) for the state-conditioned observation densities. A commonly    used CD unit is the triphone, which is a model of a phone in the context of    left and right phones. The number of observed triphones in the training data    is usually very large, with many triphones having very little training data,    resulting in poor estimates of the model parameters. One solution to this problem    is to use HMM state clustering where the states in a cluster share a set of    parameters, such as a set of Gaussians [ 1 ].    The HMM parameters are usually computed by maximum-likelihood (ML) estimation    using the expectation-maximization (EM) algorithm [ 2 ].    The EM algorithm is an iterative procedure that recomputes the model parameters    given their current estimates so as to increase the likelihood of the training    data at each iteration. This algorithm is sensitive to the values of the initial    parameters, and guarantees only a locally optimal solution. Different systems    use different approaches to initialize and train the model parameters. However,    to the author's knowledge, there has been no clear comparative description in    the literature of the effect of different initialization and training procedures    on speech recognition performance. Such a description is of value for anyone    training an HMM based speech recognition system. We believe the studies reported    in this paper provide useful insight into the issue of HMM parameter initialization    and training.    In this paper, we present the Gaussian Merging-Splitting (GMS) algorithm for HMM training. In this method, iterative Gaussian splitting and EM training is used to initialize the required number of Gaussians in each HMM state cluster. Starting with a single Gaussian,  Gaussian splitting is used to increase the number of Gaussians at each stage of training until the required number of Gaussians is reached. In addition, at each stage, Gaussians are iteratively merged until each Gaussian has a minimum amount of training data. The GMS algorithm results in a variable number of Gaussians in each HMM state cluster. The number of Gaussians is automatically decided subject to a constraint on the maximum number in each state cluster. Detailed experimental results show the effect of Gaussian splitting and merging, and compare the performance with our previous training algorithm.    SRI's Previous Training Approach          SRI's DECIPHER      speech recognition system is based on HMM state clustering where the states    in each cluster share the same set of Gaussians or Genone [ 1 ].    Each state in a cluster has a different mixture weight distribution to these    shared Gaussians. The HMM states are clustered separately for each phone. This    is done by first training a phonetically tied mixture (PTM) system, where all    states in a phone share the same set of 100 Gaussians. The states in this phone    are then clustered using bottom-up agglomerative clustering. For clustering,    the distance between two states is given by the weighted-by-counts increase    in entropy of the mixture weight distribution (to the shared 100 Gaussians)    due to merging the two states [ 1 ].    The mixture weight distribution of the merged state is easily computed as the    weighted-by-counts average of the individual distributions.    The Gaussians in each state cluster are initialized using the corresponding    100 PTM Gaussians. The 100 Gaussians in each phone are used to initialize the    required number for each state cluster through a series of steps involving the    selection of the most likely Gaussians for each state cluster, and also Gaussian    merging. Details of the algorithm can be found in [ 1 ].    This approach poses a potential problem for the initial values of the Gaussians in the state clusters and hence the final models. The 100 PTM Gaussians cover the entire acoustic space for a particular phone. Each state cluster for this phone covers only a small part of this large acoustic space. Thus, the PTM Gaussians may not be appropriate for initializing the Gaussians in the individual state clusters, and may result in inefficient use of the parameters. To address this issue, we studied an initialization algorithm based on Gaussian splitting.    Gaussian Splitting          We implemented a new initialization scheme based on the splitting strategy    commonly used in vector quantization [ 3 ].    In this approach, we first estimate a single Gaussian model for each Genone.    Given the segmentation of data into HMM states, the ML estimate of these (single)    Gaussians is globally optimal. We then split the Gaussian for each Genone into    two by slightly perturbing the mean of the Gaussian along the direction of the    standard deviation vector, and reestimate the model by further EM training.    This process of splitting and retraining is repeated until the required number    of Gaussians is achieved. At each stage, we can choose how many Gaussians to    split. Thus, if there are currently  n  Gaussians that we want to increase    to  m  Gaussians, then we split the  m - n  Gaussians having    the largest average variance. This average, computed by using the geometric    mean, is a measure of the likelihood of the training data modeled by that single    Gaussian. The Gaussian with the largest variance is the one for which the training    data likelihood is minimum. Since our goal is to maximize the training data    likelihood, splitting this Gaussian is intuitively appealing. Gaussian splitting    is also used in the Cambridge University HTK system [ 4 ].    However, in that system, the Gaussian with the largest amount of data is split    as opposed to the one with the largest variance. We do not believe that this    difference will significantly affect the final recognition results.    Let the mean and variance of Gaussian  i  in a Genone be denoted as     and    , respectively. Let the parameters for the  Gaussians resulting from splitting Gaussian  i  be denoted as    , and    . Then       where     is a vector whose  k th component is the standard deviation of the  k th dimension of the original Gaussian,  i , and     is a small positive number. We set     to 0.001 in our experiments.   Each state  q  in a state cluster has a separate mixture weight distribution to the Gaussians in that Genone, resulting in a Gaussian mixture model for the state given by    . When we split Gaussian  i , the mixture weight     of state  q  for Gaussian  i  is divided equally into the mixture weights for the resulting Gaussians.   The Gaussian splitting initialization approach can be configured in a variety of ways. For example, we may split all Gaussians at each stage, or may split only the single largest variance Gaussian, or may do something in between these extremes.    Gaussian Merging          If there is too little training data segmented into an HMM state cluster, then the Gaussians in the corresponding Genone will not be well estimated. To ensure robust Gaussian estimation, we used a Gaussian merging algorithm. In this method, the Gaussians in a Genone are iteratively merged using bottom-up agglomerative clustering until all Gaussians have at least a threshold amount of data. The optimum value of this threshold is experimentally determined. Gaussian merging results in an automatically selected variable number of Gaussians per Genone. The clustering distance we used between two Gaussians,     and    , is given by the weighted-by-counts increase in entropy due to merging the Gaussians:         where     and     are speech data counts, and    ,     and     are the determinants of the covariance matrices for the individual Gaussians, and the merged Gaussian, respectively.     can easily be computed from the sufficient statistics of the individual Gaussians.    The GMS Algorithm          In the GMS algorithm, Gaussian merging is done before each Gaussian splitting operation. This guarantees that at all stages of the algorithm, the Gaussians are robustly estimated for all Genones. In the GMS algorithm, the user must specify the number of Genones and the maximum number of Gaussians per Genone. The GMS algorithm iteratively increases the number of Gaussians using merging and splitting until the maximum number of Gaussians is reached. Since the amount of training data per Genone varies, and Gaussians are merged until all Gaussians have a threshold of data, the number of Gaussians per Genone usually varies with the Genones after a certain number of merging and splitting operations. Genones with lesser training data have fewer Gaussians and vice versa. The number of Gaussians that are split thus varies with the Genones as the algorithm progresses.    Experimental Results           Data Description    We conducted detailed experiments using the Wall Street Journal (WSJ) corpus to study the effect of Gaussian splitting and merging, comparing performance with our previous training algorithm.  For training, we used the WSJ SI-284 corpus, which consists of 142 male and 142 female speakers, with about 18,000 utterances per gender. For our experiments, we used half of the male speakers and 49 utterances per speaker for a training set of 3479 utterances. Since we ran numerous training experiments in this study, we used this smaller training set to reduce the time for each experiment. However, we believe the results will extend to larger training sets.   For recognition experiments, we used three test sets. The first consists of    10 male speakers taken from the 1993 WSJ development and evaluation test sets,    each uttering about 23 sentences for a total of 230 sentences and 3645 words.    This set is referred to subsequently as WSJ93. A 20,000-word bigram language    model was used for this set. The second test set was the 10-speaker male subset    of the 1994 WSJ S0 development test set and used a 5000-word bigram language    model. This set consisted of 209 sentences with 3330 words, and is referred    to as WSJ94S0. The third test set was the male subset of the 1995 North American    Business News H3-C0 (Sennheiser microphone) development set and used a 60,000-word    bigram language model. This set had 152 sentences with 3904 words, and is referred    to as NABN95. All recognition experiments were run using word lattices [ 5 ]    that we had previously generated for these test sets.    The speech features we used were 12 mel-frequency cepstral coefficients (MFCCs) together with their first- and second-order time derivatives, and the normalized energy along with its first- and second-order derivatives to give a 39-dimensional speech feature vector.    Effect of Gaussian Splitting    To estimate how well the Gaussians in a Genone model the training data, we can measure the number of feature vectors in each Gaussian. If some Gaussians model most of the data, and the others only a few data points, it might be an indication of inefficient parameter usage caused by poor initial conditions. Let     denote the number of feature vectors in the  i th Gaussian of some Genone, and     be the total number of feature vectors for that Genone. The entropy of the distribution     gives an estimate of how the data is distributed into the Gaussians. The entropy is computed as         where  M  is the number of Gaussians in the Genone. The maximum value of this entropy is    , and it is achieved when the data is equally distributed into the Gaussians.   We trained a Genone-based HMM using the old initialization approach (Section  2 )    and the Gaussian splitting approach (Section  3 ).    These models had 991 HMM state clusters, with each cluster sharing a Genone    with 32 Gaussians. Thus, the maximum entropy for each Genone is 5 (      ). As explained previously, the Gaussian splitting approach can be configured    in a variety of ways. For example, we may split all Gaussians at each stage,    or may split only the single largest variance Gaussian, or may do something    in between these extremes. We experimented with many of these approaches. There    was not a very significant difference in performance between these methods,    and so we decided on a simple strategy that splits all Gaussians at each stage    until we have the desired number of Gaussians per Genone.    Figure  1  plots the entropy for each    Genone in the HMM trained with our old initialization algorithm (see Section  2 ),    and the solid line in Figure  2  plots    the entropies for the Gaussian splitting approach. The  x -axis is the    Genone index, and the Genones are ordered from left to right on the axis according    to an increasing amount of data per Genone. As we can see from the figures,    the entropy for many Genones is much less than the maximum of 5 in the old approach,    showing that for these Genones, the Gaussians are underutilized. However, the    entropy for all Genones trained using the splitting approach is very close to    5. Clearly, the two initialization approaches have dramatically different influences    on the model parameters.               Figure 1:  Entropy for Genones with the old training algorithm                 Figure 2:  Entropy for Genones with the Gaussian splitting algorithm      From the figures, it appears that the Gaussian splitting approach does a better    job of utilizing the model parameters. We ran recognition experiments using    the old model and the one trained with Gaussian splitting. Table  1     shows the word error rates on the three test sets. As can be seen, Gaussian    splitting gave essentially the same performance as the old approach. One possible    problem with Gaussian splitting is that if there are not enough data points    in a Genone, then each Gaussian may wind up getting only a small amount of data,    since the data is equally distributed into the Gaussians. The Gaussian merging    algorithm described in Section  4  should    take care of this by merging Gaussians with too little data.               Table 1:  Word error rates for different initialization algorithms   Effect of Gaussian Merging    We merged the 32 Gaussians in each of the 991 Genones trained using the Gaussian    splitting algorithm. This merging was followed by one iteration of EM training    to reestimate the models, starting with the merged Gaussians. The dotted line    in Figure  2  plots the entropy for    the resulting Genones, showing that Genones with less data use fewer Gaussians    while Genones with enough data use close to the maximum of 32 Gaussians. Table  2     shows the word error rates of the new training approach with different merging    thresholds. It also replicates the word error rates obtained by using the old    training approach.               Table 2:  Comparison of word error rates with different merging thresholds      From these results, we see that the merging algorithm resulted in a small improvement for WSJ93 and WSJ94S0. At a merging threshold of 50, the performance of the new algorithm is essentially the same as that of the old algorithm. Again, we see that no significant improvement in performance was achieved.   We investigated how speaker-independent (SI) models trained using the splitting    and merging algorithms performed when they were adapted to the test speakers    using maximum-likelihood transformation based adaptation where we used block-diagonal    matrix affine transformations of the HMM mean vectors [ 6 ].    These experiments were carried out only on the WSJ93 set. Adaptation was done    in supervised mode with the 40 common sentences provided by each speaker. Table  3     gives the word error rates for both the unadapted and adapted models. We can    see clearly that Gaussian merging is critical to good adaptation performance.    It is interesting that while the difference in the performance of the unadapted    models is small, training the SI models by Gaussian splitting and merging resulted    in a 15% adaptation improvement over the SI models as compared to only 7% when    Gaussian splitting alone was used to train the SI models. This result can be    explained by the greater robustness of the SI model parameter estimates from    Gaussian merging as compared to splitting alone. Since the SI models are used    to estimate the transformations used in adaptation, poor SI model estimates    cause incorrect adaptation transformation estimates, which in turn result in    poorly estimated adapted models. However, while Gaussian merging gave better    performance than splitting alone, it is only slightly better than the old algorithm.               Table 3:  Comparison of word error rates for WSJ93 before and after adaptation using different approaches to train the SI models      Table  3  clearly shows the effect    of greater robustness due to Gaussian merging. We further investigated this    by comparing the Gaussian splitting and merging algorithm to the old approach    described in Section  2 . Since    the greater robustness of Gaussian merging would be more evident in much larger    systems, we trained an HMM system with 2027 Genones and 32 Gaussians per Genone,    using both the old training approach and the new approach (Gaussian splitting    until each Genone has 32 Gaussians, followed by iterative Gaussian merging with    a merging threshold of 50 frames). Table  4     shows the word error rates for this system, along with the corresponding word    error rates for the smaller, 991-Genone system.               Table 4:  Comparison of word error rates for systems with different numbers of parameters      From this table, we see that for both the Gaussian splitting and merging and old training algorithms, the word error rate was higher with the larger system (2027 Genones). However, the system trained with the splitting and merging algorithm degraded more gracefully than the one trained with the old algorithm. The relative increase in word error rate for the old system on WSJ93, WSJ94S0, and NABN95 was 6.8%, 13.1%, and 7.0%, respectively, as compared to 1.7%, 4.3%, and 5%, respectively, for the new algorithm. Thus, the Gaussian splitting and merging training algorithm gives more robust estimates of the model parameters than the old algorithm. This greater robustness is reflected in the fact that the word error rates are not as sensitive to the number of model parameters, creating a wider range of the number of model parameters that give similar error rates. This makes it easier to experimentally search the space of models to find the optimal one.   Finally, in Table  5 , we present    adaptation results on WSJ93 for the 2027 Genone system when the SI models were    trained with the old algorithm and the splitting and merging algorithm. The    results show that the performance both before and after adaptation was superior    with the splitting and merging algorithm.               Table 5:  Comparison of word error rates for WSJ93 before and after adaptation using different approaches to train the SI models   Training with the GMS Algorithm          After studying the individual effects of Gaussian splitting and merging, we    used the GMS algorithm to train Genonic HMMs by using the WSJ SI-284 subset    described above. We experimented with different numbers of Genones and Gaussians    per Genone. Table  6  gives the resulting    recognition word error rates. Since the GMS algorithm guarantees robust parameter    estimation, we experimented with large numbers of Gaussians per Genone. In previous    development work, we had typically used 32 Gaussians per Genone. However, in    using the GMS algorithm we decided to increase this upto 512 Gaussians per Genone.    (To allow comparison across different settings with the same total number of    Gaussian parameters, the total number of Gaussians is given in the table in    parentheses.)               Table 6:  Word error rates (%) with different number of Genones and  Gaussians per Genone. The number of Gaussians is in parenthesis.      One interesting observation we can make from Table  6     is that the optimal system has 126 Genones and 512 Gaussians per Genone. This    is significantly different from our previous systems, which typically used 1000    or more Genones and 32 Gaussians per Genone. We note, however, that we used    only a fraction of the WSJ SI-284 data to train the systems listed in Table  6 ,    and so the optimal number of parameters may be different from that of our previous    systems. However, the table still shows a preference for systems with fewer    Genones and more Gaussians per Genone. For example, comparing the 991-Genone/32-Gaussian    system with the 126-Genone/256-Gaussian system, we see that the latter has a    lower word error rate even though the number of parameters is comparable. In    fact, the 126-Genone/128-Gaussian system also has a lower error rate than the    991-Genone/32-Gaussian system even though it has fewer parameters. One explanation    for this might be that when two HMM state clusters overlap in acoustic space,    modeling them as a single cluster with more Gaussians gives better resolution    than modeling them as two clusters with half the number of Gaussians in each    cluster.    Since the optimal models trained with the GMS algorithm have fewer Genones    than our previous systems, there is the potential of a speedup for speech recognition    in addition to the improved word error rate. At any frame during the Viterbi    search, multiple hypotheses must be evaluated. For each hypothesis, the frame    likelihoods of the Gaussians in the corresponding Genone are computed and cached.    When the same Genone is used by another active hypothesis at the same frame,    the cache is used to provide the likelihoods. For systems with fewer Genones,    the same Genone is used for more of the active hypotheses at any frame, thus    reducing the number of Genone likelihood computations. However, since the number    of Gaussians per Genone is larger, each Genone likelihood computation is more    expensive. We can reduce the cost of Genone likelihood computations by using    Gaussian shortlists [ 1 ]. In this method,    vector quantization is used to divide the acoustic space into Voronoi regions.    A shortlist of Gaussians is maintained for each Genone in each region. During    recognition, the Voronoi region corresponding to the test frame is used to determine    the shortlist of Gaussians to be computed. By effectively using shortlists,    we believe that a significant speedup can be achieved using the new models trained    by the GMS algorithm. We are currently studying this issue.     Summary and Conclusion          We have introduced the GMS algorithm for HMM training and presented a detailed comparative study of the algorithm. Gaussian splitting has the property of uniformly distributing the data into the available Gaussians, while Gaussian merging results in robust parameter estimation. This robustness was shown by graceful degradation in performance when the number of parameters is larger than can be properly estimated by the available training data, and also by significantly improved performance of speaker adaptation algorithms. The GMS algorithm allows us to efficiently train HMMs by automatically determining the number of Gaussians for a given number of Genones. Since robust estimation is guaranteed, we can train systems with a very large number of Gaussians per Genone. Some interesting observations were made about the effect of varying the number of state clusters or Genones and the number of Gaussians per Genone. As a result, we found that the optimum HMM structure was very different from that used by our previous systems; the number of state clusters or Genones was significantly decreased while the number of Gaussians per Genone was significantly increased. The reduced number of Genones has the potential of giving a significant speedup during recognition.    References     1  V. Digalakis, P. Monaco, and H. Murveit, ``Genones: Generalized Mixture   Tying in Continuous Hidden Markov Model-Based Speech   Recognizers,''  IEEE Transactions on Speech and Audio Processing ,   vol. 4, no. 4, pp. 281-289, 1996.    2  A. Dempster, N. Laird, and D. Rubin, ``Maximum Likelihood from Incomplete   Data via the EM Algorithm,''  Journal of the Royal Statistical   Society , vol. 39, no. 1, pp. 1-38, 1977.    3  Y. Linde, A. Buzo, and R. Gray, ``An Algorithm for Vector Quantizer   Design,''  IEEE Transactions on Communications , vol. COM-28,   pp. 84-95, January 1980.    4  S. Young and P. Woodland, ``The Use of State Tying in Continuous Speech   Recognition,'' in  Proceedings of EUROSPEECH , pp. 2203-2206, 1993.    5  H. Murveit, J. Butzberger, V. Digalakis, and M. Weintraub, ``Large-Vocabulary   Dictation Using SRI's DECIPHER(TM) Speech   Recognition System: Progressive-Search Techniques,'' in     Proceedings IEEE International Conference on Acoustics, Speech, and Signal   Processing , pp. II 319-322, 1993.    6  L. Neumeyer, A. Sankar, and V. Digalakis, ``A Comparative Study of   Speaker Adaptation Techniques,'' in  Proceedings of EUROSPEECH ,   pp. 1127-1130, 1995.      About this document ...       Experiments with a Gaussian Merging-Splitting Algorithm for HMM Training for Speech Recognition  This document was generated using the  LaTeX 2 HTML  translator Version 96.1 (Feb 5, 1996) Copyright © 1993, 1994, 1995, 1996,   Nikos Drakos , Computer Based Learning Unit, University of Leeds.   The command line arguments were:    latex2html   -split 0 acoustic-paper.tex .  The translation was initiated by Ananth Sankar on Mon Mar 30 22:49:26 PST 1998   ...Recognition This work was sponsored by DARPA through the Naval Command and Control Ocean Surveillance Center under contract N66001-94-C-6048.            Ananth Sankar   Mon Mar 30 22:49:26 PST 1998
GX026-53-3204346	"MITRE TDT-2000 Segmentation System      Warren Greiff, Alex Morgan, Randall Fish, Marc Richards, Amlan Kundu,   (greiff, morgan, fishr, marc, akundu)@mitre.org     MITRE Corporation   202 Burlington Road   Bedford, MA 01730-1420          ABSTRACT    We present the design and development of a Hidden Markov Model for the division of news broadcasts into story segments.  Model topology, and the textual features used, are discussed, together with the non-parametric estimation techniques that were employed for obtaining estimates for both transition and observation probabilities.  Visualization methods developed for the analysis of system performance are also presented.   1. Introduction   For last year’s, TDT-3, evaluation, the MITRE system utilized a Naïve Bayes classifier [Greiff, Hurwitz & Merlino, `99].  For the TDT-2000 evaluation, we have explored the use of a fine-grained, multi-state Hidden Markov Model (HMM).  After experimentation and performance analysis with an initial design, the model topology, as well as the feature definitions used were found to be unsatisfactory in a number of respects.  In the sections that follow, we describe the topology and features that have evolved from this experience, and are used in the current version of the segmentation system.  Also described is our approach to the determination of model parameters using non-parametric kernel estimation methods.  After a brief description of how the resulting model is used for actual segmentation, we discuss visualization aids developed to support analysis of the system, and conclude with a summary of our TDT-2000 submission results, and plans for future work.   Figure 1:  HMM Topology   2. Approach   The model we have used is a 251 state Hidden Markov Model, with the topology shown in Figure 1.  States labeled, 1 to 250, correspond to the first 250 words of a story.  One extra state, labeled 251, is included to model the production of all words at the end of stories exceeding 250 words in length.  There are two principal motivations for this approach.  First, that with a model of this nature we will be able to exploit the distribution of story-lengths that is unique to each source.  For example, in the top graph of Figure 3, below, we see a histogram of story lengths (up to 250 words) for the ABC_WNT news source. The histogram displays a bimodal distribution with a clear preference for stories in the range of 30  to 80 words.  This is evidence that can be used to improve segmentation. The second motivating factor was to go beyond the approach we employed in TDT-3, which used a Naïve Bayes classifier to classify each word as to whether or not it is at a boundary.  Classification of each word was independent of all others.  The goal of the fine-grained HMM approach is to enable the exploitation of features that give strong indications of which portion of a story we are in; indications which go beyond the binary boundary/no-boundary distinction.  The HMM approach also supports the production of a single coherent segmentation sequence in place of a series of independent boundary/no-boundary decisions.    3. Features   The Hidden Markov Model is defined in terms of a set of features.  The model assigns, for each state, a probability distribution over all possible combinations of values the features may take on.  The probability assigned to value combinations is assumed to be independent of the state/observation history, conditioned on the state. The MITRE system further assumes that the value of any one feature is independent of all others, once the current state is known. Features have been explicitly designed with this assumption in mind.  Three categories of features have been used, which we refer to as  coherence  features,  trigger  features, and the  x-duration  feature.   3.1.  Coherence    We have used four coherence features.  The coher-1 feature, shown schematically in Figure 2a, is based on a buffer of 50 words immediately prior to the current word (to which the feature value corresponds).  If the current    Figure 2:  Coherence features   word does not appear in the buffer, the value of coher-1 is 0.  If it does appear in the buffer, the value is -log (s w /s) , where  s w  is the number of stories in which the word appears, and  s  is the total number of stories, in the training data.  In this way, rare words get high feature values, and common words get low feature values.  The idea behind this feature is that high values will be very unlikely to be observed at early points in the story, where the majority of the buffer corresponds to words of the previous story.  Three other features: coher-2, coher-3, and coher-4  (Figures 2b, c & d) correspond to similar features; for these, however, the buffer is separated by 50, 100, and 150 words, respectively, from the current word.  Presumably, the probability of observing a high value of coher-3, for example, would be very low in states 1 to 100, where the buffer is entirely over words of the previous story; much higher in states 150 - 250, where the buffer is entirely over words of the same story; and grow monotonically between states 100 and 150.   3.2. X-duration   This feature is based on the untranscribable portions of the audio signal, as indicated by X entries in the ASR file.  The absence of an X entry prior to a word corresponds to an x-duration feature value of 0. The existence of an X entry prior to the word gives a non-zero value, with large X entry durations corresponding to high values, and low X entry durations corresponding to low values.   3.3. Triggers   Trigger features correspond to small regions at the beginning and end of stories, and exploit the fact that some words are far more likely to occur in these positions than in other parts of a news segment.  One region, for example, is restricted to the first word of the story. For ABC_WNT, for example, the word ""finally"" is far more likely to occur in the first word of a story than would be expected by its general rate of occurrence in the training data.  For a word,  w , appearing in the input stream, the value of the feature is an estimate of how likely it is for  w  to appear in the region of interest.  The estimate used is given by:           where is the number of times  w  appeared in  R  in the training data;   is the total number of occurrences of  w ; and   is the fraction of all tokens of  w  that occurred in the region.  This estimate can be viewed as a Bayesian estimate with a beta prior.  The beta prior is equivalent to a uniform prior together with the observation of one occurrence of the word in the region out of   total occurrences.  This estimate was chosen so that: 1) the prior probability would not be greatly affected for words observed only a few times in the training data; 2) it would be pushed strongly towards the empirical probability of the word appearing in the region for words that were encountered in  R ; 3) it has a prior probability, , equal to the expectation for a randomly selected word.  The regions used for the submission were restricted to the one-word regions for: first word, second word, last word, and next-to-last word.  Limited experimentation with multi-state regions, was not fruitful.  For example, including the regions, {3,4,…,10} and {-10,-9,…,-3}, where  –i  is interpreted as  i  words prior to the end of the story, did not improve segmentation performance.  Since the current HMM topology does not model end-of-story words (earlier versions of the topology did model these states directly), trigger features for end-of-story regions are delayed.  Suppose the delay were 10 words, sufficient to cover all end-of-story regions, and the region of interest were {-4,-3,-2}.  High values for this trigger feature could be interpreted as ""a word showing marked preference for occurring 2 to 4 words prior to the end of a story, occurred 10 words ago"", and could be expected to be observed most often in states 6, 7 and 8.    4. Parameter Estimation   The Hidden Markov Model requires the estimation of transition and conditional observation probabilities.  There are 251 transition probabilities to be estimated.  Much more of a problem are the observation probabilities, there being 9 features in the model, for each of which a probability distribution over as many as 100 values must be estimated, for each of 251 states.  With the goal of developing methods for robust estimation in the context of story segmentation, we have applied non-parametric kernel estimation techniques, using the locfit library (Loader, 1999) of the R open-source statistical analysis package, which is based on the S-plus system [Venables & Ripley,    Figure 3:  Histograms  of story lengths        (raw and smoothed)   `99; Chambers & Hastie, `92, Becker, Chambers & Wilks, `88]. For the transition probabilities, it is assumed that the underlying probability distribution over story length is smooth, allowing the empirical histogram, shown at the top of Figure 3, to be transformed to the probability density estimate shown at the bottom. From this probability distribution over story lengths, the conditional transition probabilities can be estimated directly.   A similar approach is taken with regard to observations, in that conditional probabilities are deduced from an estimate of the joint probability distribution.  The following procedure was used.  First, observation values were binned.  Binning limits were set in an attempt to 1) be large enough to obtain sufficient counts for the production of robust probability estimates, and yet, 2) be constrained enough so that important distinctions in the probabilities for different feature values will be reflected in the model.  At this time, the limits have been set manually, as the result of studying the data distributions, and comparing the results of different binning strategies.  For each bin, the observation counts are smoothed by performing a non-parametric regression of the observation counts as a function of state.  The smoothed observations counts corresponding to the regression are then normalized so as to sum to the total observation count for the bin.  The result is a conditional probability distribution over states for a given binned feature value,   p(State=s|Feature=fv).   Once this is done for all bin values, each conditional probability is multiplied by the marginal probability,  p(State=s) , of being in a given state, resulting in a joint distribution,  p(fv,s),  over the entire space of  (Feature,State)  values.  From this joint distribution, the necessary conditional probabilities,  p(Feature=fv|State=s),  can be deduced directly.       Figure 4:  Likelihood of  coher-3=20 over all states.   Figure 4 gives a comparison of the conditional probability estimates,   p(fv | s),  for the feature value coher-3=20, across all states, confirming the intuition that, while the probability of seeing a value of 20 is small for all states, the likelihood of seeing it is much higher in later parts of a story than it is in early-story states.   The preceding description assumes that probabilities are smooth  across all states.  This is not always the case.  For example, exploratory analysis of the data reveals that state 1 is notably different from other states with respect to the x-duration  feature.  Because of this, a smoothing over all states is not appropriate.  Fortunately, there are abundant observations for state=1.  This allows us to estimate the conditional probability distribution over feature values for  state=1  independently, and restrict the procedure described above to the non-boundary states.  For some features, special consideration must also be given to the mid-story state, 251.   5. Segmentation   Once parameters for the HMM have been determined, segmentation is straight-forward.  The Viterbi algorithm [Rabiner, `89], is employed to determine the sequence of states most likely to have produced the observations sequence associated with the broadcast.  A boundary is then associated with each word produced from State 1 for the maximum likelihood state sequence.   The version of the Viterbi algorithm we have implemented provides for the specification of ""state-penalty"" parameters, which we have used for the ""boundary state"", state 1. In effect, the probability for each path in consideration is multiplied by the value of this parameter (which can be less than, equal to, or greater than, 1) for each time the path passes through the boundary state.  Variation of the parameter effectively controls the ""aggressiveness"" of segmentation, allowing for the tuning of system behavior in the context of the evaluation metric.       6. Visualization   The MITRE submission was the result of a number of modifications and refinements of the HMM topology, design of the features, and the estimation procedures.  This    Figure 5:  Visualization for x-duration feature.   evolution was driven by analysis of the behavior of the system, supported by visualization routines developed using the graphing capability of the R package.  Figure 5 gives an example of graphs that can be used for analysis of the role of the x-duration feature, as part of the segmentation of a specific file.  These graphs compare the maximum likelihood path produced by the HMM (lighter, thinner line) to the path that would be produced by a perfect system (thicker, darker line) -- one privy to ground-truth, as given by the boundary file.  The top graph shows the states traversed by the two systems.  The graph below shows the value of the x-duration feature corresponding to each word of the broadcast.  The third graph shows, on a log scale, how many times more likely it is that this value would be generated from the true state than in the state predicted by the system; with positive values corresponding to the true state being more likely to produce the observation, and negative values corresponding to situations for which the observation is more likely from the system’s state.  The final graph shows the cumulative sum of the values from the graph above it. Note, that a similar graph for the total probability (equal to the product of all the individual feature value probabilities) will always have an overall downward trend.  This is because the system always chooses the maximum likelihood path.  This is the path with greater likelihood than all others according to the HMM model, including the true path, to which the observations actually do correspond.  These graphs help to direct attention to potential weaknesses in the modeling. For example, towards the end of the story, we see a cluster of three points (lower right hand corner of the probability ratio graph) corresponding to observations less likely to have been generated by the true state sequence, than by the state sequence produced by the system.  This is an indication that this feature may have contributed significantly to the mistaken placement of a boundary, after the last true boundary, as shown in the top graph.   7. Results   As mentioned above, the process of getting the core HMM segmentation mechanism to an acceptable base-level of performance proved to be much more difficult than we had anticipated.  As the Oct. 10 submission deadline drew near, key issues in the implementation of the system still needed to be worked out.  Our submission system did not perform well compared to segmentation scores reported last year at TDT-3.  In the intervening time we have continued work on our approach, focusing on the analysis of system performance on the ABC_WNT news source.  At this point, preliminary results for this part of the corpus are encouraging.  After training on all but 15 files of the development set, a test on the remaining 15 produced a normalized Cseg score of .31, comparable to the scores of .31 and .33 achieved by IBM and CMU respectively, at TDT-3.     8. Future Work   Analysis of segmentation performance on sources other than ABC_WNT will be the initial focus of continued work on the fine-grained HMM approach to broadcast news segmentation.  Analysis of each feature individually will be required to obtain insights into the modeling questions that require specific attention, taking full advantage of the visualization techniques, discussed above,  to study the segmentation produced by the system on specific files.  In support of this effort, and continued efforts to improve performance for all sources, we will be looking at devising theoretically motivated methods for evaluating the value of a given feature and summarizing its impact on segmentation performance over a large number of files.  We also plan to take a closer look at the problem of probability estimation.  A number of parameters must be set before estimation can proceed.  We have found, for example, that local linear fitting with a rectangular kernel seems to give best performance, but time has not permitted the methodical comparison of alternatives such as higher degree polynomial fitting, and differently shaped kernel functions.  We would also like to more carefully examine the question of choosing the best bandwidth for smoothing, and the measurement of goodness-of-fit.  Finally, our original plan for TDT-2000 included experimentation, in the context of our HMM approach, with the use of more sophisticated textual features, such as the use of log-linear modeling for the determination of more complex trigger cues as developed in [Beeferman, Berger, & Lafferty ‘99], as well as the exploitation of features extracted directly from the audio signal, continuing work on prosodic modeling reported at the TDT-2 conference in [Stolcke, et al. ‘98]. This continues to be our goal, even though we were unable to explore these possibilities for this evaluation.   REFERENCES      [Becker, Chambers & Wilks, `88] Becker, Richard A., Chambers, John M., and Wilks, Allan R.   The New S Language.   Wadsworth & Brooks/Cole, Pacific Grove, Cal.   [Beeferman, Berger, &Lafferty ‘99] D. Beeferman, D., A. Berger, A. and Lafferty, J.  Statistical Models for Text Segmentation.   Machine Learning , vol. 34, pp. 1-34, 1999.    [Chambers & Hastie, `88] Chambers, John M. and Hastie, Trevor, J.   Statistical Models in S.   Wadsworth & Brooks/Cole, Pacific Grove, Cal., 1988.   [Greiff, Hurwitz & Merlino, `99] Greiff, Warren, Hurwitz, Laurie, and Merlino, Andrew.  MITRE TDT-3 Segmentation System.  TDT-3 Topic Detection and Tracking Conference, Gathersburg, Md, February, 2000.   [Lau, Rosenfeld & Roukos, `93] Lau, Raymond, Rosenfeld, Ronald and Roukos, Salim.  Trigger-Based Language Models: A Maximum Entropy Approach. In Proceedings of the International Conference on Acoustics, Speech and Signal Processing, pp. II 45-48, Minneapolis, MN, April, 1993.    [Loeder, `99] Loader, C.   Local Regression and Likelihood . Springer, Murray Hill, N.J., 1999.   [Rabiner, `89] L. R. Rabiner, A tutorial on hidden Markov models and selected applications in speech recognition.   Proceedings of the IEEE , vol. 37, no. 2, pp. 257-86, February, 1989.   [Venables & Ripley, `99]  Venables, W. N. and Ripley, B. D.  Modern Applied Statistics with S-PLUS .  Springer, Murray Hill, N.J., 1999.   [Stolcke, et al. ‘98] Stolcke, Anderas, Shriber, Elizabeth. Hakkani-Tur, Dilek, Tur, Gokhan, Rivlin, Ze’ev and Sonmez, Kemal.  Combining Words and Speech Prosody for Automatic Topic Segmentation, TDT-2 Topic Detection and Tracking Conference, Gathersburg, Md, February, 1999."
GX029-71-10285515	"Chapter 2    High-Throughput Genome Assembly, Modeling, and Annotation    The sequencing of microbial genomes containing several thousand genes and the eventual completion of human and other model organism genomes is the underlying driving force for understanding biological systems at a whole new level of complexity. There is for the first time the potential to understand living organisms as whole, complex dynamic systems and to use large-scale computation to simulate their behavior. Modeling  all  levels of biological complexity is well beyond even the next generation of Teraflop computers, but each increment in the computing infrastructure makes it possible to move up the biological complexity ladder and solve previously unsolvable classes of problems.       The  experimental (left) and computational (right) hierarchies  will increasingly become codependent as the research community models greater biological complexity.         The first step in the biological hierarchy is a comprehensive genome-based analysis of the rapidly emerging genomic data. With changes in sequencing technology and methods, the rate of acquisition of human and other genome data over the next few years will be ~100 times higher than originally anticipated. Assembling and interpreting these data will require new and emerging levels of coordination and collaboration in the genome research community to develop the necessary computing algorithms, data management and visualization systems.    Annotation -  the elucidation and description of biologically relevant features in the sequence -  is essential in order for genome data to be useful. The quality with which annotation is done will have direct impact on the value of the sequence. At a minimum, the data must be annotated to indicate the existence of gene coding regions and control regions. Further annotation activities that add value to a genome include finding simple and complex repeats, characterizing the organization of promoters and gene families, the distribution of G+C content, and tying together evidence for functional motifs and homologs.    As complete genomes are sequenced, the length of DNA comparison strings will change from single genes to entire genomes, with a concomitant expansion in the time to compute. In order to look at long-range patterns of expression, syntenic regions on the order of 10's of megabases become reasonable lengths for consideration. While the cycles needed to run many of these classes of analysis codes on rapidly increasing data sets is in itself a significant problem, all of these must inevitably be run on ALL data accumulated and in a recurring manner. Significant computational work is required to permit the analysis and visualization of long genomic regions needed for comparative genomic studies.       The Genome Channel Browser.   One mechanism for researchers to access and visualize the results of each day's analysis of human genome data. The effort makes daily use of multiple current high-performance computing systems to keep up with current data flow and analysis and modeling requirements.         Methods of Genome Sequence Analysis and Modeling    A new approach to sequencing genomes, whole genome shotgun sequencing, becomes possible if small fragments of sequence generated at random can be compared to each other fast enough to effectively determine sequence overlap and from this, assemble the pieces into longer contiguous regions, or contigs.    The first step in the sequence reconstruction involves finding overlaps between each fragment and the existing contigs. The presence of sequencing errors, natural sequence variations between sources as well as high repetitive DNA sequence elements make this an extremely complex challenge. If the computation can be performed and other significant technical problems can be overcome, this strategy could speed up sequencing of the human genome from about 7 to 3 years.    As an example, the proposed whole genome shotgun strategy at the Institute for Genome Research (TIGR) produced 30 million bases of DNA per day in January of 1998 which increased to 100 million base pairs per day by mid-year. To continue to grow the contiguous assemblies of sequence that emerge from the random strategy requires that each of the 200,000 fragments generated each day be compared in a very detailed way to all previous contigs. Particularly in the early phases, overlaps were rare so initially 200,000 contigs per day accumulated in the database. After enough contigs accumulated to realize substantial overlap, the sequence assembly required 4x10 12  sequence comparisons per day, each of which requires a significant fraction of a second on a standard processor. While comparing new reads to existing data is enough of a problem, periodically the entire data structure must be rebuilt, requiring an even larger number of FLOPs for a calculation that breaks existing contigs down to their underlying fragments and reassembles them    Some of the most compute intensive processes involve the use of sequence database information to calculate models of genes contained in the sequence. In the last year or so, significant growth of expressed sequence tag (EST) collections and new types of hybrid gene modeling methods that integrate EST data with pattern recognition approaches, such as GRAIL-EXP, have become available. These can be used to produce modeling of the structure of genes in genome sequence data in most cases, but require significant computing power.    Nucleic acid based reference sequences currently number about 1 million and each putative exon (gene coding segment) must be aligned to this database of reference DNA pieces. GRAIL-EXP computes multiple-gene structures on an input DNA sequence using GRAIL predicted exons, which are most consistent with the known ESTs/cDNA/ protein sequences.    It achieves this goal by modeling the multiple-gene structure prediction problem as a combinatorial optimization problem and solving it by a dynamic programming method. The algorithm runs in O(nM+n 2 K 2 ) time, where n is the number of predicted exons in the given DNA sequence, M is the average time to find all the ESTt/cDNA/proteins that match a predicted exon from the specified databases, and K is the maximum number of EST/cDNAs/proteins an exon may match.    Typically, it takes about a minute to find all the matched ESTs in the current dbEST database (about 1.2 million entries) for a predicted exon. Hence it may take up to a few days to find all the matched ESTs for all the predicted exons on a DNA of 10 million bases long. If the data rate grows to on the order of 100 million bases per day, the calculation would require about a month of time for each day's data using a single 500 megahertz processor. This is by not a one time operation -  the data needs to be reanalyzed frequently because the underlying databases of ESTs and cDNAs are growing rapidly. At 100 million bases per day, in ten days there is a billion base pairs to analyze, and in 100 days 10 billion. To reanalyze this data requires about three days on a current 1,024 node supercomputer. Bringing known protein sequences in the protein sequence database into the process makes the analysis much more useful, but due to the alignment takes an order of magnitude more time than the ESTs. Nonetheless, this should be done routinely, requiring about a month of time on a current 1024 node supercomputers.    The problem of parsing the predicted exons into genes that are most consistent with known ESTs/cDNAs/proteins, also takes significant amount of time, which will increase as the number of matched ESTs/cDNAs/proteins gets larger. To process 100 megabases of sequence will require over 10 13 Ops, assuming each predicted exon has ~1000 matched ests/ cDNA/ proteins in the database on average. When the average number of matched ESTs/cDNA/ proteins goes up to 5000, the number of needed ops will scale to over ~10 15 Ops. It would also be desirable to calculate multiple gene models for each gene, to make each model consistent with possible splice variants suggested by the underlying EST evidence. This increases the complexity of this component of the calculation to 10 17 Ops for a relatively frequently needed operation.         Methods for Large Scale Comparison of Genome Sequences    Once the basic structure of genes has been modeled, comparison of new sequences against each other or existing database is one of the most essential and revealing processes in computational genomics. Such operations relate new sequences to archival sequences that may have meaningful information about patterns in the sequence and its function. Such comparisons are the starting point for the computation of phylogenetic (evolutionary) trees of organisms or genes, pathogenicity studies for public health, polymorphism studies (e.g., of genetic defects), identification of protein motifs, model identification for gene recognition, model identification for organism classification, functional analysis of genomic/ protein sequences, and exon identification.    The analyses and inferencing often depend on the quality of the computed multiple sequence alignments (MSA's) used as input. MSA's of biological sequences, e.g., DNA, RNA, or protein sequences, entail the arrangement of many (in some cases thousands) of sequences, so that corresponding positions are aligned in vertical columns, with padding characters (nulls) added to compensate for length variations in some sequences.    The most accurate and sensitive alignments must consider gaps in the alignment (insertions and deletions) and are thus rather computationally intensive. The standard algorithm for this is Smith-Waterman, which uses dynamic programming to produce a local optimal alignment between two sequences of length M and N, and scales as O(MxN). The simple extension of these algorithms to multiple sequence alignments of K sequences requires time O(N K ). For sequence lengths in the thousands of nucleotides, this is barely feasible for 3 sequences, certainly not for thousands of sequences. Hence, common practice is to use ""progressive alignments"" which is an inefficient algorithm that adds one sequence at a time to the MSA. This is computationally tractable, but not optimal. It is especially problematic when the sequences are not closely related, e.g., in computing the Tree of Life.    Recently rediscovered Hidden Markov Models (HMM) and Stochastic Context Free Grammars (SCFG) offer the prospect of better MSAs, by also modeling higher order structures. The simplest are HMMs, which are stochastic regular grammars. SCFGs are more complex, but permit one to model nested structures, such as the stem and loop structures common in RNA. More elaborate types of grammars permit the modeling of more complex secondary and tertiary structures. To use these models, one must first estimate the many parameters of the model. The resulting model can then be used to ""parse"" the sequences, and the resulting parses transformed into multiple sequence alignments. Iterative estimation of the Hidden Markov Models entails iterative solution of computations akin to the pairwise dynamic program sequence comparison computations. At each iteration we must perform M such O(N 2 ) computations, one for each of the M sequences being aligned, and sum the results.    These independent computations with each sequence offers a clear target for parallel computation, followed by a logarithmic summation computation. This is particularly true for large sequence collections such as the ribosomal RNA alignments. Some researchers have constructed fine-grained parallel systolic algorithms for the dynamic programming computations, on specialized hardware implementations or SIMD machines. However, on MIMD machines (with greater costs for interprocessor communication and synchron- ization) coarser partitioning of the dynamic programming computations appears preferable. Furthermore, these iterative computations often find local optima, requiring multiple computations with different starting states to find (putative) global optima.    One difficulty in model estimation for methods like HMM arises from the possibility of over-fitting the very large number of parameters in these models (several per sequence position). Bayesian methods have been adopted to smooth these parameter estimates. Bayesian methods have traditionally been difficult to compute. Several researchers have resorted to Gibbs sampling methods to estimate the posterior probability distribution. These methods entail the construction and simulation of a Markov chain whose equilibrium probability distribution is equal to the target posterior distribution. The Gibbs sampling computations should be amenable to parallelization, assuming that independent parallel random number generators (PRNGs) are available. This is a subject of research activity in the Monte Carlo computation community, and are available from several research groups.    In the area of phylogenomics, insight from the evolutionary relationships of the unknown protein to others known is used to infer something about its potential function(s). In this approach, first, homologs of the unknown are identified and phylogenetic tree is constructed. Known functions of members of the group are overlaid onto the evolutionary tree and the function of the unknown is predicted by its position in the tree relative to its homologs whose functions have been characterized.    The first step in building a phylogenetic tree is to do a multiple sequence alignment on the homologous group of proteins. Once the alignment has been completed tree construction itself presents significant computational challenges. The evaluation and alignment of multiple trees, which is important for attempting to reconstruct the relationship among organisms based on several trees reflecting the relationships of groups of proteins, has been shown to be MAX SNP-hard.    For the ""Tree of Life"" computations that employ thousands of ribosomal RNA sequences, heuristic methods are a necessity. Typical computations with serial code run a few hundred hours on a workstation to accurately compute a single backbone tree of only about 100 nodes. Computation of the backbone tree involves both discrete optimization over the space of possible tree topologies and parametric optimization over the space of possible edge lengths (duration between evolutionary events). Likelihood computations are used to rank the trees. The likelihood computations are quite expensive, involving the computation of a state-vector for each node in the tree.    Divide and conquer strategies to partition the computation of the entire tree are also of interest -  as the subtrees correspond to groups of related organisms. Each such taxa is typically of particular interest to a group of researchers. Bootstrap methods (resampling the input data and recomputation) can be to evaluate the reliability of the tree. Bootstrap computations exhibit obvious parallelism, but have previously been computational intractable for problems on this size. About a thousand new rRNA sequences are added to the ""Tree of Life"" every year.    A number of methods exist for the computation of phylogenetic trees. The area is one of considerable ongoing scientific controversy. These algorithms differ in the type of data used (distance data between molecules vs. aligned sequences), the existence of global optimal tree criteria, the existence of an explicit statistical model for the evolutionary history, etc. Methods vary in computation time, their consistency (convergence to correct tree with infinite data), efficiency (rate of convergence to correct tree with finite data), bias, robustness to departures from assumed statistical models of evolution.    Robustness is of particular concern because most analyses assume (contrary to fact) that individual sequence positions evolve independently. Efficiency of the estimation technique is particularly important when dealing with phylogenies of individual genes for which only relatively short sequences are available. Some studies have suggested that available data on individual genes may not be sufficient for reliable estimation of gene phylogenies.    Many researchers believe that maximum likelihood estimation (MLE) (or perhaps related Bayesian approaches), offer the best prospect for consistent, efficient estimation of phylogenetic trees. MLE approaches also facilitate the integration of multiple types of data (e.g., different sequences, restriction fragment length polymorphism data, etc.). However, MLE (and Bayesian) have formidable floating point computation and intermediate storage requirements (e.g., each internal node in the tree requires storage equal to the sequence length times the alphabet size). It is also worth noting that the large MLE phylogeny problems have serious problems with floating point representation of portions of the likelihood computations.    Clearly, some portions of the computations, e.g., bootstrapping, clearly lend themselves to simple cluster-based parallel processing. Finer grained parallelism, e.g., by decomposing the computation of individual trees, e.g., on MPPs, has yet to be explored. There also appear to be opportunities for parallelism in the computationally intensive construction of multiple sequence alignments, e.g., via HMMs or SCFGs.              Sequence Comparisons Against Model Protein Families for Understanding Human Pathology    Searching a protein sequence database for homologues is a powerful tool for discovering the structure and function of a sequence. Amongst the algorithms and tools availabe for this task, Hidden Markov model (HMM)-based search methods improve both the sensitivity and selectivity of database searches by employing position-dependent scores to characterise and build a model for an entire family of sequences.    HMMs have been used to analyse proteins using two complementary strategies. In the first, a sequence is used to a search a collection of protein families, such as Pfam, to find which of the families it matches. In the second approach an HMM for a family is used to search a primary sequence database to identify additional members of the family. The latter approach has yielded insights into protein involved in both normal and abnormal human pathology such as Fanconi Anaemia A, Gaucher disease, Krabbe disease, polymyositis scleroderma and disaccharide intolerance II.    HMM-based analysis of the Werner Syndrome protein sequence (WRN) suggested it possessed exonuclease activity, and subsequent experiments confirmed the prediction. Like WRN, mutation of the protein encoded by the Klotho gene lead to a syndrome with features resembling ageing. However, Klotho is predicted to be a member of the family 1 glycosidase (see figure). Eventually, large-scale sequence comparisons against HMM models for protein families will require enormous computational resources to find these sequence-function correlations over genome-scale size databases.       The similarities and differences between two  plant  and  archeael  members of a family of glycosidases that includes a protein implicated in ageing.  Ribbons correspond to the beta-strands and alpha-helices of the underlying TIM barrel (red) and family 1 glycosidase domain (cyan). Amino acid side chains drawn in magenta, yellow and green are important for structure and/or function. The loop in yellow denotes a region proposed to be important for substrate recognition. The 2-deoxy-2-fluorglucosyl substrate bound at the active site of one of the enzymes is shown with carbon atoms in grey, oxygen in red and fluorine in green.                   Large-scale Calculations of Phylogenetic Trees    The calculation of phylogenetic trees is a central approach to understanding evolutionary history, a central problem in biology. Phylogenetic computations are concerned with the estimation of evolutionary trees and their reliability, and may be done from a variety of types of data: DNA sequences, ribosomal RNA sequences, protein sequences, or protein structures.    Currently construction of phylogenetic trees commences with a multiple sequence alignment (MSA), which is then used as input to the phylogenetic tree computation. Classical dynamic programming algorithms with progressive alignments can be used to do the MSAs. Hidden Markov models can improve the quality of the sequence alignments, and extensions to Stochastic Context Free Grammars have the ability to identify RNA secondary structures such as stem and loop construction.    Estimation of the phylogenetic tree itself involves searching the discrete topological space of all trees of a specified size (exponential in the number of leaves) and estimating the lengths of each edge in the phylogenetic tree (i.e. how much mutation occurred). Recent large phylogenetic computations have used 10 4  to 10 5  processor hours to explore truncated tree models. Typically, the reliability of the trees is estimated by performing a bootstrap computation over the individual sequence positions/ columns). For large computations, such as the tree of life, bootstrapping has often not been done, due to lack of computing resources. Speed and availability of computer time are presently major constraints of the ability to do these computations, as the availability of genome sequence data explodes.       Two hypothetical scenarios and the path of trying to infer the function of two uncharacterized genes in each case is traced.  (A) A gene family has undergone a gene duplication that was accompanied by functional divergence. (B) Gene function has changed in one lineage. The genes are referred to by numbers representing the species from which these genes come, and letters representing different genes within a species. The thin branches in the evolutionary trees correspond to the gene phylogeny and the thick gray branches in A (bottom) correspond to the phylogeny of the species in which the duplicate genes evolve in parallel (as paralogs). Different symbols represent different gene functions; gray (with hatching) represents either unknown or unpredictable functions.                   The Need for High-End Computing for Genome    Modeling and Annotation    The massive scale of the data flow and challenge of timely analysis is promoting significant changes in the organization of genome analysis components of the research community. A small number of specialized centers, such as the Genome Annotation Consortium, are emerging to construct the codes and systems needed to analyze the data on the scale needed for the next phase of the biological research. These groups are using current supercomputing capabilities on a continuing daily basis to keep up with current analysis and modeling needs and are rapidly recognizing the need for new computing infrastructure in the imminent future. Typical current applications, their current computational cost and community requirements are shown in Table I. These efforts are more and more serving as a focus for the broader research community by providing interface systems to access, visualize and validate the results obtained from genome scale computational analysis and modeling.         Table I. Current and Expected Sustained Capability Requirements for Major Community Genomics Codes                                    Problem Class                Sustained Capability 1999 (in FLOPS)                Sustained Capability         2000 (in FLOPS)                Sequence       assembly                >10 12                10 14                Binary       sequence comparison                10 12                >10 14                Multiple       sequence comparison                10 12                >10 14                Gene       modeling                >10 15                10 17                Phylogeny       trees                10 11                10 13                Protein       family classification                >10 10                10 12                Table I  illustrates many high-priority computational challenges associated with the analysis, modeling and annotation of genome data. The first is the basic assembly and interpretation of the sequence data itself (analysis and annotation) as it is produced at increasing rates over the next five years. There will be a never-ending race to keep up with this flow, estimated at about 200 million base pairs per day by some time in 1999, and to execute the required computational codes to locate and understand the meaning of genes, motifs, proteins, and genomes as a whole. Cataloging the flood of genes and proteins and understanding their relationship to one another, their variation between individuals and organisms, and evolution represents a number of very complex computational tasks. Many calculations such as multiple sequence alignments, phylogenetic tree generation, and pedigree analysis are NP-hard problems and cannot be currently done at the scale needed to understand the body of data being produced, unless a variety of shortcuts is introduced. Related to this is the need to compare whole genomes to each other on many levels both in terms of nucleic acid and proteins and on different spatial scales. Large-scale genome comparisons will also permit biological inference of structure and function.    Chapter 2"
GX029-79-5419511	"Chapter 3    From Genome Sequences to Protein Structures:    Comparative Modeling and Fold Assignment    The key to understanding the inner workings of cells is to learn the three-dimensional atomic structures of the some 100,000 proteins that form their architecture and carry out their metabolism. These three-dimensional (3D) structures are encoded in the blueprint of the DNA genome. Within cells, the DNA blueprint is translated into protein structures through exquisitely complex machinery- itself composed of proteins. The experimental process of deciphering the atomic structures of the majority of cellular proteins is expected to take a century at the present rate of work. New developments in comparative modeling and fold recognition will short circuit this process, that is we can learn to translate the DNA message by computer.    The success of these methods rests on a fundamental experimental discovery of structural biology: the 3D structures of proteins have been better conserved during evolution than their genome sequences. When the similarity of a target sequence to another sequence with known structure is above a certain threshold, comparative modeling methods can often provide quantitatively accurate protein structure predictions,    since a small change in the protein sequence usually results in a small change in its 3D structure .  Even when   the percentage identity of a target sequence falls below this level, then at least qualitative information about the overall fold topology can often be predicted.    In protein fold assignment, a genome sequence is computationally tested for compatibility with a library of known protein folds. Current estimates of the number of protein folds range between 800 and 15,000, but each estimate is more than a thousand times smaller than the number of proteins. The goal of fold assignment and comparative modeling is to assign each new genome sequence to the known protein fold or structure that it most closely resembles, using computational methods.    Fold assignment and comparative modeling techniques can then be helpful in proposing and testing hypotheses in molecular biology, such as in inferring biological function, predicting the location and properties of ligand binding sites, in designing drugs, testing remote protein-protein relationships. It can also provide starting models in X-ray crystallography and NMR spectroscopy.            Protein fold assignment.   A genome-encoded amino acid sequence (center) is tested for compatibility with a library of known 3D protein folds. An actual library would contain of the order of 1000 folds; the one shown here is a representation, illustrating the most common protein folding motifs. There are two possible outcomes of the compatibility test: that the sequence is most compatible with one of the known folds or that the sequence is not compatible with any known fold. The second outcome may mean either that the sequence belongs to one of the folds not yet discovered, or that the compatibility measures are not fully enough developed to detect the distant relationship of sequence to its structure.         BPI: A case study in assigning genome sequences to a known 3D protein structure.    Bactericidal permeability-increasing protein (BPI; Figure 1 ) from human white blood cells is a potent antimicrobial protein of 456 amino acid residues. Its structure, determined by X-ray crystallography, was found to be a new fold: an elongated, boomerang-shaped molecule, unlike any previously known structure. Prior to the publication of the 3D structure of BPI, its amino acid sequence was submitted to the 2nd meeting on Critical Assessment of Protein Structure Prediction methods (CASP2). Several methods of fold assignment correctly concluded that the BPI sequence was incompatible with any protein fold then in the database of known protein structures.    With the 3D structure of BPI available, it became possible to search databases of genome sequences to learn which other protein sequences are compatible with the BPI structure. In other words, it was then possible to assign other sequences to the BPI structure (Beamer, Fischer, & Eisenberg, 1998). This search uncovered 13 distant relatives of BPI in a diverse set of eurkaryotes, including rat, chicken, worm, and biomphalaria galbrata. The 13 new proteins share only 13-19 % sequence identity with BPI, below the ""twilight zone"" of marginal identification by sequence comparison methods.    The significance of this case study is that advanced computational methods can assign numerous genome sequences to the 3D structures by methods of fold assignment, short circuiting the laborious experimental determination of 3D structures. Chapter 3 discusses prospects for improving the sensitivity of fold assignment methods, so that even more distant sequence-structure relationships can be detected by computer.       A ribbon diagram of human BPI . The N-terminal domain is aqua, and the C-terminal domain is blue. A proline-rich linker residues 230-250, which connects the two domains, is shown in yellow. The highly conserved disulfide bonds between Cys 135 and Cys 175 is shown as ball-and-stick atoms.              Large-scale comparative modeling of protein structures of the yeast genome    Recently, a large-scale comparative protein structure modeling of the yeast genome was performed (Sanchez and Sali, PNAS, 1998). Fold assignment, comparative protein structure modeling, and model evaluation were completely automated. As an illustration, the method was applied to the proteins in the Saccharomyces cerevisiae (baker's yeast) genome. It resulted in all-atom 3D models for substantial segments of 1071 (17%) of the yeast proteins, only 40 of which have had their 3D structure determined experimentally. Of the 1071 modeled yeast proteins, 236 were related clearly to a protein of known structure for the first time; 41 of these have not been previously characterized at all. Many of the models are sufficiently accurate to facilitate interpretation of the existing functional data as well as to aid in the construction of mutants and chimeric proteins for testing new functional hypotheses. This study shows that comparative modeling efficiently increases the value of sequence information from the genome projects, although it is not yet possible to model all proteins with useful accuracy.    The main bottlenecks are the absence of structurally defined members in many protein families and the difficulties in detection of weak similarities, both for fold recognition and sequence-structure alignment. However, while only 400 out of the total of a few thousand domain folds are known, the structure of most globular folds is likely to be determined in less than ten years. Thus, comparative modeling will conceivably be applicable to most of the globular protein domains close to the completion of the human genome project.       Large-scale protein structure modeling.   A small sample of the 1,100 comparative models calculated for the proteins in the yeast genome is displayed over an image of a yeast cell.              Methods for Fold Assignment    Recent work on fold assignment (or ""threading"") involves two main approaches: developing potentials for fold assignment, and Hidden Markov Models (HHMs) or profile methods that descended from sequence alignment methods. The potentials can be contact potentials (potentials of mean force) or they can be more complex semi-empirical potentials, involving atomic areas and other properties. Fold assignment approaches further subdivide into two categories: (1) unipositional methods that consider probability distributions of amino acids at single sites and (2) those that consider distributions on pairs (or even triples) of amino acids within a contact distance in a given structure. Hidden Markov models to date have considered single site probability distributions. We discuss each approach in turn.    Contact potentials for threading .    Unipositional fold assignment approaches score each residue position in a template structure using local 3D environmental information such as secondary structure propensity, degree of environmental polarity, and the fraction of the residue surface buried and inaccessible to solvent. The 3D environmental information for each residue then becomes a one-dimensional profile of the tertiary structure or fold, and the compatibility of the twenty common amino acids are evaluated for each position in the 1-D profile. Optimal 1-D alignments of a probe sequence to a given structure can be determined by dynamic programming, and the subsequent score of the aligned sequence against the template are determined by the global characteristics of the sequence-environment fit, thereby tolerating locally poor scores.    Unipositional methods demonstrated impressive ability for determining similar topological folds for proteins with less than 25% sequence identity in some cases. However, only 25% of genome sequences recognize their 3D protein fold with a sufficient threshold of confidence to be considered a successful fold assignment. One reason for this modest success rate of threading is that the repertoire of folds is thought to be incomplete. This repertoire (or library) is growing modestly through the current efforts of structural biologists, and a strong Structural Genomics Initiative will give a major boost to fold assignment in the future. It has been estimated that the success rate of fold assignment algorithms will increase to roughly 50% once these missing folds are identified structurally. For the remaining 50% of genome sequences to be assigned to folds, there must be advances in the directions discussed here.    The first advance is to move to multi-positional compatibility functions. Pairwise threading potentials typically consider the propensity of two amino acids to be within a specified distance using a score function compiled from a database of structures. Additional features can be used in addition to the identify of the amino acids, such as the secondary structure type, relative exposure to water, relative position, and local atomic density. The identification of the important features is emerging from interdisciplinary collaborations among protein scientists, physicists, and computer scientists, and often involves the use of computational benchmarks, as discussed below. Some attempts have been made to go beyond pairwise potentials, but determination of higher order probability distributions is limited by the data available from the present number of structures. New structures made available from the Structural Genomics Initiative would provide some assistance in this regard, however the number of new structures is unlikely to dramatically increase the order of probability distributions that can be reliably estimated. Therefore further improvements in pairwise and other potentials of mean force will rest on better identification of the relevant physical effects determining the relation of sequence to structure, and on improved algorithms to extract information about these effects from limited data.    Alignment of a genome sequence to 3D structure using pairwise potentials is more difficult than using unipositional potentials. Branch and bound algorithms have been shown to yield the optimal alignment when they converge, but since the general threading problem for multipositional potentials is NP complete, branch and bound algorithms will not converge in all cases. Nevertheless, they are often extremely useful. Approximations can also be employed, such as the frozen approximation, in which one assumes that the interaction of test sequence position j with the amino acid k' of the target structure would be similar to k in the sequence. Once the sequence is optimally aligned using the frozen approximation, the multipositional compatibility function is used to score the sequence-structure match. However, this current state-of-the-art alignment has proved insufficient in the blind prediction experiment CASP2, and improved methods are essential to reach accurate protein models.    Hidden Markov Models    HMM's consider single site probability distributions for amino acids, but have the added feature of a Markovian transition matrix between ""hidden"" states. The hidden states effectively perform a choice among a set of position dependent amino acid probability distributions. In contrast to threading methods, HMM's do not use an explicit scoring function to score the match of an amino acid with its environment, nor do they typically consider pairwise interactions. HMMs rely heavily on position specific scoring functions that, in combination with the hidden Markov states, match appropriate probability distributions to sequence positions. Prior knowledge about amino acid probability distributions can be incorporated in a Bayesian framework for HMM's using ""Dirichlet prior"" probability distributions.    HMM's can be used for fold identification by performing a standard sequence based homology search using the probe sequence to generate homologous sequences. These sequences can be used to construct an HMM based on the probe, and then the sequences from a library of folds can be matched against the HMM. Similarly, one can construct separate HMM models for each member of a library of folds, and then score the probe sequence against each model. Construction of HMM models is typically an iterative process involving successive periods of modeling building, searching with the given model, and model refinement. Alignment to a HMM can be performed in an efficient recursive manner, similar to dynamic programming.    A variety of methods has been applied to the problem of scoring the match of a sequence to a structure. These include both analytical methods such as Markov Random Fields, and neural networks, and highly empirical energy-like functions. These range from unipositional functions with three or greater environmental descriptors, to multipositional functions that consider the attributes of two or more amino acids at a time. Multipositional functions are potentially more sophisticated since a score is based on the compatibility of multiple amino acids in the test sequence with multiple positions in the target structure.    Even after perfect fold assignment can be achieved, there remains a computational bottleneck in providing a predicted 3D structure: proper alignment of the sequence to the structure. Alignment with unipositional compatibility functions, which add the independent contributions of a single position of a test sequence to a single position in the target fold, offers the advantage of using well-established dynamic-programming algorithms to find the optimal alignment, although poor gap and insertion penalty parameters can render this optimum somewhat arbitrary. HMM's offer effective position dependent insertion/deletion penal- ties as well as an efficient alignment procedure, but ignore more than single site probabilities, as do other unipositional compatibility functions. Pairwise threading potentials add an additional order of statistics (the pairwise probability distributions between amino acids), but with an increase in computational cost for the alignment step. Allowing only a limited number of gaps between secondary structure elements, and exhaustively enumerating all possible resulting threadings, has been implemented for multipositional compat- ibility functions and has been successful for a subset of interesting cases.         Methods for Comparative Modeling    Comparative modeling remains the only method at present that can provide models with an rms error lower than 2Å. All current comparative modeling methods consist of four sequential steps. The first step is to identify the proteins with known 3D structures that are related to the target sequence. The second step is to align them with the target sequence and to pick those known structures that will be used as templates. The third step is to build the model for the target sequence given its alignment with the template structures. In the fourth step, the model is evaluated using a variety of criteria. If necessary, the alignment and model building are repeated until a satisfactory model is obtained. The main difference between the different comparative modeling methods is in how the 3D model is calculated from a given alignment (step 3 above).    The original and still widely used method is modeling by rigid body assembly. The method constructs the model from a few core regions, and loops and side-chains, which are obtained from dissected related structures. This assembly involves fitting the rigid bodies on the framework, which is defined as the average of the C a  atoms in the conserved regions of the fold. Another family of methods, modeling by segment matching, relies on approximate positions of conserved atoms from the templates to calculate the coordinates of other atoms. This is achieved by the use of a database of short segments of protein structure, energy or geometry rules, or some combination of these criteria. The third group of methods, modeling by satisfaction of spatial restraints, uses either distance geometry or optimization techniques to satisfy spatial restraints obtained from the alignment of the target sequence with homologous templates of known structure. In addition to the methods for modeling the whole fold, numerous other techniques for predicting loops and side-chains on a given backbone have also been described. These methods can often be used in combination with each other and with comparative modeling techniques.    Perhaps the most promising comparative model building technique (step 3 above) is the comparative modeling by satisfaction of spatial restraints. The reason is that this approach is based only on optimization of an objective function, and it thus allows an efficient exploration of various representations of protein structure, methods of optimization, and objective function forms. The computational complexity of this approach is directly tied to methods such as global optimization described in the next chapter. This flexibility is essential for improving comparative protein modeling. It will also facilitate simultaneous use of different sources of information when calculating a model of a given protein. For example, a model may be constructed that is consistent with the template structures, potentials of mean force, NMR restraints, cross linking experiments, site-directed mutagenesis data, etc. Boundaries between comparative modeling, fold assignment, ab initio folding simulations, ligand docking, NMR and X-ray structure refinement will be blurred.    The best comparative techniques can generally produce models with good stereochemistry and overall structural accuracy that is slightly higher than the similarity between the template and the actual target structures, when the modeling alignment is correct. The errors in comparative models can be divided into five categories: (1) Side-chain packing errors. (2) Distortions and rigid body changes in regions that are aligned correctly (e.g., loops, helices). (3) Distortions and rigid body changes in insertions (e.g., loops). (4) Distortions in incorrectly aligned regions (loops and longer segments with low sequence identity to the templates). (5) Incorrect fold resulting from an incorrect choice of a template. The consequence of these errors is that the comparative method can result in models with a main-chain rms error as low as 1Å for 90% of the main-chain residues, if a sequence is at least 40% identical to one or more of the templates. In this range of sequence similarity, the alignment is mostly straightforward to construct, there are not many gaps, and structural differences between the proteins are usually limited to loops and side-chains. When sequence identity is between 30% and 40%, the structural differences become larger, and the gaps in the alignment are more frequent and longer. As a result, the main-chain r.m.s. error rises to ~ 1.5 Å for about 80% of residues. The rest of the residues are modeled with large errors because the methods generally cannot model structural distortions and rigid body shifts, and cannot recover from misalignments. Insertions longer than about 8 residues usually cannot be modeled accurately at this time, while shorter loops frequently can be modeled successfully. Model evaluation methods are frequently successful in identifying the inaccurately modeled regions of a protein. To put the errors into perspective, we list the differences among experimentally determined structures of the same protein: the 1.0Å accuracy of main-chain atom positions corresponds to X-ray structures defined at a low-resolution of about 2.5 Å and with an R-factor of about 25%, as well as to medium-resolution NMR structures determined from 10 inter-proton distance restraints per residue.    Future improvements of comparative modeling should aim to (1) model proteins with lower similarities to known structures (e.g. , less than 30% sequence identity), (2) to increase the accuracy of the models, and (3) to make modeling fully automated. The improvements are likely to include simultaneous optimization of side-chain and backbone conformations in side-chain modeling, simultaneous optimization of a loop and its environment in loop modeling, and simultaneous optimization of the alignment and the model. At the same time, better potential functions and possibly better optimizers are needed. The potential function should guide the model away from the templates in the direction towards the correct structure. An addition of atomic or residue based potentials of mean force to the homology-derived scoring could be one way of achieving this goal. This is a difficult problem, as illustrated by the fact that no present force field or potential of mean force can produce a model with a main-chain rmsd from the X-ray structure smaller than about 1Å, even when the starting conformation is the X-ray structure itself. For example, molecular dynamics simulations in solvent generally have a main-chain rmsd of more than 1Å, and the most detailed lattice folding simulations result in models with an rms error larger than 2Å. Since most of the main-chain atoms in two homologs with at least 40% sequence identity usually superpose with an r.m.s.d. of about 1Å, it is currently better to aim to reproduce the template structures as closely as possible rather than to venture away from the templates in the search for a better model.    The major factor that limits the use of comparative modeling in the cases of less than 30% sequence identity is the alignment problem, as discussed in the fold recognition problem (Figure 1A). In principle, the alignment can be derived by any of the sequence or sequence/ structure alignment methods, but in practice even careful manual editing frequently results in significant alignment errors. At 30% sequence identity, the fraction of incorrectly aligned residues is about 20% and this number rises sharply with further decrease in sequence similarity. This limits the usefulness of comparative modeling because no current modeling technique can recover from an incorrect input alignment. It would appear that fold recoginition methods are a natural solution to the alignment problem in comparative modeling. However, while these techniques are successful in identifying related folds, they appear to be somewhat less successful in generating correct alignments, although improvements in alignment for fold recognition is a goal of future work. To reduce the errors in the model stemming from the alignment errors, iterative changes in the alignment during the calculation of the model are needed. Provided the objective function is capable of distinguishing a good model from a bad one, the iterative realignment and re-selection of templates will minimize the effect of errors in the initial alignment and selection of templates. A case in point is provided by the generation of the RUVB model based on a remotely related E. coli  d  structure.                   The Need for Advanced Computing for    Fold Assignment and Comparative Modeling    Several fundamental issues remain to amplify the effectiveness of fold assignment and comparative modeling. Both can be addressed with broader computational resources and better communication among protein scientists and computational scientists. A primary issue in fold assignment is the determination of better multipositional compatibility functions which will extend fold assignment into the ""twilight zone"" of sequence homology. In both fold assignment and comparative modeling, better alignment algorithms that deal with multipositional compatibility functions are needed. A move toward detailed empirical energy functions and increasingly sophisticated optimization approaches in comparative modeling will occur in the future. As these future directions develop, computational bench- marks will be important. These are sets of distantly related pairs of proteins, having similar folds, but very different amino acid sequences. New methods for fold recognition and comparative modeling are developed without the use of these pairs, and then tested on this set, with the goal of assigning each sequence to its proper fold, and further refining that fold for accurate structure. The value of computational benchmarks is that they permit unbiased development of new functions.    The availability of tera-scale computational power will further extend fold assignment in the following ways: the alignment by dynamic programming is performed in order L 2  time, where L is the length of the sequence/structure. For 100,000's of sequences and 10,000's of structures (each of order 10 2  amino acids long), an all to all comparison of sequence to structure using dynamic programming would require on order of 10 13  operations. Genuine teraflop computing capability could make comparisons on this scale, and indeed one or two orders of magnitude bigger, routine today. The use of multipositional functions that will scale as L 3  or L 4  depending on the complexity of the compatibility function, and may require 10 15  to 10 17  FLOPS, and will likely need an effective search strategy in addition. The next generation of 100 teraflop computers addresses both fundamental issues in reliable fold assignment. First, an increase in complexity in alignment algorithms for multipositional functions that scale beyond L 2 , and secondly the development of new multipositional compatibility functions whose parameters are derived by training on large databases with multiple iterations, resulting in an increase in sensitivity and specificity of these new models.    Availability of tera-flop computing will greatly benefit comparative protein structure modeling of both single proteins and the whole human genome. Once an alignment is determined, comparative modeling is formally a problem in molecular structure optimization. The objective function is similar in complexity to typical empirical protein force fields used in ab initio global optimization prediction and protein folding, and scales as M 2 , where M is the number of atoms in the model. A typical calculation for a medium sized protein takes in the order of 10 12  Flops. More specifically, the increased computing power is likely to improve the accuracy of comparative models by aleviating the alignment problem and the loop modeling problem, at the very least.    It is probable that the impact of the alignment errors will be decreased by performing independent comparative modeling calculations for many different alignments. Perhaps as many as 1000 different alignments will be explored in this way. This would in essence correspond to a conformational search with soft constraints imposed by the alignment procedure. Such a procedure would increase the computational cost to 10 15  Flops for a single protein, and to 10 20  Flops for the human genome.    Specialized and time consuming loop modeling procedures can be used after the initial comparative models are obtained by standard techniques. Such specialized procedures typically need about 5000 energy function evaluations to obtain one loop conformation. It is standard to calculate an ""ensemble"" of 25-400 loop conformations for each loop sequence. Thus, the total number of function evaluations is on the order of 10 6  for prediction of one loop sequence. Applying these procedures to the whole human genome would take on the order of 10 18  Flops.      Chapter 3"
GX262-63-0775528	"Understanding and Responding to Earthquake Hazards Carol A. Raymond, Paul R. Lundgren and Soren N. Madsen Jet Propulsion Laboratory 4800 Oak Grove Drive Pasadena, CA 91109  John B. Rundle Colorado Center for Chaos & Complexity University of Colorado Boulder, CO 80309  Abstract  Understanding the earthquake cycle and assessing earthquake hazards is a topic of both increasing potential for scientific advancement and social urgency. A large portion of the world's population inhabits seismically-active regions, including the megacities of Los Angeles and Mexico City, and heavily populated regions in Asia. Population growth will exacerbate the potential for huge earthquake-related casualties. However, powerful new tools to observe tectonic deformation have been developed and are being deployed with encouraging results for improving knowledge of fault system behavior and earthquake hazards. In the future, the coupling of complex numerical models and orders of magnitude increase in observing power promises to lead to accurate targeted, short-term earthquake forecasting. Dynamic earthquake hazard assessments resolved for a range of spatial scales (large and small fault systems) and time scales (months to decades) will allow a more systematic approach to prioritizing the retrofitting of vulnerable structures, relocating populations at risk, protecting lifelines, preparing for disasters, and educating the public. The suite of spaceborne observations needed to achieve this vision has been studied, and the derived requirements have defined a set of mission architectures and enabling technologies that will accelerate progress in achieving the goal of improved earthquake hazard assessments.  sion era, the modeling environment will rapidly evolve to achieve revolutionary advances in understanding the emergent behavior of fault systems. This in turn will enable finer temporal resolution (dynamic) earthquake hazard assessments on the scale of individual faults and fault systems. Dynamic earthquake hazard assessment, coupled with rapid postearthquake damage assessments will enable more effective disaster preparedness for, and response to, large damaging earthquakes. II. ELEME NTS OF A GLOBAL EARTHQUAKE SATELLITE OBSERVING SYSTEM  I. EARTHQU  AKE HAZARD ASSESSMENT IN THE VISION ERA  Three decades ago, earthquake prediction was thought to be an achievable goal. Such optimism has all but vanished in the face of current understanding of the complexity of the physics of earthquake fault systems. The advent of dense geodetic networks in seismically active regions (e.g., SCIGN, the Southern California Integrated Global Positioning System [GPS] Network), and satellite interferometric synthetic aperture radar (InSAR) has resulted in great progress in understanding fault ruptures, transient stress fields, and the collective behavior of fault systems, including transfer of stresses to neighboring faults following earthquakes [1]. These improved observations of surface deformation, coupled with advances in computing and information technology, have stimulated numerical simulations of fault systems that attempt to reveal system behavior. As InSAR and GPS data become more spatially and temporally continuous in the Vi-  Efforts to advance understanding of earthquake physics require detailed observations of all phases of the earthquake cycle (pre-, co-, and post-seismic), across multiple fault systems and tectonic environments, with global distribution. Satellites offer the best way to achieve global coverage and consistent observations of the land surface. While ground seismometer and GPS networks are and will remain critical, the synoptic view of the deforming crust that is possible using satellite data drives the need for a global earthquake satellite observing system. In addition, knowledge of the character of the shallow subsurface is critical to assessing expected ground accelerations. Other types of geophysical data may also shed light on the subsurface processes. The different types of measurements that might comprise a global earthquake satellite system are discussed below. A. Surface Deformation Measurements  Measurement of surface change (displacement) constitutes a powerful tool for resolving the deformation fields resulting from tectonic strain. Surface deformation includes other components besides tectonic strain, such as surface motion due to groundwater storage and retrieval [2]. The InSAR technique relies on correlated image-pairs to derive displacements to the resolution of the radar wavelength. If topography is known, two images can be used to derive a map of the displacement in the range direction. A second image pair obtained from a different look direction (i.e., ascending   versus descending) improves resolution of vertical and horizontal displacements. If topography is not known, three images can be differenced to derive the topography and its change. The accuracy of the measurement depends on several factors, including radar signal to noise, orbit determination precision, and removal of signal path delays caused by the interference of ionospheric electron density and tropospheric water vapor. All of these errors must be minimized to achieve long-term absolute accuracy of interseismic strain accumulation. B. Subsurface Characteristics  sion will provide data to test existing hypotheses. III. S PATIAL AND TEMPORAL MEASUREMENT REQUIREMENTS  The type of material in the shallow subsurface, and its saturation, affect the ground acceleration experienced as a result of a particular earthquake. Directivity of seismic energy during fault rupture can result in quite different patterns of deformation. Liquifaction, the sudden release of water from saturated, permeable layers, is of particular concern in coastal landfill areas, and on steep slopes. Mapping the degree of saturation in the shallow subsurface will help determine landslide hazards, and may allow the liquifaction hazard to be folded into the overall dynamic earthquake hazard assessment, scaled by the degree of saturation of the vulnerable layers. Radar sounders, along with InSAR displacements, can provide data to characterize the subsurface. C. Electromagnetic and Thermal Anomaly Precursors  The remainder of the paper will focus on measurement of surface deformation, as this has emerged as the top priority for space-based observation of the earthquake cycle. LIDAR (Light Detection and Ranging) systems can provide precise measurements of surface change through clear air and even beneath vegetation canopy. Wide-swath LIDAR is a promising technique for future observing systems. Detailed requirements for InSAR data gathering have been collected to support three main objectives: long-term measurement of interseismic strain accumulation (to <1 mm/yr resolution), detailed maps of coseismic deformation to define the fault rupture, and measurement of slow, transient deformation such as post-seismic relaxation and stress transfer following earthquakes, aseismic creep, and slow earthquakes. To maximize correlation between scenes, especially at interannual time scales, an L-band system is preferred. The midterm and far-term requirements are presented in Table 1. TABLE 1 R EQUIREMENTS FOR SURFACE DEFORMATION MEASUREMENTS  Minimum Displacement accuracy 3D displacement accuracy Displacement rate Temporal Accessibility Daily Coverage Map region Spatial resolution Geo-location accuracy Swath Data latency in case of event 25 mm instantaneous 50 mm (1 week) 2 mm/yr (over 10 y) 8days 6x106 km2 60 latitude 100 m 25 m 100 km 1 day  Goal 5 mm instantaneous 10 mm (1 day) 1 mm/yr (over 10 y) 1day Global (land) Global 10 m 3m 500 km 2 hours after acq.  Many claims have been made concerning the correlation of magnetic fields, electric fields and sesimicity, including precursory electromagnetic signals. Mechanisms to produce such correlative variations include movement of fluids in fault zones as a result of stress changes preceding ruptures, and piezomagnetic effects of stress field changes. Improvements in data quality and quanitity over the past 40 years has led to a substantial decrease in the correlated signals [3]. Magnetic anomalies associated with mainshocks are welldocumented and can be accounted for by piezomagnetic effects. The subject of precursory electromagnetic signals, and a satisfactory mechanism to explain them, requires more laboratory and field research, as well as high-quality continuous ground and satellite magnetic field data series with proper reference control. Recognizing subtle signals generated at the surface against the background of the highly dynamic external magnetic field at satellite altitude is challenging. These correlations are likely best tested using carefully configured ground networks in seismogenic zones. A strong emphemeral infrared (IR) thermal anomaly was observed near the epicenter of the October, 1999 Hector Mine earthquake. This and other suggested correlations of thermal IR anomalies and earthquakes has been studied with inconclusive results. As with electromagnetic anomalies, more robust correlations and plausible mechanisms are needed to assess this potential stress indicator. The current ASTER (Advanced Spaceborne Thermal Emission Radiometer) mis-  Observing interseismic strain accumulation drives the need for very precise long-term accuracy. To distinguish between blind thrust and shallow faults, and their hazards, requires deformation rates to be resolved at the 1mm/yr level over 10 years. Achieving this accuracy requires mitigating the tropospheric and ionospheric noise in the images, as well as reducing orbit errors. Fortunately, the process is steady, so stacking and filtering techniques can be used to remove these sources of noise [4]. Consequently, the length of the data series is more important than the revisit frequency and is on the order of 10 years for an L-band system.   Observation of coseismic deformation drives the need for precise instantaneous accuracy and short revisit times. Exponentially-decaying post-seismic processes will obscure the coseismic signals with time following the event. Also, good spatial resolution is needed to precisely map the decorrelation and displacement close to the rupture. Transient post-seismic strain, as well as aseismic creep and slow earthquakes drive the need for frequent revisit times to capture these events. These requirements can be met by a constellation of 6-24 SAR satellites in LEO or LEO+ (1325 km) orbits, or by 3-6 geosynchronous SARs [5]. A few LEO+ satellites can optimize most of the requirements, but to achieve the very short revisit requires a larger investment. IV. DYNAMIC EARTHQUAKE HAZARD ASSESSMENTS  ACKNOWLEDGMENT A portion of this work was performed at the Jet Propulsion Laboratory, California Institute of Technology, under contract with NASA. The Global Earthquake Satellite System Study is funded by the NASA Earth Science Enterprise. REFERENCES [1] F. Pollitz, C. Wicks and W. Thatcher, ""Mantle flow beneath a continental strike-slip fault: Postsesimic deformation after the 1999 Hector Mine earthquake,"" Science, v. 293, pp. 1814-1818, 2001. [2] G. Bawden,W. Thatcher, R. S. Stein, K. W. Hudnut, and G. Peltzer, ""Tectonic contraction across Los Angeles after removal of groundwater pumping effects,"" Nature, v. 412, pp. 812-815, 2001. [3] M. J. S. Johnston, ""Review of electric and magnetic fields accompanying seismic and volcanic activity,"" Surveys in Geophysics, v. 18, pp. 441-475, 1997. [4] D. T. Sandwell and Y. Fialko, ""Requirements for observing slow crustal deformation due to tectonic and volcanic processes in the presence of tropospheric noise and decorrelation,"" Final Technical Report, GESS Requirement Definition Study, unpublished. [5] S. N. Madsen, C. Chen and W. Edelstein, ""Radar options for global earthquake monitoring,"" this volume.  The underlying stress-strain dynamics of fault systems is generally unobservable, but this obstacle can be surmounted by comparing observations to numerical simulations to test and improve models of fault system behavior. Scaled measures of strain, such as the Local Ginzburg Criterion"" (LGC), a normalized measure of surface shear strain across faults, appears to be a proxy for the unobservable coulomb failure function that governs fault rupture. Developing and evolving models of complex fault systems and creating a community modeling environment will be key to exploiting the revolutionary advances in observing capability that are expected within the next 20 years. Capable models will ingest the observations in real-time and may adjust the earthquake hazard assessments based on the emerging system behavior. While predicting the time, location and size of a particular earthquake will remain elusive, much higher fidelity earthquake forecasts appear within reach. V. D ISASTER RESPONSE  Temporal revisit times on the order of hours following an event are required to effectively support disaster response efforts. While displacement maps are useful for understanding the dynamics of the rupture and to predict the transient post-seismic behavior, decorrelation maps will be most useful to the emergency workers on the ground. Decorrelation maps will indicate changes in the built environment, and zones of intense shaking that can focus response efforts. InSAR has the advantage of being an all-weather capability that is immune to illumination conditions. To satisfy the requirements for disaster response support, a dense LEO or LEO+ constellation, or 3-6 geosynchronous satellites will be needed. Such a constellation could provide global accessibility with 24 hour revisit time, while the geosynchronous constellation would allow a staring capability that would reveal the details of transient post-seismic behavior and could be particularly useful in the hours and days following a great earthquake to assess the stress transfer and loading of neighboring fault systems."
GX262-18-7840956	"GLOBAL.EARTHQUAKE.SATELLITE.SYSTEM  GESS A 20-YEAR PLAN TO ENABLE  EARTHQUAKE  PREDICTION  MARCH 2003   50  GLOBAL.EARTHQUAKE.SATELLITE.SYSTEM  Geosynchronous Architecture  CHAPTER FOUR  T  he most ambitious concept of the GESS study entails the operation of a  geosynchronous SAR constellation. While the deployment of such a constellation would be a massive under taking and require that major technological challenges be overcome, a constellation of high-altitude SAR satellites would offer advantages over lower-altitude sensors in the contexts of system operational flexibility and instantaneous accessibility of target areas on the ground (Figure 4.1). A system with such capabilities might be considered as a mission for the next decade. Geosynchronous orbits are unique because their orbital periods are equal to one Ear th day. A geostationary orbit is a special kind of geosynchronous orbit in which the orbital inclination is zero; viewed from the rotating Ear th, a satellite in a geostationar y orbit appears to remain fixed in the same position in the sky at all times. This proper ty makes geostationary orbits ideal for such applications as communications and meteorology, but it in fact makes them unusable for SAR missions. This is because the principle of aper ture synthesis -- the ver y principle for which SAR is named -- requires relative motion between the sensor and the scene under obser vation. When such motion exists, as it does for an inclined geosynchronous orbit, however, fine resolution can be obtained even from ver y great distances. Although a geosynchronous satellite follows an elliptical trajectory as dictated by Kepler 's laws of motion, due to the relative motion of the Ear th and the satellite, the nadir point traces out a ""figure-8"" pattern on the ground once per day (see Figure 4.2). Since radar instruments can also acquire images both during the day and at night, and unaffected by cloud cover, a high-altitude SAR constellation may be well suited to the task of 24-hour global hazard monitoring. Most of the Earth's sur face could be kept in view nearly continuously by a constellation of geosynchronous satellites.   GEO Sys t e m P a rameters A geosynchronous SAR at an altitude of about 35,800 km would be more than an order of magnitude farther from the Earth than any SAR mission to date. W hile this highaltitude vantage point would allow a geosynchronous SAR to view a large ground area, it would also require a large physical antenna and a great deal of transmitted power. The antenna size requirements are driven by the need to resolve the range-Doppler ambiguities that are inherent in a pulsed radar system; the power requirements are driven by signal-to-noise ratio (SNR) considerations. We envision a system with a 30-m-diameter L-band aperture antenna that transmits 60 kW of peak power over relatively long 1-ms pulses. The boresight of this antenna would be kept pointed in the nadir direction and the beam steered electronically to look either left or right. Any part of the sensor footprint could be illuminated with no more than 8 of electronic steering -- from this altitude, the limb of the Earth is only 9 from nadir. Data could therefore be acquired for  GEOSYNCHRONOUS.ARCHITECTURE  51  areas between 1000 and 6500 km ground range from nadir on either side of the platform ground track, corresponding to groundincidence angles of 10.666.4. The antenna could also be steered to areas with ground squint angles up to 60 on either side of the ground track with less than 8 of electronic steering in the azimuth direction. With such a wide swath, some of the system parameters would have to change a great deal between the near range and the far range. The bandwidth of the system might var y between 80 MHz at the steepest incidence angles to 10 MHz further out. A split-spectrum approach might be employed as in the LEO+ case in order to characterize the effects of the ionosphere. The signal polarization might also change over the sensor footprint. We envision that the spacecraft would be yaw steered to keep the antenna in a nominal HH polarization state for broadside acquisition geometries, implying that the polarization state would be mixed in squinted geometries.  100 90  Figure 4.1 Cumulative land coverage from both LEO+ and GEO constellations of three satellites. The GEO constellation achieves over 90%  Percent Cumulative Land Coverage  80 70 60 50 40 30 20 10 0 3 LEO+ 3 Geosynchronous 3 6 9 12 15 18 21 24  land coverage in approximately half the time of the LEO+ constellation.  Time (hours)   52  GLOBAL.EARTHQUAKE.SATELLITE.SYSTEM  Figure 4.2 Trace of a geosynchronous satellite at a 60 orbit inclination (shown as figure-8). Instantaneous field of view for a 5500-km SAR swath is also shown.  Other system parameters would change over the platform orbit as the velocity of the platform relative to an obser ver on the Earth varies. For an orbital inclination of 60, the relative velocity of the satellite would var y by a factor of two between about 1500 m/s when the satellite is at high latitudes and 3000 m/s when it is over the equator. The pulse repetition frequency (PRF) of the system would therefore var y between about 125 Hz and 250 Hz. Relevant system parameters are summarized in Table 4.1. Note that these relative velocities and PRFs are significantly lower than those of LEO systems.  Orbit , C o v e rage , C o nstella tion Although a geosynchronous SAR would have a ver y large footprint, its ""figure-8""  ground track would always remain in a fixed set of longitudes. A single geosynchronous sensor would consequently be unable to provide global coverage on its own. W hile the longitude-locked behavior of a geosynchronous SAR might prove advantageous if the mission objective were to provide coverage only of the Western hemisphere, the aim of this study has been the development of system whose scope is global. Consequently, we concentrate here on the use of multiple geosynchronous sensors. One architecture would be a constellation of ten satellites divided into five groups of two (Figure 4.3). The two satellites in each group would follow the same ground track and would be phased 180 apart, resulting in an interferometric repeat time of 12 hours. In   GEO ORBIT  GEOSYNCHRONOUS.ARCHITECTURE  53  Table 4.1 Geosynchronous  Altitude Inclination Inter ferometric Repeat Period VISIBLE SW AT H  35788 km 60 1 day  parameters.  Look Angle Ground Incidence Angle Ground Range From Nadir Slant Range Ground Squint Angle Footprint Area Subswath Width INSTR U MENT   1.68.0  10.666.4  10006500 km 3589239224 km  60 79,000,000 km2 400 km nominal  Antenna Diameter Electronic Steering Wavelength Polarization Peak Transmit Power Pulse Duration Bandwidth Pulse Repetition Frequency PERFORMANCE  30 m  8 in azimuth and elevation 24 cm (L-band) varies with squint angle 60 kW 1 ms 1080 MHz 125250 Hz  Ground Range Resolution Stripmap Azimuth Resolution Nominal SNR Nominal Range Ambiguity Level Nominal Azimuth Ambiguity Level  20 m nominal varies over orbit (220 m nominal) 10 dB 30 dB 20 dB   54  GLOBAL.EARTHQUAKE.SATELLITE.SYSTEM  our initial analyses, we assume that the five groups would be equally spaced in longitude, though in an optimized constellation design, the groups might be unevenly spaced for better performance over high-priority areas. Each satellite would be in a nearly circular geosynchronous orbit with a 60 inclination. At any given time, this ten-satellite constellation would have about 80% or more of the Earth's surface in view. Approximately 20% of the surface would be in view of one or more satellites continuously, and approximately 80% of the surface would be in view nearly continuously (about 90% of the time). Gaps in coverage would be less than one hour for about 70% of the surface to less than five hours for 100% of the surface. The areas associated with longest gaps in coverage would be located mainly near the poles  and at certain longitudes near the equator. For a great majority of the Earth, a 20-m resolution image could be acquired on average within 10 minutes of an emergency event. The 3-D displacement accuracy of the system would be excellent (Figure 4.4). Because the antenna beam could be squinted both for ward and backward via electronic steering, data could be acquired from a great diversity of viewing angles. Moreover, because a target area could be viewed for long periods of time, a great deal of data could be acquired of this area, albeit at the expense of acquiring data of other areas. Typically, hundreds of images of a target area could be acquired during each interferometric repeat cycle. These data could be combined in a least-squares estimation procedure in order to reduce noise induced by  Figure 4.3 The maximum revisit time for a constellation of ten GEO satellites, five ground tracks, 12-hour phasing, at an inclination of 60.  Percentage Coverage (numbers in boxes)   GEO temporal decorrelation and tropospheric and ionospheric effects. We expect that 3-D displacement accuracies of a few millimeters could be achieved in 2436 hours.  GEOSYNCHRONOUS.ARCHITECTURE  55  Instrument Mo des Most of the time, full-resolution capability would not be required. Rather, the instrument might instead be operated in various interferometric ScanSAR modes that could yield data over swaths thousands of kilometers wide (each subswath would be up to 400 km wide). Because the antenna beam would be electronically steered exclusively, data could even be acquired from nonadjacent subswaths-perhaps on opposite sides of the ground track-or from beams squinted  both for ward and backward. The large antenna footprint would make ScanSAR ground patches hundreds of kilometers wide in the along-track dimension. Moreover, with ScanSAR bursts up to several minutes long, the instrument operation could easily be timed well enough for repeat-pass ScanSAR interferometr y. Alternately, the instrument could operate in a standard stripmap mode, or the antenna beam could be steered to dwell on particular areas of interest in a spotlight mode so that large amounts of high-resolution data from various viewing angles could be collected over key seismogenic areas. Between these different modes, the instrument would allow great flexibility in operation.  Figure 4.4 Worst-case component of the 3-D vector displacement accuracy for a constellation of ten geosynchronous SAR satellites. The units are relative to the single-image line-of-sight displacement accuracy. For each satellite pass, an image is assumed to have been acquired of the visible ground points every 30 in ground-point Percentage Coverage (numbers in boxes)  azimuth. Maximum 3-D Displacement Accuracy (cm)   56  GLOBAL.EARTHQUAKE.SATELLITE.SYSTEM  The system might have numerous modes that are tailored to different situations and changing priorities. A typical operational plan might involve the daily or twice-daily creation of multiple maps of certain highpriority seismogenic areas (as described in the LEO+ scenario) as well as low-resolution global maps ever y few days, or highresolution global maps ever y few weeks. The operational plan could also be modified as needed in response to global hazard events and current conditions.  Pe r f o r manc e Because the cur vature of the relative platform motion changes a great deal over the orbit, the stripmap-mode azimuth resolution depends on the satellite latitude and location of the target in the sensor footprint. W hen the instrument is looking towards the outside of each loop in the figure-8 pattern, the stripmap-mode azimuth resolution can be as coarse as 20 m, yet it can be finer than 2 m when the instrument is looking towards the inside. That is, in some cases, the orbit cur vature would effectively cause the antenna beam to dwell on a particular ground area in a manner similar to that of a spotlight-mode SAR acquisition, even when the antenna beam is kept pointing toward broadside. Azimuth resolutions of around 2 m or better might also be obtained anywhere in the swath or along the orbit when the instrument is operating explicitly in spotlight mode. The processing involved for attaining 2-m resolution would not be trivial, however. The required synthetic aperture length for such a resolution would be more than 200 km, over which the cur vature of the platform motion would change significantly.  In fact, a different azimuth-compression reference function would be required for each along-track sample position. Moreover, the slant-range depth of focus might be even smaller than the nominal slant-range resolution of around 1020 m if 2-m resolution is desired. Another difficulty of highresolution processing would be posed by the variability of the ground scene and of the atmosphere over the time required by the satellite to traverse the long synthetic aperture distances; up to 25 min may be required in some situations. In practice, 2-m resolution may be very difficult to achieve, though 510-m azimuth resolutions should be readily attainable. Note that while fine spatial resolution is not always required for geophysical applications, high-resolution data can be averaged spatially in order to mitigate the effects of temporal decorrelation in interferometric data. Indeed, as in the LEO+ case, we expect that the interferometric displacement accuracy will be limited more by temporal decorrelation as well as by tropospheric or ionospheric effects than by the instrument performance. Given the large amount of data that a geosynchronous system can acquire of a targeted area over a short period of time, though, it is not unreasonable to expect that with sufficient averaging, the displacement accuracy can be reduced to the level of a few millimeters for specific target sites. O ver larger areas, we expect that subcentimeter displacement accuracies would be typical. The instrument would also allow for excellent 3-D displacement accuracy over much of the Earth since most points on the ground could be imaged from diverse sets of   GEO viewing angles. This diversity of viewing angles comes from the both the lateral cur vature of the ground track as well as the variation in ground squint angles that are possible through electronic beam steering from high altitudes.  GEOSYNCHRONOUS.ARCHITECTURE  57  ground stations might be able to handle all of the data from an entire constellation.  Geosynchr o nous SAR P a yload Description The geosynchronous SAR instrument is dominated by the ver y large deployable antenna. Because of the large antenna area, our mission concept integrates the SAR antenna with the spacecraft structure and subsystems as illustrated in Figure 4.5. The 30-m-diameter antenna aperture is deployed with horizontal booms and then tensioned to maintain flatness with two asymmetric axially deployed masts and tensioning cables. The antenna aperture is constructed from flexible membrane material that is integrated with the active electronics for proper beam formation and transmit/receive signal amplification. The antenna flatness must be maintained to within 1/20 of a wavelength, or roughly 1 cm across the entire aperture. This type of flatness requirement can be achieved using the axial booms and membrane tensioning cables. D ue to the challenge of such a flatness requirement, the proper calibration of the full array is essential for successful beamforming. The principal array error sources include feed errors caused by variations in electronic components, and element displacement caused by mechanical deformation of the array surface. The removal of displacement errors is somewhat problematic. Only an external calibration system can address the problem of element displacement. There is a trade-off between mechanical rigidity (and thus, surface accuracy) and mass. In order to minimize the launch mass, it is desirable to tolerate greater  Da ta Ra tes and V o lume Although the footprint of a geosynchronous SAR's accessible area would be much larger than that of a sensor in a lower orbit, the instantaneous data rates of the two would be comparable (on the order of 100200 Mbs). This is because neither sensor would be able to acquire full-resolution data over its entire footprint simultaneously; data would need to be collected over smaller subswaths. W hile the subswaths of a geosynchronous SAR would be considerably wider than those of a LEO SAR, the Earth-relative velocity is slower. Despite comparable data rates, because the geosynchronous sensor is almost always in sight of land, even when the sensor itself is over water, the instrument could acquire data nearly continuously. The total data volume could therefore be on the order of 12 TB per satellite per day, or around 510 PB per satellite over the satellite's relatively long 15-year life. These values would increase by a factor of two or four if multiple radar polarizations were used. Such data rates and volumes would require specialized downlink and data-handling facilities. Since geosynchronous satellites would be in view continuously from low-latitude ground stations for long periods of time, however, one or two dedicated ground stations could be used to downlink the data from each satellite or each pair of satellites in the same ground track. Furthermore, with appropriate intersatellite communication, one or two   58  GLOBAL.EARTHQUAKE.SATELLITE.SYSTEM  surface errors. The added complexity of an external calibration system must be weighed against the benefit of lighter structural elements. There are several approaches to external calibration. An onboard metrolog y system (either optical or RF) can be used. Feed errors include all (manufacturing, thermal or aging related) amplitude and phase errors in feed networks, interconnects and T/R modules. There are several proven approaches to compensating for feed errors using loop-back calibration loops. If the feedback paths are properly designed to eliminate leakage from adjacent elements, this calibration can be performed continuously during normal radar operation. Thin-film solar arrays provide power to the antenna and spacecraft. These arrays are an integral part of the system configuration and share the same structural elements. One solar array is an annular ring formed around  the perimeter of the radar antenna aperture. Solar arrays also partially cover the rear or zenith-pointed side of the spacecraft, stretching up the tensioning cables as shown in Figure 4.5. The solar array ""tent configuration"" provides a large surface area for solar power collection from any sun orientation but will also result in high solar pressure on the solar arrays making station keeping more challenging and costly. To minimize the effect, we have lengthened the axial boom for steeper angles of the rear-facing solar array. We have also reduced the size of the solar arrays as much as possible. The top of the solar array ""tent "" is open because the thermal subsystem requires a window to space for radiating excess thermal energy. The power system also includes sufficient batteries to operate for short periods in eclipse. On the tips of each mast are propulsion modules for orbit maintenance. The antenna is mounted  Figure 4.5 Geosynchronous SAR large deployable antenna concept.  Selected Subsystems of Spacecraft Bus  Pre-tensioned Cables (x 24)  Thin-Film Solar Arrays Horizontal Booms (x 12) L-band RF Membrane Antenna Aperture  Telescoping Booms (x 2) Propulsion Modules ( 2)   GEO to a centralized spacecraft bus that also houses the radar central processor/controller. The high-radiation environment of the geosynchronous orbit poses a substantial risk to the SAR electronics, and degrades the performance and reliability of the solar arrays. Long-term exposure can cause device threshold shifts, increased power consumption, and device damage. Radiation-hardened devices and shielding can mitigate total dose effects. The expected total dose for a 15-year mission behind 30 mils of aluminum is 15 Mrad for a geosynchronous orbit inclined to 60. Clearly, this presents a major technical challenge. The total dose decreases exponentially with shielding, and therefore the expected total dose behind 100 mils of aluminum is reduced to 600 krad. This becomes a more manageable problem. However, only limited shielding can be employed for the electronics on the antenna. Greater levels of shielding will be very massive and bulky, and perhaps not compatible with membrane antenna technology. Therefore, we consider the radiation environment to be one of the biggest challenges to realizing this system. A more detailed description of the radar system architecture and antenna design will be presented in Chapter 6. The spacecraft bus contains the subsystems required to perform all spacecraft housekeeping functions and radar control and data handling functions. The bus also supports the deployable booms, masts, antenna aperture, and solar arrays. Twelve horizontal booms deploy, support, and tension the antenna aperture and solar array. These booms each have a fully deployed length of 19 m and are supported by, as well as stowed for launch in,  GEOSYNCHRONOUS.ARCHITECTURE  59  the spacecraft bus. Self-rigidizable springtape-reinforced (STR) inflatable booms of 10 inches (0.25 m) in diameter were selected as the baseline booms. Two highstiffness, telescoping Able Deployable Articulated Masts (ADAM) developed by AEC-Able provide axial support for the large antenna and solar arrays. The masts will be deployed axially from the top and the bottom of the spacecraft bus. The masts are asymmetric; the nadir-pointing mast has a fully deployed length of 19 m and the upper mast used to support the solar arrays has a length of 38 m. This configuration was chosen to maximize solar array efficiency and to reduce mass and solar pressure effects. Each ADAM mast has a linear mass density of 1.3 kg/m and is stowed in a dedicated canister for launch. The canister for stowing the upper ADAM mast also acts as the central mandrel for packing the antenna aperture. This means that the membrane aperture will be wrapped around the upper canister for launch. The antenna aperture is formed by three layers of thinfilm membranes. The 30-m-diameter membrane antenna aperture is integrated with all of the distributed transmit, receive, and control electronics including T/R modules, true-time-delay (TTD) digital transceivers, power converters, and signal-distribution and interconnect technologies. Twenty-four pre-tensioned cables are used to stiffen the horizontal booms. The upper ADAM mast supports 12 of these cables, and the lower ADAM mast supports the other 12 cables. The solar arrays that provide all power for the instrument and spacecraft housekeeping functions are made with flexible, amorphous thin-film technology that is 13% efficient. The areal mass density of these solar arrays   60  GLOBAL.EARTHQUAKE.SATELLITE.SYSTEM  is assumed to be about 0.63 kg/m2. The front annular-ring-shaped solar array will be packaged for launch and deployed in space together with the membrane antenna aperture. The rear arrays will be deployed and supported by the 12 upper cables. Two propulsion modules are located at the tips of the ADAM masts. Each propulsion platform consists of hydrazine thrusters, solar-electric ion propulsion engines, and supporting elements of the propulsion subsystem. The total dr y mass of the integrated geosynchronous SAR flight system is roughly 4500 kg (with contingency). A breakdown of mass and power is provided in Table 4.2. The geosynchronous SAR integrated flight system will be packaged for launch as shown in Figure 4.6. The total launch stack height is 8 m, which will fit in a Delta IV fairing. The large antenna must be stowable with high packaging-efficiency in order to physically fit into the spacecraft launch vehicle. Since low mass and low stow-volume are driving requirements, a flexible mem-  brane antenna architecture was selected. Rigid honeycomb panels, such as that used for SIR-C and SRTM antennas, were not considered due to their larger mass and stowage volume.  Mission Design Because of the large area of the antenna, our concept of the mission involves the integration of the antenna and the spacecraft structure as just described, with rigidizable booms, deployed radially as illustrated in Figure 4.7. An alternative, more-traditional design involving a separate articulated solar array was examined but not selected because of the mechanical problems involved. The solar arrays of the current working design are sized such that sufficient power would be available regardless of the satellite orientation with respect to the Sun. The spacecraft would also carr y enough batter y capacity for three minutes of instrument operation during eclipse. This would allow images with azimuth resolutions of 10 m or better to be obtained in emergency situations if necessar y. Eclipses would occur once per day during each of two month-long seasons per year, with eclipses of up to 70 minutes duration. The large central opening in the solar arrays over the back of the radar antenna aperture is required for the radiators that would provide thermal control. Gimbaled propulsion units with both chemical and ion thrusters as well as their propellant tanks would be located on azimuthally rotating platforms at the ends of the main masts. These thrusters would be used for orbit maintenance and for dumping the built-up momentum of the reaction wheels used to provide attitude control and yaw steering. The reaction wheels themselves  Figure 4.6 Major components of a GEO SAR flight system. Central Mandrel (with the membrane aperture wrapped around) Propulsion Box #1  Spacecraft Bus  Propulsion Box #2   GEO SY STEM/SUBSY S TEM N O . OF UNITS UNIT P O W ER ( W )  GEOSYNCHRONOUS.ARCHITECTURE  61  TO TA L P O W ER ( W )  U NIT M ASS ( k g)  TO TA L MASS ( k g)  Table 4.2 Geosynchronous SAR mass and power estimate.  Spacecraft Spacecraft Bus Flex Solar Array Batteries Propulsion Modules Radar Antenna Structure Antenna Membrane Aperture ADAM Mast (nadir) ADAM Mast (zenith) Able Mast Canisters Horizontal Boom End Cap Radar Electronics Central Processor (CPU) Digital Receivers T/R Modules Feed Probes/Interconnects Optical Fiber Distribution Power Distribution Cabling Power Converters Spacecraft Total (Dry) Contingency (30%) Spacecraft Total (Dry) with Contingency Propellant Star 48 V engine for orbit circularization Launch Mass Launch Vehicle Capability (Delta IV 4050 Heavy) to GTO Launch Mass Margin (30%) 1 61 15616 15616 1 1 244 1000 50 2 36 1 1 2 12 24 0 0 0 0 0 0 1 12 2 2 1091 158 0 2621  8226 1091 1894 0 5241 0 0 0 0 0 0 0 28050 1000 3050 24000 0 0 0 0 36276 10883 47159 25 0.1 0.006 0.001 30 90 0.1 4.242 24.7 49.4 20 9.5 0.25 1166 76 214 154  2810 1166 909 428 307 387 152.7 24.7 49.4 40 114 6 286 25 6 94 16 30 90 25 3482 1045 4527 752 2455 7734 11000 3266   62  GLOBAL.EARTHQUAKE.SATELLITE.SYSTEM  Figure 4.7 The geosynchronous SAR flight system will undergo a four-step deployment sequence: 1. The system is separated from the fairing of the launch vehicle. See Figure 4.7(a) and (b). 2. The 12 horizontal booms and the two ADAM masts deploy. See Figure 4.7(c). The 24 tensioning cables also deploy simultaneously with the ADAM masts. These cables will be tensioned to a pre-determined level when the masts are fully extended. 3. The membrane antenna aperture, together with the annular front solar array, start to deploy by the actuation of a set of wire and pulley mechanisms. The various stages of this deployment process are illustrated in Figure 4.7(d) and (e). The fully deployed membrane aperture and ring-shape solar array can be seen in Figure 4.7(f ). 4.The rear solar arrays are deployed by a second set of wireand-pulley mechanisms. Figure 4.7(g) shows the half-deployed cone-shaped solar array and Figure 4.7(h) shows the fully deployed array. It should be noted that in these two figures a full cone is shown for this array; however, the current baseline design uses only a partially filled configuration. (g) Deploying cone-shaped solar array. (h) Deployed GESS system. (e) Deploying membrane aperture. (f ) Fully deployed aperture. (c) Booms and masts deploy. (d) Unrolling of membrane aperture. (a) Ejection from the fairing. (b) Ready to deploy.   GEO would be located near the spacecraft center of mass. Also located at the end of the nadir mast would be a telecommunications package, including a high-gain antenna, and an Earth sensor for attitude control. The packages located here would be relatively small compared to the size of the radar antenna, so they would not interfere with the operation of the instrument. Precise knowledge of the platform position and attitude would be obtained from the Earth sensor, Sun sensors, star trackers, and a reverse GPS system. Aside from the technological difficulties posed by the instrument, several key issues need to be addressed in the overall mission design. Because of the great distance of the radar from the Earth and the consequent need for large amounts of power (up to 37 kW DC total for the spacecraft, including 28 kW for the instrument, in addition to power for telecom, solar-electric ion propulsion, but not including contingency), a large area would be required for the solar arrays. Not only would the solar arrays constitute a large fraction of the total spacecraft mass, they would present a large surface area over which solar pressure would exert forces that perturb the spacecraft trajector y and attitude. Several thruster fires per day would be needed to maintain orbit control tightly enough for high-precision repeat-pass interferometr y. It is not clear how thruster firings, whether chemical or electric, would affect the flatness of the radar antenna and the subsequent system performance. The spacecraft mass, including contingency, would be around 4500 kg. In order to maintain an adequate launch vehicle margin of 30%, we require a launch vehicle such as a Delta IV 4050 Heavy, as well as a Star  GEOSYNCHRONOUS.ARCHITECTURE  63  48 V upper stage (2455 kg). The launch vehicle puts the spacecraft into geosynchronous transfer orbit (GTO), and the Star 48 V provides the apogee burn to produce a final geosynchronous orbit. We further note that more-optimal designs of several subsystems could potentially result in mass savings of at least several hundred kilograms.  Co s t A rough-order cost estimate for the GESS geosynchronous SAR mission concept was derived from Team X cost models and technolog y projections. With a technology cutoff year of 2010, a 5-year development, and 15-year mission operations, the total cost for the first satellite is in the range of $12 billion. A ten-satellite constellation will cost roughly $810 billion.  Fu t u re Oppor t unities S eismology from Space Using Ka-Band GEO S a tellit e  Broadband digital seismometers -- the current state of the art in seismology -- offer continuous three-component surface displacements with sensitivities in the micron range. Although much of the world is covered with var ying densities of broadband seismometers, they remain isolated to discrete point locations. A satellite-based, continuous measurement of surface displacements with temporal and spatial sensitivities appropriate for seismic waves (submillimeter displacements, 0.011-Hz sampling) would represent an important step for ward in understanding earthquake physics and solid Earth structure and dynamics. Current models for 3-D velocity structure of the   64  GLOBAL.EARTHQUAKE.SATELLITE.SYSTEM  Earth are based on propagation of seismic waves as measured at discrete points. Spatially continuous mapping of seismic wave phases from large earthquakes would allow great improvements in crustal and lithospheric heterogeneity models, and would be synergistic with current developments in super computing models of 3-D Earth structure and seismic wave propagation (Figure 4.8). Detailed images of complex wave interference would generate significant improvements in basin seismic resonance and earthquake hazard assessment. By seeing the rupture unfold and obser ving the amplitudes of the seismic waves generated along the rupture, more-precise models would be determined for the rupture dynamics. This should lead to a fundamental improvement in our knowledge of how earthquakes initiate and why they stop. The development of a radar mission for monitoring seismic waves involves tremendous technical challenges, but it is not outside the realm of possibility as a long-term goal. For now, we assume that the science requirements for such a mission dictate a horizontal resolution of 10 km, a vertical resolution of 100 m, and a temporal resolution sufficient for following 5 km/s seismic waves. W hile most spaceborne imaging radars employ aperture synthesis techniques for obtaining high horizontal resolution, the high-temporal resolution and wide-area coverage requirement of this application preclude the use of such techniques. That is, temporal sampling must occur on time scales finer than those possible for a high-altitude sensor to traverse a synthetic aperture length. Lowaltitude sensors would be unable to provide sufficient coverage. We consequently envision a geostationar y, real-aperture Ka-band system. That is, in contrast to the geosynchronous  SAR, this platform must be maintained so that it does not move relative to the Earth surface. A constellation of three to five satellites could provide coverage for all equatorial and moderate-latitude regions of the Earth. A short wavelength is desired so as to maximize the interferometric sensitivity to small surface displacements and to minimize the required antenna area, but the frequency must not be so great as to cause pulse-to-pulse decorrelation through frequency drift. We select the Ka-band frequency of 35 GHz (8.57-mm wavelength) for this analysis as a compromise. Therefore, in order to obtain 10-km horizontal resolution at a latitude of 60, an antenna with a diameter of approximately 110 m is required. Assuming nominal system parameters, the high antenna gain implies that the singlepulse SNR will be greater than 40 dB for incidence angles up to 68 with 50 kW of radiated power and a normalized backscatter coefficient 0 of 20 dB. For this computation, we assume that a 1-s gated continuous wave (CW ) pulse is transmitted. (Range resolution is not required, as successive pulses are to be interferometrically combined.) Thermally induced phase noise will be much less than the 2.2 (25 dB SNR) threshold required for an interferometric vertical accuracy of 100 m. Successive pulses for each 10 km ground cell will need to be spaced by approximately 1 s if the wave speed is 5 km/s. Given the time between pulses, tropospheric artifacts may pose a significant limitation on the achievable vertical accuracy of the system (see Chapter 5, Optimizing the Measurement). O ver 1 s, the tropospheric delay can var y by approximately 100 m or more, depending on local condi-   GEO  GEOSYNCHRONOUS.ARCHITECTURE  65  Figure 4.8 Modeled propagation of seismic waves from the deep, magnitude 8.2 Bolivia earthquake of June 9, 1994. The earthquake was so large that it produced a permanent displacement of the surface of the Ear th of several millimeters near the epicenter in Bolivia. (Komatitsch and Tromp, 2002)  tions. Another factor limiting the system's vertical accuracy may be pulse-to-pulse decorrelation caused by changes of the ground surface between pulses; for example, from disturbances of the local vegetation by the wind. This effect will depend on the scene and its dominant scattering mechanisms at Ka-band. Moreover, it is not clear how the scattering centers of surface features such as vegetation move in response to a seismic wave propagating through the ground. The total surface area that can be obser ved by the system during a seismic event is determined by the number of resolution cells that can be scanned within the interferometric repeat time of one second. We assume that the radar antenna is electronically steered to transmit pulses towards different ground cells at a 30-kHz pulse repetition frequency (PRF) during a transmit window equal to the 0.25-s round-trip pulse travel time. The radar  then receives each of the transmitted pulses in turn. The total area covered in 1 second is approximately 1012 m2, equivalent to a 1200-km-diameter circle. The most important technological hurdle for such a system will likely be the design, construction, and deployment of a large, high-frequency antenna capable of fast, accurate electronic steering. Advanced onboard processing hardware and algorithms will also be required since the system must detect and respond adaptively to seismic events in real time. Stringent requirements may also be placed on spacecraft control and pointing given the solar pressure exerted on the large antenna. Note that if the horizontal resolution were relaxed from 10 km to 30 km, the antenna diameter could be reduced by a factor of three, and the area coverage rate would increase by a factor of nine.   JPL 400-1069 03/03"
GX261-98-2515418	"GLOBAL.EARTHQUAKE.SATELLITE.SYSTEM  GESS A 20-YEAR PLAN TO ENABLE  EARTHQUAKE  PREDICTION  MARCH 2003   28  GLOBAL.EARTHQUAKE.SATELLITE.SYSTEM  Enhanced Low-Earth Orbit CHAPTER THREE  T  he GESS enhanced low-Ear th orbit concept, or LEO+, proposes the operation  of one or more L-band SAR systems at an altitude of approximately 1325 km, significantly higher than those of most current SAR platforms. The higher platform altitude affords a wider visible area to the sensor: two 800-km swaths, each comprising seven subswaths, from 300 to 1100 km on either side of the satellite ground track. The large viewable area enables the system to access the entire Ear th quickly, reducing the inter ferometric repeat period to six days and allowing for much finer InSAR temporal resolution than is available from other current or pending SAR missions. The higher altitude also offers better orbit stability, another impor tant consideration for repeat-pass inter ferometr y. A LEO+ mission could be flown using existing conventional technology, though the system would require a slightly larger antenna and greater power than LEO SAR systems. The radar would be capable of generating high-resolution single-subswath images in a standard stripmap mode and provide lower-resolution, wider-swath images from multiple subswaths in inter ferometric ScanSAR mode. In stripmap mode, the five-look image resolution would be 30  30 m or better. The inter ferometric  sur face-displacement accuracy would depend mainly on the temporal correlation proper ties of the sur face under obser vation, as well as on atmospheric effects, but with appropriate calibration and post-processing, nominal line-of-sight displacement accuracies better than 1 cm can be achieved. The instrument would acquire right-looking and left-looking data on ascending and descending passes, so it would be possible to synthesize 3-D maps of sur face displacement from multiple inter ferograms with different viewing angles. Over targeted areas, a high-resolution 3-D displacement map comprising ten or more individual images could be generated in under 12 days, and global maps generated annually.   LEO+ Sys t e m P a rameters The radar would transmit 10 kW of peak power from a 3.5  13.5 m aperture L-band (24 cm wavelength) antenna that is mechanically steered to a fixed position looking either to the left or the right of the platform direction of motion. Each subswath on a given side would be illuminated through 10 of electronic beam steering. Because the system performance would degrade somewhat with increasing slant range and incidence angle, the inner four subswaths (14) on each side are denoted ""primar y beams"" while the outer three subswaths (57) are denoted ""extended beams."" Across the entire swath, the ground incidence angle varies from 15 to 47. Instrument parameters and performance measures for each subswath are summarized in Table 3.1.  ENHANCED.LOW-EARTH.ORBIT  29 29  A split-spectrum approach would be employed for ionospheric correction (see Payload Description for further detail) so that transmitted pulses occupy two distinct subbands of the 80 MHz L-band frequency allocation. The subband pulse bandwidth would be 20 MHz in subswaths 37, while the steep incidence angles in subswaths 1 and 2 would require a somewhat larger total bandwidth in order to maintain the required ground-range resolution. (Because the surface would also reflect more radar energ y at steep incidence angles, SNR performance would not be sacrificed.) The pulse repetition frequency would be nominally around 1200 Hz.  Figure 3.1 The maximum revisit time for any given spot for a single LEO+ SAR at approximately 1325 km altitude and 100 inclination, with a six-day repeating ground track. The percentage of the Earth's surface that would be revisited within a given time frame is given in the legend below.  Percentage Coverage (numbers in boxes)   30  GLOBAL.EARTHQUAKE.SATELLITE.SYSTEM  Table 3.1 LEO+ beam summary.  1  2  3  4  5  6  7  Ground Range from Nadir (km, near/far) Look Angle (deg, near/far) Incidence Angle (deg, near/far) Transmit Power (peak, kW ) Pulse Duration (s) Bandwidth (MHz) Polarization Pulse Repetition Frequency (Hz) Ground Range Resolution (m, near/far) Single-Look Azimuth Resolution (m) Minimum SNR Assuming Model Soil Surface (dB) Maximum Range Ambiguity Level (dB) Maximum Azimuth Ambiguity Level (dB) Data Rate (Mb/s)  300/450  445/595  590/740  735/835  830/930  925/1025  1020/1090  12.7/18.5  18.4/23.7  23.5/28.4  28.2/31.1  31.0/33.6  33.5/36.0  35.8/37.4  15.4/22.6  22.3/29.1  28.8/35.0  34.8/38.6  38.4/42.0  41.8/45.2  45.0/47.2  10.0  10.0  10.0  10.0  10.0  10.0  10.0  50.0 17.0 HH 1240  50.0 12.0 HH 1191  50.0 10.0 HH 1233  50.0 10.0 HH 1178  50.0 10.0 HH 1224  50.0 10.0 HH 1176  50.0 10.0 HH 1220  28.8/19.9  28.5/22.3  26.9/22.6  22.7/20.8  20.9/19.4  19.5/18.3  18.4/17.7  6.0  6.3  6.0  6.3  6.1  6.3  6.1  10.4  9.8  9.6  11.1  11.0  10.3  10.3  35.0  36.0  29.5  32.8  26.8  23.7  20.1  22.1  20.9  21.8  20.4  21.7  20.5  21.5  142.7  124.6  128.6  95.2  105.9  107.8  84.9  Orbit , C o v e rage , and C o nst ella tions The satellite would be launched into a nearly circular, sun-synchronous terminator orbit at an altitude of 1325 km and an inclination of 101; this orbit has a six-day repeating ground track (Figure 3.1). The satellite would be controlled under tight attitude and trajector y constraints in order to facilitate repeat-  pass interferometric processing. Given the satellite orbit and the capabilities of the radar instrument, 85% of the Earth's surface would be viewable by the satellite within 24 hours, and 100% of the surface would be viewable within 60 hours. This quick-response capability of the SAR could provide timely data in the crucial hours and days following an earthquake or other natural disaster.   LEO+ A constellation of identical satellites in phased, node-spaced orbits could provide even shorter interferometric repeat times and event-response times (see Figure 3.2). With a constellation of four satellites, the interferometric repeat period could be reduced to 36 hours, and an image of any point on the Earth could be formed by at least one satellite within about 12 hours of an event (or six hours for 85% accessibility). Multiple satellites could also be placed at different orbital inclinations in order to enhance the achievable 3-D surface displacement accuracy. That is, while near-polar orbits are required to provide Earth coverage at high latitudes, they do not offer much diversity in viewing angle at very low and ver y high latitudes. Hence, in equatorial regions of the Earth for example, the north south component of surface displacement could not be very accurately determined. Therefore, in addition to the satellite(s) inclined 101, one or more satellites could be placed in lower-inclination orbits in order to  ENHANCED.LOW-EARTH.ORBIT  31 31  increase the orthogonality of the directions from which different areas are mapped (Figure 3.3). A more ambitious constellation of 36 satellites could reduce the interferometric repeat time to four hours and could allow most points on the Earth to be imaged within around two hours or less.  Instrument and Op er a t ional Modes Each satellite could be operated in both high-resolution, single-subswath stripmap modes and wide-area, multiple-subswath interferometric ScanSAR modes (100-m resolution at eight looks). Note that in the interferometric ScanSAR modes, the instrument would need to be timed such that corresponding ScanSAR bursts are aligned between successive orbit passes. This mode of operation has not been demonstrated, but we expect that it is feasible. If the instrument collects data for one-third of the time it is over land, its operational duty cycle would be approximately 10% on average,  100 90 80  Figure 3.2 A comparison of cumulative land coverage by different  Percent Cumulative Land Coverage  70  LEO and LEO+ 60 50 1 LEO + 40 30 1 LEO 20 10 0 6 12 18 24 30 36 42 48 54 60 66 72 Time (hours) 2 LEO 4 LEO 8 LEO 2 LEO + 4 LEO +  constellations. A four-satellite LEO+ constellation covers 90% of the Earth in six hours.   32  GLOBAL.EARTHQUAKE.SATELLITE.SYSTEM  Figure 3.3 Maximum 3-D accuracy from two LEO+ satellites at 101 and 30  inclinations, assuming 1 cm line-of-sight displacement.  Percentage Coverage (numbers in boxes)  though the duty cycle might be significantly higher for some orbits. Each year, the instrument 's operational plan would most likely include the collections of different global data sets in various high- and low-resolution modes for archive in an interferometric librar y (see Table 3.2). The satellite would also be tasked to collect data as frequently as possible from important seismogenic areas, such as Southern California and other parts of western North America, in addition to other seismogenic zones in South America, Asia, the Mediterranean, and others. A typical six-month operational plan might consist of 36 days spent acquiring a global, low-resolution, primar y-beam, interferometric ScanSAR data set, and 144 days spent acquiring four to six high-resolution maps of western North America and two to three high-resolution maps of other key areas. As  possible, data would also be acquired over other areas to fill in coverage for global highresolution maps. The ground resolution depends on which operational mode is in use. The ground resolution for both primary (1-4) and extended beams (5-7) is 30 m in stripmap mode, and 100 m in ScanSAR. The number of looks available also changes for different modes both across and between subswaths. The primar y beams have five looks in stripmap mode, and 14 in ScanSAR mode; while the extended beams have five looks in stripmap mode, but 18 looks for ScanSAR. The ground swath for each beam can be calculated from the ground range values in Table 3.1. For stripmap mode, the ground swath in beams 1-7 in kilometers is: 150, 150, 150, 100, 100, 100, 70. In ScanSAR mode, however, the primar y-beam ground swath is 535 m, and the extendedbeam ground swath is 260 m. Additional   LEO+ instrument parameters do not var y for subswath or operational mode:  RF peak power is 10 kW  Average orbit duty cycle is 10%  Peak DC power is 3816 W An illustration of the operational modes and beams is shown in Figure 3.4.  ENHANCED.LOW-EARTH.ORBIT  33 33  Pe r f o r mance W hile the studied system parameters do not represent a final, fully optimized design, they maintain a signal-to-noise ratio of at least 10 dB over the entire visible area, assuming a model scattering profile for a soil surface and incidence angles determined by a nominal spherical Earth. The preliminar y design yields a range ambiguity level below 30 dB in the primar y subswaths and 20 dB in the extended subswaths, and an azimuth ambiguity below 20 dB in any subswath. Note also that the overall performance is generally better in the middle of a subswath than at its edges, and that the subswath widths can be increased slightly if reduced performance is acceptable in the extended areas. For the stripmap modes, the nominal along-track (azimuth) resolution would be 6 m, and the nominal cross-track (range) resolution projected  onto the ground would be 30 m or better. Performance parameters are summarized in Table 3.1, referenced previously in the System Parameters section. The interferometric displacement accuracy of the system would be highly dependent upon the properties of the surface and the atmosphere at the time data is acquired. Under ideal conditions, line-of-sight displacement accuracies of a few millimeters might be possible from a single interferogram at 30 m resolution, but the performance would degrade considerably in the presence of temporal decorrelation or atmospheric variability. Temporal decorrelation comes about when, rather than bulk displacement, the surface exhibits random change that makes the interferometric phase noisier. Decorrelation-induced phase noise tends to become more severe as the temporal baseline is increased, but it can also be reduced through averaging, either spatially or over multiple interferometric pairs. Atmospheric artifacts result from the spatial and temporal variations in the effective radar signal path length related to changes in the propagation properties of the troposphere and ionosphere. These artifacts are more difficult to remove,  Global Primary-Beam ScanSAR Global Extended-Beam ScanSAR Priority-Area Primary-Beam Stripmap Priority-Area Extended-Beam Stripmap Global Fill-In Primary-Beam Stripmap  (6 days) * (2 sides) / (0.333 over-land duty cycle) (6 days) * (2 sides) / (0.333 over-land duty cycle) (6 days) * (2 sides) * (4 beams) (6 days) * (2 sides) * (3 beams) (6 days) * (2 sides) * (4 beams) (Targeted Area Time) (0.333 over-land duty cycle) (6 days) * (2 sides) * (3 beams) (Targeted Area Time) (0.333 over-land duty cycle)  36 days 36 days 48 days 36 days 96 days  Table 3.2 LEO+ operational modes.  Global Fill-In Extended-Beam Stripmap  72 days   34  GLOBAL.EARTHQUAKE.SATELLITE.SYSTEM  Flight Direction  Figure 3.4 LEO+ operational modes. Nadir Track  1325 km Altitude  300 km 150 km Primary Beams  535 km Stripmap 100 km Stripmap 260 km  ScanSAR  Extended Beams  but some mitigation strategies are possible (see the Atmospheric Mitigation section in Chapter 5). We expect that average-case accuracies of a few centimeters or better would be possible from a single interferogram, and that these accuracies might be reduced to the subcentimeter level with the proper combination of multiple data sets. (This underscores the need for an InSAR mission that can acquire large amounts of data over short time periods for targeted areas.) We also expect subcentimeter 3-D displacement accuracies for regions between 30-70 latitude, though as described previously, one of the 3-D displacement components may be indeterminable for other latitudes if data are acquired only from satellites at the same near-polar orbital inclination.  data volume collected each day would be about 950 Gb, or 119 GB. O ver five years, the satellite would collect more than 200 TB of data that would need to be archived. For downlink and instrument-storage sizing, the maximum instantaneous data rate of 320 Mb/s and a 25% instrument duty cycle would yield 250 Gb of data per orbit. See the Ground Data System section in this chapter for more detail.  Pa yload Descr i ption The LEO+ mission described here consists of an L-band SAR instrument on a dedicated spacecraft. The radar antenna, consisting of ten lightweight rigid panels and antenna deployment structure, comprises the majority of the radar instrument 's 640 kg mass (including 30% contingency). The radar sensor electronics subsystem, which generates the transmit waveform and receives the return echoes, include the RF electronics, data handling electronics, and timing and control electronics.  Da ta Ra tes and V o lumes The average instantaneous data rate of the satellite would be approximately 105 Mb/s, so assuming a 10% instrument duty cycle, the   LEO+ Radar Sensor Elec tronics  ENHANCED.LOW-EARTH.ORBIT  35 35  The Radio Frequency (RF) electronics perform the transmit chirp generation, upconversion, filtering, and amplification during signal transmission (Figure 3.5). They also provide amplification, downconversion, and filtering of the received echo. The instrument uses the full 80 MHz frequency allocation by transmitting and receiving a single linear polarization (HH) chirp in two frequency subbands (split-spectrum) with 70 MHz separation to permit ionospheric corrections similar to the L1/L2 GPS approach. The aggregate bandwidth of both subbands is up to 20 MHz.  Subharmonic sampling will be used to combine the two subbands into a minimumrate data stream using the least amount of hardware. An NCO-based direct digital synthesizer (DDS) generates multiple chirp waveforms in a small and power-efficient package. Solid-state power amplifiers (SSPAs) are used as the radar transmitter. SSPA technolog y is ver y mature at L-band, and several hundred watts to several kilowatts of RF power (over relatively narrow bandwidths) can be readily achieved. For an active phasedarray architecture, the transmit power is generated using transmit/receive (T/R) modules distributed on the antenna. In this configura-  Figure 3.5 Radar electronics for the LEO+ SAR payload.   36  GLOBAL.EARTHQUAKE.SATELLITE.SYSTEM  tion, we assume roughly 25 W per module, where 420 modules are distributed along the array to achieve the 10 kW minimum transmit power. The front-end electronics control the signal routing of the primar y, redundant, and test/calibration signals. The receiver downconverts the echo received from the antenna and frequency translates the two splitspectrum subbands using a subharmonic sampling technique to produce two concatenated frequency bands at range-offset video. Gain control provides high dynamic range. These signals are then routed to the data handling system for digitization and storage. The data handling hardware consists of the high-speed analog-to-digital converter (ADC), data buffer and block floating-point quantizer (BFPQ). For the subharmonic sampling receiver, the ADC sample clock is only required to be 50 MHz, with an analog bandwidth of 80 MHz, sampling at 8 bits per sample. The BFPQ converts the 8-bit data to 4-bit data and the buffer reduces the peak data rate to interface with the solid-state recorder (SSR). Formatting includes embedding a synch word, frame count, and spacecraft data (GPS, time) into the data stream. Only one high-rate data channel is required. The radar control, timing, and telemetr y hardware includes a central processor unit (CPU), telemetr y processor, spacecraft interface module, radar control and timing unit (CTU), and power module. A dedicated CPU is implemented to control and manage the instrument functions and data flow. This approach ensures a simple interface to the spacecraft and aides in ground testing of the instrument. W hile the CTU generates deterministic subsecond timing parameters, the CPU controls operations for time scales greater than one second (ScanSAR control  parameters, radar mode, data flow). The dedicated CPU will be able to easily handle the control algorithm to calculate and store in a look-up table the beam position for ScanSAR operation. Based upon the command word generated by the CPU, the CTU generates the timing signals necessar y to control the radar, including pulse repetition frequency (PRF), receiver protection and gain control, antenna phase shifter settings, and data window position. The radar electronics will be housed in two separate chassis, one for the RF electronics and one for the digital electronics. Each subsystem has its own dedicated power distribution unit to convert the raw spacecraft voltages to the required DC voltages and to condition and distribute them to the subsystems. Full block redundancy of the radar electronics is implemented to achieve the five-year mission lifetime. The RF electronics consist of primar y and redundant subassemblies and the Redundancy/Built-In-Test select switch matrix, each packaged in a separate shielded enclosure. Surface-mount RFIC/MMIC technology in microstrip circuits ensures cost-effective, lowmass packaging. The digital electronics will reside in a standard VME chassis. Standard VME architecture enables the use of several existing commercial-off-the-shelf (COTS) hardware assemblies to reduce cost and risk. These include the CPU board, the Telemetry Processor Board, the spacecraft I/O interface board, and the power distribution and conditioning board. The custom digital hardware uses FPGA technology to reduce size and power while increasing flexibility of the design. Radar A n tenna  The antenna performs the beam steering and transmission function as well as highpower amplification on transmit and low-noise   LEO+ Bus Panel Frames  ENHANCED.LOW-EARTH.ORBIT  37 37  Fi g u r e 3 . 6 LEO+ deplo y ed an tenna using a Folding Arms Locking Elbows  RA D A R S AT - 2 modified truss struc t ur e .  Fixed Panels  A-Frames  amplification on receive. The antenna is a corporate-fed planar phased array with deployable antenna structure (Figure 3.6). The use of many distributed T/R modules on the antenna provides inherent redundancy since random failures of the T/R modules result in a graceful degradation of radar performance. The Antenna Subsystem consists of the RF aperture (antenna panels) and the deployment structure. W hen stowed, the antenna is folded into ten panels, each measuring 1.35 m  3.5 m. The antenna width (3.5 m) was selected such that it could be accommodated in several existing launch vehicles. The radiating elements consist of half-wavelength microstrip patch radiators. To minimize grating lobes, an element spacing of 0.7  is selected so there are 21 (elevation)  80 (azimuth) radiating elements in the full array. The radiating elements are single polarization (HH) and combined into 1 (elevation)  4 (azimuth) element subarrays that are each driven by a single transmit/receive (T/R) module. The T/R modules, with integrated 4-bit phase-shifters, are distributed over each antenna panel to achieve elevation steering as well as to minimize losses. The 420 T/R modules are organized in panels,  which contain 21 (elevation)  2 (azimuth) T/R modules (42 modules). This configuration enables one phase shifter per elevation element. Although there is no azimuth steering requirement, the antenna does have the capability of limited azimuth scanning. To facilitate ground testing and in-flight performance monitoring, an RF Built-In Test Equipment (BITE) capability is included in the T/R module. A small portion of the transmit signal is coupled to a BITE port and routed to the receiver to monitor the transmitter performance of the antenna. Alternatively, an in-band caltone signal can be routed through the BITE feed and coupled into the T/R module's LNA to test the receive portion of the system. This calibration feature can be implemented as either a special test sequence during a non-data-taking mode or else incorporated directly into the data-taking mode. A broadband corporate feed network distributes the RF signals to and from the antenna elements. The coaxial array feed distributes the RF signals to each panel. Within each panel is a microstrip panel feed to distribute the RF signals to each subarray. Conventional multiwire harness cabling distributes the DC power and control signals to each panel.   38  GLOBAL.EARTHQUAKE.SATELLITE.SYSTEM  Table 3.3 Instrument mass and power for GESS LEO+ mission. QUANTIT Y MASS (KG)  PEAK DC POWER ( W )  STANDBY POWER ( W )  ORBIT POWER AVG ( W )  Antenna Subsystem T/R Modules Panels (Aperture, Panel RF Feed, Frame, Hinges) Antenna Panel Electronics Antenna DCDC Converters Antenna Array RF Feed Antenna Power and Control Cabling Deployment Structure Interface Structure and Launch Support Actuators and Release Mechanisms Thermal Blankets (MLI) Radio Frequency Electronics Subsystem Chirp Generator Frequency Synthesizer Upconverter Driver Receiver Red and BITE Select Power Distribution Housing, Cabling, and Misc. Digital Electronics Subsystem Timing and Control Unit ADC/Buffer BFPQ CPU Telemetry S/C I/F and Data Formatter Power Housing, Cabling, and Misc. Ra dar To tal 30% Margin GESS R a dar To tal w/ Margin 2 2 2 2 2 2 2 1 2 2 2 2 2 1 2 1 420 10 10 67 -- -- 1 s/c s/c s/c  433.0 43.0 298.0 10.0 10.0 10.0 15.0 47.0 0.0 0.0 0.0 29.0 2.0 3.0 2.0 4.0 2.0 1.0 3.0 12.0 31.0 2.0 3.0 2.0 2.0 2 .0 2.0 3 .0 15.0 493.0 147.9 640.9  2783.3 2066.7 0.0 160.0 556.73 0.0 0.0 0.0 0.0 0.0 0.0 91.1 12.0 20.0 7.0 28.0 3.0 12.0 9.1 0.0 61.3 7.0 10.0 6.0 10.0 10.0 6.0 12.3 0.0 2935.7 880.7 3816.4  13.3 0.0 0.0 10.7 2.7 0.0 0.0 0.0 0.0 0.0 0.0 11.1 0.0 10.0 0.0 0.0 0.0 0.0 1.1 0.0 41.3 7.0 0.0 0.0 10.0 10.0 6.0 8.3 0.0 65.7 19.7 8 5.4  278.3  9.1  6 .1  293. 5 88.1 381.6   LEO+ The antenna structure is a deployable truss structure, which provides both support and strength to the panels and maintains flatness of the full array. The structure and deployment mechanisms must be reliable and lightweight and must deploy such that the antenna is flat, structurally stiff, and thermally stable. The flatness requirement of the antenna is onesixteenth of a wavelength (1.4 cm) to minimize antenna pattern distortion. Two competing truss structures are suitable for this application. A modified deep-truss structure (as used in Seasat and RADARSAT-2) has extensive heritage and is considered relatively low risk. The edge-truss structure offers a ver y compact stowed envelope although it is less mature technologically. Both are ver y lightweight and can meet all GESS structural requirements. The spacecraft provides the interface, launch support structure, and thermal blankets. The instrument mass and power are shown in Table 3.3. Heritage  ENHANCED.LOW-EARTH.ORBIT  39 39  mented in the GESS LEO+ mission for cost and risk reduction. For instance, based on LightSAR prototyping activities related to the antenna panel, T/R module, and structure, the total antenna mass density is projected to be roughly 10.2 kg/m2, which is a significant improvement over the SIR-C L-band panels (23 kg/m2) and the SRTM C-band outboard antenna (20 kg/m2).  Mission Design The design of the LEO+ mission is summarized below. O verall objectives and subsystem requirements were defined, and traded against, to achieve this baseline design. The LEO+ spacecraft block diagram showing the subsystems is illustrated in Figure 3.7. The fiveyear mission duration requires the use of functional redundancy in design. In order to mitigate overall risk, the spacecraft design uses full redundancy. The original LEO+ study used a launch date of July 2006, which corresponds to a technology cutoff date of 2003. This means that all technology items must be at a TRL of 6 by the beginning of Phase C/D. Another constraint is that the spacecraft must sur vive through the short eclipse seasons that occur each year. The power systems were designed to meet that requirement. Additionally, the LEO+ orbit radiation environment is somewhat more harsh than LEO. For study purposes, we assumed a radiation value of 24 krad behind 100 mils per year, with a radiation design margin of two. The current mission was designed for the use of a Delta II launch vehicle. The spacecraft would need either a two-axis gimbal High-Gain Antenna (HGA) or two antennas. These gimbals are required to permit continuous data taking without much performance degradation.  W hile the instrument is based on existing technology, it represents a major leap for ward in measurement capability. The GESS L-band SAR instrument derives much design and hardware heritage from its SIR-C predecessor. SIR-C experimentally validated several new SAR techniques including ScanSAR, spotlight SAR, and repeat-pass interferometr y. Technolog y development activities following the success of SIR-C have focused on reducing the mass, power, and cost of similar instruments to enable a future free-flyer. The Advanced Radar Technology Program (ARTP) in collaboration with LightSAR demonstrated numerous L-band SAR component technologies with significant reductions in mass and power. Many of these technologies can be imple-   40  GLOBAL.EARTHQUAKE.SATELLITE.SYSTEM  Figure 3.7 LEO+ spacecraft block diagram showing the subsystems.  Sp a c e c r a ft D e s c r i p t i o n The LEO+ system concept has been formulated to keep overall costs low by using existing commercial designs (hardware and software) to the maximum extent possible. Applying this approach to the entire endto-end system design has kept the spacecraft requirements in a range that can be satisfied by any one of several readily available satellite busses that can be obtained from industr y. To minimize cost and development schedule, the approach is to use existing designs with  limited modifications to support the mission characteristics. The system design requires the bus to provide the following functions:  Radar instrument commanding  Antenna deployment initiation command  Onboard storage of radar data  Data handling capacity to accommodate instrument peak data rates  Data downlink for instrument telemetr y  Global Positioning System data  Radar instrument power  Attitude and articulation control  Capability to handle large instrument   LEO+ MISSION OBJECTIVE  ENHANCED.LOW-EARTH.ORBIT  41 41  COMMAND AND DA T A SUBSY S TEM (CDS)   mm-level surface change detection accuracy per year (cm-level for any given interferometric pair). Six day repeat coverage.  Global accessibility.  Five-year mission duration. MISSION AND INSTR U MENT CHARA CTERISTICS   100 Gbits / orbit average.  Two orbits storage.  150 Mbps data rate. INSTR U MENT PO WER   10 kW peak RF output.  2936 W DC input in science mode, 66 W in standby mode (3816 W with 30% uncertainty, 85 W with 30% uncertainty). STRUCTURES   L-band frequency.  Single polarization.  200-m orbit tube radius.  Left/right-looking, sun synchronous.  Up to 20 MHz combined bandwidth split in the 80 MHz available L-band bandwidth to mitigate ionospheric delay problems.  Incidence angle range 15.447.2. ORBIT DESIGN   SAR antenna consists of rigid honeycomb panels with back-up truss, 13.5  3.5 m deployed, stowed in 1.35  3.5  1.5 m assuming 10 panels.  433 kg mass estimate (current best estimate with no contingency) for SAR antenna panel.  Total instrument mass estimated to be 493 kg, including the RF antenna and data electronics boxes (641 kg with 30% contingency). TELEC O M   Baseline six-day repeat orbit of 1325 km.  6 am/6 pm orbit baseline. AT TITUDE CONTR O L SUBSY S TEM (A CS)   Pitch and yaw pointing control to within  0.05 (180 arcsec), 3 sigma. Roll pointing control to within  0.1 (360 arcsec), 3 sigma.  Pitch and yaw pointing knowledge to within  0.025 (90 arcsec), 3 sigma. Roll pointing knowledge to within  0.05 (180 arcsec), 3 sigma. These are half of the pointing control requirement.  Pointing stability to within  10 arcsec/sec, 3 sigma per axis. This supports the pointing control requirements.  Repeat orbit position to within 200 m, 3 sigma.  Real-time orbit position knowledge to within 20 m, 3 sigma, which supports the repeat orbit position requirement.  Slew about the roll axis through 64 within 5 or 10 minutes.   Support a minimum data rate of 105 Mbps (preferably 320 Mbps using two channels at 160 Mpbs), into 11.3 m ground stations in South Dakota (EROS Data Center), Alaska (Alaska SAR Facility), with Svaalbard, Norway as backup.  Provide a low-rate S-Band T TC link. PR OPULSION   Provide 143 m/s of velocity for a 1500-kg spacecraft (based upon assumed bus mass and Team X estimates).  Provide 20 kg for miscellaneous attitude control functions.  Unload reaction wheels if necessary. Normal unloading is by torquer bars.  Provide initial tipoff rate reduction during launch.  Provide many very small orbit correction maneuvers similar to TOPEX.  Functional redundancy.   42  GLOBAL.EARTHQUAKE.SATELLITE.SYSTEM  Figure 3.8 System elements for launch, flight, and ground operations.  LAUNCH SERVICES  FLIGHT SEGMENT  Launch Vehicle (Delta II)  Commercial Spacecraft Bus  L-Band SAR Instrument  GROUND SEGMENT  Uplink Station  Global Network of Downlink Stations  GESS Operations Center  GESS Data Planning and Management GESS SAR Data Catalog and Archive  Customers  SAR Data Processing  A sur vey has been completed that indicates several manufacturers can supply satellite busses that meet or exceed the LEO+ requirements with little or no modifications to existing designs. All of the busses sur veyed have substantial flight heritage and utilize space qualified components and technologies. Some of the available busses are productionoriented designs that will provide substantial cost and schedule efficiencies. The cost-saving approach for selecting a bus for nominal design requires that the bus requires little modification, has substantial space flight heritage, is compatible with multiple launch vehicles, has redundancy and sufficient margins to accommodate unexpected changes in the designs, uses radiation-hard standard or commercial parts when possible, and has welldefined interfaces to allow parallel development and testing of the instrument.  La u n c h Ve h i c l e The LEO+ launch vehicle will be obtained by the NASA Expendable Launch Vehicle Office and provided to the project using NASA ELV Office procurement and quality assurance processes. The NLS-Medium launch vehicle (Delta II) was assumed, as it can place the spacecraft into the desired orbit (Figure 3.9). A Delta IV would provide ample mass and volume margin.  A ssembly, Te st , and L a unch Opera t ions (AT&L O ) Once the LEO+ system design is complete and has passed the critical design review (CDR), production of the major elements will proceed concurrently. The spacecraft bus, radar instrument, and ground segment systems and their components will complete a rigorous test program during development and build-up for flight. All subsystems will   LEO+ be thoroughly tested prior to deliver y to AT&LO (Figure 3.10). The objectives are to verify system level requirements, functional interfaces, and nominal performance of the integrated flight segment configuration in flight-like conditions. The main objectives for AT&LO are:  Provide an integrated, test validated, flight-ready space segment consisting of the spacecraft bus, radar instrument, and launch vehicle, which is capable of being launched on the scheduled launch date.  Plan and implement traceable, repeatable, and comprehensible test activities.  Demonstrate an ability to support the spacecraft and the mission objectives with functionally validated ground operations and data processing systems. The AT&LO program shall test or demonstrate the following:  Compliance of the integrated flight segment with system-level design and functional requirements.  Nominal flight segment performance in ambient and expected environmental conditions (of launch and flight), with baseline representative operational sequences; and, predictable performance in selected contingency conditions.  Compatibility with the launch vehicle and launch systems interface requirements.  Compatibility with the ground segment and operations systems.  Verified spacecraft capability to receive and process commands, and to clock out execution time for commands or command sequences that cannot be fully tested in ambient/ground environments. In addition to traditional integration and test support, the AT&LO organization will also support a postlaunch on-orbit commis-  ENHANCED.LOW-EARTH.ORBIT  43 43  Figure 3.9 The LEO+ payload in stowed configuration in a Delta II launch vehicle fairing.  sioning phase to checkout and calibrate the end-to-end flight and ground system. The AT&LO activity will conclude upon completion of the operational acceptance review, which occurs at the end of the commissioning phase. A T&L O A pproach  The AT&LO approach targets several areas, including the early use of a system testbed to evaluate and confirm avionics architecture and end-to-end Information System design in order to avoid surprises and risks later in integration and testing (I&T). The spacecraft bus manufacturer will be responsible for the spacecraft bus I&T, and will deliver a fully integrated and tested bus. The radar system I&T would follow the incremental build and test approach. Final flight element I&T would be conducted at the spacecraft bus contractor facility. No thermal/vacuum tests at system level, no postenvironmental antenna deployment test, and only limited mechanical/integrity verifications   44  GLOBAL.EARTHQUAKE.SATELLITE.SYSTEM  Figure 3.10 Satellite integration and test flow.  SUBSYS TEM LE VEL Integrate Subsystems Ambient Electrical Test Initial Mass Properties, Alignments Initial Environmental: EMC, Vibe, Thermal Assemble/Integrate Subsystems & Structure  SYSTEM LEVEL  Comprehensive System Test: Electrical, Thermal Cycling  Install Solar Arrays and Deployables  Vibration and Acoustic Testing  Install Solar Arrays and Deployables  Comprehensive System Test  Slow Deployables Test  Launch Configuration Thermal Vacuum Test  Comprehensive System Test  Integrate Radar Instrument  Remove Solar Array and Deployables  Package and Ship to Launch Site  JPL Delivers Radar Instrument  Launch Site Activities  Final Assembly and Inspection  Final Functional Testing  Launcher Mating  Fueling  are assumed. There will be maximum use of system engineering in other WBS elements. The launch will be from the Western Test Range ( Vandenberg Air Force Base).  Gr ound Da ta S y st em and Produc ts The GESS Ground Data System (GDS) is designed to support the disaster management community (Figure 3.8). Specifically, the data latency for disaster response is two hours or less for time-tagged raw data and six hours or less for Level 1 data products, which could be utilized directly by the disaster response teams. Two downlink stations are planned to capture all the raw data being acquired, the EROS Data Center (EDC) in South Dakota and the Alaska SAR Facility (ASF). The Svaalbard Ground Station in Nor way will ser ve as a backup downlink station. EDC  would be the central data archiving and processing center, whereas another center, such as JPL, would ser ve as the GDS development site and backup data archiving and processing facility. JPL would have the capability to handle both the standard data product delivery as well as special event product generation. Level 0 product generation may be done at the downlink station and transmitted directly to the users when needed to reduce the data transmission overhead via EDC. In the following section, we will summarize the ground data system requirements and corresponding design impact on the GDS for the LEO+ mission. We will describe the output products based on inputs from the seismology community. Next, we describe the external interfaces to the GDS and the distributed software architecture. The distributed nature and   LEO+ scalability of the GDS architecture designed for the LEO+ mission can be easily expanded to support a geosynchronous mission. We describe the hardware architecture of the GDS, which is composed entirely of commercial off-the-shelf (COTS) products. Ground Data S y stem Requirements  ENHANCED.LOW-EARTH.ORBIT  45 45   Atmospheric path delay model (from meteorological ser vices)  Ground truth data (from external sources) necessar y for calibration Browse products will be generated for all Level 1a, 1b, and 2 products. Ancillar y data may be bundled with any level data product deliver y. Ground Data S y st em Int er faces  Based on the functional requirements for the LEO+ mission, the ground data system requirements and design impacts are summarized in Table 3.4. Data Pro duc t Definitions  Two primar y user communities with different requirements will be supported: those with radar processing capabilities, and researchers relying on geophysical Level 2 products. The former group of users would request the raw radar data to process themselves. In addition, they will need ancillar y data for the purpose of calibration such as the removal of atmospheric effects. The second group of researchers who have no interest or capability to process their own radar data prefer to work directly with the geo-coded differential interferograms to extract the deformation measurements. Therefore, as shown in Table 3.5, the data products are defined to ser ve both of these user communities. All level data products have accompanying metadata, which includes the ancillar y data and quality, calibration, and processing parameters. Quick-look data (without corrections) will also be available. Ancillar y data needed for processing includes:  Satellite orbit information derived from onboard GPS  Ground reference GPS (from mission operations)  The GESS GDS is an integrated SAR processing, product deliver y, and archiving system. The GDS interfaces with the following components:  Spacecraft operations  Science users  Program management  Algorithm developers and calibration engineers  Ancillar y data sources The high-level GDS boundaries and external interfaces are shown in Figure 3.11. Spacecraft operations provide satellite-tasking information (instrument on/off times and modes) to the GDS. This information is catalogued, used for internal processing and made available via a Web-based GIS interface (interactive map) and subscription. It also provides ground station tasking and downlinked data. Science users access the GDS through a Web portal. This portal provides product and processing request capability, as well as other features such as data mining and education and outreach. Program management accesses the GDS through a Web portal to view metrics and provide processing priorities etc. Algorithm developers submit basic algorithms and refinements through a Web-enabled configuration management interface. The GDS actively acquires and ingests ancillar y files required for product processing.   46  GLOBAL.EARTHQUAKE.SATELLITE.SYSTEM  Table 3.4 Ground data system requirements. Based on the functional requirements for the LEO+ mission, the ground data system requirements and design impacts are summarized here.  REQUIREMENT S  DESIGN IMP A CT  At least two downlink stations  Distributed system architecture Secure and reliable network connections Process raw data at more than one location  Duty cycle up to 2025%  200250 Gbits of data per orbit Parallel processing environment Distributed high-speed storage devices  13 orbits per day and six days in a repeat cycle  Online real-time data storage over 30 TB of data Six days' worth of data available on line  Fast downlink (320 Mbps) required  Network upgrades at ASF and Svaalbard Identify other possible stations  Single Data Archiving Center with a backup site  Develop operational concepts with EDC Design near online storage devices Use of DVD and high density magnetic media with jukeboxes  System interfaces to those with radar processing capability and individual researchers without the capability Access to ancillary data  Develop capability to interface with various data access methods Fault tolerance with real-time data deliver Support special orders of various level products Negotiate interfaces with ancillary data providers Develop redundant interfaces during emergency  Level 0 in compliance with EOS-HDF  Develop metadata standards Participate in HDF version 5 development  Latency for time-tagged raw data is 24 hours Latency for calibrated data products is six days  320 Mbps downlink reception capability Parallel/Beowulf/clustered processors Smart online data management system Reliable interfaces to ancillary data repositories  In emergency, two hours for raw and six hours for Level 1 Easy to use user interface  Capability to handle special processing Data mining Web interfaces to access data Single interface to access all data levels Data and metadata standards  Five-year mission lifetime  Reliable system maintenance and upgrades Develop cost-effective operational concept   LEO+ LEVEL D EFINITION  ENHANCED.LOW-EARTH.ORBIT  47 47  Table 3.5 Data products  0 1a  Reformatted raw signal data with associated radar headers. Processed single-look complex (SLC) data, browse imagery from multi-look SLC data, browse interferogram generated with most recent data-take from archive, and associated radar headers.  definitions.  1b 2 Ancillary data  Interferogram and correlation map with associated radar headers. Calibrated three-dimensional displacement map in standard map projections. Satellite orbit information derived from onboard GPS data and ground reference GPS stations (from mission operations), atmospheric path delay model (from meteorological services), and any ground truth information (from external source) necessary for calibration. May be bundled with any level data product delivery.  So ft wa re A r chitec ture  The software architecture supports a distributed implementation allowing for any number of receiving stations to be integrated into the system. In addition, this architecture is scalable in order to meet the performance requirements of a LEO+ mission or a geosynchronous mission. Additional designs may also be studied. Alternate architectures such as a direct broadcast approach may be viable, if data quality and calibration can be ensured. The current design will allow for one or more ground data systems to be deployed. All ground systems will contain the same software, and will be configured to archive products long term at the EDC, the central data archiving center. The EDC will contain that master catalog of data products acquired throughout the mission and will include both online and offline storage of the data products and metadata. Ground receiving stations will be capable of receiving the products, performing basic data processing, and archiving the products at the EDC. The product catalogs will be designed to reside on one or more hosts allowing scala-  bility in the catalog. A typical scenario would be to build three product catalogs representing L0, L1, and L1+ data products. The data products will be stored on a network attached storage (NAS) file system so that the data products always appear local to the system creating an online archive. Two key user interfaces will be created. The science user interface will allow scientists to enter product requests. Requests for previously captured data takes will be processed and products staged for download by the data distribution function. Requests for products that have not been acquired will be recorded by the system and scheduled for notification and distribution to the user once acquired. In addition, an operational user interface will also be created which will allow system operators to manage the data system. In addition to the principal site of EDC, the software will also support the creation of a replicated site in the event that the EDC is unavailable. The replicated site will allow for products to be archived and queried at that site should the EDC be unavailable. The replicated site will contain a short-term ""online""   48  GLOBAL.EARTHQUAKE.SATELLITE.SYSTEM  archive. Should the archive be unavailable, products will be captured by the replicated site, and then moved to the EDC once available. The site will also contain a master copy of the catalog indicating what products are available in the system. Hardw a re A r chit ec ture  The GDS hardware system is composed of COTS products utilizing COTS operating systems. The hardware architecture is scalable in order to meet the performance requirements of a LEO+ mission or a geosynchronous mission. The physical interface between co-located machines is a high-speed switched network, now specified as Gigabit Ethernet but soon to be 10 Gigabit Ethernet. The design is also adaptable to the new InfiniBand architecture, a high-speed I/O protocol that is five times or more faster than the 10 Gigabit Ethernet, when it becomes widely available. Ground stations and the processing centers  communicate through the Internet at the highest available bandwidth connections provided by the NASA Integrated Ser vices Network (NISN) or commercial providers. The hardware system supports fully distributed processing, access, and control. The database is fully mirrored at an off-site location and is continuously updated. The main repository and archive for the database, online processed products, and long-term archived L0 product will be centralized at EDC. Mirror sites and Web caching at multiple locations will facilitate periods of high demand access to processed products. Operational Scenario  The GESS GDS is fully responsive to the published operational scenarios. The GDS will be operated and ser viced by EDC once the GDS system is delivered. The operational  Figure 3.11 Ground data system boundaries and interfaces.  GESS Satellite Tasking  Downlink  Ground Station Tasking  Ancillary Data Sources  Ground Data System  Science Users  SAR Science  Program Management   LEO+ and ser vice concepts will follow the current model of EDC. Through the GDS Science and Management interfaces, processing priorities will be set (as in the case of the response to a targeted seismic event) and the processed products from the various mapping campaigns will be segregated into virtual collections for distribution and browsing (Table 3.6). 14 days 36 days 36 days 48 days 48 days  ENHANCED.LOW-EARTH.ORBIT  49 49  FIRST 6 MONTHS  Table 3.6 Ground data  Checkout ScanSAR Global ScanSAR Extended Beams Global High-resolution (Strip) Targeted areas High-resolution (Strip) Extended Beam Targeted Areas  system operational scenarios.  Total 182 days  Geosynchr o nous GDS To scale the GDS to support a geosynchronous mission essentially increases the duty cycle from 25% to 100% radar on-time. This increases the procurements and downlink costs significantly and would more than double the cost of the LEO+ system described here. RECURRING 6 MONTH SCENARIO  36 days 144 days  ScanSAR Global High-resolution (Strip) Target Areas  Mission C o st The total mission cost for the LEO+ system is in the range of $400500 million. The JPL Project Design Center (PDC), also known as Team X, which is a concurrent engineering process for proposal development and mission definition, developed the spacecraft and mission costs. The Team X subsystem engineers used grass-roots estimates and parametric models to estimate the costs. The basis of the L-band SAR instrument is a grass-roots estimate developed by experts from the JPL Radar Science and Engineering Section. The following assumptions apply to the costs:  All costs are in FY02 $M.  The mission starts in September 2003. The mission launches in August 2006.  Phase A is nine months, Phase B is 12 months, Phase C/D is 25 months, and Phase E is 60 months.  This is a Class-B mission using commercial and military 883B parts.   It will have full redundancy for a five-year mission duration.  The spacecraft will be supplied by industry and built as a protoflight. The instrument will be built by JPL.  Phase A, B, C, D, E will have 30% reser ves. There is no reser ve on the launch vehicle cost. These cost study results should be considered a departure point for more in-depth study. By iterating the science requirements with the user community, and the mission design with mission architects, significant cost reductions may be available. Identifying descope options is an important first step in this process. Additional cost savings, as well as additional mission value, should be explored through partnerships with other government agencies (such as NPOESS, DoD) and international programs, including ground stations and flight elements.   JPL 400-1069 03/03"
GX261-74-14113532	"GLOBAL.EARTHQUAKE.SATELLITE.SYSTEM  GESS A 20-YEAR PLAN TO ENABLE  EARTHQUAKE  PREDICTION  MARCH 2003   14  GLOBAL.EARTHQUAKE.SATELLITE.SYSTEM  Scientific Motivation  CHAPTER TWO  T  he requirements for a global ear thquake obser vational system are  derived from current scientific understanding of earthquake physics, crustal rheology, and fault interactions, the societal benefits of defining and mitigating seismic hazard, and aiding in disaster response following large earthquakes. In simple terms, ear thquakes are generally viewed as being one component of a longer cycle in which a given section of a fault accumulates stress due to plate tectonic driving forces, releases that stress during an ear thquake, and then begins the cycle anew. Since these time scales are on the order of seconds for the coseismic portion and centuries for the interseismic phase, we rarely obser ve a complete cycle. When multiple events do repeat on a given fault segment, significant variation in time scale and earthquake size is the rule. Further complicating our understanding of ear thquakes is that they do not occur in isolation. Ear thquakes located nearby in space and time induce additional forces into a given fault system, either through the static stress changes induced coseismically, or through temporally evolving postseismic stress changes. Since seismology is essentially confined to the coseismic realm, geodesy is the principal means of measuring the response of the fault and lithosphere during the inter- and postseismic part of the ear thquake process. GPS networks have already had a tremendous impact on understanding the ear thquake cycle. A space-based system for monitoring crustal deformation is the logical next step to achieve revolutionar y advances in ear thquake science needed to develop a better predictive capability.  Inset: Modeled seismic cycle deformation. (Rundle and Kellogg, 2002) Background: Interferogram from Antofagasta, Chile, earthquake. (Pritchard et al., 2002)   SCIENCE S C I E N T I F I  C  .  M  O  T  I  V  A  T  I  O  N  15  GESS Science In v e stiga t ions and Requirements The GESS science requirements derive directly from the GESS investigations that addressed the current and future state of our understanding of earthquake physics, and the measurements necessar y (and practical) to advance our understanding (see page 98). Some of the investigations present theoretical or scenario-based models that predict specific spacetime behavior of seismicity and patterns of crustal deformation. These studies placed requirements on resolving different classes of lithospheric models and time scales of pre- and postseismic deformation. Other studies presented examples from the current principal satellite SAR system, the European Space Agency 's (ESA) ERS satellites, which have formed the basis for much of our current understanding of SAR interferometr y, both in terms of performance and in terms of the types of information and applications that are possible. These examples impact both the single image and interferogram data requirements, and also illustrate methods for overcoming some of the error sources through data stacking, time series inversion, or atmospheric modeling. Finally, applications goals such as earthquake disaster response also impact the system requirements. Before examining the main scientific questions regarding earthquakes, it is worth summarizing how these pieces fit together and their historical context. Our current understanding and the direction we see as necessar y to understanding the earthquake process are directly linked to the recent past. Much of our understanding of earthquakes comes from seismology, both in terms of their spacetime magnitude, and from understanding the characteristics of the earthquake rupture kinemat-  ics and dynamics. Understanding coseismic rupture kinematics has benefited from the use of high-quality geodetic data, in particular the applications of InSAR. Advances in GPS and InSAR data in conjunction with several significant earthquake sequences (LandersHector Mine, California; IzmitD uzce, Turkey) in the 1990s provided important insight into their coseismic ruptures, and also provided important new observations and model constraints on complex ruptures, triggered earthquake sequences, and aftershocks. The Landers earthquake was the first application of InSAR to crustal deformation. Examination of the complex rupture and aftershocks of the Landers event stimulated development of models based on stress shadowing and stress migration in the crust and upper mantle to explain the space-time occurrence of these triggered events. The case was similar for the IzmitDuzce and Manyi Kokoxili, Tibet, earthquake sequences. Highquality space geodetic data (particularly from InSAR) allowed obser vation of spatial and temporal behavior of the crust following large earthquakes that forced re-examination of the crustal response and the forces governing earthquakes. The insights gained from these event data sets have in turn boosted a debate regarding the time-var ying state of stress in the crust, and have fueled fresh examination of the physics of the earthquake cycle on fault systems. Theoretical models that examine earthquake clustering and stress evolution predict spatial and temporal deformation signals that could be measurable with future satellite systems. This could lead to significant advances in our ability to constrain the locations of future earthquakes.   16  GLOBAL.EARTHQUAKE.SATELLITE.SYSTEM  Figure 2.1 Evolution of Coulomb stresses prior to an earthquake. Each figure shows the progression of the surface Coulomb stress due to earthquakes and deep fault creep on a fault segment that will experience a future ear thquake. Warm colors indicate that the change in stress favors a future earthquake. Thus, in addition to the steady-state tectonic loading of the future earthquake segment, the positive Coulomb stress caused by the surrounding fault segments increases the likelihood of an event on the future earthquake segment. (Sammis and Ivins, 2002) Fault Creep Future Earthquake Future Earthquake Seismic Slip Seismic Slip Future Earthquake  Significant improvement in obser vation of earthquake crustal deformation provided by GPS and InSAR during the past decade placed critical constraints on some existing models and forced significant revision of others. Perhaps the most significant inference we can draw from these advances is that the feedback loop between data and models is critical, and that future advances will require better data, particularly InSAR data. As stated in Chapter 1, we solicited studies to define requirements for an obser vational system that could address specific outstanding questions in earthquake science. The results of the studies are discussed here. In the following section, we have renumbered the original six study questions slightly, combining questions 3 and 4 to emphasize the relationship between complex and triggered earthquakes, and postseismic processes.  1. Ho w does the crust def orm during the int e rseismic pe ri od bet w een ear thquak es and what ar e its te m p o r a l char ac t e r i stics (if any) b e f o r e major ear thquak es?  Detecting signals precursor y to large earthquakes has been one of the most sought after and debated aspects of earthquake physics. Obser vations of precursor y signals have been sporadic and often without a clear link to the subsequent earthquake. In the cases where the connection is clear, the measurements have generally been point location measurements, sometimes requiring measurement sensitivities that are not possible with satellite systems. At the core of this debate is whether or not earthquakes are fundamentally predictable. Some have argued that the crust is continuously in a state of self-organized criticality (SOC) with the probability of earthquake size and location remaining steady. Sammis and   SCIENCE S C I E N T I F I  C  .  M  O  T  I  V  A  T  I  O  N  17  Ivins (2002) and Rundle and Kellogg (2002) argue, instead, that earthquake systems have ""memor y,"" with large earthquakes moving the crust away from SOC through ""stress shadowing"" (Fig-ure 2.1). This provides testable obser vations of seismicity and late seismic cycle deformation that could be measured both seismically and with radar interferometr y (Figure 2.2). The stress shadow models for the earthquake cycle (Figure 2.1) predict that when the surrounding crust is moved away from SOC less background seismicity is expected, but as a future earthquake approaches an increase in surrounding activity should occur. The basis for this model is the seismicity and stress shadow models derived for the large earthquake sequences of the 1990s described previously. The exciting aspect of these recent seismic cycle models is that they predict temporally and spatially var ying deformation patterns in the termination regions of locked fault segments. These models can constrain earthquake fault system behavior, and should be of a magnitude measurable with radar satellite systems. Part of the model for individual faults and fault systems consists of sections that experience either continuous or transient creep. Creep, or aseismic slip, describes slip on fault surfaces that does not produce seismic waves, or discernible shaking. W hile some creeping fault segments are recognized, and several such segments are monitored locally in wellinstrumented regions such as California, many creeping faults are still unknown. InSAR is a valuable measurement technique for detecting and measuring the spatial and temporal characteristics of creeping faults (Figures 2.3 and 2.4), including strike-slip faults (Sandwell and  Figure 2.2 Comparison of the predicted deformation due to stress buildup and release for a large simulated San Andreas earthquake, as observed by C-band InSAR. The bottom panel differFive years pre-earthquake. ences the pre- and postseismic signals to show the level of precursory deformation expected, defining the segment of the fault that will rupture. (Rundle and Kellogg, 2002)  Five years postear thquake.  Difference.  Contour Interval = Radar Wavelength   18  GLOBAL.EARTHQUAKE.SATELLITE.SYSTEM  Figure 2.3 A portion of an interferogram at Mt. Etna, Italy, showing anticline growth and fault creep (data from 19931996, from ERS-1 and ERS-2, courtesy ESA). One color cycle represents 2.8 cm of surface displacement in the radar line-of-sight (LOS). Incidence angle for this image is approximately 23 from vertical toward the west-southwest. The anticline and fault both show approximately 3 cm of LOS displacement. (Lundgren, 2002) Anticline Growth Fault Creep  Fialko, 2002; Burgmann et al., 2002; Lundgren, 2002) as well as blind thrusts (Lundgren, 2002). If the motion is steady, stacking (averaging) InSAR data can reduce many of the transient and systematic errors in a series of interferograms (Sandwell and Fialko, 2002). To detect variations in the rate of deformation, least-squares network inversions can be used to calculate an InSAR time series (Figure 2.4), with a relative deformation map at each InSAR data acquisition (Burgmann et al., 2002; Lundgren, 2002). To be able to detect any precursor y deformation and to discriminate between even relatively simple models of locked versus creeping areas on faults requires a measurement accuracy of less than 1 mm per year (Zebker and Segall, 2002; Fielding and Wright, 2002).  Requirements The requirements for detecting these signals requires both wide swath (on the order of 100 km), and detailed spatial sampling (10100 m). Also required is long-term temporal continuity (over decades) but at fine enough temporal sampling (several days) that precursor y phenomena can be separated from the coseismic, postseismic, and aftershock signals that accompany a large earthquake (i.e., Figure 2.2). Similarly, to monitor creep processes on faults, long time span interferograms (more than seven years) are most important for resolving rates at the 1 mm/yr level (Sandwell and Fialko, 2002). However, detecting transient deformation requires weekly or more frequent measurements to improve temporal resolution and reduce atmospheric noise.   SCIENCE S C I E N T I F  I  C  .  M  O  T  I  V  A  T  I  O  N  19  2. Ho w do ear thquak e ruptur es ev ol v e b o th kinematica lly and dynamic ally and what c o ntr o ls the ear thquake siz e ?  To start to address the question of when and where a future earthquake will occur, and how big it will be, requires an improved understanding of earthquake physics. This starts with more precise knowledge of the coseismic ruptures: how does the slip grow over the fault plane in both time and magnitude, and what controls these parameters? Questions encompassed by this include understanding how earthquakes nucleate and what causes them to stop. Although answering this question has traditionally been the realm of continuum mechanics and seismology, surface deformation has increasingly played a part in improving kinematic and dynamic coseismic models. InSAR has provided detailed surface deformation maps that place tight constraints on the spatial distribution of slip on the fault plane, thus allowing seismic data to better  define the temporal evolution of the slip when joint seismic and geodetic inversions are calculated (Olsen and Peyrat, 2002; DeLouis et al., 2002). The location and slip vectors of the coseismic slip for large earthquakes are important in constraining the temporal characteristics of the earthquake rupture, thus defining the driving force for subsequent postseismic crustal response, afterslip, and the locations and sizes of aftershocks. High-density surface displacements as revealed through InSAR have been used over the past decade to place powerful constraints on coseismic slip maps. W hen combined with other seismic data, the resulting inverse models can image the propagation of the rupture in space and time, and place important constraints on the fault dynamics. Repeat orbit interferometr y alone cannot meet the temporal requirements for directly imaging the seismic wave propagation and rupture dynamics near the fault. How-  Figure 2.4 Observed surface creep across the southern Hayward fault in Fremont, 40 Alignment Array  California. Blue circles show alignment array data which captured a 2 cm InSAR Time Series  Right Lateral Slip (mm)  30  20  creep event in February 1996. Red points display  10  an InSAR time series where the change in  0 1990 1992 1994 1996 Time (year) 1998 2000 2002  range has been projected onto a fault parallel vector. The time series is the result of an inversion using 45 interferograms. Error bars represent the scatter in adjacent pixels. (Lienkaemper et al., 1997)   20  GLOBAL.EARTHQUAKE.SATELLITE.SYSTEM  Izmut Earthquake  Duzce Earthquake  Figure 2.5 Complex slip and fault interaction for the 1999 IzmitDuzce, Turkey, earthquakes (magnitude 7.5 and 7.3, respectively). The two photos are at the same location (indicated with a circle on the panel to the right). The photo on the left is the small fault offset at the eastern end of the Izmit rupture. The right photo shows the much larger normal fault motion that occurred during the Duzce earthquake (photos courtesy of the Seismological Society of America). Middle panel shows each earthquake's sur face ruptures (red, Izmut; green, Duzce), hypocenters, and the traces of the modeled fault planes. The lower panels show the individual and combined slip on the fault planes. Notice how the Duzce slip area fills in the area immediately to the east of the Izmit rupture. The model was derived from the joint inversion of InSAR and seismic data. (Delouis et al., 2000) 50 0 20 29 E  40 km  41 N  IZMIT  DUZCE  40.5 N  30 E  31 E  40 km  Hypocenter  120  250  400  600  800  Slip (cm)   SCIENCE S C I E N T I F  I  C  .  M  O  T  I  V  A  T  I  O  N  21  ever, coseismic interferograms do provide unprecedented images of the surface deformation. This allows creation of detailed models of the slip heterogeneity that help identify rupture asperities, or barriers, and the physical controls on earthquake rupture growth and termination. Slip maps, such as those for the IzmitD uzce sequence (Figure 2.5) are important input parameters for models of stress loading on nearby fault systems.  Requirements Coseismic InSAR requires coherent SAR images taken as soon as possible before and after an earthquake in order to minimize the effects due to postseismic and possible precursor y deformation transients. D ue to the large signal, atmospheric noise is not as corrupting an error source for large earthquakes. For earthquakes such as Izmit, cultivated, vegetated areas were problematic for maintaining correlation between interferograms of C-band ERS data (Fielding and Wright, 2002). This problem would be mitigated by both more frequent repeat data, and with L-band radar (Price et al., 2002). A repeat time of one to three days would be optimal, with a repeat of one week offering significant improvements relative to current systems. 3. What c o ntr o ls the spac etime char ac t e r i stics of co m p l e x ear thquak es , trigger ed ear thquak es, and their af t e rshocks , and ho w ar e the y r e lat ed t o postseismic pr oc esses?  Many large earthquakes cluster in space and time. Understanding the process that accounts for an initial earthquake triggering secondar y events may reduce hazards, and lead to more accurate forecasts.  The physical parameters that control the spatial and temporal separation of events are poorly understood, such as the seven-year delay of the LandersHector Mine earthquakes over the tens of kilometers separating these events (Figure 2.6), or the three months that separated the IzmitD uzce sequence, whose coseismic ruptures overlapped. In addition to static stress changes caused by a large earthquake, stress rates caused by creeping faults or volcanic processes can also affect seismicity (Toda et al., 2002). Triggered earthquakes pose a significant hazard and are potentially the best candidates to constrain in space and time, since the master event provides the largest change in stress to the local fault systems. At the present, understanding of these events is hampered by incomplete knowledge of the pre-existing physical properties of the neighboring fault systems, and of the evolution of the crustal stresses over time scales of minutes to years that separate coupled earthquakes. The initial conditions cannot be directly measured at present. InSAR could provide detailed measurements of the coseismic and postseismic deformation that would place better constraints on stress diffusion models, and refinements of fault interaction models, that could lead to better-constrained predictions of triggered earthquakes. Recent obser vations, principally driven by GPS and InSAR, have revealed complex and relatively fast (days to years) near-field postseismic crustal deformations. These measurements have refined understanding of the different processes (afterslip, poroelastic, viscoelastic) that play a role in the diffusion of stress, both along the fault plane and within the surrounding crust and mantle (Figure 2.6). The detailed, spatially continuous surface deformation measurements   22  GLOBAL.EARTHQUAKE.SATELLITE.SYSTEM  Figure 2.6 Calculated coseismic and postseismic changes in Coulomb stress associated with the 1992 Landers earthquake sequence. (a) Calculated coseismic Coulomb stress changes shown both for the top ground surface and for a cross-sectional view of the model along the Hector Mine (HM) rupture surface (sur face encompassed by black within yellow line). The Hector Mine hypocenter is shown as a black star. The Joshua Tree (JT ), Landers (L), and Big Bear (BB) rupture sur faces are shown as white lines on the top ground sur face. The lower crust lies between the brittle/ductile transition (b/d trans) at 18 km depth and the Moho at 28 km depth. (b) Calculated combined coseismic and seven years of postseismic Coulomb stress changes if viscous flow occurs predominantly in the upper mantle. (c) Calculated postseismic Coulomb stress changes due solely to viscous flow during the seven years following Landers (19921999). (Freed and Lin, 2001) (c) Postseismic only (19991992) (b) Coseismic and postseismic (1999) (a) Coseismic (1992) 20 km  BB JT L  HM b/d trans Moho  Less Likely to Fail  More Likely to Fail  1.0  0.5  0.0 Coulomb Stress (bars)  0.5  1.0   SCIENCE S C I E N T I F  I  C  .  M  O  T  I  V  A  T  I  O  N  23  provided by InSAR are an important tool for recognizing these deformation patterns and interpreting the physical processes that cause them.  Requirements To measure the rapid postseismic deformation and afterslip following a large earthquake (and between the triggered events) requires weekly revisit times. Because time scales of earthquake pairs can be from minutes to years, detecting changes in surface deformation requires similar time scales. Therefore, repeat measurements from one to three days would be better. The more frequent the measurements, the better we can understand earthquake and fault interactions more completely. In addition, frequent sampling allows for larger data sets. This improves signal resolution through stacking and time series computations that reduce the effects of atmospheric and other noise sources. Larger separations in time over greater spatial scales also dictate wide swath coverage over longer time periods, of order one decade. The subtle amplitudes seen for postseismic deformation associated with the Landers earthquake require resolution of deformation rates down to 1 mm/yr. 4. Ho w c an w e identify and mitigat e loc a l seismic hazar d (such as liquefac tion)?  During an earthquake, the distribution of damage is not uniform and depends on the size and frequency of ground shaking, as well as other factors such as building construction. The reduction in loss of life and property, both during the earthquake and in the time following it, can be mitigated by understanding the areas that are most prone to severe damage,  and in identifying the degree of damage as quickly as possible after wards. One important contribution of GESS to earthquake hazard assessment lies in the application of space-based technologies to response efforts by local and federal agencies immediately following a large earthquake. Identifying liquefaction is by definition a post-event analysis. Shinozuka et al. (2002) compared attempts at identifying liquefaction and the ability to differentiate between liquefaction and the effects of ground shaking as the cause of building damage for the 2001 Gujarat, India, and Izmit earthquakes. They found that for the large rural areas of the Gujarat earthquake, the well-documented liquefaction obser ved in the field was detectable with panchromatic instruments in particular. In the case of the Izmit earthquake, comparison of before and after images for both panchromatic and ERS SAR data demonstrated accurate detection of heavily damaged structures, although the cause of damage, whether ground failure (liquefaction) or severe shaking, could not be differentiated. Tobita et al. (2002) discussed the use of InSAR data, together with basin models, to estimate the liquefaction susceptibility of earthquake-prone local areas as a function of the saturation of the upper 20 m of the subsurface. Bawden et al. (2001) have shown the ability of geodetic data (InSAR and GPS) to detect surface deformation due to groundwater discharge and recharge in local basins in the Los Angeles region. Integration of tectonic and hydrologic modeling is needed and recommended to resolve tectonic deformation that is occurring against the noise background of hydrologic variations at similar scales and   24  GLOBAL.EARTHQUAKE.SATELLITE.SYSTEM  amplitudes. Further, such an integrated model will contribute to identifying and scaling liquefaction hazards to determine the total seismic hazard. These studies will also provide useful information on the natural periods of soil sites for earthquake site response analysis.  Requirements For detection of major liquefaction events, and major building damage during disaster response efforts, resolution of 10 m optical and 15 m SAR is acceptable. A smaller pixel size would enable a more complete assessment of ground failure and structural damage. For rapid earthquake response, revisit times of less than one day are best, both in terms of the response time and the quality of the damage maps. For liquefaction susceptibility and earthquake site response studies, subcentimeter resolution of surface change at spatial scales of tens of kilometers with revisit times on the order of a few days would be needed. 5. A r e ther e non-seismic pr ecursor y phenomena that may enable and impr ov e ear thquak e pr edic t ion?  There are numerous geophysical phenomena other than surface deformation that have been associated with seismic events.These include: very low-frequency ( VLF), ultra low-frequency (ULF), and extremely low-frequency (ELF) magnetic fields obser ved on the ground and in space, high-frequency electric fields (including earthquake lights), and thermal anomalies obser ved with satellite sensors. There are individual events, such as the 1989 Loma Prieta earthquake ELF magnetic field, or the warming obser ved coincident with the Hector Mine earthquake, that appear significantly correlated with seismicity. But controversy remains regarding the statistical  significance of the relationship of these anomalous signals to seismic events, particularly as earthquake precursors. The ver y small number of occurrences of these phenomena that are properly referenced to background noise, and which have a clear spatial and temporal relation to specific earthquakes, confounds a systematic approach to investigating the possible sources. An unusual and unique thermal warming was obser ved by Landsat just 18 hours prior to the Hector Mine earthquake of October 16, 1999 near the Hector Mine fault break (Crippen, 2002). Comparison of the October 15, 1999 scene to the September 29, 1999 preceding scene shows that greatest warming in a zone that intersects the Hector Mine fault break (Figure 1.2). Limited Landsat coverage of the same region does not reveal a similar pattern for the Landers earthquake (1992), but no scene was acquired within 14 days of the Landers quake, and the spatial and radiometric resolutions and repeat coverages were inferior in the earlier Landsat satellites. The Hector Mine warming has also been reported in GOES geosynchronous weather satellite data through a series of images taken ever y 30 minutes at 5-km resolution. They show an unusual (but subtle) heating trend a few hours before the earthquake. Earthquake-associated thermal ""anomalies"" have previously been reported by others, but without the spatial or temporal clarity of ""signal"" possibly indicated by the Hector Mine obser vations. Thermal emissions associated with earthquakes have been attributed to changes in fluid flow near fault zones resulting from rupturing of flow barriers as the crust approaches its yield strength (e.g., Hamza, 2001). W hile pressure-driven fluid flow   SCIENCE S C I E N T I F  I  C  .  M  O  T  I  V  A  T  I  O  N  25  within a shallow fault zone could generate a thermal anomaly of the scale and amplitude obser ved, a high permeability of the affected layers would be required for a precursor y signal within one month of a main shock (E. Ivins, personal communication, 2003). This mechanism has been proposed as a means of generating both thermal and electromagnetic anomalies associated with the Loma Prieta earthquake (Fenoglio et al., 1995). To date, no clearly quantified relationship has emerged between thermal emission signals and earthquakes, either preseismically or coseismically. If thermal anomalies precede earthquakes by hours to days, satellite observations will require both high temporal (hourly) and high spatial (< 100 m) resolution to capture the signal. Precursor y quasicontinuous electric and magnetic fields associated with earthquakes, when they can be confidently obser ved, appear to arise from electrokinetic effects of fluid flow (Fenoglio et al., 1995; Park, 1996). Coseismic signals obser ved near the epicenter may reflect piezomagnetic effects ( Johnston, 1997). W hereas a strong signal was obser ved by Magsat at 4 Hz for a M 7.2 earthquake in Tonga in 1980, a search for magnetic field signals of recent earthquakes using three currently orbiting high-precision magnetic field satellites did not identify any promising correlations (Taylor and Purucker, 2002). The mechanism proposed for Loma Prieta, invoking the motion of a conductive fluid resulting from rupture of impermeable layers, has also been proposed to explain transient thermal anomalies. Progress in understanding the relationship of electromagnetic and thermal emissions to the earthquake cycle requires  high-quality, frequently updated obser vations, and verifiable models that satisfy multiple obser vational constraints.  Requirements High spatial (< 100 m) resolution thermal measurements between 3 and 15 microns, updated hourly to daily, are needed to capture putative ephemeral thermal anomalies associated with earthquakes. Continuous magnetic and electric field measurements at DC to 800 Hz frequency are needed to test whether variations are correlated with seismic activity. Most importantly, these signals must be systematically isolated from natural background noise in a consistent manner, and evaluated simultaneously with crustal stress inferred from surface deformation measurements and fluid motion in the crust inferred from timevar ying gravity. The detailed science requirements discussed above constitute a complete set of obser vations that contribute to understanding earthquake physics and the earthquake cycle. However, consistent with the recommendations of the SESWG report and the wider community, we have focused our mission architecture on obser ving surface deformation, as this is deemed the highest payoff measurement to study earthquake physics. We focus on InSAR rather than LIDAR for three reasons. InSAR is an all-weather capability that can efficiently map the globe using a wide swath. It also measures topographic change to fractional wavelength accuracy. Its major limitation is in dense vegetation, and loss of correlation due to major surface disruption or vegetation change unrelated to tectonics. LIDAR can provide ver y precise ""bare-earth""   26  GLOBAL.EARTHQUAKE.SATELLITE.SYSTEM  Figure 2.7 Science measurement requirements for surface 3-D Displacement Accuracy (mm)  0.1  TRANSIENT DEFORMAT ION   Stress transfer 5  displacement.   Triggered earthquakes INTERSEISMIC STRAIN   Aseismic slip  Slow earthquakes  Postseismic relaxation  Afterslip   Steady state 10   Requires long-time series (10-yrs)  20   Static rupture COSEISMIC OFFSETS DYNAMIC RUPTURES  50 10  2  10  1 Revisit Frequency (days)  0.1  10  2  topography beneath vegetation. Its limitations are inoperability in cloudy air, a narrow footprint, and less-precise surface change detection. The InSAR technique has clear advantages for measuring long-term surface deformation globally. However, the LIDAR technique is likely to be important for local and regional-scale sur veys of paleoseismic landforms, and for change detection beneath vegetation canopy. The derived requirements for monitoring surface deformation are summarized on the science roadmap of Figure 2.7.  Disast er M a nagemen t A Global Earthquake Satellite System could contribute to managing earthquake disasters in two ways: by enabling higher spatial and temporal resolution hazard maps, and by  providing timely and valuable information following an earthquake. Hazard assessments are currently used proactively to guide both building codes and disaster preparedness. Spatio-temporal granularity of hazards assessments would allow prioritizing of retrofitting projects according to relative seismic risks. Similarly, emergent behavior of a fault system indicating increasing potential for fault rupture would allow preparations to focus on specific geographic areas and infrastructure assets. In a post-event scenario, GESS would provide maps showing major damage and mapping peak accelerations to accurately assess the magnitude of the damage and guide first-response teams. It would also provide data for real-time mapping of changes in stress on neighboring fault systems to assess potential triggered seismicity.   SCIENCE S C I E N T I F  I  C  .  M  O  T  I  V  A  T  I  O  N  27  The needs of the disaster management community drive the latency requirements for downlinking data and producing data products. Data must reach the ground ver y quickly, and be processed into interferograms within hours of an event to maximize its effectiveness. Direct downlink to users and a distributed processing environment enable this scenario. Data and data products must be released immediately on the Web, and online catalogs of recent data acquisitions, interferograms, and deformation time series must be maintained to expedite processing. Near-line archives of a decimated complete data set are also required to facilitate new analyses. Maps would be produced showing areas where the radar returns have decorrelated to indicate changes in the built environment, as well as maps of peak accelerations showing locations of major damage.  A robust community modeling environment is necessar y to support the disaster management community. The community model would provide a sanctioned way to identify emergent behavior of a fault system and adjust hazard maps. Processing of data would be expedited following anomaly detection (precursors), and ground networks deployed to further investigate and monitor fault behavior. Following an event, the model would produce an estimate of the new stress field. The customers for this information are anticipated to be the USGS, the Federal Emergency Management Agency (FEMA), California Office of Emergency Ser vices (CA OES), local governments, and schools. Enlightening the general public to the dynamic nature of crustal deformation and therefore the hazards they must live with should lead to greater overall preparedness and thus fewer losses.   JPL 400-1069 03/03"
GX261-57-14921080	"The Shallow Magmatic System of Klauea Volcano By Peter F. Cer velli and Asta Miklius  Abstract The shallow magmatic system of Klauea Volcano currently consists of two distinct summit magma reservoirs, a vent at Pu`u ``, and a conduit connecting that vent to the shallower of the two summit reservoirs. Global Positioning System (GPS) measurements and leveling surveys record long-term subsidence over a magma reservoir near the southeast border of the summit caldera, centered no deeper than about 3.5 km below ground level (2.5 km below sea level). The yearly volume loss from this source is, at most, 2.5 million m3, approximately 2 percent of the annual eruption output from Pu`u ``. Electronic borehole tiltmeters image another, much shallower magma reservoir about 0.5 km east of Halemaumau at a depth of 500 to 700 m above sea level (500 to 700 m below ground level). The deformation from this shallow reservoir is episodic and short-lived, each event persisting for only a few days. Four of these deformation events, which occurred from 2000 to 2002, are highly self-similar and appear to reflect a brief interruption of magma supply. We conclude, from the style of deformation and from the timing of seismicity during these events, that the conduit from the summit to Pu`u `` leaves from the shallower reservoir. From there it runs horizontally along the rift zone until it intersects the surface topography at Pu`u ``. We estimate the radius of this cylindrical conduit at about 2.75 m. rates of south-flank motion (max 8 cm/yr) and summit subsidence (max 11 cm/yr), punctuated by episodes of faster motion associated with earthquakes and intrusions (Delaney and others, 1993). Delaney and others (1993, 1998) and Owen and others (2000), who summarized the data up to 1996, modeled the sources of deformation as consisting of slip along lowangle south-flank faults, deep rift-zone opening, and deflation in the summit caldera. We interpret deformation measurements between 1996 and the present but do not attempt to model all the sources contributing to the deformation patterns observed on Klauea. Instead, we concentrate on the summit area because it gives the most information about the structure of the shallow magmatic system. Long-term summit deformation during the current eruption is primarily vertical, consisting of persistent subsidence in the southern part of Klauea's caldera. Numerous workers have attributed similar patterns of subsidence in the past to a deflating magma reservoir at about 3- to 5-km depth (Mogi, 1958; Fiske and Kinoshita, 1969; Tilling and Dvorak, 1993). Delaney and others (1993) and Owen and others (2000) determined that most of the current subsidence is attributable to this deflating reservoir but that rifting of the summit area is required by the horizontal data. Rifting, in fact, may cause about 35 percent of the subsidence. Recent data, primarily leveling and GPS, collected since 1996 clearly image the deflating magma reservoir in the southern caldera, although we do not model the contribution from rifting. Superimposed on the long-term subsidence are self-similar deformation events, which image a shallow magma reservoir slightly east of Halemaumau (fig. 1). These episodes, and what they imply about the shallow magmatic system beneath Klauea, are the primary focus of this chapter.  Introduction In this chapter, we infer the structure of the shallow magmatic system beneath Klauea summit, as well as the location and dimensions of the feeder conduit to Pu`u ``. Toward this end, we use various data sets, including continuous Global Positioning System (GPS) and borehole tilt, leveling campaigns, seismic measurements, and estimates of lava flux and gas effusion. All of these data lead to a fundamental observation about the magma system--there exists a quasisteady long-term mode of deformation interrupted by occasional short-lived episodes of deformation both at Klauea's summit and at Pu`u ``. Many of these episodes share striking similarities that suggest a corresponding similarity of process, which, if understood, could provide clues about the structure of Klauea's shallow magmatic system. Over the course of the current Pu`u `` eruption, the deformation observed at Klauea has been characterized by high  Data Continuous GPS The U.S. Geological Survey's Hawaiian Volcano Observatory (HVO), Stanford University, and the University of Hawai`i operate a network of continuously recording dualfrequency GPS receivers on Klauea (fig. 1). Data from these receivers are processed in daily batches with the Gipsy/Oasis II software package (Lichten and Border, 1987). Time series  U.S. Geological Survey Professional Paper 1676  149   Figure 1. Klauea Volcano, Island of Hawai`i, showing locations of stations in electronic-tilt, leveling, and continuous Global Positioning System (GPS) networks and seismic stations mentioned in text. Digital elevation model used to construct figure is not recent enough to include Pu`u `` and effects of its eruption. Tiltmeter at station POC is located on nor thwest flank of Pu`u ``. Contour interval, 500 ft.  from two selected summit stations (UWEV, AHUP) are shown in figure 2. We filtered all the time series to minimize the effect of reference-frame errors and to calculate long-term station velocities (Cervelli and others, 2002a). This procedure allows geologic offsets, such as earthquakes a n d i n t r u s i o n s , t o ex i s t i n t h e t i m e s e r i e s w i t h o u t b i a s i n g estimates of the long-term deformation rates. The horizontal velocities of the Klauea continuous GPS stations, after subtracting the velocity of a continuous station on Mauna Kea (approx 50 km north of Klauea's caldera), are mapped in figure 3. No evidence exists for active deformation at Mauna Kea; and so its station velocity serves as a proxy for the velocity of the Pacific Plate. The predominant horizontal signal in the GPS data is a persistent southeastward migration of Klauea's south flank (fig. 3). Maximum rates of about 8 cm/yr are observed at station KAEP on the south coast (fig. 1). Station UWEV, north of the topographic caldera but within the structural caldera, and station AHUP, south of the caldera, both show southeastward motion of about 2.5 cm/yr. Station AHUP is moving slightly faster than station UWEV, leading to a  small (0.5 cm/yr) extension across the caldera. The rate at station KOSM, in the southwest rift zone, is nearly 5 cm/yr and is directed much more easterly than at the other summit stations. Since 1996, many geologic events, including two eastrift-zone intrusions and several earthquakes, have affected the GPS time series. The long-term deformation pattern, however, seems unperturbed by these events. Indeed, even the largest event, the January 1997 Npau Crater eruption, introduced only a transient signal into the time series. Apart from horizontal motion, the continuous GPS network is also sensitive to long-term vertical signals. The GPS data show uplift along the coast of about 3 cm/yr and subsidence in the south caldera of more than 4 cm/yr. The sign change between subsidence and uplift runs between the Koa`e and Hilina fault systems (fig. 3). In the modeling and discussion that follows, we consider only the four GPS stations located around Klauea's summit: AHUP, KOSM, MANE, and UWEV (fig. 1). This subset of stations is sufficient to address the question of magma-system structure that is the primary focus of this chapter.  150 The Pu`u ``-Kpaianaha Eruption of Klauea Volcano, Hawai`i: The First 20 Years   0.08 RELATIVE POSITION, IN METERS 0.04 0 -0.04 0.04 0 -0.04 0.1 0  EAST  UWEV  0.08 0.04 0 -0.04  EAST  AHUP  NORTH  0.04 0 -0.04  NORTH  UP  0.1 0  UP  -0.1 -0.2 1996  1997  1998  1999 TIME  2000  2001  2002  -0.1 -0.2 1996  1997  1998  1999 TIME  2000  2001  2002  Figure 2. Time series from two summit Global Positioning System (GPS) stations with respect to a fixed Pacific Plate. Station AHUP is south of Klauea's summit caldera; station UWEV is just nor thwest of topographic caldera (fig. 1). Striking depar ture from long-term trend in early 1997 is effect of Npau Crater eruption (eruptive episode 54). Note long-term subsidence at station AHUP that exceeds 5 cm/yr. Effect of episodic events discussed in text is too shor t-lived to appear on these time series.  Leveling A leveling traverse crossing Klauea's summit and upper rift zones (fig. 1) is measured nearly annually. Leveling surveys are conducted to first-order, second-class standards (Federal Geodetic Control Committee, 1984) and have an expected error propagation of 2 mm/km1/2. Loop closures in the network permit us to empirically calibrate the expected error; on Klauea's summit, we achieve about 2.2 mm/km1/2. Selected time series of elevations relative to a reference station, HV023, northwest of Klauea's caldera (fig. 1) show a steady rate of subsidence from 1996 through 2002 (fig. 4),  with a transient perturbation resulting from the January 1997 Npau Crater eruption. Contours of average rates of elevation change across the summit (fig. 5) show broad, asymmetric subsidence, with the locus of maximum subsidence in the southern caldera. The maximum subsidence rate is about 6 cm/yr, slightly lower than the 8 cm/yr measured during the early part of the Pu`u `` eruption from 1983 to 1990 (Delaney and others, 1998). The rate of vertical motion at station HVO23 over this time period is unknown. The vertical GPS data seem to systematically exceed the rates implied by the leveling data at the GPS stations by about 5 mm/yr. This small discrepancy likely reflects slow subsidence at leveling reference station, HVO23.  Figure 3. Klauea Volcano showing vectors of horizontal Global Positioning System (GPS) velocity from 1996 to 2002 with respect to a fixed Pacific Plate. Error ellipses at 95-percent-confidence level are scaled by repeatability about constant-velocity model. Major signal depicted is southeastward displacement of south flank, which reaches a maximum at coast and decays nor thward.  The Shallow Magmatic System of Klauea Volcano 151   Figure 4. Time series of elevations at leveling stations with respect to reference station HVO23 (fig. 1). A, Stations along line crossing summit caldera. B, Stations on rift zones. Station KF45 is on upper east rift zone; station CB75-74 is on southwest rift zone. Data from par tial sur vey of leveling network just a f t e r e a r l y 1 9 9 7 N  p a u C ra t e r er uption mar k only depar ture from steady rate of subsidence in summit area over this interval. These early 1997 data were not used in calculating average ver tical velocities.  Figure 5. Summit of Klauea Volcano, showing contours of ver tical-defor mation rates and horizontal Global Positioning System (GPS) velocities from 1996 to 2002. Contours, in 0.01-m/yr inter vals, are with respect to reference station HVO23. Ver tical velocities at GPS stations (red) in meters per year. Ver tical GPS velocities systematically exceed those from leveling data by about 5 mm/yr, probably because of subsidence at reference station HVO23.  152 The Pu`u ``-Kpaianaha Eruption of Klauea Volcano, Hawai`i: The First 20 Years   7  EXPLANATION RADIAL TILT, IN MICRORADIANS 6 5 4 3 2 1 0 UWE N46W SDH S38W IKI N97E POC N38W SDH 2 rad UWE IKI  0  2  4  6  8  10  12  14  16  18  20  22  24  26  28  30  32  34  36  38  40  42  44  46  48  DURATION, IN HOURS  Figure 6A. Time series of tilt from borehole tiltmeter stations at summit and Pu`u `` (POC) (fig. 1) for September 2426, 2000. We plot component of tilt radial to source, that is, tilt in the direction that, on average, maximizes magnitude of signal. Inset map shows tilt vectors (in black) at summit for hour-long inflation. In white are tilts predicted by model for inflating point source at location shown by red circle. At Pu`u ``, maximum tilt points away from cone's summit crater area.  Electronic Tilt HVO operates a network of 12 electronic tiltmeters, each set into a borehole ranging in depth from 5 to 40 m. The locations of the six tiltmeters that monitor Klauea's summit and Pu`u `` are shown in figure 1. Data from the tiltmeters are logged on site at a 1-minute sampling rate and telemetered back to the observatory every 10 minutes. The first electronic borehole tiltmeter (sta. UWE, fig. 1) was installed in 1998. The tiltmeter located on Pu`u `` (sta. POC, fig. 1), just northwest of its rim, was installed in early 2000. Since then, it has recorded about 10 tilt events that started at Klauea's summit and propagated toward the vent, indicating a change in pressure over the entire shallow magmatic system. Four of these events are highly self-similar. At both the summit and at Pu`u ``, these 2- to 3-day-long events are characterized by a period of slow deflation, then rapid inflation, followed by another period of slow deflation, bringing the final tilt close to the pre-event level. Three of these events were associated with a surge in the effusion rate at Pu`u ``, the one exception being the December 2001 event. The three stages of the surge-type tilt events start at the summit and propagate to Pu`u `` (figs. 6A6C). The initial deflation of 1 to 3 microradians at stations UWE and POC (fig. 1) takes 8 to 20 hours. The time between the onset of the tilt change at station UWE and the onset at station POC ranges from 1.5 to 2.5 hours. The inflationary stage is very rapid, with station UWE gaining 6 to 10 rad in 1 hour. Station POC starts inflating 20 to 30 minutes later and takes from 5 to 14 hours to gain 4 to 6 rad. The final deflation, its rate decaying exponentially, takes from 8 to more than 20 hours. At the summit, an intense burst of seismic energy precedes the rapid inflation by a few minutes. Seismicity ceases  just before the inflation begins, and resumes again as the inflation reaches its peak (figs. 6B6D). At the vent, seismic energy seems to decrease with the initial inflation and then increases as the inflation continues. Low-frequency seismicity remains relatively high as the tilt slowly decays to the preevent level. At Pu`u ``, the April 2002 deformation event (fig. 6D) differed from previous events in several ways. The initial deflation there was much greater than at station UWE (fig. 1), and the final deflation included numerous small tilt oscillations. During the deflationary periods of the oscillations, markedly increased seismic activity was observed. This oscillatory behavior may be related to increased pressure in Pu`u ``'s shallow magmatic system during this time and does not necessarily imply a different process from the other events. Tilt vectors at the summit clearly point to a source of deflation and inflation slightly east of Halemaumau (fig. 6). At Pu`u ``, tilt directions vary slightly more but generally point toward and away from the crater. Of the four deformation events described above, three were associated with eruptive surges, but the December 2001 event had no discernible effect on the eruption. The first signs of increased lava flux at the eruption site were generally observed shortly after Pu`u `` started inflating.  Model Clearly, the deformation field from the long-term source at the summit is qualitatively different from the deformation field associated with the episodic events, both in terms of their time scales and their spatial patterns. For this reason, we model the two modes of deformation separately. The longThe Shallow Magmatic System of Klauea Volcano 153   term deformation consists most the caldera and upper rift zones, south-flank motion, whereas the of a short-lived (~2 days) signal magmatic system.  ly of persistent subsidence in combined with the effects of episodic deformation consists throughout Klauea's shallow  General Methods To model the observed deformation fields, we use basic elasticity theory to predict tilts and displacements from point sources (Anderson, 1936; Mogi, 1958). Though mathematically simple, point sources are known to approximate the effects of a finite spherical magma reservoir quite well (McTigue, 1987). For the episodic source, the strength of the data, consisting of three tilt vectors, is insufficient to resolve fine detail in the reservoir structure. Thus, models more complex than a point source are probably not warranted. For the long-term source, we are interested in identifying the deformation arising from magma withdrawal beneath the southern caldera,  as opposed to, for example, rifting. For this reason, the point source is again appropriate because more complex magmareservoir models have a greater potential for absorbing deformation signals arising from other sources. We represent the Earth as a homogenous, linear, isotropic, elastic half-space. Although this representation of the Earth ignores surface topography, the effect of topography on a shield volcano like Klauea is small (McTigue and Segall, 1988). Nonetheless, for episodic deformation events, which appear to be quite shallow, we include a topographic corr e c t i o n f o r t wo r e a s o n s . F i r s t , u s e o f a t o p o g r a p h i c c o r r e c t i o n p e r m i t s t h e e s t a b l i s h m e n t o f a ve r t i c a l d a t u m , w h i c h i s e s sential when the source depth is not much greater than the scale of the topography. Second, approximation of a topographic correction for the vertical displacements (and tilts) from a point source is straightforward (Williams and Wadge, 2001). The effect of elastic heterogeneity is also small, alt h o u g h fa i l u r e t o i n c l u d e r e a l i s t i c e l a s t i c p r o p e r t i e s m a y b i a s source depths as too shallow by about 10 percent (Johnson and others, 2001).  0 10 FREQUENCY, IN HERTZ 8 6 4 2  4 NPT NPT  8  12  16  20  24  28  32  36  40  44  48  0 12 10 8 6 4 2 0 FREQUENCY, IN HERTZ 8 6 4 2 0 0 4 8 12 16 20 24 28 32 36 40 44 48 STC  EXPLANATION UWE N46W SDH S38W IKI N97E POC N38W SDH 2 rad UWE IKI  RADIAL TILT, IN MICRORADIANS  DURATION, IN HOURS  Figure 6B. Time series of radial component of tilt and seismic energy spectra for May 2022, 2001. Spectrograms are from stations NPT, near Halemaumau, and STC, near Pu`u `` (fig. 1). Inset map shows tilt vectors (in black) at summit for hour-long inflation. In white are tilts predicted by model of inflating point source at location shown by red circle.  154 The Pu`u ``-Kpaianaha Eruption of Klauea Volcano, Hawai`i: The First 20 Years   To invert the deformation fields for source location and strength, we cast the inversion problem as a nonlinear optimization (Cervelli and others, 2001). Specifically, we seek a source model that minimizes the difference between model predictions and observations. For the optimization, the quantity to be minimized is the mean square error, defined as MSE = rT -1 r / , where r is the residual vector (difference between observation and prediction),  is the data-covariance matrix, and  is the number of degrees of freedom (the number of data points minus the number of model parameters). A 2 test can be applied to the MSE to check whether the data have been fitted within errors at some confidence level. To be meaningful, however, the 2 test requires that the data covariance be well known. For the tilt data, the covariance is poorly known, because we have not accounted for uncertainties in tiltmeter azimuth, the scale factor from millivolts to microradians, and other factors. In the case of the leveling and GPS data, the data covariance is better known but still may be off by a scale factor. Moreover, the 2 test strictly applies only when a linear relation exists between model parameters and data. For a point source, only the vol10 FREQUENCY, IN HERTZ  ume change is linear. Therefore, even though we use the MSE as a quantitative representation of the misfit, we do so only as a matter of convenience and not because of the statistical properties of the MSE under the circumstances described above.  Long-Term Deformation The long-term pattern of deformation evident at Klauea's summit, well characterized by the continuous GPS and leveling data, is quite complex. This observation is not surprising because at least three separate geologic processes lead to significant deformation signals in this region: (1) motion of Klauea's south flank, (2) deflation in the summit magmatic system, and (3) rifting in the rift zones. Although south-flank motion and rifting introduce potentially large deformation signals in the summit region, we can partially separate them from magmatic deflation, the primary concern of this chapter. This separation is possible because the patterns of deformation from the two other sources differ considerably from the radial symmetry of a deflating magma reservoir. We are mindful, 40 48 56 64 72  0 NPT  8  16  24  32  8 6 4 2 0  EXPLANATION RADIAL TILT, IN MICRORADIANS UWE N46W 8 SDH S38W IKI N97E 6 POC N38W UWE SDH 2 rad 4 IKI  2  0 FREQUENCY, IN HERTZ 8 6 4 2 0 0 8 16 24 32 40 48 56 64 72 STC  DURATION, IN HOURS  Figure 6C. Time series of radial component of tilt and seismic energy spectra for December 811, 2001. Inset map shows tilt vectors (in black) at summit for hour-long inflation. In white are tilts predicted by model of inflating point source at location shown by red circle.  The Shallow Magmatic System of Klauea Volcano 155   however, as discussed in detail below, that a point source may absorb signals from other deformation sources. We use the full covariance of the leveling data (rnadttir and others, 1992), which accounts for the correlations among these data introduced by summing the section-height differences. The covariances from both the GPS and leveling data are scaled by their repeatability about a constant velocity. Inverting these two data sets for a single point source results in a model located in the southern caldera (figs. 7A7B) at 2.5 km below sea level and deflating at about 2.5 million km3/yr. H e n c e f o r wa r d , w e r e f e r t o t h i s d e f o r m a t i o n s o u r c e a s t h e ""south-caldera magma reservoir."" The residual (fig. 7C) between model predictions (fig. 7B) and observations (fig. 7A) shows that, although most of the subsidence is attributable to a point source, significant deformation remains unexplained by the simple point-source model. This result is consistent with our expectation. The horizontal component of the GPS data shows a clear southeastward displacement, almost certainly related to south-flank deformation, that cannot be explained by a symmetrical deflation. The long-wavelength signal in the vertical residual suggests an 0  extensional process coincident with the rift zones. The closed contours of the residual near the center of the caldera may result from minor deflation at the more shallow episodic source discussed in detail below. We expect that modeling all of the summit deformation with only a point source of deflation will somewhat bias our estimate of the point source's strength and location. Specifically, we expect that our depth and volume-change estimates will be biased on the high side, because deepening and strengthening the source will widen the wavelength of the predicted data, enabling the single point source to soak up some of the signal from the other deformation sources. To estimate how large this bias might be, we conducted an experiment, using the deformation model of Owen and others (2000). This model is complete in the sense that it contains deformation sources corresponding to each of the structures thought to contribute to summit deformation--a slipping dcollement, opening rift zones, and a deflating magma chamber. We began our experiment by calculating the deformation predicted by the model at each of the leveling benchmarks and GPS stations. Then we inverted this synthetic data set as 40  8  16  24  32  48  56  64  72  10  FREQUENCY, IN HERTZ  8 6 4 2 0  NPT  EXPLANATION UWE N46W SDH S38W IKI N97E POC N38W SDH 2 rad UWE  RADIAL TILT, IN MICRORADIANS  12 10 8 6 4 2 0 10  IKI  FREQUENCY, IN HERTZ  8 6 4 2 0 0  STC  8  16  24  32  40  48  56  64  72  DURATION, IN HOURS  Figure 6D. Time series of radial component of tilt and seismic energy spectra for April 47, 2002. Inset map shows tilt vectors (in black) at summit for hour-long inflation. In white are tilts predicted by model of inflating point source at location shown by red circle.  156 The Pu`u ``-Kpaianaha Eruption of Klauea Volcano, Hawai`i: The First 20 Years   15521' 0  15518' 1 KILOMETER  15515'  15521'  15518'  15515'  15521'  15518'  15515'  -1  -2  -4 -5  -6  -3  -4  -5  -1  0  0  1940'  -1  -2 -3  -1  -1 -2  -1  5 cm/yr  A  5 cm/yr  B  5 cm/yr  C  Figure 7. Comparison of obser ved deformation at Klauea's summit (fig. 1) from 1996 to 2002 and deformation predicted by model of spherical source located beneath southern caldera (red dot), about 2.5 km below sea level. A, 1-cm/yr contours of subsidence from leveling data relative to reference station HVO23, and horizontal motions of Global Positioning System (GPS) stations. B, Predicted displacements. C, Residual between model prediction and observation, that is, deformation not accounted for by point-source model.  above for a single point source. The synthetic data contain signals from all the deformation sources; we attempt to model the synthetic data, using only a point source. As expected, the inversion led to a source deeper and stronger than the original source given by Owen and others (2000), because the single point source absorbed some of the signal from the other deformation sources. The horizontal position was not significantly biased. Owen and others' point source was at 1.7-km depth (below the top of an elastic half-space, a datum different from the one used elsewhere in this chapter), whereas the depth resulting from our experiment was 2.2 km. More significantly, our estimated volume-change rate was almost twice that of Owen and others. Our initial estimates of volume-change rate and depth are clearly too high. Conservatively stated, the reservoir depth is no deeper than 2.5 km and is probably closer to 2 km below sea level; the volume-change rate is about 106 m3/yr and is certainly no greater than 2.5 million m3/yr. We remain confident of our estimated horizontal position, which is well constrained by the circular symmetry of the deformation field. The question of exactly how much of the vertical-deformation signal at the summit is explained by deflation of the southcaldera magma reservoir remains murky; however, we can say that a significant fraction of the vertical deformation (approx 25 percent) is attributable to other sources.  Episodic Deformation We estimate the location and volume change of the magma reservoir that deforms during episodic events by using the total magnitude and orientation of the tilt signals at stations UWE, IKI, and SDH (fig. 1) for rapid-inflation intervals. The tilt vectors for these intervals are especially well resolved because they are large and last only about an hour. The short duration limits contamination of the signal by various noise sources with predominantly diurnal frequencies. We account  for uncertainties in the tilt vectors by considering the scatter about the mean for short periods of time before and after rapid-inflation intervals, resulting in a (diagonal) covariance matrix that we can use to appropriately weight the data in the subsequent inversion. Model predictions and observations for each of the four deformation events and the horizontal locations of the source models are plotted in figure 6. The depths of the four sources vary considerably over a range of 130 to 450 m below ground level, or 975 to 650 m above sea level (fig. 8). This depth variation could represent a deepening of the source region over time or, alternatively, may simply reflect a large uncertainty in the depth estimate. Indeed, forward models reveal that the current summit tilt network does not provide good resolution of the depth of sources within about 1 km of the surface. Convinced that the four deformation events share a common horizontal coordinate, we performed an inversion, using all the data simultaneously to invert for a single three-dimensional coordinate plus four volume changes corresponding to the four different events. To the extent that the errors contaminating the data are not correlated from event to event, this procedure should strengthen the inversion considerably. Three two-component tiltmeters observing the four events give 24 data points to estimate the seven model parameters, in contrast to the event-by-event analysis, which gives only 6 data points to estimate four model parameters per event. The inversion is probably not strengthened as much as it seems, because many of the errors contaminating the data are systematic over time; for example, any azimuth misalignment of the tiltmeter does not change from event to event. Nonetheless, we expect some improvement by using this ""stacked"" source-inversion method, given that at least some of the errors are not temporally correlated. The green circle in figure 8 is the stacked estimate for the horizontal coordinate of the deformation source. We refer to this source as the ""Halemaumau magma reservoir"" because of its proximity to the prominent crater. The green line in figure 8 The Shallow Magmatic System of Klauea Volcano 157   shows the stacked estimate of the depth--about 850 m above sea level (350 m below ground level). Our estimated location of the Halemaumau magma reservoir agrees closely with that inferred from broadband seismic data (Ohminato and others, 1998). However, our estimate of the reservoir depth is somewhat shallower; Ohminato and others imaged a finite spherical source centered about 200 m above sea level (950 m below ground level), with a radius of about 0.5 km. As discussed below, we do not believe that this discrepancy is statistically significant at the 95-percent-confidence level. Another advantage of stacking the data over time is that we then have enough data points to estimate uncertainties in the model parameters by using the bootstrap method (Efron and Tibshirani, 1993). Briefly, this method involves randomly resampling the data vector (with replacement) numerous times and then estimating a new optimal source model for each resample. The empirical probability distribution of the model parameters is given by the parameter distribution resulting from the bootstrap. Histograms for the three model coordinates show that the horizontal coordinates are well constrained, but the depth shows a more asymmetric, long-tailed range of possibilities (fig. 9). Also shown in figure 9 is a scatter plot of depth versus volume change for the largest deformation event (May 2001). The clear correlation between these two model parameters implies that the parameters are not separable, given the current spatial distribution of data. Qualitatively, this result means that increased volume change can compensate for deeper sources, but only up to a point; the data do constrain the depth to be shallower than about sea level at a high confidence level. In the following discussion, we use the volume changes from four episodic deformation events to estimate magmasupply rates. The geometry of the current tilt network results in poor source-depth resolution and a high correlation between depth and volume change. To obtain unbiased estimates of           volume change, we decided to fix the source depth to 700 m above sea level (450 m below ground surface). This depth is not completely arbitrary because it lies well within the 95percent-confidence level estimated by the bootstrap method, although it is not the most probable depth. Inversions in a homogeneous source tend to bias source depths as too shallow, but this bias is probably only about 50 m. Our primary reason for choosing the 700-m depth is that this depth agrees well with the depth estimates from other geophysical methods (Ohminato and others, 1998) and, moreover, does not contradict the tilt data. As of this writing, we are installing a new tiltmeter north of Halemaumau that should help resolve the source-depth question.  Discussion We interpret these tilt events as resulting from an interruption or blockage in magma supply that affects both the Halemaumau magma reservoir and the conduit to Pu`u ``. At the onset of the interruption, deflation begins at the summit (and slightly later at Pu`u ``) as lava continues to exit the system through flank vents on Pu`u ``. This deflation is the result of continued withdrawal at a time of interrupted supply. When the interruption ends, rapid inflation ensues as the accumulated and overpressurized magma below the locus of interruption rushes up into the shallow magma system. We do not speculate here about the cause of the interruption, except to note that there are several possible explanations. If this ""blocked pipe"" model is correct, then the magnitude of the inflation should scale with the duration of the deflation times the magma-supply rate times an efficiency factor that measures what percentage of the magma supply is blocked. Assuming complete blockage, the magma-supply rate                                              Figure 8. Depth of modeled Halemaumau magma reservoir versus time for four shor t-lived tilt events. Horizontal line indicates depth estimated with ""stacked"" model, which combines data from all four events into a single inversion. Inset map shows narrow range of horizontal locations of each modeled source.            158 The Pu`u ``-Kpaianaha Eruption of Klauea Volcano, Hawai`i: The First 20 Years   NUMBER OF SAMPLES  can be estimated directly. The duration of deflation, magnitude of inflation, and estimated magma-supply rates for the four events are listed in table 1.  1200 1000 800 600 400 200 0 -15516'44""  Table 1. Characteristics of four self-similar tilt events. [Parameters are duration of deflationar y stage, estimated volume change during inflation, and corresponding magma-supply rate, assuming a blocked-pipe model.]  -15516'30"" LATITUDE W.  -15516'16""  1200 NUMBER OF SAMPLES 1000 800 600 400 200 0 1924'7"" 1924'22"" LATITUDE N. 1200 NUMBER OF SAMPLES 1000 800 600 400 200 0 100 200 300 400 500 600 700 800 900 1000 DEPTH, IN METERS VOLUME CHANGE, IN MILLIONS OF CUBIC METERS 3.0 2.5 2.0 1.5 1.0 0.5 0 100 200 300 400 500 600 700 800 900 1000 DEPTH, IN METERS 1924'36""  Other estimates of magma-supply rates come from very low frequency (VLF) measurements across lava tubes leading from Pu`u `` (Kauahikaua and others, 1996) and from SO2 e m i s s i o n m e a s u r e m e n t s f r o m t h e e r u p t i o n s i t e (Sutton and others, 2001). The VLF data estimate just the rate of lava leaving Pu`u ``; they are insensitive to the volume of magma being stored or emplaced elsewhere. Thus, the VLF measurements represent a minimum magma-supply rate into Klauea's magmatic system. Eruption-site SO2 emissions also represent a minimum estimate for similar reasons. As expected, the magma-supply rates implied by the blockedpipe model are systematically larger than those estimated by these two methods (fig. 10). The relative magnitudes of observed magma-supply rates seem to agree well with those predicted from the blocked-pipe model. Moreover, the exceptionally high rate we estimated during the May 2001 event corresponds to a large spike in SO2 emissions, although curiously this spike is absent in the VLF data. However, because VLF measurements give only the cross-sectional area of the flowing lava, the conversion to lava flux rate requires a velocity estimate. Therefore, if a direct measurement of flow velocity is unavailable, VLF measurements can miss a flux spike, provided the cross-sectional area of the flowing lava (the tube) does not increase. The tilt records give unambiguous estimates of the time lag between the onset of inflation at Klauea's summit and at Pu`u ``. Using a nominal distance of 20 km from the summit to Pu`u ``, we can estimate a propagation velocity. The tilt records also provide a direct estimate of the instantaneous flux rate into the shallow magmatic system during the inflationary stage. The propagation velocity is probably a reasonable proxy for the flow velocity through the conduit from the summit to Pu`u ``, because, owing to the blockage, the conduit Figure 9. Probability distribution of model parameters (latitude, longitude, depth), estimated with bootstrap method. Plot of depth versus volume change (for May 2000 event) shows close correlation between these parameters. All depths below sea level; ground surface is about 1,100 m above sea level.  The Shallow Magmatic System of Klauea Volcano 159   was in a state of low pressure before the onset of inflation. Moreover, if the propagation were really a pressure pulse, we would expect the velocity of the pulse to approximate the P-wave velocity of the magma, more than 1,000 m/s (Murase and McBirney, 1973). If we assume that the flux rate through the conduit were the same as that measured at the summit, then we can estimate the radius of the conduit. Of course, this estimate assumes a cylindrical conduit, but this geometry seems likely for thermal a n d m e c h a n i c a l r e a s o n s . A f t e r t h e i n i t i a l d i ke i n t r u s i o n , w e suggest that magma flow quickly centralized into a cylindrical conduit, much as a fissure eruption rapidly evolves into a single circular vent. Poiseuille flow, frequently used to model the flow of viscous fluid through a pipe, gives the following relation: r = (2 Q /  u)1/2, where r is flow veloc root of the even given the conduit radius, Q is the flux rate, and u is the ity. Because the radius is a function of the square ratio of flux to velocity, it is fairly well constrained, large uncertainties in Q and u. We estimate a mean  conduit radius of 2.750.5 m (table 2). A radius of this size, in combination with the maximum flow velocities and a typical viscosity for basaltic magma (100 Pas), leads to Reynolds numbers averaging about 1,750. This result implies laminar flow, possibly explaining why the conduit can survive the high flow velocities associated with these deformation events. Laminar, rather than turbulent, flow may also explain the observed absence of volcanic tremor along the inferred (see below) path of the conduit. Table 2. Estimates of conduit radius based on inferred magma-supply rate and velocity for four self-similar tilt events discussed in text.                                                                                                                          Figure 10. Comparison of magma-supply rates estimated from blocked-pipe model with (1) estimates of lava-effusion rates derived from SO2 emissions from the eruption site, and (2) estimates of lava flux from Pu`u `` based on ver y low frequency (VLF) measurements across lava tubes.  160 The Pu`u ``-Kpaianaha Eruption of Klauea Volcano, Hawai`i: The First 20 Years                                           Figure 11. Detail of station NPT (fig. 1) spectrogram and station UWE radial tilt from April 5, 2002, tilt event. Note burst of seismic energy preceding inflation, followed by marked pause in seismicity until well after inflationary stage is underway.  Neither the episodic events nor the long-term deformation gives any direct indication of the depth of the conduit from Klauea's summit to Pu`u ``. We consider two possible scenarios. First, the conduit could leave the Halemaumau magma reservoir and run subhorizontally until it intersects t h e s u r fa c e t o p o g r a p h y a t P u ` u `  `  . S e c o n d , t h e c o n d u i t could leave from the deeper south-caldera reservoir and run obliquely upward to the vent. Both the summit and Pu`u `` deflated more or less concurrently during the first phase of each deformation event, and so the interruption must have occurred below both the conduit and the Halemaumau magma reservoir. Although this information is insufficient to decide between these two possible scenarios, the fact that the interruption affected the whole shallow magmatic system leads to several interesting observations. A closeup of the spectrogram from a seismometer located at station NPT, just north of Halemaumau (fig. 1), during the April 2002 deformation event is shown in the upper part of figure 11, and the radial tilt at station UWE in the lower part. Note that an intense burst of seismic energy begins about 19 minutes before inflation is first recorded by the tiltmeter, and then ends abruptly about 15 minutes after it began and 4 minutes before inflation. We interpret this burst of seismic energy as volcanic tremor resulting from magma flow at or  near the point of interruption as the blockage is breached. Another burst of seismic energy appears on the spectrogram about 4 minutes into the inflation at the summit and lasts for another 45 minutes, ending approximately simultaneously with the inflation. This second burst of seismic energy could have resulted from tremor associated with flow out of the Halemaumau magma reservoir into the conduit toward Pu`u ``. Other equally plausible hypotheses remain because the nature of tremor is not well understood and the locus of tremor is extremely difficult to determine. For example, the second burst of seismic energy could be a combination of numerous small summit earthquakes, triggered by the sudden inflation, and tremor from flow into a conduit toward Pu`u `` situated well below the Halemaumau magma reservoir. Indeed, since the location of the blockage is unknown, the conduit to Pu`u `` could originate from the south-caldera reservoir and still not contradict the data. The connection between the south caldera reservoir, the Halemaumau magma reservoir, and Pu`u `` can be represented by two simple models: (1) a Y-shaped model with two branches from the south-caldera reservoir, one to the Halemaumau magma reservoir, the other to Pu`u ``; and (2) a -shaped model characterized by a single ""vertical"" conduit from the south-caldera reservoir to the Halemaumau magma The Shallow Magmatic System of Klauea Volcano 161   reservoir and a single ""horizontal"" conduit from the Halemaumau magma reservoir to Pu`u ``. We favor the -shaped model, for the following reasons. First, a shallow conduit is more likely than a deep conduit to remain open during the frequent pauses that have characterized many intervals of this eruption (see Heliker and Mattox, this volume), because a deep conduit would undergo significantly higher lithostatic pressures. During a pause, magma pressure within the conduit is presumed to drop; without high magma pressure, it is difficult to see how a deep conduit could remain open. Yet, after every pause to date, the eruption at Pu`u `` has resumed. Second, because the Halemaumau magma reservoir shows marked inflation during episodic deformation events and then deflates back to pre-event levels, the question arises of where the excess magma goes during deflation. A Y-shaped system implies that the magma drains back down, because there is no place else for it to go. In contrast, in a -shaped system, the excess magma simply makes its way to Pu`u `` and erupts onto the surface. It is difficult to understand why the Halemaumau magma reservoir would remain a persistently active part of the magmatic system under the Y-shaped scenario. Finally, shallow intrusions into the upper east rift zone are known to affect both the Halemaumau magma reservoir and Pu`u `` (Cervelli and others, 2002b). These dikes are probably too shallow to intersect a deep conduit to Pu`u ``, suggesting that a -shaped system is more plausible.  Acknowledgments We thank Rick Hoblitt for detailed early discussions a b o u t t h e b l o c ke d - p i p e i n t e r p r e t a t i o n o f t h e e p i s o d i c t i l t events. The manuscript could not even have been started without the efforts of Mike Lisowski, who built the current electronic borehole tiltmeter network on Klauea and was also instrumental in building the continuous GPS network. Mike had help; Maurice Sako and Fran Coloma contributed especially long hours of labor and planning. Maurice also led the small army of people needed to conduct the annual leveling surveys, and we thank all of them. Paul Okubo and Jean Battaglia provided us with spectrograms and helped us understand how to interpret them. Jim Kauahikaua and Jeff Sutton gave us the measurements we needed to compare our predicted flux estimates with some realistic benchmark. Thanks to Paul Segall and Susan Owen for their contributions to the Stanford/HVO GPS network. Michael Bevis and James Foster kindly provided data from the University of Hawai`i's Klauea GPS network. Sigurjon Jonsson and Jim Kauahikaua gave quick and thorough reviews; their comments greatly improved the manuscript. Finally, we thank the editors, Christina Heliker, Don Swanson, and Jane Takahashi, for taking on the task of organizing this volume and bringing it to completion.  References Cited Anderson, E.M., 1936, Dynamics of the formation of cone-sheets, ring-dykes, and cauldron-subsidences: Royal Society of Edinburgh Proceedings, v. 56, p. 128157. rnadttir, Thra, Segall, Paul, and Matthews, M.V., 1992, Resolving the discrepancy between geodetic and seismic fault models for the 1989 Loma Prieta, California, earthquake: Seismological Society of America Bulletin, v. 82, no. 5, p. 22482255. Cervelli, P.F., Murray, Michael, Segall, Paul, Aoki, Yosuke, and Kato, Teruyuki, 2001, Estimating source parameters from deformation data, with an application to the March 1997 earthquake swarm off the Izu Peninsula, Japan: Journal of Geophysical Research, v. 106, no. B6, p. 1121711238. Cervelli, P.F., Segall, Paul, Amelung, Falk, Garbeil, Harold, Meertens, C.M., Owen, S.E., Miklius, Asta, and Lisowski, Michael, 2002, The 12 September 1999 Upper East Rift Zone dike intrusion at Kilauea Volcano, Hawaii: Journal of Geophysical Research, v. 107, no. B7, p. ECV 31  ECV 313. Cervelli, P.F., Segall, Paul, Johnson, K.M., Lisowski, Michael, and Miklius, Asta, 2002, Sudden aseismic fault slip on the south flank of Kilauea volcano: Nature, v. 415, no. 6875, p. 10141018. Delaney, P.T., Miklius, Asta, rnadttir, Thra, Okamura, A.T., and Sako, M.K., 1993, Motion of Kilauea Volcano during sustained eruption from the Puu Oo and Kupaianaha vents, 19831991: Journal of Geophysical Research, v. 98, no. B10, p. 1780117820.  Conclusions The shallow magma system of Klauea consists of two distinct magma reservoirs: the Halemaumau reservoir at a depth of about 700 m above sea level, and the south-caldera reservoir at a depth of about 2,500 m below sea level. The south-caldera reservoir is slowly deflating at a rate of no more than about 2.5106 m3 per year, which is about 2 percent of the total volume of lava erupted from Pu`u `` each year. The Halemaumau magma reservoir undergoes characteristic, episodic deflation-inflation events that we interpret as resulting from an interruption in magma supply from depth. These events occur with no obvious periodicity. The duration of the deflation and the magnitude of the inflation permit estimates of the total flux through the shallow magma system. Our flux estimates show a good qualitative agreement with other flux estimates inferred from VLF and SO2-emission measurements. The location of the conduit from Klauea's summit to Pu`u `` is not well resolved by the available geodetic data. We argue, however, that the conduit probably emanates from the Halemaumau magma reservoir at a depth of about 700 m below the surface, with an inferred radius of about 2.75 m.  162 The Pu`u ``-Kpaianaha Eruption of Klauea Volcano, Hawai`i: The First 20 Years   Efron, Bradley, and Tibshirani, R.J., 1993, An introduction to the bootstrap (Monographs on Statistics and Applied Probability, no. 57): New York, Chapman & Hall/CRC, 436 p. Fiske, R.S., and Kinoshita, W.T., 1969, Inflation of Kilauea Volcano prior to its 19671968 eruption: Science, v. 165, no. 3891, p. 341349. Johnson, K.M., Segall, Paul, and Cervelli, P.F., 2001, Analytical methods for including vertical and lateral heterogeneity of elastic properties in dislocation models and applications to two c o s e i s m i c G P S d a t a s e t s [ a b s . ] : E o s ( A m e r i c a n G e o p hy s i c a l Union Transactions), v. 82, no. 47, supp., p. F296F297. Kauahikaua, J.P., Mangan, M.T., Heliker, C.C., and Mattox, T.N., 1996, A quantitative look at the demise of a basaltic vent; the death of Kupaianaha, Kilauea Volcano, Hawai`i: Bulletin of Volcanology, v. 57, no. 8, p. 641648. Lichten, S.M., 1990, Estimation and filtering for high-precision GPS positioning applications: Manuscripta Geodetica, v. 15, p. 159176. Lichten, S.M., and Border, J.S., 1987, Strategies for high-precision global positioning system orbit determination: Journal of Geophysical Research, v. 92, no. B12, p. 1275112762. McTigue, D.F., 1987, Elastic stress and deformation near a finite spherical magma body; resolution of the point source paradox: Journal of Geophysical Research, v. 92, no. B12, p. 1293112940. McTigue, D.F., and Segall, Paul, 1988, Displacements and tilts from dip-slip faults and magma chamber beneath irregular surface topography: Geophysical Research Letters, v. 15, no. 6, p. 601604. Mogi, Kiyoo, 1958, Relations between the eruptions of various volcanoes and the deformation of the ground surfaces above them: University of Tokyo, Earthquake Research Institute Bulletin, v. 36, no. 2, p. 99134.  Murase, Tsutomu, and McBirney, A.R., 1973, Properties of some common igneous rocks and their melts at high temperatures: Geological Society of America, v. 84, no. 11, p. 35633592. Ohminato, Takao, Chouet, B.A., Dawson, P.B., and Kedar, Sharon, 1998, Waveform inversion of very long period impulsive signals associated with magmatic injection beneath Kilauea Volcano, Hawaii: Journal of Geophysical Research, v. 103, no. B10, p. 2383923862. Okada, Yoshimitsu, 1985, Surface deformation due to shear and tensile faults in a half-space: Seismological Society of America Bulletin, v. 75, no. 4, p. 11351154. Owen, S.E., Segall, Paul, Lisowski, Michael, Miklius, Asta, Denlinger, R.P., and Sako, M.K., 2000, Rapid deformation of Kilauea Volcano; global positioning system measurements between 1990 and 1996: Journal of Geophysical Research, v. 105, no. B8, p. 1898318998. Segall, Paul, Cervelli, P.F., Owen, S.E., Lisowski, Michael, and Miklius, Asta, 2001, Constraints on dike propagation from c o n t i n u o u s G P S m e a s u r e m e n t s : J o u r n a l o f G e o p hy s i c a l Research, v. 106, no. B9, p. 1930119317. Sutton, A.J., Elias, Tamar, Gerlach, T.M., and Stokes, J.B., 2001, Implications for eruptive processes as indicated by sulfur dioxide emissions from Klauea Volcano, Hawai`i, 1979 1997: Journal of Volcanology and Geothermal Research, v. 108, no. 14, p. 283302. Tilling, R.I., and Dvorak, J.J., 1993, Anatomy of a basaltic volcano: Nature, v. 363, no. 6425, p. 125133. Williams, C.A., and Wadge, Geoffrey, 2000, An accurate and efficient method for including the effects of topography in three-dimensional elastic models of ground deformation with applications to radar interferometry: Journal of Geophysical Research, v. 105, no. B4, p. 81038120.  The Shallow Magmatic System of Klauea Volcano 163   164 The Pu`u ``-Kpaianaha Eruption of Klauea Volcano, Hawai`i: The First 20 Years"
GX261-42-10800161	"GLOBAL.EARTHQUAKE.SATELLITE.SYSTEM  GESS A 20-YEAR PLAN TO ENABLE  EARTHQUAKE  PREDICTION  MARCH 2003   4  GLOBAL.EARTHQUAKE.SATELLITE.SYSTEM  Earthquake Hazard Assessment in the Future  CHAPTER ONE  U  nderstanding the earthquake cycle and assessing ear thquake hazards is a  topic of both increasing potential for scientific advancement and societal urgency. A large por tion of the world's population inhabits seismically active regions, including the megacities of Los Angeles, Tokyo, and Mexico City, and heavily populated regions in Asia. Fur thermore, the recent devastating Gujurat earthquake in India and the New Madrid series of ear thquakes in the U.S. underscore the vulnerability of areas not thought to be tectonically active. Population growth will exacerbate the potential for huge ear thquake-related casualties, and economic losses of tens of billions of dollars will likely occur as a result of future large events. Since ear thquake losses, human and material, are primarily the result of structural failures, enforcing appropriate building codes and retrofitting structures can reduce the overall hazard.  Knowledge of the overall ear thquake hazard, and more specific regional and local ear thquake risk (at the scale of fault systems) is needed to effectively mitigate these ear thquake hazards. A global ear thquake observing system will monitor the behavior of interacting fault systems, identify unknown (subsur face) faults, guide new models of the deforming crust, and verify those dynamic models. This knowledge will translate into tangible societal benefits by providing the basis for more effective hazard assessments and mitigation efforts.  Inset and background: Effects of the Northridge, California, earthquake. (Robert Eplett, CA OES)   HAZARDS During the last decades, powerful new tools to obser ve tectonic deformation have been developed and deployed with encouraging results for improving knowledge of fault system behavior and earthquake hazards. In the future, the coupling of complex numerical models and orders of magnitude increase in obser ving power promises to lead to accurate, targeted, shortterm earthquake forecasting. D ynamic earthquake hazard assessments resolved for a range of spatial scales (large and small fault systems) and time scales (months to decades) will allow a more systematic approach to prioritizing the retrofitting of vulnerable structures, relocating populations at risk, protecting lifelines, preparing for disasters, and educating the public. The suite of spaceborne obser vations needed to achieve this vision has been studied, and the derived requirements have defined a set of mission architectures and enabling technologies that will accelerate progress in achieving the goal of improved earthquake hazard assessments. Three decades ago, earthquake prediction was thought to be an achievable goal. Such optimism has all but vanished in the face of current understanding of the complexity of the physics of earthquake fault systems. The advent of dense geodetic networks in seismically active regions (e.g., SCIGN, the Southern California Integrated Global Positioning System Network), and satellite interferometric synthetic aperture radar (InSAR) from the European Remote Sensing (ERS) satellites, have resulted in great progress in understanding fault ruptures, transient stress fields, and the collective behavior of fault systems, including transfer of stresses to neighboring faults following earthquakes (Freed and Lin, 2001; Pollitz and Sacks, 2002). These improved obser vations of surface deformation, coupled with advances in compu-  EARTHQUAKE.HAZARD.ASSESSMENT  5  tational models and resources, have stimulated numerical simulations of fault systems that attempt to reveal system behavior. As InSAR and Global Positioning System (GPS) data become more spatially and temporally continuous in the future, the modeling environment will rapidly evolve to achieve revolutionar y advances in understanding the emergent behavior of fault systems. This in turn will enable finer temporal resolution (dynamic) earthquake hazard assessments on the scale of individual faults and fault systems. D ynamic earthquake hazard assessment, coupled with rapid postearthquake damage assessments will enable more effective management of seismic disasters. The Global Earthquake Satellite System (GESS) study began with the requirements generated for the LightSAR mission, as well as those generated in an EarthScope workshop focused on InSAR ( J. B. Minster, personal communication, 2001). EarthScope is a National Science Foundation (NSF) initiative, carried out in partnership with the United States Geological Sur vey (USGS) and NASA, to study crustal deformation in North America. NASA's proposed contribution to the initiative is an InSAR satellite. Under EarthScope, NSF will field an array of approximately 1000 GPS monitoring sites across western North America, one or more strainmeters, and several deep drill holes near the San Andreas fault. The USGS will upgrade and expand its digital seismic network as its contribution. The synergistic combination of these measurements and InSAR-obser ved surface deformation is expected to yield major advances in understanding of the crustal structure and rheology of the continent. W hereas the requirements for a near-term InSAR satellite are well understood, the future needs, which are not well defined, are the driver for our study. Therefore, we have exam-   6  GLOBAL.EARTHQUAKE.SATELLITE.SYSTEM  ined the outstanding questions concerning the physics and forecasting of earthquakes, and used these as the basis of a Request for Proposals, issued by JPL, to fund studies that defined measurement requirements for an obser ving system that could answer them. These questions are: 1. How does the crust deform during the interseismic period between earthquakes and what are its temporal characteristics (if any) before major earthquakes? 2. How do earthquake ruptures evolve both kinematically and dynamically and what controls the earthquake size? 3. W hat controls the spacetime characteristics of complex earthquakes and triggered earthquakes and aftershocks? 4. What are the sources and temporal characteristics of postseismic processes and how does this process relate to triggered seismicity? 5. How can we identify and map earthquake effects postseismically or identify regions with a high susceptibility to amplified ground shaking or liquefaction/ground failure? 6. Are there precursor y phenomena (potential field, electromagnetic effects, or thermal field changes) preceding earthquakes that could be resolved from space? Incorporating this community input, we have formulated a more stringent set of requirements for measurement of surface deformation that will answer questions 14, and we consider approaches to addressing questions 5 and 6. The drivers for these requirements are discussed below and in Chapter 2.  all phases of the earthquake cycle (pre-, co-, and postseismic), across multiple fault systems and tectonic environments, with global distribution. Satellites offer the best way to achieve global coverage and consistent obser vations of the land surface. W hile ground seismometer and GPS networks are and will remain critical, the synoptic view of the deforming crust that is possible using satellite data drives the need for a global earthquake satellite obser ving system. In addition, knowledge of the character of the shallow subsurface is critical to assessing expected ground accelerations. Su r face Deformation Measurements  Elements of a Global Ear t hquak e Sa tellit e Obser v ing S y st em Efforts to advance understanding of earthquake physics require detailed obser vations of  Measurement of surface change (displacement) constitutes a powerful tool for resolving the deformation fields resulting from tectonic strain (Figure 1.1). Surface deformation includes other components besides tectonic strain, such as surface motion due to groundwater storage and retrieval (Bawden et al., 2001). The InSAR technique relies on correlated image-pairs to derive displacements to the resolution of a fraction of the radar wavelength. If topography is known, two images can be used to derive a map of the displacement in the range direction. Additional image pairs obtained from different look directions (i.e., ascending versus descending) improve the resolution of vertical and horizontal displacements. If topography is not known, three images can be differenced to derive the topography and its change. The accuracy of the measurement depends on several factors, including the radar signal-to-noise ratio (SNR), orbit determination precision, and removal of signal path delays caused by the variations in spurious ionospheric electron density and tropospheric water vapor. All of these errors must be minimized to achieve long-term absolute accuracy of interseismic strain accumulation.   HAZARDS  EARTHQUAKE.HAZARD.ASSESSMENT  7  Figure 1.1 Ear thquakes can cause significant surface deformation, such as this meter offset from an earthquake in the California desert. (Robert Eplett, CA OES)  S ubsur fac e Charac t e ristics  The type of material in the shallow subsurface, and its saturation, affect the ground acceleration experienced as a result of a particular earthquake. Directivity of seismic energ y during fault rupture can result in quite different patterns of deformation. Liquefaction, the sudden release of water from saturated, permeable layers, is of particular concern in coastal landfill areas, and on steep slopes. Mapping the degree of saturation in the shallow subsurface will help determine landslide hazards, and may allow the liquefaction hazard to be folded into the overall dynamic earthquake hazard assessment. Radar sounders, along with InSAR displacements, can provide data to augment surface measurements that seek to characterize the subsurface. Elec tromagnetic and Thermal A nomal y Precursors  Many claims have been made concerning the correlation of magnetic fields, electric fields, and seismicity, including precursor y electromagnetic signals. Mechanisms to produce such correlative variations include move-  ment of fluids in fault zones as a result of stress changes preceding ruptures, and piezomagnetic effects of stress field changes. Improvements in data quality and quantity over the past 40 years have led to a substantial decrease in the correlated signals ( Johnston, 1997). Magnetic anomalies associated with main shocks are well documented and can be accounted for by piezomagnetic effects. The subject of precursor y electromagnetic signals, and a satisfactor y mechanism to explain them, requires more laboratory and field research, as well as high-quality continuous ground and satellite magnetic field data series with proper reference control. Recognizing subtle signals generated at the surface against the background of the highly dynamic external magnetic field at satellite altitudes is challenging. These correlations are likely best tested using carefully configured ground networks in seismogenic zones. A weak infrared (IR) thermal anomaly was obser ved near the epicenter of the October 1999 Hector Mine, California, earthquake (Figure 1.2). This and other suggested correlations between thermal IR anomalies and   8  GLOBAL.EARTHQUAKE.SATELLITE.SYSTEM  Figure 1.2 Landsat data for Mojave Desert, California, on October 15, 1999, hours before the Hector Mine earthquake. The visible scene is on the left, and the thermal difference between October 15 and an image from September 29, 1999 is shown at right. A weak thermal anomaly intersects the fault segment that broke in the Hector Mine earthquake (yellow line). (R. Crippen, JPL)  Ag Fields Broadwell Dry Lake  Ag Fields  Rugged Topo  earthquakes have been studied with inconclusive results. As with electromagnetic anomalies, more robust correlations and plausible mechanisms are needed to assess this potential stress indicator. The current Advanced Spaceborne Thermal Emission Radiometer (ASTER) and Landsat ETM+ instruments have good spatial resolution, and may provide data to test existing hypotheses, but coverage is sparse.  Sp a t i a l and Te mporal Measurement Requirements The primar y focus of the GESS study was the measurement of surface deformation, as this has emerged as the top priority for space-based obser vation of the earthquake cycle. Light detection and ranging (LIDAR) systems can provide precise measurements of vertical surface change through clear air and even beneath vegetation canopies. Wideswath LIDAR is thus a promising technique for complementing InSAR (Hofton and Blair, 2002; Chao et al., 2002), especially in vegetated areas.  Detailed requirements for InSAR data gathering have been collected to support three main objectives: long-term measurement of interseismic strain accumulation (to <1 mm/yr resolution), detailed maps of coseismic deformation to define the fault rupture, and measurement of transient deformation such as postseismic relaxation and stress transfer following earthquakes, aseismic creep, and slow earthquakes. To maximize correlation between scenes, especially at interannual time scales, an L-band system is preferred. The mid-term and far-term requirements are summarized in Table 1.1. Obser ving interseismic strain accumulation drives the need for ver y precise long-term accuracy. To distinguish between hazards from blind thrust and shallow faults requires deformation rates to be resolved at the 1 mm/yr level over 10 years. Achieving this accuracy requires mitigating the tropospheric and ionospheric noise in the images, as well as reducing orbit errors. Fortunately, the strain accumulation process is steady, so stacking and filtering techniques can be used to remove   HAZARDS these sources of noise. Short repeat periods enable frequent data acquisitions to support these needs. A promising approach to mitigate the tropospheric water vapor delay is to combine the radar obser vations with other atmospheric data to derive the water vapor content along the radar line-of-sight. For interseismic strain measurements, the length of the data series may be more important than the revisit frequency and the requirement is on the order of 10 years for an L-band system. Obser vation of coseismic deformation drives the need for precise instantaneous accuracy and short revisit times. Exponentially decaying postseismic processes will obscure the coseismic signals with time following the event. Also, good spatial resolution is needed to precisely map the decorrelation and displacement close to the rupture. Transient postseismic strain, as well as aseismic creep and slow earthquakes, drive the need for frequent revisit times to capture these events. Chapter 2 discusses the measurement needs in greater detail.  EARTHQUAKE.HAZARD.ASSESSMENT  9  Co n c e p t Mission Archit ec tur e s The scientific requirements for studying earthquakes drive two main components of a proposed Global Earthquake Satellite System: accurate, high-resolution surface deformation measurements; and timely, global coverage. Interferometric synthetic aperture radar techniques provide spatially continuous observations of surface movements in the form of high-resolution displacement maps. InSAR produces unique, spatially continuous, distributed obser vations. The line-of-sight components of surface displacements can be determined to fractional-wavelength accuracies over hundreds of kilometers at high resolutions (tens of meters). Three-dimensional  vector displacement information can be derived by combining ascending, descending, right-looking, and left-looking data. A key performance parameter for a disaster and hazard monitoring system is the timely access to and coverage of the target area. InSAR deformation maps can only be generated when the SAR sensor passes overhead and a prior reference data set exists; therefore, the instantaneous field of view (accessible area), and the likelihood that any given target will be covered within a given time are crucial design parameters. As such, two point designs were selected early in the study to provide innovative radar mission architectures that add perspective to the traditional and tested low-Earth orbit (LEO) missions flown at altitudes from 560870 km. Most LEO SAR designs to date, including those of the widely used ERS 1 and 2 satellites, have involved swath widths of around 100 km, and therefore have required orbit repeat periods of around 3040 days in order to provide global coverage. With the use of ScanSAR techniques (Tomiyasu, 1981), as on RADARSAT and the Shuttle Radar Topography Mission (SRTM), the SAR swath can be extended significantly at the expense of image resolution. This can be a worthy trade, as characterizing coseismic fault rupture requires rapid accessibility -- the ability to map a specified target area at a critical time -- but only moderate resolution. However, to implement repeat-pass interferometr y with a ScanSAR system, the along-track ScanSAR bursts would have to be precisely aligned between orbits. This has not been done before. Increasing the satellite elevation can also enhance the accessibility of a SAR sensor, as doing so generally increases the area the satellite can view at any given time. Generally,   10  GLOBAL.EARTHQUAKE.SATELLITE.SYSTEM  Table 1.1 Requirements for sur face deformation measurements.  MINIMUM  GO AL  Displacement Accuracy 3D Displacement Accuracy Displacement Rate Temporal Accessibility (Science) Temporal Accessibility (Disaster) Daily Coverage Map Region Spatial Resolution Geolocation Accuracy Swath Data Latency in Case of Event  25 mm instantaneous 50 mm (1 week) 2 mm/yr (over 10 yr) 8 days 1 day 6  106 km 2  5 mm instantaneous 10 mm (1 day) <1 mm/yr (over 10 yr) 1 day or less 2 hrs Global (land) Global 330 m 3m 500 km Minutes to hours   60 latitude 50100 m 25 m 100 km 1 day  it is found that a SAR will only operate satisfactorily if it has a certain minimum antenna area. That area, A, is  where  is the velocity of the satellite relative to the Earth,  is the wavelength, R is the range to the target, c is the speed of light,  is the incidence angle, and k is a weighting factor that depends on the specific sidelobe requirements and is generally on the order of 1.42.0. As the range R increases with platform altitude more quickly than the velocity  decreases, the antenna size must increase with orbit elevation. However, the accessible area increases as well. Thus, to the extent that the mission cost is not 100% dominated by the radar aperture size, one will achieve greater efficiency in terms of accessible area per dollar by raising the elevation of the satellite. As past SAR system studies have focused on elevations in the range 560820 km, and the performance of such systems is fairly well  understood, we have studied the placement of a SAR satellite in a higher, ""enhanced LEO"" configuration (LEO+) at an altitude of 1325 km. This design is largely evolutionar y relative to present and past LEO SAR systems. The orbit is a proven TOPEX-class orbit, and the radar hardware could be built from existing technology. However, the higher altitude affords a much larger accessible area than traditional LEO systems. By increasing the satellite elevation even higher for the purpose of improving its accessibility, one can imagine operating a SAR in a geosynchronous orbit (Figure 1.3). Such a system provides an enormous instantaneous field of view, and is also able to provide data at ver y high resolution, in contrast to optical sensors at those altitudes. However, the technological challenges are significant not only because of the very large active antenna aperture required, but also due to issues relating to processing the extremely long apertures, in particular in   HAZARDS higher resolution modes (210 m horizontal). As a SAR uses the relative motion between itself and the target to achieve high resolution, synthetic aperture formation will be impossible from a geostationar y geometr y, where the radar location is fixed in Earth body fixed coordinates (EBFC). However, when the inclination of the orbit is not zero, the satellite will be moving in EBFC. We have primarily studied circular orbits with inclinations between 50 and 65. In these cases, the ground track will resemble that shown in Figure 1.3 (a figure eight). In terms of the Earth surface area that is in view from a single satellite at a given time, a geosynchronous satellite will outperform a LEO-type satellite by two orders of magnitude, thus requiring far fewer satellites to cover the globe entirely at all times. The trade-study comparing LEO-type systems to geosynchronous SAR systems is, however, complicated for several reasons. A geosynchronous SAR would require an extremely large antenna aperture, which would involve the use of technologies that are not yet mature. A geosynchronous SAR would also differ from a LEO SAR in its coverage characteristics. Contrar y to LEO satellites, a geosynchronous satellite can be placed to provide focused regional coverage for a limited set of Earth longitudes. A minimum of three geosynchronous satellites will be required for global coverage. The radar processing required for a geosynchronous SAR would also differ quite dramatically from that of a LEO system because of the peculiar characteristics of geosynchronous orbits, as well as atmospheric changes over the long integration times that arise from the long apertures and low relative velocities. It will also be necessar y to address dynamic atmospheric (troposphere and ionosphere)  EARTHQUAKE.HAZARD.ASSESSMENT  11  correction, which is presently not well understood and not tested at all. In addition, we study constellations based on those two point designs. The constellations provide insight as to what future systems could provide in terms of an operational mapping capability. Constellations of satellites capable of providing obser vations on a ver y frequent basis (many obser vations each day) were studied for the LEO+, MEO (medium Earth orbit), and geosynchronous cases. In these evaluations, the relevant performance measure was the likelihood that a given position on the ground would be mapped within a given time. The constellations were also assessed for accuracy in providing 3-D displacement measurements. A key concern in repeat-pass interferometr y is so-called temporal decorrelation. W hile InSAR measurements reflect the collective displacement of all scatterers within a given image resolution cell -- typically tens of meters wide to fractional-wavelength accuracy -- the technique breaks down when the scattering centers within the resolution cell experience different displacements, or when the dominant scatterers change from one obser vation to the next. For example, the vegetation in the resolution cell might induce temporal decorrelation. At longer wavelengths, the radar returns would come mainly from plant branches and trunks, so the signal might decorrelate over periods of weeks to months. At short wavelengths, the radar echoes might come primarily from the leaves, which can decorrelate in seconds as the leaves move with the wind. Precipitation and the freezing or thawing of the ground will also introduce significant temporal decorrelation. Longer wavelengths tend to exhibit better correlation properties over extended time periods. In rela-   12  GLOBAL.EARTHQUAKE.SATELLITE.SYSTEM  Figure 1.3 Orbit and ground trace of a geosynchronous satellite at a 50 orbit inclination (figure eight). Instantaneous field of view for a 5000-km SAR swath is shown (blue). Orbital path and instantaneous field of view for a LEO+ SAR is also shown (pink).  tion to vegetation, longer wavelengths tend to look through the lighter components, such as leaves, to primarily ""see"" the more stable elements such as branches, trunks, and the ground. The frequency trade-off is counterbalanced by issues such as the ionosphere, and the antenna size. These factors suggest that L-band (approximately 24 cm wavelength) is a good compromise for the frequency selection. The designs presented are based on a single polarization design, to keep cost at a minimum. It is conceivable that a polarimetric capability would allow forming interferograms from polarimetric combinations that would reduce the decorrelation from vegetation. Also, to bridge the two extreme design points of LEO+ and geosynchronous, we performed a parametric analysis indicating key performance parameters at altitudes in between. Interestingly, the analysis hints that for future around-the-clock monitoring, medium Earth orbit (MEO) configuration, with somewhat smaller antennas and reduced costs relative to geosynchronous, might offer a ver y capable and effective trade-off.  The scientific requirements outlined in Table 1.1 can be met by various SAR architectures. The report details those architectures in the following chapters. The most promising concepts are a constellation of six to twentyfour SAR satellites in LEO or LEO+ (1325 km) orbits, or three to six geosynchronous SARs. A few LEO+ satellites can optimize most of the requirements, but ver y short revisit times require larger constellations.  Exp e c t ed B e nefits Impro v ed Ear thquake Hazard Assessments  Current seismic hazard assessments rely on historical earthquake catalogs to predict the statistical probability of future earthquakes. However, there is a spectrum of crustal deformation driven by plate motions that is transient and/or aseismic. Our incomplete knowledge of the deformation budget is a major obstacle to improving predictive capabilities. It is difficult to verify predictive models against infrequent and sparse seismic and geodetic data. There is a debate as to whether the crust is in a constant state of self-orga-   HAZARDS nized criticality in seismic zones, or whether the crust approaches and retreats from that state in a cyclic pattern; the answer has profound implications for the predictability of earthquakes. One promising model posits that normalized surface shear strain across faults, obtainable from dense InSAR data, appears to be a proxy for the unobser vable stress-strain dynamics that govern fault rupture (Rundle et al., 2002). The ability to resolve surface deformation to the centimeter level over the entire globe will result in hundreds of earthquakes each year that can be analyzed to test and improve predictive models (Melbourne et al., 2002). Community models will produce dynamic earthquake hazard assessments by using obser vations in real time, mining the data, and adjusting the earthquake hazard assessments based on the emerging model system behavior. This will allow more effective use of portable ground networks or arrays of instruments (laser strainmeters, seismometers, magnetometers) to capture information on transient fault behavior leading up to an event. W hile predicting the time, location, and size of a particular earthquake will remain elusive, much higher fidelity earthquake forecasts appear within reach. The total seismic risk includes the likelihood of a particular seismic event, and the response of any particular site to the seismic waves generated. The worst damage occurs in regions of directed seismic energy, and liquefaction (the sudden liquification of permeable sedimentar y layers) often amplifies the damage. Very precise surface deformation measurements will help to identify aquifer discharge and recharge, and can provide information on the saturation of vulnerable subsurface sedimentar y layers (Tobita et al., 2002). This knowledge can be folded into the  EARTHQUAKE.HAZARD.ASSESSMENT  13  earthquake hazard assessments to produce a localized, dynamic measure of seismic risk.  Disast er Management The dynamic earthquake hazard assessments described will provide the disaster management community with information to focus mitigation efforts. Such efforts include prioritizing retrofitting projects to protect lifelines and infrastructure, educating the public, staging emergency supplies, and establishing mobile communication networks. Earthquake hazard assessment models should be interfaced with decision support systems to guide mitigation efforts. Temporal revisit times on the order of hours following an event are required to effectively support disaster response efforts. Mapping zones of decorrelation will be most useful to the emergency workers on the ground. Areas that decorrelate between interferograms obtained prior to a seismic event and those that span the event indicate changes in the built environment, and zones of intense shaking that can focus response efforts. InSAR has the advantage of being an all-weather capability for either day or night, an important consideration for obtaining time-critical measurements. Radar-equipped uninhabited aerial vehicles may play an important role in disaster response efforts. A SAR constellation would allow a staring capability that would reveal the details of transient postseismic behavior and could be particularly useful in the hours and days following a great earthquake to assess the stress transfer and loading of neighboring fault systems, potentially predicting large damaging aftershocks and triggered earthquakes.   JPL 400-1069 03/03"
GX058-95-16645126	"Next   Previous   Contents     2. Traditional Artificial Intelligence         Traditional AI is based around the ideas of logic, rule systems, linguistics, and the concept of rationality.  At its roots are programming languages such as Lisp and Prolog. Expert systems are the largest successful example of this paradigm.  An expert system consists of a detailed knowledge base and a complex rule system to utilize it.  Such systems have been used for such things as medical diagnosis support and credit checking systems.      2.1 AI class/code libraries        These are libraries of code or classes for use in programming within the artificial intelligence field.  They are not meant as stand alone applications, but rather as tools for building your own applications.              ACL2     Web site:   www.telent.net/cliki/ACL2     ACL2 (A Computational Logic for Applicative Common Lisp) is a theorem prover for industrial applications. It is both a mathematical logic and a system of tools for constructing proofs in the logic.  ACL2 works with GCL (GNU Common Lisp).        AI Kernel     Web site:   aikernel.sourceforge.net   Sourceforge site:   sourceforge.net/projects/aikernel/     The AI Kernel is a re-usable artificial intelligence engine that uses natural language processing and an Activator / Context model to allow multi tasking between installed cells.          AI Search II     WEB site:   www.bell-labs.com/topic/books/ooai-book/     Submitted by:   Peter M. Bouthoorn   Basically, the library offers the programmer a set of search algorithms that may be used to solve all kind of different problems. The idea is that when developing problem solving software the programmer should be able to concentrate on the representation of the problem to be solved and should not need to bother with the implementation of the search algorithm that will be used to actually conduct the search. This idea has been realized by the implementation of a set of search classes that may be incorporated in other software through  C++ 's features of derivation and inheritance.  The following search algorithms have been implemented:        depth-first tree and graph search.   breadth-first tree and graph search.   uniform-cost tree and graph search.   best-first search.   bidirectional depth-first tree and graph search.   bidirectional breadth-first tree and graph search.   AND/OR depth tree search.   AND/OR breadth tree search.       This library has a corresponding book, ""  Object-Oriented Artificial Instelligence, Using C++ "".        Aleph     Web site:   web.comlab.ox.ac.uk/oucl/research/areas/machlearn/Aleph/     This document provides reference information on A Learning Engine for Proposing Hypotheses (Aleph). Aleph is an Inductive Logic Programming (ILP) system. Aleph is intended to be a prototype for exploring ideas. Aleph is an ILP algorithm implemented in Prolog by Dr Ashwin Srinivasan at the Oxford University Computing Laboratory, and is written specifically for compilation with the YAP Prolog compiler        Chess In Lisp (CIL)     Web site: *found as part of the CLOCC archive at:   clocc.sourceforge.net       The CIL (Chess In Lisp) foundation is a Common Lisp implementaion of all the core functions needed for development of chess applications.  The main purpose of the CIL project is to get AI researchers interested in using Lisp to work in the chess domain.            DAI     Web site:   starship.python.net/crew/gandalf/DNET/AI/     A library for the Python programming language that provides an object oriented interface to the CLIPS expert system tool. It  includes an interface to COOL (CLIPS Object Oriented Language) that allows:    Investigate COOL classes   Create and manipulate with COOL instances   Manipulate with COOL message-handler's   Manipulate with Modules           FFLL     Web site:   ffll.sourceforge.net     The Free Fuzzy Logic Library (FFLL) is an open source fuzzy logic class library and API that is optimized for speed critical applications, such as video games. FFLL is able to load files that adhere to the  IEC 61131-7 standard.        HTK     Web site:   htk.eng.cam.ac.uk     The Hidden Markov Model Toolkit (HTK) is a portable toolkit for building and manipulating hidden Markov models.  HTK consists of a set of library modules and tools available in C source form. The tools provide sophisticated facilities for speech analysis, HMM training, testing and results analysis. The software supports HMMs using both continuous density mixture Gaussians and discrete distributions and can be used to build complex HMM systems.  The HTK release contains extensive documentation and examples.        JACK     Web site:   www.pms.informatik.uni-muenchen.de/software/jack/     JACK is a new library providing constraint programming and search for Java.    JACK consists of three components:   - JCHR: Java Constraint Handling Rules. A high-level language to write constraint solvers.   - JASE: Java Abstract Search Engine. A generic search engine for JCHR to solve constraint  problems.   - VisualCHR: An interactive tool to visualize JCHR computations.     Source and documentation available from link above.        LK     Web site:   www.cs.utoronto.ca/~neto/research/lk/     LK is an implementation of the Lin-Kernighan heuristic for the Traveling Salesman Problem and for the minimum weight perfect matching problem. It is tuned for 2-d geometric instances, and has been applied to certain instances with up to a million cities. Also included are instance generators and Perl scripts for munging TSPLIB instances.   This implementation introduces ``efficient cluster compensation'', an experimental algorithmic technique intended to make the Lin-Kernighan heuristic more robust in the face of clustered data.          Nyquist     Web site:   www.cs.cmu.edu/afs/cs.cmu.edu/project/music/web/music.html       The Computer Music Project at CMU is developing computer music and interactive performance technology to enhance human musical experience and creativity. This interdisciplinary effort draws on Music Theory, Cognitive Science, Artificial Intelligence and Machine Learning, Human Computer Interaction, Real-Time Systems, Computer Graphics and Animation, Multimedia, Programming Languages, and Signal Processing. A paradigmatic example of these interdisciplinary efforts is the creation of interactive performances that couple human musical improvisation with intelligent computer agents in real-time.        OpenCyc     Web site:   www.opencyc.org     OpenCyc is the open source version of Cyc, the largest and most complete general knowledge base and commonsense reasoning engine. An ontology based on 6000 concepts and 60000 assertions about them.        PDKB     Web site:   lynx.eaze.net/~pdkb/web/   SourceForge site:   sourceforge.net/projects/pdkb     Public Domain Knowledge Bank (PDKB) is an Artificial Intelligence Knowledge Bank of common sense rules and facts. It is based on the Cyc Upper Ontology and the MELD language.          Python Fuzzy Logic Module     FTP site:   ftp://ftp.csh.rit.edu/pub/members/retrev/     A simple python module for fuzzy logic. The file is 'fuz.tar.gz' in this directory. The author plans to also write a simple genetic algorithm and a neural net library as well. Check the 00_index file in this directory for release info.        QUANT1     Web site:   linux.irk.ru/projects/QUANT/     QUANT/1 stands for type QUANTifier. It aims to be an alternative to Prolog-like (Resulutional-like) systems. Main features include a lack of necessity for eliminating Quantifiers, scolemisation, ease of comprehension, large scale formulae operation, acceptance of nonHorn formulaes, and Iterative deeping. The actual library implemented in this project is called ATPPCF (Automatic Theorem Prover in calculus of Positively Constructed Formulae).  ATPPCF will be a library (inference engine) and an extension of the Predicate Calculus Language as a new logical language. The library will be incorporable in another software such as TCL, Python, Perl. The engine's primary inference method will be the ""search of inference in language of Positively Constructed Formulas (PCFs)"" (a subset of Predicate Calculus well translated in both directions). The language will be used as scripting language to the engine. But there will be possibility to replace it with extensions languages of main software.          Screamer     Web site:   www.cis.upenn.edu/~screamer-tools/home.html     Screamer is an extension of Common Lisp that adds support for nondeterministic programming. Screamer consists of two levels. The basic nondeterministic level adds support for backtracking and undoable side effects.  On top of this nondeterministic substrate, Screamer provides a comprehensive constraint programming language in which one can formulate and solve mixed systems of numeric and symbolic constraints. Together, these two levels augment Common Lisp with practically all of the functionality of both Prolog and constraint logic programming languages such as CHiP and CLP(R). Furthermore, Screamer is fully integrated with Common Lisp. Screamer programs can coexist and interoperate with other extensions to Common Lisp such as CLOS, CLIM and Iterate.        SPASS     Web site:   spass.mpi-sb.mpg.de     SPASS: An Automated Theorem Prover for First-Order Logic with Equality  If you are interested in first-order logic theorem proving, the formal analysis of software, systems, protocols, formal approaches to AI planning, decision procedures, modal logic theorem proving, SPASS may offer you the right functionality.        ThoughtTreasure     Web site:   www.signiform.com/tt/htm/tt.htm     ThoughtTreasure is a project to create a database of commonsense rules for use in any application. It consists of a database of a little over 100K rules and a C API to integrate it with your applications. Python, Perl, Java and TCL wrappers are already available.      Torch     Web site:   www.torch.ch     Torch is a machine-learning library, written in C++.  Its aim is to provide the state-of-the-art of the best algorithms.  It is, and it will be, in development forever.      Many gradient-based methods, including multi-layered perceptrons, radial basis functions, and mixtures of experts.  Many small ""modules"" (Linear module, Tanh module, SoftMax module, ...) can be plugged together.                Support Vector Machine, for classification and regression.                Distribution package, includes Kmeans, Gaussian Mixture Models, Hidden Markov Models, and Bayes Classifier, and classes for speech recognition with embedded training.                  Ensemble models such as Bagging and Adaboost.    Non-parametric models such as K-nearest-neighbors, Parzen Regression and Parzen Density Estimator.                     Torch is an open library whose authors encourage everybody to develop new packages to be included in future versions on the official website.                      2.2 AI software kits, applications, etc.            These are various applications, software kits, etc. meant for research in the field of artificial intelligence. Their ease of use will vary, as they were designed to meet some particular research interest more than as an easy to use commercial package.          ASA - Adaptive Simulated Annealing     Web site:   www.ingber.com/#ASA-CODE   FTP site:   ftp.ingber.com/       ASA (Adaptive Simulated Annealing) is a powerful global optimization C-code algorithm especially useful for nonlinear and/or stochastic systems.    ASA is developed to statistically find the best global fit of a nonlinear non-convex cost-function over a D-dimensional space. This algorithm permits an annealing schedule for 'temperature' T decreasing exponentially in annealing-time k, T = T_0 exp(-c k^1/D). The introduction of re-annealing also permits adaptation to changing sensitivities in the multi-dimensional parameter-space. This annealing schedule is faster than fast Cauchy annealing, where T = T_0/k, and much faster than Boltzmann annealing, where T = T_0/ln k.            Babylon     FTP site:   ftp.gmd.de/gmd/ai-research/Software/Babylon/     BABYLON is a modular, configurable, hybrid environment for developing expert systems. Its features include objects, rules with forward and backward chaining, logic (Prolog) and constraints. BABYLON is implemented and embedded in Common Lisp.        cfengine     Web site:   www.iu.hio.no/cfengine/     Cfengine, or the configuration engine is a very high level language for building expert systems which administrate and configure large computer networks. Cfengine uses the idea of classes and a primitive form of intelligence to define and automate the configuration of large systems in the most economical way possible. Cfengine is design to be a part of computer immune systems.        CLEARS     Web site: ???? (anyone know where to find this anymore)     The CLEARS system is an interactive graphical environment for computational semantics. The tool allows exploration and comparison of different semantic formalisms, and their interaction with syntax. This enables the user to get an idea of the range of possibilities of semantic construction, and also where there is real convergence between theories.        CLIG     Web site:   www.ags.uni-sb.de/~konrad/clig.html     CLIG is an interactive, extendible grapher for visualizing linguistic data structures like trees, feature structures, Discourse Representation Structures (DRS), logical formulas etc. All of these can be freely mixed and embedded into each other. The grapher has been designed both to be stand-alone and to be used as an add-on for linguistic applications which display their output in a graphical manner.        CLIPS     Web site:   www.ghg.net/clips/CLIPS.html     CLIPS is a productive development and delivery expert system tool which provides a complete environment for the construction of rule and/or object based expert systems.      CLIPS provides a cohesive tool for handling a wide variety of knowledge with support for three different programming paradigms: rule-based, object-oriented and procedural.  Rule-based programming allows knowledge to be represented as heuristics, or ""rules of thumb,"" which specify a set of actions to be performed for a given situation. Object-oriented programming allows complex systems to be modeled as modular components (which can be easily reused to model other systems or to create new components).  The procedural programming capabilities provided by CLIPS are similar to capabilities found in languages such as C, Pascal, Ada, and LISP.        EMA-XPS - A Hybrid Graphic Expert System Shell     Web site:   www.iai.uni-wuppertal.de/EMA-XPS/       EMA-XPS is a hybrid graphic expert system shell based on the ASCII-oriented shell Babylon 2.3 of the German National Research Center for Computer Sciences (GMD). In addition to Babylon's AI-power (object oriented data representation, forward and backward chained rules - collectible into sets, horn clauses, and constraint networks) a graphic interface based on the X11 Window System and the OSF/Motif Widget Library has been provided.        FOOL & FOX     Web site:   rhaug.de/fool/   FTP site:   ftp.informatik.uni-oldenburg.de/pub/fool/       FOOL stands for the Fuzzy Organizer OLdenburg. It is a result from a project at the University of Oldenburg. FOOL is a graphical user interface to develop fuzzy rulebases.  FOOL will help you to invent and maintain a database that specifies the behavior of a fuzzy-controller or something like that.    FOX is a small but powerful fuzzy engine which reads this database, reads some input values and calculates the new control value.        FUF and SURGE     Web site:   www.cs.bgu.ac.il/research/projects/surge/index.htm   FTP site:   ftp.cs.bgu.ac.il/pub/fuf/     FUF is an extended implementation of the formalism of functional unification grammars (FUGs) introduced by Martin Kay specialized to the task of natural language generation. It adds the following features to the base formalism:    Types and inheritance.    Extended control facilities (goal freezing, intelligent  backtracking).    Modular syntax.     These extensions allow the development of large grammars which can be processed efficiently and can be maintained and understood more easily.  SURGE is a large syntactic realization grammar of English written in FUF. SURGE is developed to serve as a black box syntactic generation component in a larger generation system that encapsulates a rich knowledge of English syntax. SURGE can also be used as a platform for exploration of grammar writing with a generation perspective.        The Grammar Workbench     Web site: ???   www.cs.kun.nl/agfl/           Seems to be obsolete??? Its gone from the site, though its parent  project is still ongoing.   The Grammar Workbench, or GWB for short, is an environment for the comfortable development of Affix Grammars in the AGFL-formalism. Its purposes are:     to allow the user to input, inspect and modify a grammar;    to perform consistency checks on the grammar;    to compute grammar properties;    to generate example sentences;    to assist in performing grammar transformations.            GSM Suite     Web site:   www.slip.net/~andrewm/gsm/     The GSM Suite is a set of programs for using Finite State Machines in a graphical fashion. The suite consists of programs that edit, compile, and print state machines. Included in the suite is an editor program, gsmedit, a compiler, gsm2cc, that produces a C++ implementation of a state machine, a PostScript generator, gsm2ps, and two other minor programs. GSM is licensed under the GNU Public License and so is free for your use under the terms of that license.        Illuminator     Web site:   documents.cfar.umd.edu/resources/source/illuminator.html     Illuminator is a toolset for developing OCR and Image Understanding applications.  Illuminator has two major parts: a library for representing, storing and retrieving OCR information, heretofore called dafslib, and an X-Windows ""DAFS"" file viewer, called illum. Illuminator and DAFS lib were designed to supplant existing OCR formats and become a standard in the industry. They particularly are extensible to handle more than just English.  The features of this release:     5 magnification levels for images   flagged characters and words   unicode support -- American, British, French, German,  Greek, Italian, MICR, Norwegian, Russian, Spanish, Swedish,  keyboards    reads DAFS, TIFF's, PDA's (image only)   save to DAFS, ASCII/UTF or Unicode   Entity Viewer - shows properties, character choices,  bounding boxes image fragment for a selected entity, change  type, change content, hierarchy mode           Isabelle     Web site:   isabelle.in.tum.de     Isabelle is a popular generic theorem prover developed at Cambridge University and TU Munich. Existing logics like Isabelle/HOL provide a theorem proving environment ready to use for sizable applications. Isabelle may also serve as framework for rapid prototyping of deductive systems. It comes with a large library including Isabelle/HOL (classical higher-order logic), Isabelle/HOLCF (Scott's Logic for Computable Functions with HOL), Isabelle/FOL (classical and intuitionistic first-order logic), and Isabelle/ZF (Zermelo-Fraenkel set theory on top of FOL).        Jess, the Java Expert System Shell     Web site:   herzberg.ca.sandia.gov/jess/     Jess is a clone of the popular CLIPS expert system shell written entirely in Java. With Jess, you can conveniently give your applets the ability to 'reason'. Jess is compatible with all versions of Java starting with version 1.0.2. Jess implements the following constructs from CLIPS: defrules, deffunctions, defglobals, deffacts, and deftemplates.          learn     FTP site:   sunsite.unc.edu/pub/Linux/apps/cai/     Learn is a vocable learning program with memory model.         LISA     Web site:   lisa.sourceforge.net     LISA (Lisp-based Intelligent Software Agents) is a production-rule system heavily influenced by JESS (Java Expert System Shell). It has at its core a reasoning engine based on the Rete pattern matching algorithm. LISA also provides the ability to reason over ordinary CLOS objects.        NICOLE     Web site:   nicole.sourceforge.net     NICOLE (Nearly Intelligent Computer Operated Language Examiner) is a theory or experiment that if a computer is given enough combinations of how words, phrases and sentences are related to one another, it could talk back to you. It is an attempt to simulate a conversation by learning how words are related to other words. A human communicates with NICOLE via the keyboard and NICOLE responds back with its own sentences which are automatically generated, based on what NICOLE has stored in it's database. Each new sentence that has been typed in, and NICOLE doesn't know about, is included into NICOLE's database, thus extending the knowledge base of NICOLE.        Otter: An Automated Deduction System     Web site:   www-unix.mcs.anl.gov/AR/otter/     Our current automated deduction system  Otter is designed to prove theorems stated in first-order logic with equality.  Otter's inference rules are based on resolution and paramodulation, and it includes facilities for term rewriting, term orderings, Knuth-Bendix completion, weighting, and strategies for directing and restricting searches for proofs.   Otter can also be used as a symbolic calculator and has an embedded equational programming system.        PVS     Web site:   pvs.csl.sri.com/     PVS is a verification system: that is, a specification language integrated with support tools and a theorem prover. It is intended to capture the state-of-the-art in mechanized formal methods and to be sufficiently rugged that it can be used for significant applications. PVS is a research prototype: it evolves and improves as we develop or apply new capabilities, and as the stress of real use exposes new requirements.        SNePS     Web site:   www.cse.buffalo.edu/sneps/   FTP site:   ftp.cse.buffalo.edu/pub/sneps/     The long-term goal of The SNePS Research Group is the design and construction of a natural-language-using computerized cognitive agent, and carrying out the research in artificial intelligence, computational linguistics, and cognitive science necessary for that endeavor. The three-part focus of the group is on knowledge representation, reasoning, and natural-language understanding and generation. The group is widely known for its development of the SNePS knowledge representation/reasoning system, and Cassie, its computerized cognitive agent.          Soar     Web site:   ai.eecs.umich.edu/soar/       Soar has been developed to be a general cognitive architecture. We intend ultimately to enable the Soar architecture to:    work on the full range of tasks expected of an intelligent agent, from highly routine to extremely difficult, open-ended problems   represent and use appropriate forms of knowledge, such as procedural, declarative, episodic, and possibly iconic   employ the full range of problem solving methods    interact with the outside world and    learn about all aspects of the tasks and its performance on them.      In other words, our intention is for Soar to support all the capabilities required of a general intelligent agent.      TCM     Web site: ???   www.cs.utwente.nl/~tcm/   FTP site:   ftp.cs.vu.nl/pub/tcm/       TCM (Toolkit for Conceptual Modeling) is our suite of graphical editors. TCM contains graphical editors for Entity-Relationship diagrams, Class-Relationship diagrams, Data and Event Flow diagrams, State Transition diagrams, Jackson Process Structure diagrams and System Network diagrams, Function Refinement trees and various table editors, such as a Function-Entity table editor and a Function Decomposition table editor.  TCM is easy to use and performs numerous consistency checks, some of them immediately, some of them upon request.          WEKA     Web site:   lucy.cs.waikato.ac.nz/~ml/       WEKA (Waikato Environment for Knowledge Analysis) is an state-of-the-art facility for applying machine learning techniques to practical problems. It is a comprehensive software ""workbench"" that allows people to analyse real-world data. It integrates different machine learning tools within a common framework and a uniform user interface. It is designed to support a ""simplicity-first"" methodology, which allows users to experiment interactively with simple machine learning tools before looking for more complex solutions.              Next   Previous   Contents"
GX001-65-9619083	"A CRONYMS   &  A BBREVIATIONS         A     B       C       D       E       F       G       H       I       J       K       L       M      N       O       P       Q       R       S       T       U       V       W       X       Y    Z            Numeric      4WM   Four Wave Mixing            A       AAMI   Association for the Advancement of Medical Instrumentation   AAPM   American Association of Physicists in Medicine   ACR (ACRii)   Absolute Cryogenic Radiometer (second upgrade)   ACS   American Chemical Society   ACTS   Automated Computer Time Service   ADCL   Accredited Dosimetry Calibration Laboratory   ADMIT   Analytical Detection Methods for the Irradiation Treatment of foods   AECL   Atomic Energy Canada Limited    AEDC   Arnold Engineering Development Center   AFB   Air Force Base   AFGL   Air Force Geophysics Laboratory   AFM   Atomic Force Microscope   AFOSR   Air Force Office of Scientific Research   AFPL   Air Force Phillips Laboratory   AFRRI   Armed Forces Radiobiology Research Institute   AI   Associative Ionization   AIAA   American Institute for Aeronautics and Astronautics   AIGER   American Industry Government Emissions Research Consortium   AIP   American Institute of Physics   ALG   Advanced Lithography Group   AM1   First Launch in the morning series of EOS platforms   AMMAC   Mexican Metrology Association   AMO   Atomic, Molecular and Optical   AMS   Accelerator-Mass Spectrometry   ANS   American Nuclear Society   ANSI   American National Standards Institute   ANSOM   Apertureless Near-Field Scanning Optical Microscopy   ANVIS   Aviator Night Vision Imaging System   APAS   Astrophysical, Planetary and Atmospheric Sciences    APHIS   Animal and Plant Health Inspection Service   APOMA   American Precision Optics Manufacturers Association   APRF   Army Pulse Radiation Facility   APS   American Physical Society  or  Advanced Photon Source   APT   Annular Proton Telescope   ARO   Army Research Office   ARPES   Angle Resolved Photoelectron Spectroscopy    ART   Algebraic Reconstruction Technique   ASCA   Advanced Satellite for Cosmology & Astrophysics   ASCA   Japan-NASA X-ray Satellite   ASME   American Society for Mechanical Engineers   ASSI   Airglow Solar Spectrometer Instrument   ASTER   Advance Spaceborne Thermal Emission and Reflectance Radiometer   ASTM   American Society for Testing and Materials   AT&T   Atlantic Telephone & Telegraph   ATD   Above-Threshold Dissociation   ATI   Above-Threshold Ionization   ATLAS   Atmospheric Laboratory for Applications and Science   ATP   Advanced Technology Program   ATW   Accelerator Transmutation of Waste   AURA   Association of Universities for Research in Astronomy   AXAF   Advanced X-ray Astrophysical Facility   AXAF-I   Imaging Advanced X-ray Astrophysical Facility   AXAF-S   Spectroscopy Advanced X-ray Astrophysical Facility             B     BARC   Bhabha Atomic Research Centre   BB   Blackbody   BB   Broad-bandwidth   BBIR   Broad Band Infrared   BBO   Beta-Barium Borate   BBXRT   Broad-Band X-Ray Telescope   BCC   Broadband Calibration Chamber   BCS   Bardeen-Cooper-Schrieffer theory of superconductivity   BCS   Bragg Crystal Spectrometer   BEB   Binary-encounter-Bethe   BEC   Bose-Einstein Condensation   BED   Binary-encounter-dipole   BEEM   Ballistic Electron Emission Spectroscopy   BEV   Bundesamt für Eich- und Vermessungswesen, Vienna, Austria   BFRL   Building & Fire Research Laboratory   BGSM   Bowman Gray School of Medicine   BIB   Blocked Impurity Band   BIFL   Burst Integrated Fluorescence   BIPM   Bureau International des Poids et Mesures   BL   Beam Line at SURF-III   BMDO   Ballistic Missile Defense Organization   BNL   Brookhaven National Laboratory   BNM   National Bureau of Metrology, France   BOMAB   Bottle Manikin Absorber   BRDF   Bidirectional Reflectance Distribution Function   BSDF   Bidirectional Scattering Distribution Function   BSS   Beta Secondary System   BSS.2   Beta-Particle Secondary Standards System   BTI   Bubble Technology, Inc.   BWO   Backward Wave Oscillator   BXR   BMDO Transfer Radiometer            C     CAD   Computer Aided Design   CAM   Computer Aided Machining   CAMOS   Committee on Atomic, Molecular and Optical Sciences   CAO   Carlsbad Area Office   CARB   Center for Advanced Research in Biotechnology   CARS   Coherent Anti-Stokes Raman Spectroscopy   CASS   Calibration Accuracy Support System   CAST   Council of Agricultural Science and Technology   CBNM   Central Bureau for Nuclear Measurements   CCD   Charged Coupled Device   CCDM   Consultative Committee for the Definition of the Meter   CCDS   Consultative Committee for the Definition of the Second   CCE   Consultative Committee on Electricity   CCEMRI   Consultative Committee for Ionizing Radiations, CIPM   CCG   Calibration Coordination Group   CCP6   Collaborative Computational Project 6   CCPR   Consultative Committee on Photometry and Radiometry   CCRI   Comité Consultatif des Rayonnements Ionisants   CCTF   Consultative Committee for Time and Frequency   CDRH   Center for Devices and Radiological Health   CEL   Correlated Emission Laser   CEMRC   Carlsbad Environmental Monitoring Research Center   CERES   Clouds and Earth's Radiant Energy System   CFS   Constant-Final-State Spectroscopy   CHEM-1   Chemistry   CIAQ   Committee on Indoor Air Quality   CIE   Commission Internationale De L'Éclairage   CIPM   International Committee of Weights and Measures, France   CIRMS   Council on Ionizing Radiation Measurements and Standards   CIRRPC   Committee on Interagency Radiation Research and Policy Coordination   CIS   Constant-Initial-State Spectroscopy   CMC   Calibration Measurement Capabilities   CMR   Colossal Magnetoresistance   CNIF   Californium Neutron Irradiation Facility   CNRF   Cold Neutron Research Facility   CODATA   Committee on Data for Science and Technology   CORM   Council for Optical Radiation Measurements   COSPAR   Committee on Space Research   CPIC   International Physics Center, Elba   CPT   conjugation, parity, and time   CPU   Central Processing Unit   CR   Cascaded Rectifier Accelerator   CRADA   Cooperative Research and Development Agreement   CRCPD   Conference of Radiation Control Program Directors   CRDS   Cavity-Ring-Down Spectroscopy   CRI   Cambridge Research Instrumentation   CRP   Coordinated Research Program   CRTs   Cathode Ray Tubes   CRYRING   Electron Accelerator and Storage Ring Facility (Stockholm, Sweden)   CSDA   Continuous-Slowing-Down Approximation   CSEWG   Cross Section Evaluation Working Group   CSI   Compton Scatter Imaging   CSIC   Consejo Superior de Investigaciones Científicas   CSIR   Council of Scientific and Industrial Research   CSTL   Chemical Science and Technology Laboratory   CT   Computed Tomographic   CTI   Critical Technologies Institute   CU   University of Colorado   CVD   Chemical Vapor Deposition     cw (CW)   continuous wave   CY   Calendar Year            D     DARPA   Defense Advanced Research Project Agency   DEC   Digital Electronics Corporation   DMA   Defense Mapping Agency   DNA   Deoxyribose Nucleic Acid  or  Defense Nuclear Agency   DOC (DoC)   Department of Commerce   DOD (DoD)   Department of Defense   DOE (DoE)   Department of Energy   DOELAP   Department of Energy Laboratory Accreditation Program   DORT   Discrete Ordinates Code   DPPC   dipalmitoylphosphatidylcholine   DRAM   Dynamic Random Access Memory   DRIP   Detector Response Intercomparison Program   DSA   Digital Subtraction Angiography   DSC   differential scattering cross section   DVM   Digital Voltmeter   DUV   Deep Ultraviolet            E      EBIS   Electron Beam Ion Source   EBIT   Electron Beam Ion Trap   ec   electron-capture   ECP   Effective Core Potential   ECPR   Electrically-Calibrated Pyroelectric Radiometer   ECR   Electron Cyclotron Resonance   ECRIS   Electron-Cyclotron-Resonance Ion Source   ECS   Energy-Corrected-Sudden   ECSED   Electronic Commerce in Scientific and Engineering Data   EDX   Energy-Dispersive X-ray Analysis   EEEL   Electronics & Electrical Engineering Laboratory   EELS   Electron Energy Loss Spectroscopy   EEO   Equal Employment Opportunity   EEP   Einstein Equivalence Principle   EM   Environmental Management   ENEA   Ente per le Nuove Tecnologie, L'Energia E L'Ambiente   ENDF   Evaluated Nuclear Data File   ENDL   Evaluated Nuclear Data Library   ENSDF   Evaluated Nuclear Structure Data File   EOS   Earth Observing System   EPA   Environmental Protection Agency   EPIC   Earth Polychromatic Imaging Camera   ERATO   Exploratory Research for Advanced Technology Office   EROS   Electric Resonance Optothermal Spectrometer   ESA   European Space Agency   ESB   Electrical Substitution Bolometer   ESDIAD   Electron-Stimulated Desorption Ion Angular Distributions   ESO   European Southern Observatory   ESR   Experimental Storage Ring  or  Electrical Substitution Radiometer   ESR   Electron Spin Resonance (EPR now preferred)   ETI   Environmental Technology Initiative   ETRAN   Monte Carlo computer code for Electrons and Photons through Extended Media   EURADOS   European Radiation Dosimetry Group   EURATOM   European Nuclear Energy Organization   EUROMET   A European collaboration in measurement standards   EUV   Extreme Ultraviolet   EUVE   Extreme Ultraviolet Explorer   EUVL-LLC   EUV Lithography - Limited Liability Corporation    EXAFS   Edge X-ray Absorption Fine Structure            F     FAA   Federal Aviation Administration   FACSS   Federation of Analytical Chemistry and Spectroscopy Societies   FAD   FASCAL Absolute Detector   FARCAL   Facility for Advanced Radiometric Calibrations   FASCAL   Facility for Automatic Spectroradiometric Calibrations   FCCSET   Federal Coordinating Council Science, Engineering and Technology   FCDC   Fundamental Constants Data Center   FCPM   Fundamental Constants and Precision Measurements   FDA   U.S. Food and Drug Administration   FEA   Field-Emitter Arrays   FEDs   Field-Emitter Displays   FEL   Free Electron Laser   FEMA   Federal Emergency Management Agency   FET   Field Effect Transistor   FIMS   Fissionable Isotope Mass Standards   FIR   Far Infrared   FLIR   Forward Looking Infrared Radiometer   FM   Frequency Modulation   FNR   Ford Nuclear Reactor   FOS   Faint Object Spectrograph   FOV   Field of View   FPC   Fullerene Production Chamber   FR   Filtered Radiometer   FRET   Fluorescence Resonant Energy Transfer   FT   Fourier Transform   FT-IRAS   Fourier Transform-Infrared Reflection Absorption Spectroscopy   FTIR   Fourier Transform Infrared   FTMS   Fourier Transform Microwave Spectroscopy   FTMW   Fourier Transform Microwave   FTS   Fourier Transform Spectroscopy   FUSE   Far Ultraviolet Spectroscopic Explorer   FUV   Far Ultraviolet   FWHM   Full Width at Half Maximum   FY   Fiscal Year            G      GAMS 4 (GAMS4)   NIST High Resolution Spectrometer   GDRIMS   Glow-Discharge Resonance Ionization Mass Spectrometry   GEC   Gaseous Electronics Conference   GFP   Green Fluorescent Protein    GHRS   Goddard High Resolution Spectrograph   GIM   Grazing-Incidence Monochromator   GINGA   Japanese X-ray Satellite   GMR   Giant Magnetoresistance   GOES   Geostationary Operational Environmental Satellite   GPIB   General Purpose Instrumentation Bus   GPS   Global Positioning System   GRI   Gas Research Institute   GRT   Germanium Resistance Thermometer   GSFC   Goddard Space Flight Center   GSI   Gessellschaft für Schwerionenforschung   GVHD   Graft-Versus Host Disease            H      HACR   High Accuracy Cryogenic Radiometer   HALO   Hypersonic Aircraft Launch Option   HBM   Hybrid Bilayer Membrane   HCCD   Mono-deuterated Acetylene   HDR   High Dose Rate   HFIR    High Flux Isotope Reactor   HID   High Intensity Discharge   HIRDLS   High Resolution Dynamics Limb Sounder   HPGe   High Purity Germanium   HPLC   High Pressure Liquid Chromatographic   HPS   Health Physics Society   HPSSC   Health Physics Society Standards Committee   HR3DCT   High Resolution 3-D Computed Tomography HRTS   High Resolution Telescope Spectrograph   HSST   Heavy Section Steel Technology   HST   Hubble Space Telescope   HTBB   High-Temperature Black Body   HTD   Heat Transfer Division   HTS   High-Temperature Superconductivity   HUT   Hopkins Ultraviolet Telescope   HVL   Half-Value Layer            I     IAEA   International Atomic Energy Agency   IAG   International Association of Gravity   IAU   International Astronomical Union   IC   Integrated Circuit   ICAMDATA   International Conference on Atomic and Molecular Data   ICP   Inductively Coupled Plasma   ICPEAC   International Conference on the Physics of Electronic and Atomic Collisions   ICRM   International Committee for Radionuclide Metrology   ICRP   International Commission on Radiological Protection   ICRU   International Commission on Radiation Units and Measurements   ID (id or i.d.)    inside diameter   IDMS   Isotope Dilution Mass Spectrometry   IEC   International Electrotechnical Commission   IEEE   Institute of Electrical and Electronics Engineers   IEN   Istituto Elettrotecnico Nazionale (Italy)   IES   Illumination Engineering Society   IESNA   Illumination Engineering Society of North America   IGC   International Gravity Commission   IHPRPTP   Integrated High Payoff Rocket Propulsion Technology Program   ILL   Institut Laue Langevin   ILS   International Laser Spectroscopy   IMECE   International Mechanical Engineering Congress and Exposition  IMGC   Istituto di Metrologia ""G. Colonnetti"" (Italy)   IMS   Institute for Molecular Science   INDC   International Nuclear Data Committee   INISO-TTC   Experimental Radiochromic Film   INM   Institute National de Metrologie   INMM   Institute for Nuclear Materials Management   INMRI-ENEA   Istituto Nazionale di Metrologia delle Radiazioni Ionizzanti - Ente per le Nuove Tecnologie   IPNS   Intense Pulsed Neutron Source   IPSN   L'Institut de Protection et de Sûreté Nucléaire   IPTS   International Practical Temperature Scale   IQEC   International Quantum Electronics Conference   IQI   Image Quality Indicators   IR   Infrared   IRAS   Infrared Astronomical Satellite  or  Infrared Reflection Absorption Spectroscopy   IRB   Institutional Review Board   IRDCF   Infrared Detector Calibration Facility   IRMM   Institute of Reference Materials and Measurements   ISCC   Inter-Society Color Council   ISO   International Organization for Standardization   ISP   International Specialty Products   ISCC   Information System to Support Calibrations   ISS   International Space Station   ISSI   International Space Science Institute (Bern, Switzerland)   ITAMP   International Meeting of Theory of Atomic and Molecular Physics   ITL   Information Technology Laboratory   ITEP   Institute for Theoretical and Experimental Physics   ITER   International Thermonuclear Experimental Reactor   ITS   International Temperature Scale   ITU   International Telecommunication Union   IUCr   International Union of Crystallography   IUE   International Ultraviolet Explorer   IVBT   Intravascular Brachytherapy   IVR   Intramolecular Vibrational Relaxation   IVR   Intramolecular Vibrational Redistribution   IWG   Investigators Working Group            K    KCDB   Key Comparison and Calibration Database            J    JAERI   Japan Atomic Energy Research Institute   JANNAF   Joint Army-Navy-NASA-Air Force   JCCRER   U.S.-Russia Joint Coordinating Committee  on Radiation Effects Research   JCGM   Joint Committee for Guides on Metrology   JCMT   James Clerk Maxwell Telescope   JET   Joint European Torus   JILA   formerly Joint Institute Laboratory for Astrophysics   JEOL   JEOL USA, Inc.   JPL   Jet Propulsion Laboratory            L      LAGOS   Laser Gravitational-Wave Observatory in Space   LAMPF   Los Alamos Meson Physics Facility   LAMSCAL   Large Area Monochromatic Source for Calibrations   LANL   Los Alamos National Laboratory   LANSCE   Los Alamos Neutron Scattering Center   LASP   Laboratory for Atmospheric and Space Physics, University of Colorado   LBIR   Low Background Infrared Radiometry   LBL   Lawrence Berkeley Laboratory   LBRS   Low Background Reference System   LCD   Liquid Crystal Display   LCIF   Laser-Collision-Induced Fluorescence   LED   Light Emitting Diode   LEED   Low Energy Electron Diffraction   LEI   Laser-Enhanced Ionization   LET   Linear Energy Transfer   LF   Low Frequency   LIF   Laser Induced Fluorescence   LIGO   Laser Interferometric Gravitational-Wave Observatory   LISA   Laser Interferometer Space Antenna   LLNL   Lawrence Livermore National Laboratory   LMR   Laser Magnetic Resonance   LMRI   Laboratoire de Mesure des Rayonnements Ionisants  (France)   LO   Laser Optics   LORAN-C   A Radio Navigation System Operated by the U.S. Coast Guard   LPMs   Light-Particle Monitors   LPRI   Laboratoire Primaire des Rayonnements Ionisants, Gif-sur-Yvette, France   LPRT   Light-Pipe Radiation Thermometers   LPTF   Laboratoire Primaire du Temps et des Fréquencies   LS   Liquid Scintillation   LSC   Liquid Scintillation Counting   LTE   Local Thermodynamic Equilibrium   LT   Low-Temperature   LTEC   Lamp Testing Engineers Conference   LTG   Low-Temperature Growth   LVIS   Low Velocity Intense Source   LWIR   Long Wave Infrared            M      MARLAP   Multi-Agency Radiological Laboratory Procedures   MARS   Multiple-Angle Reference System   MBE   Molecular Beam Epitaxy   MBIR   Medium Background Infrared Facility   MBOS   Molecular-Beam Optothermal Spectrometer   MCNP   Monte Carlo Neutron Photon (computer code)   MCQDT   Multi Channel Quantum Defect Theory   MCT   Mercury-cadmium-telluride   MCU   Mobile Calibration Unit   MDRF   Materials Dosimetry Reference Facility   MEA   Materials Engineering Associates   MEDEA   Microelectronics Development for European Applications   MEIBEL   Merged Electron-Ion Beam Energy Loss   MEL   Manufacturing Engineering Laboratory   MEMS   Micro Electro Mechanical Systems   MET   Medium Energy Telescope   MEVVA   Metal Vapor Vacuum Arc   MIDAS   Modular Interactive Data Acquisition System   MIL   Military   MIM   Metal-Insulator-Metal (Diode)   MIRD   Medical Internal Radiation Dose (committee)   MIRF   Medical and Industrial Radiation Facility   MISR   Multi-angle Imaging Spectroradiometer    MIT   Massachusetts Institute of Technology   MJD   Modified Julian Date   MMI   Mallinckrodt Medical, Inc.   MOBY   Marine Optical Buoy   MOCVD   Metal Organic Chemical Vapor Deposition   MODIL   Manufacturing Operations Development & Integration Laboratory   MODIS   Moderate Resolution Imaging Spectrometer   MOEMS   Micro Optp Electro Mechanical Systems   MOKE   Magneto-Optical Kerr Effect   MOPITT   Measurement of Pollution In The Troposphere   MOS   Marine Optical System  or  Marine Optical Spectrograph   MOSFET   Metal Oxide Semiconductor Field Effect Transistor   MOT   Magneto Optical Trap   MQDT   Multichannel Quantum Defect Theory   MPD   Multiphoton Detector   MPI   Multiphoton Ionization   MPP   Multi-Pinned Phasing   MQA   Measurement Quality Assurance   MQDT   Multichannel Quantum Defect Theory   MQSA   Mammography Quality Standards Act   MRA   Mutual Recognition Arrangement   MRI   Magnetic Resonance Imaging   MRT   Minimal Resolvable Temperature   MSEL   Materials Science and Engineering Laboratory   MSX   Midcourse Space Experiment   MTG   Methanol To Gasoline   MURR   University of Missouri Research Reactor   MW   Microwave            N     NAMP   National Analytical Management Program   NAMT   National Advanced Manufacturing Testbed   NAPM   National Association of Photographic Manufacturers   NAS   National Academy of Sciences   NAS/NRC   National Academy of Sciences/National Research Council   NASA   National Aeronautics and Space Administration   NATO   North Atlantic Treaty Organization   NBS   National Bureau of Standards, now NIST   NBS-4   Older Primary Frequency Standard (retains the NBS name)   NBS-6   Previous Primary Frequency Standard (retains the NBS name)   NBSR   National Bureau of Standards' Reactor (retains the NBS name)   NCAR   National Center for Atmospheric Research   NCI   National Cancer Institute   NCNR   NIST Center for Neutron Research   NCRP   National Council on Radiation Protection and Measurements   NCSCANS   National Steering Committee for the Advanced Neutron Source   NCSL   National Conference of Standards Laboratories   ND   Neutron Density   NDT   Nondestructive Testing   Nd:YAG   Neodymium: Yttrium-Aluminum-Garnet (YAG doped with Nd)   NEANDC   Nuclear Energy Agency Nuclear Data Committee   NEANSC   Nuclear Energy Agency Nuclear Science Committee   NEC   Nippon Electric Corporation   NED   Nuclear Effects Directorate   NEI   Nuclear Energy Institute   NELAC   National Environmental Laboratory Accreditation Conference   NEOS   Newport Electro-Optic Systems   NERI   Nuclear Energy Research Initiative  NESDIS   National Environmental Satellite Data and Information Service   NEWRAD   New Radiometry   NG6, NG-6   Monochromatic Beam Line near the end of NG6   NG6M   Neutron Guide No. 6   NG7   Neutron Guide No. 7   NGS   National Geological Society   NICE-OHMS   Noise-Immune Cavity Enhanced Optical Heterodyne Molecular Spectroscopy   NI&D   Nuclear Interactions and Dosimetry   NIDR   National Institute of Dental Research   NIF   National Ignition Facility   NIH   National Institutes of Health   NIM   Normal-Incidence Monochromator  or  National Instrumentation Methods   NIOF   Neutron Interferometry and Optics Facility    NIPDE   National Initiative for Product Data Exchange   NIR   Near Infrared   NIST   National Institute of Standards and Technology   NIST-7   Former NIST Primary Frequency Standard   NIST-F1   Current NIST Primary Frequency Standard   NISTAR   NIST Advanced Radiometer   NMI   National Metrology Institute  or  National Measurement Institute   NML   National Measurement Laboratory (Japan)   NMR   Nuclear Magnetic Resonance   NMS   Natural Matrix Standard   NOAA   National Oceanic and Atmospheric Administration   NOAO   National Optical Astronomy Observatory   NOBCChE   National Organization for the Professional Advancement of Black Chemists and Chemical Engineers   NORA   Non-Overlapping Redundant Array   NORAMET        A North American regional collaboration in national measurement standards and services   NPL   National Physical Laboratory (U.K.)   NPOESS   National Polar-orbiting Operational Environmental Satellite   NPSS   Nuclear and Plasma Sciences Society   NRC   National Research Council   NRC   Nuclear Regulatory Commission   NRCC   National Research Council Canada   NREL   National Renewable Energy Laboratory   NRIP   NIST Radiochemistry Intercomparison Program   NRL   Naval Research Laboratory   NRLM   National Research Laboratory of Metrology (Japan)   NRRS   Near Resonance Rayleigh Scattering   NSA   National Security Agency   NSBP   National Society of Black Physicists   NSCANS   National Steering Committee for the Advanced Neutron Source   NSF   National Science Foundation   NSLS   National Synchrotron Light Source, Brookhaven National Laboratory   NSOM   Near-Field Scanning Optical Microscopy   NTSB   National Transportation Safety Board   NVIS   Night Vision Imaging System   NVLAP   National Voluntary Laboratory Accreditation Program            O      OAI   Optical Associates, Inc.   OCLI   Optical Cooling Laboratory Incorporated   OD   Optical Density   OE   Optical Engineering   OFS   Österreichisches Forschungszentrum   OIML   International Organization of Legal Metrology   OMEGA   24-Beam Laser Facility at Rochester   OMH   National Office of Measures (Hungary)   ONR   Office of Naval Research   OPA   Optical Parametric Amplifier   OPO   Optical Parametric Oscillator   OPTCON   International conference sponsored by 3      agencies:  Optical Society of America; Society of Photo-optical      Instrumentation Engineers; and Institute of Electrical and Electronics      Engineers   ORM   Office of Radiation Measurement   ORELA   Oak Ridge Electron Linear Accelerator   ORNL   Oak Ridge National Laboratory   OSA   Optical Society of America   OSL   Optical Stimulated Luminescence   OSRD   Office of Standard Reference Data   OSTP   Office of Science and Technology Policy   OTD   Optical Technology Division            P      PA   Proton Affinity   PADE   Parallel Applications Development Environment   PARCS   Primary Atomic Reference Clock in SPACE   PC   Personal Computer   PCB   polychlorinated biphenyl   PDE   Product Data Exchange   PDML   Photovoltaic Device Measurement Laboratory   PE   Performance Evaluation   PECVD   Plasma-Enhanced Chemical Vapor Deposition   PES   Photoelectron Spectroscopy   PET   Positron Emission Tomography   PFID   Perturbed Free Induction Decay   PID   Proportional, Integral, and Derivative   PIXE   Particle Induced X-ray Emission PL   Physics Laboratory   PMG   Precision Measurement Grant   PMMA   polymethylmethacrylate   PMS   Particle Measurement System   PMT   Photomultiplier Tube    PNL   Pacific Northwest Laboratory   PNNL   Pacific Northwest National Laboratory   PNR   Polarized Neutron Reflectivity   POC   Physical Optics Corporation   POP   Plasma Oscillation Probe   POPA   Panel on Public Affairs of American Physical Society   PREP   Professional Research Experience Program   PRF   Petroleum Research Fund   PRL   Physical Review Letters   PRM   Precision Radiation Measurement   PS   Polystyrene   PSD   Photon-Stimulated Desorption   PSL   Polystyrene-latex   PT   Performance Testing   PTB   Physikalisch-Technische Bundesanstalt  (Germany)   PTCA   Percutaneous Transluminal Coronary Angioplasty   PTFE   Polytetrafluoroethylene   PTI   Proxima Therapeutics, Inc.   PUDs   Paired Uranium Detectors   PWR   Pressurized-Water Reactor   PWS   Primary Working Standard   PZT   Piezoelectric Transducer            Q      QA/QC   Quality Assurance/Quality Control   QCD   Quantum Chromodynamics   QED   Quantum Electrodynamics   QELS   Quantum Electronics and Laser Science   QFT   Quantum Field Theory   QMD   Quantum Metrology Division   QPD   Quantum Physics Division            R      RAC   Research Advisory Committee   R&D   Research & Development   RBE   Relative Biological Efficiency   RBS   Rutherford Backscattering   RDP   Rubidium Di-Hydrogen Phosphate   REDA   Resonant-Excitation-Double-Autoionization   REI   Rad Elec, Inc.   RESL   Radiological and Environmental Science Laboratory   RF (rf)   Radio Frequency   RHEED   Reflection High Energy Electron Diffraction   RIMS   Resonance Ionization Mass Spectrometry   RKR   Rydberg-Klein-Rees   RNA   Ribonucleic Acid   ROSAT   Roentgensatellit   ROSPEC   Rotating Spectrometer for Neutrons   RS-232   An IEEE Standard Bus   RTC   Radiochromic Film Task Group   RTP   Rapid Thermal Processing            S      SACR   Space-based Active Cavity Radiometer   SAMs   Self-Assembled Monolayers   SANS   Small-Angle Neutron Scattering   SBIR   Small Business Innovation Research   SCAMPY   Scanning Micro Pyrometer   SCATMECH   Consortium of 14 U.S. Semiconductor Manufacturers   SCC   Standards Coordinating Committee  or  Spectral Calibration Chamber   SCF   Spectral Comparator Facility   SCLIR   Secondary Calibration Laboratories for Ionizing Radiation   SDI   Strategic Defense Initiative   SDIO   Strategic Defense Initiative Organization   SDL   Space Dynamics Laboratory   SEAWIFS (SeaWiFS)   Sea-Viewing of Wide Field Sensor   SEBA   Standards' Employees Benefit Association   SEM   Scanning Electron Microscope   SEMATECH   Consortium of 14 U.S. Semiconductor Manufacturers   SEMPA   Scanning Electron Microscopy with Polarization Analysis   SFA   Sachs Freeman and Associates   SFCP   Special Foreign Currency Program   SFG   Sum Frequency Generation   SI   Système International d' Unités        or  International System of Units   SIA   Semiconductor Industry Association   SID   Society for Information Display   SIMS   Secondary Ion Mass Spectrometry   SIRCUS   Spectral Irradiance and Radiance Calibration with Uniform Sources   SIRREX-3   SeaWiFS Intercalibration Round-Robin Experiment   SKACR   Superconducting Kinetic-inductance Absolute Cryogenic Radiometer   SLMs   Synthetic Layer Microstructures   SME   Solar Mesosphere Explorer   SM   Single Molecule   SNOM   Scanning Near Field Optical Microscope   SOG   Spin-on Glass   SOLSPEC   Solar Spectrometer   SOLSTICE   Solar Stellar Irradiance Comparison Experiment   SPIE   Society of Photo-optical Instrumentation Engineers   SPP   Storage Photostimulable Phosphor   SQL   Structured Query Language   SRAM   Static Random Access Memory   SRDP   Standard Reference Data Program   SRM   Standard Reference Material   SSBUV   Shuttle Solar Backscatter Ultraviolet   SSC   Superconductor Super Collider   SSDL   Secondary Standard Dosimetry   SSPM   Solid State Photomultipliers   SSRCR   State Suggested Regulations for Controlling Ionizing Radiations   SSTR   Solid State Track Recorder   SSUV   Shuttle Solar Ultraviolet   STARR   Spectral Tri-function Automated Reference Reflectometer   STD   Standard   STM   Scanning Tunneling Microscope   STP   Standard Temperature Pressure   STScI   Space Telescope Science Institute   SUNY   State University of New York   SURF   Summer Undergraduate Research Fellowship  program   SURF-III (SURF II)   The NIST  Synchrotron Ultraviolet Radiation Facility  Electron Storage Ring   SUSIM   Solar Ultraviolet Spectral Irradiance Monitor   SVGL   Silicon Valley Group Lithography   SWIXR   Short Wave Infrared Transfer Radiometer   SWIR   Short Wave Infrared   SXR   SeaWiFS Transfer Radiometer            T      TAG   Technical Advisory Group    TAI   International Atomic Time   TAMOC   Theoretical Atomic, Molecular, and Optical Physics Community   TASSII   Total and Spectral Solar Irradiance Investigation   TC   Technical Committee   TCAP   Time-Correlated Associated Particle   TEPC   Tissue Equivalent Proportional Counter   TEXT   Texas Experimental Tokamak   TFTC   Thin-Film Thermocouple   TGM   Toroidal-Grating Monochromator   TIMED   Thermosphere Ionosphere Mesosphere Energetics and Dynamics   TLC   Thin Layer Chromatography   TLD   Thermoluminescent Detector   TMA   Tri-methyl-aluminum   TOF   Time-of-Flight Spectrometer   TOMS   Total Ozone Mapping Spectrometer   TOP   Time-Orbiting-Potential Trap   TPB   Tetraphenyl Butadiene   TPD   Temperature Programmed Desorption   TQM   Total Quality Management   TRIGA   Training, Research and Isotope Reactor, General Atomics   TRU   Transuranic   TuFIR   Tunable Far Infrared (Radiation)   TXR   Thermal-infrared Transfer Radiometer            U      UA   University of Arizona   UARS   Upper Atmosphere Research Satellite   UCN   Ultra Cold Neutron   UDC   University of the District of Columbia   UHV   Ultrahigh Vacuum   UK (U.K.)   United Kingdom   UPS   Ultraviolet Photoelectron Spectroscopy   URL   Uniform Resource Locator   US   United States   USA   United States of America   USAIDR   U.S. Army Institute of Dental Research   USCEA   U.S. Council for Energy Awareness   USDA   U.S. Department of Agriculture   USFDA   U.S. Food and Drug Administration   USGCRP   United States Global Change Research Program   USNA   U.S. Naval Academy   USNC   United States National Committee   USNO   U.S. Naval Observatory   USSR   Union of Soviet Socialist Republics   UTC   Coordinated Universal Time   UV   Ultraviolet   UV-B (UVB)   Ultraviolet-B            V      VCO   Voltage-Controlled Oscillator   VCSEL   Vertical-Cavity Surface-Emitting Laser   VDG   Van de Graaff   VEEL   Vibrational and Electronic Energy Levels   VET   Vibration Energy Transfer   VIS   Visible   VLA   Very Large Array   VLBI   Very Long Baseline Interferometer (or Interferometry)   VNIIFTRI   National Scientific and Russian Research Institute for Physical, Technical and Radiotechnical Measurements   VNIIM   Mendeleyev Institute of Metrology   VNIIOF   All-Union Research Institute for Optical and Physical Measurements   VR   Vibrationally Resonant   VRML   Virtual Reality Markup Language   VR-SFG   Vibrationally Resonant Sum-frequency Generation   VUV   Vacuum Ultraviolet   VXR   Visible Transfer Radiometer             W      WAFAC   Wide-Angle Free-Air Chamber   WERB   Washington Editorial Review Board   WG   Working Group   WHC   Washington Hospital Center   WHO   World Health Organization   WIPP   Waste Isolation Pilot Plant   WISE   Women in Science and Engineering   WKB   Wentzel-Kramers-Brillouin   4WM   Four Wave Mixing   WMO   World Metrological Organization   WPMA   Working Party on Measurement Activities   WSTC   Westinghouse Science and Technology Center   WWV   Call letters for NIST short-wave radio station in Colorado   WWVB   Call letters for NIST lf radio station in Colorado   WWVH   Call letters for NIST short-wave radio station in Hawaii   WWW (www)   World Wide Web   WYSIWYG   What You See Is What You Get            X      XANES   X-ray Absorption Near-Edge Structure   XPS   X-ray Photoelectron Spectroscopy   XROI   X-Ray Optical Interferometer   XSW   X-ray Standing Wave   XTE/PCA   X-ray Timing Explorer/Proportional Counter Array   XUV   Extreme Ultraviolet            Y      YAEL   Yankee Atomic Environmental Laboratory   YAG   Yttrium-Aluminum-Garnet   Yb:YAG   Ytterbium: Yttrium-Aluminum-Garnet (YAG doped with Yb)   YBCO   Yttrium-Barium-Cuprate                  Inquiries or comments:  www@physics.nist.gov   Online:  May 1995   -    Last update:  March 2002"
GX256-28-5467338	"Handwritten Document Image Analysis at Los Alamos: Script, Language, and Writer Identification Judith Hochberg, Kevin Bowers, Michael Cannon, and Patrick Kelly Mail Stop B265, Los Alamos National Laboratory, Los Alamos, NM 87545 {judithh, tmc, kelly}@lanl.gov kbowers@eecs.berkeley.edu Abstract A system for automatically identifying the script used in a handwritten document image is described. The system was devel oped using a 496-document dataset repre senting six scripts, eight languages, and 281 writers. Documents were character ized by the mean, standard deviation, and skew of five connected component features. A linear discriminant analysis was used to classify new documents, and tested using writer-sensitive cross-validation. Classifi cation accuracy averaged 88% across the six scripts. The same method, applied within the Roman subcorpus, discriminated English and German documents with 85% accuracy. Pilot results indicate that a vari ation of the method may be applicable to writer identification. 1. Introduction Script and language identification are important parts of the automatic processing of document images in an international en vironment. A document's script (e.g., Cyrillic or Roman) must be known in order to choose an appropriate optical character recognition (OCR) algorithm. For scripts used by more than one language, knowing the language of a document prior to OCR is also helpful. And language identification is crucial for further processing steps such as routing, indexing, or translation. For scripts such as Greek, which are used by only one language, script identifi cation accomplishes language identifica tion. For scripts such as Roman, which are used by many languages, it is normally as sumed that script identification will take place first, followed by language identifi cation within the script (e.g. [1]). Alterna tively, it may be possible to skip script identification as an intermediate step, rec ognizing languages directly regardless of their script. To the best of our knowledge, script identification has never been attempted for handwritten documents. Because of the dramatic individual differences in handwrit ing, we found a feature-based approach to be most successful, in contrast to the tem plate matching we have previously applied to machine printed documents [2-3]. In the spirit of Wilensky et al. [4], each document was characterized by a single feature vec tor, containing summary statistics taken across the document's black connected components. The documents were then classified using linear discriminant analysis. The main focus of this work was script identification: the method was 88% accu rate in distinguishing among six scripts, in cluding challenging pairs of related (and vi sually similar) scripts such as Ro man/Cyrillic and Chinese/Japanese. We also took a first look at language identifica tion within the Roman script: the method was 85% accurate for English versus Ger man documents. Finally, we report promising pilot results (80% accuracy for a rough implementation) on a variation of our method applied to writer identification from free text. 2. Data We assembled a corpus of 496 hand written documents from six scripts: Arabic, Chinese, Cyrillic, Devanagari, Japanese, and Roman. The scripts are illustrated in Figure 1. For the most part, document im ages were obtained from foreign language speakers we were acquainted with or whom we contacted through the Internet. Over   Cyrillic  Roman  Chinese  Japanese  Arabic  Devanagari  Fig 1. Examples of six handwritten scripts 75% of the documents we collected were 'natural' -- letters, lecture notes, official documents, etc. The remaining documents were written on request. 281 different writ ers were represented in the corpus. Around a third of the documents had at least one document quality issue such as ruling lines, line curvature, line skew, char acter fragmentation, or brevity (fewer than 100 connected components). Character fragmentation and ruling lines were ad dressed in preprocessing. We did not at tempt to correct for the other phe nomena, but simply included all documents in the training and testing process in order to per form a realistic test of the classification method. 3. Script identification Connected components. The basic element of the analysis was the eight-con nected black component. After finding all the components in a document image, un usually small or large components were fil tered out in order to remove speckle, ruling lines, and outsize components in general. Some filtering criteria were absolute (e.g., removing components with height or width less than three pixels), and some were rela tive (e.g., removing components with height or width more than four standard deviations above the document mean). Features. Once filtering was completed, several features were extracted from the remaining components. To develop the feature set we studied the document images and determined which visual features guided our human script identification. The final set of features was:      relative Y centroid relative X centroid number of white holes sphericity aspect ratio (height/width)  For each of the five connected compo nent features, three document summary statistics were calculated: the mean, stan dard deviation, and skew. This created a fifteen-element vector for each document. Discrimination. The classification method used a collection of linear discrimi nant functions. A separate Fisher linear discriminant [5] was trained to separate each possible pair of scripts in the dataset (Arabic vs. Chinese, Arabic vs. Cyrillic, etc.). New documents were classified by applying each individual linear discriminant to the document's feature vector, while keeping track of the results. The document   was then assigned to the class receiving the most ""votes"". The classifier was tested through writersensitive cross-validation. For each writer, the classifier was trained on all data except that writer's documents. Then the writer's documents were classified using the trained classifier. We calculated the percentage of documents correctly classified for each script, and averaged these percentages to produce an overall accuracy figure unbiased by the scripts' sample sizes. Results. The linear discriminant analysis was 88% accurate. Table 1 breaks down these results by script, and also presents the cross-classification matrix. The individual percentages for the different scripts were pleasingly uniform, especially since the amount and quality of data available for the different scripts varied considerably. When documents were misclassified, the errors were sensible: Roman and Cyrillic tended to be confused, and likewise Chinese and Japanese. Character fragmentation adversely af fected classification: 90% of documents with no fragmentation, or only mild frag mentation, were correctly classified, com pared to 81% of documents with moderate or severe fragmentation (F = 5.21, p < 0.05). Ruling lines also appeared to affect classification -- 89% of unruled documents were correctly classified, compared to 81% of ruled documents -- although this differ ence was just short of statistical signifi cance (F = 3.18, p = 0.07). Of the 366 doc uments in the corpus with no or mild frag mentation, and without ruling lines, 91% were classified correctly. 4. Language identification  Method. Of the Roman script documents in the corpus, 107 were in English and 58 in German. Using the same prepro cessing, feature selection, and classification techniques described for script identifica tion, we attempted to distinguish between these two groups. We also tried to identify the languages directly, using a single dis criminant analysis for the seven-way dis crimination among Arabic, Chinese, Cyril lic, Devanagari, Japanese, German, and English. Results. For the two-way (English vs. German) task, correct identification aver aged 85% (84% for English, 86% for Ger man). For the seven-way task, 80% of English and German documents were cor rectly identified by language. Interestingly, overall Roman identification improved to 93% when the two languages were split apart. It may be that the heterogeneity of the combined Roman group adversely af fected the script classifier's performance, so that dividing the group into two smaller, more homogeneous groups helped. Classi fication of the other scripts was not affected by the Roman split. 5. Writer identification While experimenting with a variation of the method described in sections 3 and 4, we obtained exciting pilot results for writer identification. We present them here, knowing that they are extremely prelimi nary, in the hopes that they may inspire further research. Method. Connected components were identified, filtered, and features extracted as described above, producing a five-element vector per component. Then a k-means cluster analysis was performed across the  Script Arabic Chinese Cyrillic Devanagari Japanese Roman Average  % correct 89% 87% 88% 88% 86% 91% 88%  Table 1. Script identification results Classified as Arabic 51 0 1 0 3 2 Chinese 0 104 0 0 6 1 Cyrillic 0 0 49 1 0 9 Devanagari Japanese 3 2 0 8 2 0 22 1 0 63 0 3 Roman 1 8 4 1 1 150   entire training set, resulting in 256 clusters, or connected component types. Now each document could be represented as a his togram of cluster occurrences, and docu ments compared to each other on the basis of their histogram similarity, using a dis tance metric such as Kullback-Leibler entropy [6]. The pilot study analyzed the 282 docu ments in our corpus whose writers had contributed more than one document to the corpus. Using the histogram comparison method, we determined how many of these documents were most similar to another document by the same writer -- a one-near est-neighbor classifier. Related work by Wilensky et al. on Chinese writer identification from free text also used a nearest-neighbor algorithm, rep resenting each document by a feature vector similar to the ones we used for script and language identification [4]. Results. As shown in Table 2, roughly 80% of docu ments by multi-document writers were closest to another document by the same writer. This was particularly impres sive given that (at the time) the corpus con tained 466 documents, representing 260 dif ferent writers. In other words, the odds of picking another document by the same writer by chance were extremely low. In comparison, Wilensky et al.'s accu racy was much higher (98% or better), but their task was much easier since it involved only fifteen writers and 106 documents. In addition, their feature selection (in contrast to ours) was optimized for writer identifica tion. 5. Conclusion The feature set and classifier we devel Script  oped served to discriminate scripts with 88% accuracy. While not as accurate as script identification for machine printed document images [2-3], this result exceeded our initial expectations given the variability of handwritten documents. Classification accuracy was higher for documents without fragmented characters and ruling lines. Language identification for English ver sus German was 85% accurate once Roman identity was known, and 80% accurate when script and language identification were performed together. It would be worthwhile to explore the language identi fication task more thoroughly. Pilot work showed 80% accuracy for writer identifica tion. We believe that these results could be improved with a more ju dicious selection of features. Since the pi lot study was a casual offshoot of our main line of research, the features used to repre sent documents were chosen for their utility in script identification, not writer identifi cation. In fact, we were careful not to choose features that showed substantial in dividual variation, such as white component sphericity. A more fully-developed algo rithm along these lines could be useful in the intelligence field or in forensics. References [1] Spitz, A. L (1977). Determination of the script and language content of docu ment images. IEEE Transactions on Pattern Analysis and Machine Intelligence 19:235-245. [2] Hochberg, J., Kelly, P., Thomas, T., Kerns, L. (1997). Automatic script identification from document images using cluster-based templates. IEEE Transactions on Pattern Analysis and Machine In -  Table 2. Pilot results for writer identification # documents by # correctly matched % correctly matched multi-document writers Arabic 14 12 86% Chinese 99 79 80% Cyrillic 37 29 78% Devanagari 19 16 84% Japanese 3 3 100% Roman 110 87 79% total 282 226 80% total # writers 260   telligence 19176-181. [3] Patent: Script identification from images using cluster-based templates (5,844,991). [4] Wilensky, G., Crawford, T., Riley, R. (1997). Recognition and characteriza tion of handwritten words. In Doermann, D. (ed.): Proceedings of the 1997 Sympo sium on Document Image Understanding Technology. College Park, MD: University of Maryland Institute for Advanced Com puter Studies, pp. 87-98 [5] Duda, T., Hart, P. (1973). Pattern Classification and Scene Analysis. New York: John Wiley & Sons, pp. 114-118. [6] Deco, G. & D. Obradovic (1996). An Information-Theoretic Approach to  Neural Computing. New York: Springer, p. 10. Acknowledgments Los Alamos National Laboratory is op erated by the University of California for the United States Department of Energy under contract W-7405-ENG-36. We would like to thank Steve Dennis, Ron Ri ley, and Greg Wilensky for their help. Most of all, we would like to thank the many people from around the world who shared with us their writing and that of their friends and families."
GX254-49-8427006	"Word Spotting: Indexing Handwritten Manuscripts Indexing George Washington's manuscripts R. Manmatha Multimedia Indexing and Retrieval Group Center for Intelligent Information Retrieval University of Massachusetts, Amherst http://ciir.cs.umass.edu/research/wordspotting/   Isaac Newton Manuscript Newton's manuscripts are being scanned and will be annotated at a co st - 40po unds/ page Note the varying orientation of the lines  c a u s es complication s  Courtesy: Stefan Rueger, Imperial College   Newton - Closeup  Newton was a religious man. This page lists his religious beliefs as a set of articles (axioms). (Supposedly) a lot of his writing is on religion and alchemy.   Word Spotting: Indexing Handwritten Manuscripts.  Index historical documents written by a single author  Make an index like one at the back of a printed book.   Examples  Presidential papers at the Library of Congress.  Isaac Newton's manuscripts.  Margaret Sanger's correspondence at Smith and NYU.   Variation in single author's writing is small.  R. Manmatha and W. B. Croft, Wordspotting: Indexing Handwritten Manuscripts, in Intelligent Multimedia Information Retrieval, ed. Mark Maybury, AAAI/MIT Press, 1997.   An Example Manuscript (George Washington)   One Possible Approach  Recognize words (e.g. optical character recognition  OCR).  Use text indexing and retrieval.  Handwriting recognition is a hard unsolved problem.  Handwritten manuscripts are noisy and the  handwriting is variable. This makes it challenging.   The Idea Document Word Image Extraction  Match  ... Tagging  Index ... that (p.12, p.45, ...) ... the (p.12, p.34, ...) ...  Clustering Indexing  ""the""  ...  ""that""   People Working  Current  Toni Rath, J. Jeon  Grad Students  A. Maguire, J. Rothfeder, K. Srivastav  Undergraduates   Previously  Shaun Kane, Andrew Lehman, Elizabeth Partridge  REU's (site REU award)  Nitin Srimal  Grad Student.  Joshua Sarro, Eric Mulvihill and Liz Yon  Undergraduates  Fangfang Feng  staff programmer   Overview  Scanning  Segmentation  Often have little control over this process.  Developed a new scale-spaced segmentation algorithm  discussed last year.   Word Similarity (Matching)  Preprocessing  Pruning  Matching/Clustering   Indexing/Database Management  User Interface  How does one map images to ASCII?  Alternatively what does a visual index look like?   Collection  Have roughly 6,400 scanned pages of George Washington's manuscripts from the Library of Congress.  Scanned in 8 bit graylevel at 300dpi.  Scanned from microfilm  Quality not as good as scanning from original.  For example, boundary artifacts, noise etc.   Probably done for reasons of cost, fragility of manuscripts and security.   Compression Artifacts  Edge Image created from Jpeg compressed version  Edge Image created from lossless tiff compressed version Moral: Scanning is important. Our work so far has been limited to jpeg images. In the process of getting lossless tiffs from the Library of Congress.   Word Similarity  Pruning  Eliminate most matches using Area, Aspect Ratio and Descenders  Results  87% possible matches pruned  94% relevant matches retained   Preprocessing Example Original Cleaned and Cropped Slant Corrected   Matching  Template matching     SSD  Sum of Squared Differences XOR EDM [Danielsson 1980] AEDM - Affine corrected EDM (align images, then EDM)   Transformation recovery  SLH (affine transformation) [Scott & Longuet-Higgins 1991]  Shape Context (thin-plate spline transformation) [Belongie et al. 2000]   Feature matching/alignment  Dynamic Time Warping for feature alignment and comparison (e.g. projection profiles)   Matching Considerations.  George Washington manuscripts     Some variation in the way words are written. Preprocessing is important. Representation for matching is important. Alignment is important.   Some standard techniques don't work so well on this problem.  Examples: SSD, Shape Context   Things which work reasonably well.  AEDM - EDM after an approximate affine alignment.  Dynamic Time Warping of projection profiles of words.  We think this can be improved even further.   AEDM Align Images up to an affine transform. Then  Image 1 Image 2 XOR AEDM Error measure: sum over difference pixels in XOR i m ag e; p i xels in ""blob s "" are weighted more heavily   Effects of Preprocessing  0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 Precision With Without  AEDM with and without preprocessing.   Dynamic Time Warping of Projection Profiles.   Projection Profiles Original Images Cleaned, deslanted, cropped Projection profiles 1 0.9 0.8 0.7 0.6 0.5  0.4  0.3  0.2  0.1  0  0  50  100  150  200  250  300  350  400  450  500  1  0.9  0.8  0.7  0.6  0.5  0.4  0.3  0.2  0.1  0  0  50  100  150  200  250  300  350  400  450  1  0.9  0.8  0.7  0.6  0.5  0.4  0.3  0.2  0.1  0  0  50  100  150  200  250  300  350   Similarity of Projection Profiles What makes projection profiles similar? 1 0.9 0.8 0.7 0.6 0.5  Find correspondences in projection profiles  0.4  0.3  0.2  0.1  0  0  50  100  150  200  250  300  350  400  450  500  1  0.9  0.8  0.7  obvious correspondences no obvious correspondences  0.6  0.5  0.4  0.3  0.2  0.1  0  0  50  100  150  200  250  300  350  400  450  ? 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 0 50 100 150 200 250 300 350  Dynamic Time Warping can recover non-linear alignment between two time series Projection Profiles are more stable.   Dynamic Time Warping Find minimum-cost assignment path recovered path ) Cost(x,y  Generate match error (or score) Use sum of assignment cost along recovered path Use recovered path to align images for a subsequent matching algorithm  =  Cost for assigning column x to row y   Using Alignment for Matching Original Images Recovered Alignment  Warped projection profiles 1 0.9 0.8 0.7 0.6  Warped images 0 50 100 150 200 250 300 350 400 450 500  0.5  0.4  0.3  0.2  0.1  0  1  0.9  0.8  0.7  0.6  0.5  0.4  0.3  0.2  0.1  0  0  50  100  150  200  250  300  350  400  450  500   Computing Error Measures Warped projection profiles 1 0.9 0.8  Error measures 1. Sum of abs. differences:  p(i)=  0.7  0.6  0.5  0.4  0.3  0.2  0.1  0  0  50  100  150  200  250  300  350  400  450  500  1  0.9  1 e= n 1 e= n  i{1 .. n }     | d (i ) |  0.8  q(i)=  0.7  0.6  0.5  0.4  2. Euclidean distance 0 50 100 150 200 250 300 350 400 450 500  0.3  0.2  0.1  0  i=1,2,..,n  d (i )  2  i{1 .. n }  Difference d(i)=p(i)-q(i): 0.5 0.4 0.3 0.2 0.1 0 -0.1  3. Kullback-Leibler div.:  -0.2  e= 0 50 100 150 200 250 300 350 400 450 500  -0.3  -0.4  -0.5  i{1..n}    p(i) p(i) * log q(i )   Retrieval Results Alexandria Recruits 6/8 retrieved in top 20 Template used matters. Don't know why yet.  Template Dependence  10/11 retrieved in top 10  8/8 retrieved in top 8   Problem 1 1 0.9 0.8 0.7 0.6  0.5  Same word?  0.9  0.8  0.7  0.6  0.5  0.4  0.4  0.3  0.3  0.2  0.2  0.1  0.1  0  0  20  40  60  80  100  120  140  160  180  200  0  0  50  100  150  200  250  300  no!  Probably need additional features to distinguish words with similar projection profiles. Note, we only use 1D projection profiles.   Variation between Pages  Example of cleaner page with less noise  Example of more difficult page. Faded Ink, Noise   Comparison of Matching Algorithms Test Set  10 pages, 15 queries 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 P r e cis io n XO R SS D AE DM SLH Ra ndom DTW DTW - a ll im a ge s   Comparison of Matching Algorithms Difficult Test Set  10 pages 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 P re cis ion AE DM DT W  DTW - Works better on difficult test set   Computational  Matching expensive operation  O(n2).  In 6,400 images, roughly 1.8 million words, no. of possible matches ~ 2.5 * 1012.  Our current pruning reduces this by a factor of 10.  Clustering matches could also reduce this substantially.  Highly parallelizable.   User Interface  How does one map index images to ASCII?  Have people do this manually.  Read a few pages, recognize them using ASR.  Align these pages to the handwritten ones.  Currently working on this.   Use a visual index.  People aren't used to a visual index.  How does one create such a visual index?   Demonstration  See http://ciir.cs.umass.edu/research/wordspotting/index. html   Conclusions and Future Work  Have a reasonable segmentation algorithm.  Currently working on effective matching techniques.  Hard Problem.  Dynamic Time Warping of the right representation shows great promising.  DTW on additional features will probably improve performance  Computational Processing Issues.   Database Issues.  User Interface Issues.  In principle, Word Spotting should work in other languages."
GX011-24-9834250	"Next:    Technical Reports   Up:    Publications    Previous:    Publications          Publications in Print      B.K. Alpert, High-Order Quadratures for Integral Operators with       Singular Kernels,   J. Computational and Applied Mathematics  60       (1995) pp. 367-378.     D.M. Anderson and S.H. Davis,        Local Fluid Flow and Heat Transfer Near Contact Lines,         J. Fluid Mech.  268 (1994) pp. 231--265.     D.M. Anderson and S.H. Davis,       Fluid Flow, Heat Transfer, and Solidification Near Tri-junctions,         J. Crystal Growth  142 (1994) pp. 245--252.     D.M. Anderson and S.H. Davis,       The Spreading of Volatile Liquid Droplets on Heated Surfaces,         Phys. of Fluids  7 (1995) pp. 248--265.     D.M. Anderson and M.G. Worster,       Weakly-Nonlinear Analysis of Convection in a Mushy Layer During the       Solidification of Binary Alloys,         J. Fluid Mech.  302 (1995) pp. 307--331.     D.M. Anderson and M.G. Worster,       A New Oscillatory Instability in a Mushy Layer During the Solidification       of Binary Alloys,         J. Fluid Mech.  307 (1996) pp. 245--267.     M.A. Anuta, D.W. Lozier and P.R. Turner, The MasPar MP-1 As a       Computer Arithmetic Laboratory,   Journal of Research of the       National Institute of Standards and Technology  101 (1996)       pp. 165--174.     I. Beichel, J. Bernal, F. Sullivan, C. Witzgall, Computational       Geometry, Encyclopedia of Operations Research and Management       Science, S. Gass and C. Harris (Ed.), Kluwer Academic Publishers,       Norwell, MA, 1996.     I. Beichel, J. Bernal, F. Sullivan, C. Witzgall, Voronoi/Delaunay       Construct, Encyclopedia of Operations Research and Management       Science, S. Gass and C. Harris (Ed.), Kluwer Academic Publishers,       Norwell, MA, 1996.     I.M. Beichl, and F. Sullivan, Coping with Degeneracies in       Delaunay Triangulation, IMA Volumes in Mathematics and its       Applications,   Modeling, Mesh Generation and Adaptive Numerical Methods       for Partial Differential Equations , ed. J.E. Flaherty,       I. Babuska, W. D. Henshaw, J. E. Oliger, T. Tezduyar,       Springer Verlag, New York (1995).     I.M. Beichl, A.Y. Teng, J.L. Blue, Parallel Monte Carlo Simulation       of MBE Growth,         Proceedings of the International Parallel Processing       Symposium, (IPPS) 1995 .     I.M. Beichl and F. Sullivan, Tree-lookup for partial sums OR:       How can I find this stuff quickly?,   Computational Science and Engineering        (1996).     J. Bechhoefer and S.A. Langer,       Can Elasticity Stabilize a Moving Nematic-Isotropic Interface?,         Physical Review B  51 (1995) p. 2356.     B. Bialecki and K.A. Remington, Fourier Matrix Decomposition Methods        for the Least Squares Solution of Singular Neumann and Periodic        Hermite Bicubic Collocation Problems,   SIAM Journal on        Scientific Computing  16 (1995) pp. 431--451.     B. Bialecki, G. Fairweather and K.A. Remington, Fourier Methods        for piecewise Hermite Bicubic Collocation,   East-West Journal of        Numerical Mathematics  2 (1994) pp. 1--20.     J.L. Blue, G.T. Candela, P.J. Grother, R. Chellappa, and C.L. Wilson,       Evaluation of Pattern Classifiers for Fingerprint and OCR       Applications,   Pattern Recognition    27 , 1994, 485-501.     J.L. Blue, I. Beichl, and F.E. Sullivan, Faster Monte Carlo       Simulations,   Physical Review E    51 , 1995, R867-R868.     W.J. Boettinger, A.A. Wheeler, B.T. Murray, and G.B. McFadden,       Prediction of Solute Trapping at High Solidification Rates Using         a Diffuse Interface Phase-Field Theory of Alloy Solidification,          Mat. Sci. and Eng. ,   A178  (1994) pp. 217-223.     P.T. Boggs and J.W. Tolle, Convergence properties of a class of       rank-2 updates,   SIAM J. on Optimization  4 (1994) pp. 262--287.     P.T. Boggs, Interior point methods, in   Encyclopedia of       Operations Research , S. Gass and C. Harris, eds.,  Kluwer Academic       Publishers, Dordrect, (1996).     P.T. Boggs, J.W. Tolle, and A.J. Kearsley, A truncated SQP       algorithm for large scale nonlinear programming problems, in         Advances in Optimization and Numerical Analysis: Proceedings of the       Sixth Conference on Numerical Analysis and Optimization , S. Gomez and       J.-P. Henart, eds., Kluwer Academic Publishers, Dordrect, (1994)       pp. 69--78.     P.T. Boggs and J.W. Tolle, Sequential Quadratic Programming,         Acta Numerica , (1995) pp. 1--51.     P.T. Boggs, J.W. Tolle, and A.J. Kearsley, On the convergence of       a trust region SQP algorithm for nonlinearly constrained optimization       problems, in   System Modeling and       Optimization , J. Dolezal and J. Fidler, eds., Chapman &       Hall, London, 1996, pp. 3--12.     R.F. Boisvert, Shirley Browne, Jack Dongarra and Eric Grosse,       Digital Software and Data Repositories for Support of Scientific       Computing, in   Advances in Digital Libraries , Springer-Verlag,       NY, 1995, pp. 61-72.     R.F. Boisvert,  A Web Gateway to a Virtual Mathematical Software  ,       Repository,   Electronic Proceedings of the Second International       World Wide Web Conference , 1994.     R.F. Boisvert, NIST's GAMS: A Card Catalog'' for the Computer User,          SIAM News , vol. 27, no. 8, October 1994, pp. 1,8.     R.F. Boisvert, The Architecture of a Virtual Mathematical Software       Repository,   Mathematics and Computers in Simulation        36 (1994) pp. 269-279.     R.J. Braun, G.B. McFadden, and S.R. Coriell,        Morphological instability in phase-field       models of solidification,   Phys. Rev. E  49 (1994) pp. 4336--4352.     L.N. Brush, G.B. McFadden, and S.R. Coriell,        The effect of crystalline anisotropy on       pattern formation in laser-melted thin silicon films,         J. Crystal Growth  137 (1994) pp. 355--374.     R.J. Braun, B.T. Murray,       W.J. Boettinger and G.B. McFadden, Lubrication Theory for Reactive Spreading        of a Thin Drop,   Physics of Fluids ,   7  (1995) pp. 1797-1810.     T. J. Burns,       Does a Shear Band Result from a Thermal Explosion?,   Mechanics of        Materials  17 (1994), pp. 261--271.     T. J. Burns,       A Simple Criterion for Discontinuous Plastic Deformation in Metals       at Very Low Temperatures,   J. Mech. Phys. Solids  42 (1994),        pp. 797--811.     T.J. Burns,       Connections Between Localized Behavior in Plasticity and in Combustion,       in   Material Instabilities, Theory and Application , R.C. Batra and        H.M. Zbib, eds., AMD-Vol. 183, MD-Vol. 50, American Society of Mechanical       Engineers, New York, 1994, pp. 87--93.     A.S. Carasso, Overcoming Hölder Continuity in Ill-Posed       Continuation Problems,   SIAM Journal on Numerical Analysis        31 (1994) pp. 1535--1557.     A.S. Carasso, Slow Evolution from the Boundary: A New Stabilizing       Constraint in Ill-Posed Continuation Problems,   American Mathematical       Society Proceedings of Symposia in Applied Mathematics  48 (1994) pp. 269--273.     A. A. Chernov, S.R. Coriell, and B.T. Murray, Morphological Stability of a         Vicinal Face Induced by Liquid or Step Flow, in   Crystal Growth Mechanism       in Atomic Scale  12, T. Nishinaga, ed., Atagawa, Shizuoka       Prefec., 1994, pp. 163--167.     A.A. Chernov, S.R. Coriell, and B.T. Murray, Kinetic Self-Stabilization        of a Stepped Interface: Growth into a Supercooled Melt,   Journal of        Crystal Growth ,   149  (1995) pp. 120-130.     V.T. Coppola and B.R. Miller,       Synchronization: an alternative to normalization for perturbed nonlinear       Hamiltonians,   International Journal of Nonlinear Dynamics , (1995)       (in press).     S.R. Coriell, R.F. Boisvert, G.B. McFadden, L.N. Brush and J.J. Favier,       Morpological stability of a binary alloy during directional       solidification: initial transient,   Journal of Crystal        Growth  40 (1994) pp. 139--147.     S.R. Coriell, A.A. Chernov, B.T. Murray, and G.B. McFadden, Convection and          Morphological Stability during Directional Solidification, in   Proc.       Second Microgravity Fluid Physics Conference , CP-3276,       NASA, Wash. D.C., 1994, pp. 175--180.     S.R. Coriell, B.T. Murray, and A.A. Chernov, Kinetic Self-Stabilization        of a Stepped Interface: Binary Alloy Solidification,   Journal of        Crystal Growth ,   141  (1994) pp. 219-233.     S.R. Coriell, B.T. Murray, A.A. Chernov and G.B. McFadden, Effects of        Shear Flow and Anisotropic       Kinetics on the Morphological Stability of a Binary Alloy,          Met. Mater. Trans. ,   27A  (1996) pp. 687-694.     S.R. Coriell, B.T. Murray, G.B. McFadden, and K. Leonartz,        Convective and morphological stability during directional        solidification of the succinonitrile-acetone system, in          Free Boundaries in Viscous Flows , The IMA Volumes in       Mathematics and Its Applications 61, R.A. Brown and S.H. Davis,        eds., Springer-Verlag, New York, 1994, pp. 99--112.     R. W. Davis, E. F. Moore, M. R. Zachariah, D. R. F. Burgess and T. J. Burns,       Numerical Modeling of Particle Contaminants in CVD Reactors, in   Proceedings       of the 13th International Conference on Chemical Vapor Deposition Reactors ,       Los Angeles, CA, May 1996.     A. Elipe, B.R. Miller and M. Vallejo,       Bifurcations in a non-symmetric cubic potential,        Astronomy and Astrophysics    300  (1995), pp. 722--725.     M.D. Garris, J.L. Blue, G.T. Candela, D.L. Dimmick, J.C. Geist,        P.J. Grother, S.A. Janet, C.L. Wilson, ""Public Domain Optical Character       Recognition,""   Proceedings of Document Recognition II, SPIE ,       San Jose, February, 1995.     M.D. Garris, J.L. Blue, G.T. Candela, D.L. Dimmick, J. Geist,       P.J. Grother, S.A. Janet, and C.L. Wilson, Off-line Handwriting       Recognition from Forms,''   Proceedings of IEEE International       Conference on Systems, Man, and Cybernetics , Vancouver, B.C.,       October, 1995.     J. Gary and M. Hamstad, On the Far-field Structure of Waves       Generated by a Pencil Lead Break on a Thin Plate,   Journal       of Acoustic Emission , 12 (1994), pp 157-170.     R.E. Goldstein and S.A. Langer,       Nonlinear Dynamics of Stiff Polymers,         Physical Review Letters  75 (1995) p. 1094.     F.Y.  Hunt, Approximating the Invariant Measures of Randomly Perturbed       Dissipative Maps,   J. Math. Anal. and Appl.   198 (1996) pp.  534-551.     F.Y.  Hunt, J.F.  Douglas, J.  Bernal, Probabilistic computation of       Poiseuille flow velocity field,   J. Math. Phys.   36 (1995)        (5), pp.  2386-2400     F.Y.  Hunt, R.D.  McMichael, Analytical Expressions for Barkhausen        Jump Size Distributions,   IEEE Transactions on Magnetics  30,6       (1994)     F.Y.  Hunt, Existence, Approximation and Convergence of Invariant       Measures of Randomly Perturbed Maps in   Differential Equations       and Applications to Biology and to Industry , M.  Martelli, K. Cooke,       E.  Cumberbatch, B. Tang, and H. Thieme, eds. World Scientific 1996     D. Josell, S.R. Coriell, and G.B. McFadden,        Evaluating the zero creep conditions for        thin film and multilayer thin film specimens,          Acta Metall. et Mater.  43 (1995) pp. 1987--1999.     B.L. Johnson and S.A. Langer,       Magnetic field confinement effects in a classical two-dimensional       interacting electron gas,   Physical Review B  49 (1994) p. 7511.     S.J. Lomonaco,Jr., The Modern Legacies of Thomson's       Atomic Vortex Theory in Classical Electrodynamics, American        Mathematical Society Society (AMS) Proceedings of Symposium in        Applied Mathematics, Vol. 51,1996, pp 145 - 166. (Invited paper)     S.J. Lomonaco,Jr. and R.E. Sabin,       Metacyclic Error-Correcting Codes,        Journal of Applicable Algebra in Engineering, Communications,       and Computing, Vol. 6, No. 3, 1995, pp 191-210.     D.W. Lozier,   http://math.nist.gov/nesf , the Web version       of D.W. Lozier and F.W.J. Olver,       Numerical Evaluation of Special Functions, in   Mathematics of       Computation 1943--1993: A Half-Century of Computational Mathematics ,       W. Gautschi, ed., American Mathematical Society, 1994, pp. 79--125.     D.W. Lozier,   http://math.nist.gov/stssf , the Web version       of D.W. Lozier, Software Testing Service for Special Functions:       A Proposal and Request for Comments, presented in Minisymposium       on Computational Aspects of Special Functions, SIAM Annual Meeting,       October 23--26, 1995.     D.W. Lozier, Software Needs in Special Functions,   Journal       of Computational and Applied Mathematics  66 (1996) pp. 345--358.     B.R. Miller and V.T. Coppola,       Non-canonical transformations of nonlinear Hamiltonians.       in   Hamiltonian Dynamical Systems , D. Schmidt, S. Dumas, K. Meyer, eds.       Springer Verlag, New York, 1995, Springer-Verlag. pp. 283--290.     B.T. Murray, A.A. Wheeler, and M.E. Glicksman, Simulations of Experimentally       Observed Dendritic Growth Behavior using a Phase-Field Model,          Journal of Crystal Growth ,   154  (1995) pp. 386-400.     W.F. Mitchell, Review of PLTMG: A Software Package for Solving       Elliptic Partial Differential Equations, User's Guide 7.0'',         Mathematics of Computation  64 (1995), pp. 1343--1345.     W.F. Mitchell, Refinement Tree Based Partitioning for       Adaptive Grids,   Proceedings of the   SIAM Conference       on Parallel Processing for Scientific Computing , SIAM, 1995,       pp. 587--592.     W.F. Mitchell, MGGHAT: Hierarchical Finite Element Multilevel       Adaptive Solution of Elliptic Partial Differential Equations,         Proceedings of the   IMACS World Congress on       Computational and Applied Mathematics , 1994,       pp. 356--359.     O.M. Omidvar, J.L. Blue, and C.L. Wilson,       Improving Neural Network Performance for Character and        Fingerprint Classification by Altering Network Dynamics'',          Proceedings of The World Congress on Neural Networks ,        Washington, DC, July, 1995.     K.A. Remington,  NIST SP2       Primer  : a       hypertext companion to programming on NIST's IBM PowerParallel System 2,       announced June 1995.     B.W. Rust and D.P. O'Leary, Confidence Intervals for Discrete        Approximations to Ill-Posed Problems,   Journal of Computational       and Graphical Statistics  3 (1994) pp. 67--96.     B.W. Rust and F.J. Crosby, Further Studies on the Modulation of       Fossil Fuel Production by Global Temperature Variations,         Environment International  20 (1994) pp. 429--456.     B.W. Rust, Perturbation Bounds for Linear Regression Problems,         Computing Science and Statistics  26 (1994) pp. 528-532.     H. Rushmeier, G. Ward, C. Piatko, P. Sanders, and B. Rust, Comparing       Real and Synthetic Images: Some Ideas About Metrics, in   Rendering       Techniques '95 , P.M. Hanrahan and W. Purgathofer, eds.,        Springer-Verlag/Wien, New York, 1995, pp. 82-91.     B.V. Saunders, An Algebraic Grid Generation System for Interface        Tracking, in   Numerical Grid Generation in Computational Fluid       Dynamics and Related Fields , N.P. Weatherill, P.R. Eiseman, J. Hauser,       and J.F.  Thompson, eds., Pineridge Press LTD, Swansea, UK, 1994,       pp. 49-60.     B.V. Saunders, A Boundary Conforming Grid Generation System for        Interface Tracking,   Computers and Mathematics With Applications ,       Vol. 29, No. 10, 1995, pp. 1-17.     B.V. Saunders, A Boundary-Fitted Grid System for Interface Tracking,       in   Numerical Grid Generation in Computational Field Simulations ,       B.K. Soni, J.F. Thompson, J. Hauser, and P.R. Hauser, eds., NSF       Engineering Research Center for Computational Field Simulation,       Mississippi State, Mississippi, 1996, pp. 599-608.     U. Seifert and S.A. Langer, Hydrodynamics of Membranes: the Bilayer       Aspect and Adhesion,   Biophysical Chemistry  49 (1994) p. 13.     R.F. Sekerka, S.R. Coriell, and G.B. McFadden, Stagnant film       model of the effect of natural convection on the dendrite operating       state,   J. Crystal Growth  154 (1995) pp. 370--376.     H. Singh and A. Youssef,       Mapping and Scheduling Heterogeneous Task Graphs Using Genetic        Algorithms,         Fifth Workshop on Heterogeneous Computing at the Tenth        International Parallel Processing Symposium,  Honolulu, Hawai,         April 1996.     A.C. Skeldon, G.B. McFadden, M.D. Impey, D.S. Riley,       K.A. Cliff, A.A. Wheeler, and S.H. Davis, On long-wave       morphological instabilities in directional solidification,         IMA J. Appl. Math.  6 (1995) pp. 639--652.     S. van Vaerenbergh, S.R. Coriell, G.B. McFadden,        B.T. Murray, and J.C. Legros,       Modification of morphological stability threshold by Soret       diffusion,   J. Crystal Growth  147 (1995) pp. 207--214.     J.A. Warren and B.T. Murray, Ostwald Ripening and Coalescence of a Binary       Alloy in Two Dimensions using a Phase-Field Model,          Modeling and Sim. in Matls Sci. ,   4  (1996) pp. 215-229.     A.A. Wheeler, G.B. McFadden, and W.J. Boettinger,        Phase-Field model of a eutectic alloy,         Proceedings of the Royal Society of       London, Series A.  452 (1996) pp. 495--525.                        Next:    Technical Reports   Up:    Publications    Previous:    Publications              Generated by boisvert@nist.gov on Mon Aug 19 10:08:42 EDT 1996"
GX012-88-1962549	"Information Access Division (IAD) Publications List     Below are links to the complete listings of publications, and papers as a result of the research performed by Information Access Division's Retrieval Group staff.    The publications with a letter after the coding number (such as 101A, or 205C) are available by the Superintendent of Documents through the  U.S. Government Printing Office .    The documents with a coding system without a letter (such as 101, 205) are available in electronic format below.  If a paper copy is required, please send your request to the  Patricia Flanagan  .                                                  2001       #341   Scholtz, J., (2001),   Evaluation Methodologies: A Framework for collaboration between HCI Research and Practice,   NOT PUBLISED YET, Keywords: HCI funding, HCI practice, HCI research, information exchange.     #348   Ressler, Sandy, (2001),   A Web-based 3D Glossary for Anthropometic Landmarks,   will appear in the proceedings of the HCI International 2001, New Orleans, LA, August 5-10, 2001, Keywords: arm reach volume, Frankfort plane, SAE G3, viewpoints, VRML browsers, web3d.               2000        #411  Over, P.,  The TREC Interactive Track; An Annotated Bibliography,    Information Processing and Management 37 (2001), 369-381,   Keywords< Information retrieval, TREC, Interactive.     #412   Morse, E. L., Steves, M. P.,  A Visualization Approach to Dealing with Log Data,   Published on website of the Workshop on Data Log Mining of the Computer Supported Cooperative Work Conference    #413  McCabe, R.,   Data Format for the Interchange of Fingerprint, Facial, and Scar mark and Tattoo Information (ANSI/NIST/ITL 1 2000),  NIST Special Publication 500-245    #414  Moon, H., Phillips, J. P.,  Computational and Performance Aspects of PCA-Based face Recognition Algorithm,   NISTIR 6486, March 8, 2000 and to be published in Perception    #415   Zhao, W., Chellappa, R., Phillips, J. P.,  Subspace Linear Discriminant Analysis for Face Recognition,   IEEE TransImage Processing (2000)    #416   Garris, M. D., McCabe,R. M.,  NIST Special Databasel 27, Fingerprint Minutiae from Latent and Matching Tenprint Images,   NISTIR 6534, Gaithersburg, Maryland     #417   Grother, P., Casasent, D.,  Optical Path Difference Measurement Techniques for SLMs,  Optics Communication, August 2000    #418  Grother, P., Casasent, D.,  New MTE Measurement Method for Electrically-addressed SLMs,   to be published in Applied Optics    #388   Scholtz, J., (2000),   Adaptation of Traditional Usability Testing Methods for Remote testing,   Will be in proceedings of the Hawaii International Conference on System Science,  HICSS 34, Keywords: Remote evaluation, usability, web site design.     #399    Hawkings, D., Voorhees, E., Craswell, N., Bailey, P., (2000),    Overview of the TREC-8 Web Track,  In proceedings NIST Special Publication 500 246:The Eighth Text REtrieval Conference(TREC 8), pgs 131-150.  Keywords:     #400     Hersh, W., Over, P. D., (2000),    TREC-8 Interactive Track Report,  In proceedings NIST Special Publication 500-246: The Eighth Text REtrieval Conference(TREC 8), pg 57-64. Keywords:     #401   Voorhees, Harman, D., (2000),    Overview of the Eighth Text Retrieval Conference (TREC-8),  In proceedings NIST Special Publication 500-246: The Eighth Text REtrieval Conference(TREC 8), pgs 1-24.  Keywords:    #277  Buckley, C. & Voorhees, E. (2000, July).  Evaluating Evaluation Measure Stability.  In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Athens, Greece, pp. 33-40.   #276  Voorhees, E. & Tice, D. (2000, July).   Building a Question Answering Test Collection.  In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Athens, Greece, pp. 200-207.    #292  Martin, A., Przbocki, M., Doddington, G., Reynolds, D., (2000).   The NIST Speaker Recognition Evaluation - Overview, methodology, systems, results, perspectives (1998).     Speech Communications   31 (2000) pages 225-254 (as submitted for publication)  PDF Format     #293  Bird, S., Day, D., Garofolo, J., Anderson, J., Laprun, C., Reynolds, D., (2000)  ATLAS: A flexible and extensible architecture for linguistic annotation.   In proceedings of the LREC Second International Conference on Language Resources and Evaluation 2000, Athens, Greece     #275  Voorhees, E.  & Tice, D. (2000, May/June).  The TREC-8 Question  Answering Track .  In proceedings of Language Resources and Evaluation Conference, Athens Greece.   #274  Voorhees, E.  & Tice, D. (2000, May).   Implementing a Question Answering Evaluation .  In proceedings of ""Using Evaluation Within HLT Programs: Results and Trends"", a workshop of the Second International Conference on Language Resources and Evaluation, Athens, Greece.   #273  Kantor, P. & Voorhees, E. (2000)  The TREC-5 Confusion Track: Comparing Retrieval Methods for Scanned Text .    Information Retrieval ,  2(2/3), 165-176.     #396   Scholtz, J., (2000),   The IUSR Project: Industry Usability Report,   In proceedings of the Make it Easy 2000 Conference, may 18-19, 2000 , Keywords: Procurement, software, standard report format, total-cost-of-ownership, usability.    #272  Voorhees, E.  & Harman, D. (2000).   Overview of the Sixth Text REtrieval Conference .    Information Processing and Management , 36(1), 3-35.    #293  Martin, A., Przbocki, M.,  The NIST 1999 Speaker Recognition Evaluation - An Overview.     Digital Signal Processing   Volume #10 1-3, Jan/Apr/Jul 2000 pages 1-18   PDF Format     #348   Phillips,  J. P., Martin, A., Wilson, C. L., Pryzbocki, M.,  (2000, February).    An Introduction to Evaluating Biometric Systems,  , IEEE Computer , February 2000.  (PDF format)         # 235   J. Cugini,    ""Presenting Search Results: Design, Visualization, and Evaluation"",  Proceedings of the Information Doors -- Where Information Search and Hypertext Link workshop, May 30, 2000, San Antonio, Texas (held in conjunction with the ACM Hypertext and Digital Libraries conferences).. Keywords: search engine, user interface, visualization, search results, information retrieval, usability evaluation.     #397   Scholtz, J., Mills, K. L.,   Workshop on Intelligent Environments,   In October 2000 issue,   IEEE Personal Communications Magazine,   Keywords: pervasive computing, smart spaces, smart environments, ubiquitous computing.     #371   J. Cugini, (2000),     ""Web Usability Logging: Tools and Formats"",    presented to the Tools to Support Faster and Better Usability Engineering workshop, August 15, 2000, Asheville, NC (held in conjunction with the UPA 2000 conference). Keywords: usability, logfiles, logging, remote testing.     # 369   Morse, E. L., Steves, M. P., (2000, June),  CollaLogger: A Tool for Visualizing Groups at Work,   In proceedings of the IEEE 9th International Workshop on Enabling Technologies; Infrastructure Collaborative Enterprises, WETICE, pgs 104-109, June 14-16, 2000.  Keywords: collaboration environments, exploratory sequential data analysis, software usability, visualization.        # 327   Morse, E. L., (2000),  The IUSR Project and the Common Industry Reporting Format,   In proceedings for the Conference on Universal Usability, November 15, 2000. Keywords: CIF, Common Industry Format, IUSR, procurement, software usability, usability testing.     # 332   J. Cugini, S. Laskowski, M. Sebrechts,  (2000)   Design of 3D Visualization of Search Results: Evolution and Evaluation ,  Proceedings of IST/SPIE's 12th Annual International Symposium: Electronic Imaging 2000: Visual Data Exploration and Analysis VII,  SPIE 2000,   pp. 198-210, San Jose, CA, 23-28 January 2000. Keywords: comparision of 3D and 2D, design of visualization, evaluation of visualization, information visualization, usability experiment.   #409   Watson, Grother, P.J., Casasent, D.P.,   Distortion-tolerant filter for elastic-distorted Fingerprint Matching,   NIST Interagency Report 6489, National Institute of Standards and Technology, Gaithersburg, Maryland, 2000.             1999       #407   Garris, M.D., (1999, January).   NIST Internal Report 6231 - Document Image Recognition and Retrieval: Where are We?"",  , and in proceedings of Document Recognition and Retrieval VI, SPIE volume 3651, p. 141 - 144, San Jose, January 1999. Keywords: document image recognition information retrieval.     #403    Voorhees, E., Harman, D.,(1999),    Overview of the Seventh Text retrieval Conference,  In proceedings NIST Special Publication 500-242: The Seventh Text REtrieval Conference(TREC 7), pgs 1-24.  Keywords:     #419    Hawking, D., Thistlewaite, P., Harman, D.,(1999),   Scaling Up the TREC Collection   Information Retrieval, Vol. 1, Nos. 1/2, 1999, pp. 115-137.  Keywords: test collection, very large databases, text retrieval.     #420    Banks, D., Over, P., Zhang, N-F., (1999),    Blind Men and Elephants: Six approaches to TREC data  Information Retrieval, Vol. 1, Nos. 1/2, 1999, pp. 7-34. Keywords: cluster analysis, multidimensional scaling, rank correlation.     #404    Over, P., (1999),    TREC-7 Interactive Track Report,  In proceedings NIST Special Publication 500-242: The Seventh Text REtrieval Conference(TREC 7), pgs 65-72.  Keywords:     #271  Voorhees, E. (1999).   Natural Language Processing and Information Retrieval .   In M. T. Pazienza, (Ed.),  Information Extraction:  Towards Scalable, Adaptable Systems  (pp.32-48).  Germany: Springer.  (Entry in lecture notes in artificial intelligence 1714)   #270  Schmidt, C. & Over, P.  (1999, August).    Digital Video Test Collection.   In proceedings of the Twenty-Second Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Berkeley, California, USA.      # 345   J. Cugini and J. Scholtz,     VISVIP: 3D Visualization of Paths through Web Sites ,  Proceedings of the International Workshop on Web-Based Information Visualization(WebVis'99), pp. 259-263, (in conjunction with DEXA'99, Tenth International Workshop on Database and Expert Systems Applications, eds A.M. Tjoa, A. Cammelli, R.R. Wagner) Florence, Italy, September 1-3, 1999, IEEE Computer Society. Keywords: 3D visualization, usability experiment, website development, website testing.     # 318   Damianos, L., Hirschman, L., Kozierok, R., Kurtz, J., Greenberg, A.., Holgado, R., Walls, K., Laskowski, S., Scholtz, J., (1999).   Evaluation for Collaborative Systems,   appeared in   ACM Computing Surveys  , vol. 31, no. 2es, June 1999. Keywords: case studies, computer supported cooperative work, evaluation, measures, metrics, scenario-based evaluation.      #346   M. Sebrechts, J. Vasilakis, M. Miller, J. Cugini, S. Laskowski, (1999).   Visualization of Search Results: A Comparative Evaluation of Text, 2D, and 3D Interfaces   , Proceedings of SIGIR'99, pp. 3-10,  eds. M. Hearst, F. Gey, and R. Tong, 22nd International Conference on Research and Development in Information Retrieval, Berkeley, California, August 1999.  Keywords: 3D user interface, evaluation, information retrieval, information visualization, search results.    #269  Schmidt, C. & Harman, D.  (1999, August).    Information Retrieval Library (IRLIB).   In proceedings of the Twenty-Second Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Berkeley, California, USA.   #268  Tebbutt, J.  (1999). User Evaluation of Automatically Generated Semantic Hypertext Links in a Heavily Used Procedural Manual.    Information Processing and Management,  35(1), 1-18.   #267  Downey, L. & Tice, D. (1999).   A Usability Case Study Using TREC and ZPRISE .   Information Processing and Management,  35(5), 589-603.    #290  Martin, A., Przbocki, M., Doddington, G., Reynolds, D., (1999).   The NIST 1998 Development Evaluation of Speaker Recognition on Multi-Speaker Telephone Channels    In proceedings of The Audio-and Video-based Biometric Person Authentication Conference, Washington, DC.  PDF Format    #291  Martin, A., Przbocki, M., (1999).   The 1999 NIST Speaker Recognition Evaluation, Using Summed Two-Channel Telephone Data for Speaker Detection and Speaker Tracking   In proceedings of EuroSpeech 1999, Volume #5, pgs 2215-2218    #384   Hsieh, M. L., Paek, E. G., Wilson, C. L., Hsu, K. Y., (1999, December).  Performance Enhancement of a Joint Transform Correlator Using the Directionality of a Spatial Light Modulator.   Optical Engineering  , 38(12) pages 2118-2121, December 1999.        #266  Tebbutt, J. (1999).   NIST Interagency Report 6321: The NIST LEIDER Prototype - Inserting Hypertext Links into the POMS Using Information Retrieval.  Installation Guide, User Guide and Software Documentation.   Gaithersburg, Maryland, USA: National Institute of Standards and Technology.    #347   Wilson, C. L., McCabe, R. M., (1999, May).     NIST Interagency Report 6336 - Simple Test Procedure for Image-Based Biometric Verification Systems , May 1999, Gaithersburg, Maryland: National Institute of Standards and Technology.     #350   ANSI/NIST-ITL 1-1999 -  PROPOSED DRAFT  .   American National Standard For Information Systems- Data Format for the Interchange of Fingerprint, Facial, and Scar Mark and Tattoo (SMT) Information   .(  This is in PDF Format).   This is a Revision of  ANSI/NIST-CSL 1-1993   and   ANSI/NIST-ITL 1a-1997 (NIST Interagency Report 6011)   (  In PFD format)   or   Postscript format.     #351   Phillips, J. P., (1999).    Support Vector Machines Applied to Face Recognition     NIST Interagency Report 6241   ,  November 1999 and  In Advances in Neural Information Processing Systems  11, MIT Press, pp. 803-809, 1999.    #354   Rubinfeld, M., Wilson, C. L.,  (1999, June).   NIST Interagency Report 6322 - Gray Calibration of Digital Cameras to Meet NIST Best Practice Recommendation ,  June 1999, Gaithersburg, Maryland: National Institute of Standards and Technology.    #355   Phillips,  J. P., O'Toole, A. J., Cheng, Y., Ross, B., Wild, H. A.,(1999, June).     NIST Interagency Report 6348 - Assessing Algorithms as Computational Models for Human Face Recognition ,   June 1999, Gaithersburg, Maryland: National Institute of Standards and Technology.    #393   Downey, L. L.,  Scholtz, J., ,   On the Fast Track: new Usability testing Methods for Web Sites,    NOT PUBLISHED, Keywords: remote usability testing, usability testing, web development, web site evaluation.     #394   Scholtz, J., (1999),   A Case Study: Developing a Remote, Rapid, and Automated Usability Testing Methodology for On-line Books   In proceedings of the Hawaii International Conference on System Science,  HICSS 32, Keywords: online books, navigation, usability testing, web-based documentation..     #395  Scholtz, J., (1999),   WEB Usability, the Search for a Yardstick,   In proceedings of the 3rd Conference on Human-factors and the Web,  1999, Keywords: human-computer interation, usability, web design, web evaluation.     # 326   Sheppard, C., Scholtz, J., (1999, June).   The Effects of Cultural Markers on Web Site Use,  In proceedings of the 5th Conference on Human Factors and the Web, Gaithersburg, Maryland, July 5, 1999. Keywords: cultural markers, usability, user performance, user preference, user satisfaction, web sites.     #392   Scholtz, J. C.,  Graphical User Interfaces,  John C. Webster (Ed.),  Encyclopedia of Electrical and Electronics Engineering.   Published by John Wiley. Keywords: bitmapped displays, direct manipulation, usability testing, user interface architecture, user interface toolkits, window management systems.   #408   Watson, C.I., Wilson, C.L., Paek, E.G., Grother, P.J.,   Composite Filter for LanderLugt Correlator,   NIST Interagency Report 6291, National Institute of Standards and Technology, Gaithersburg, Maryland, 1999.             1998       #398   Scholtz, J., (1998),   Kiosk-Based User Testing of Online books,   In proceedings of the Sixteenth Annual International Conference on Computer Documentation, September 1998, pgs 80-86. Keywords: Usability testing, kiosk-based testing, online books, web-based documentation, navigation.    #265  Lagergren, E., & Over, P.  (1998, August).   Comparing Interactive Information Retrieval Systems Across Sites:  the TREC-6 Interactive Track Matrix Experiment.    In proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Melbourne, Australia.    #264  Voorhees, E. (1998, August).   Variations in Relevance Judgements in the Measurement of Retrieval Effectiveness.   In proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Melbourne, Australia.    #383   Wilson, C. L., Watson, C.I. , Paek, E. G., (1998, July).     NIST Interagency Report 6184 - Effect of Resolution and Image Quality on Combined Optical and Neural Network Fingerprint Matching.  Gaithersburg, Maryland: National Institute of Standards and Technology.    #294   Garris, M. D., Klein, W.W., (1998, January).    NIST Interagency Report 6090 - Creating and Validating a Large Image Database for METTREC, Gaithersburg, Maryland: National Institute of Standards and Technology.     #295   Garris, M. D., Klein, W.W., Janet, S., (1998, January).      NIST Interagency Report 6101 - Impact of Image Quality in Machine Print Optical Character Recognition,    Gaithersburg, Maryland: National Institute of Standards and Technology.   #263  Towell, G. & Voorhees, E. (1998).   Disambiguating Highly Ambiguous Text.    Computational Linguistics,  24(1), 125-145.   #262  Harman, D. (1998, June).   The Text REtrieval Conferences (TRECs) and the Cross-Language Track.   In proceedings of the First International Conference on Language Resources and Evaluation, Granada, Spain.   #261  Tebbutt, J.  (1998, June).   Finding Links.  In proceedings of the Ninth ACM Conference on Hypertext and Hypermedia, Pittsburgh, Pennsylvania, USA.   #260  Dimmick, D., O'Brien, G., Over, P., & Rogers, W.  (1998).   Guide to Z39.50/Prise 2.0: Its Installation, Use, & Modification.   Gaithersburg, Maryland, USA: unpublished resource.   #259  Harman, D.  (1998).   The Text Retrieval Conferences.    Bulletin of the American Society for Information Retrieval,  vol#(iss#), pps.   #257  Voorhees, E. (1998).   Using WordNet for Text Retrieval.   In C. Fellbaum, (Ed.),  WordNet: An Electronic Lexical Database  (pp.285-303).  Cambridge, Massachusetts, USA: The MIT Press.    #382   Paek, E. G., Wilson, C. L., Roberts, J. W., Watson,  C.I. , (1998, January).   High-speed Temporal Characterization and Visualization of Spatial Light Modulators and Flat-panel Displays.   Optics Letter , Volume 23, No. 7, pages 546-548, April 1, 1998 and NIST Interagency Report 6108 January 1998, Gaithersburg, Maryland: National Institute of Standards and Technology.  Figures for paper:  figure 1     figure 2     figure 3      #339   Garris, M. D., Janet, S., Klein, W.W.,(1998, December).     NIST Interagency Report 6245- NIST Special Database 25 Volume 1- Federal Register Document Image Database, , December 1998.    #352   Phillips, J. P., Moon, H., Rizvi, S.A., Rauss,P. J., (1998, October).   NIST Interagency Report 6264 - The FERET Evaluation Methodology for Face-Recognition Algorithms    , October 1998, Gaithersburg, Maryland: National Institute of Standards and Technology.    #353   Phillips,  J. P., Moon, H., and Razvi, S. A., (1998, October)    NIST Interagency Report 6281 - The FERET Verification Testing Protocol for Face Recognition Algorithms ,   October 1998, Gaithersburg, Maryland: National Institute of Standards and Technology.    #379   Garris, M. D., Wilson, C. L., Blue, J. (1998).     Neural Network-Based Systems for Handprint OCR Applications ,   IEEE Trans. on Image Processing , Vol. 7, pages 1097-1112, 1998.  Paper figures:   1  ,   2 ,   3 ,   4 ,  5 ,   6 ,   7 ,    8  .  Paper tables:   1 ,    2      #389   Scholtz, J.,  (1998),   WebMetrics: A Methodology for Producing Usable Web Sites,   In proceeding for the Human Factors and Ergonomics Society Conference, 1998, Keywords: Usability testing, web design, web sites, web applications, evaluation.     #390   Steves, M. P., Scholtz, J., (1998),   Modified Field Studies for CSCW Sytems,   In SG Group Bulletin, Keywords: Collaboration technologies, Computer Supported Cooperative Work (CSCW), data visualizations, deployment techniques, evaluation methodologies.     #387   Scholtz, J., Laskowski, S. J., Downey, L., (1998),   Developing Usability Tools and Techniques for Designing and Testing Web Sites,   In proceedings for the HF & Web Conference, 1998, Keywords: remote usability testing, usabilitiy testing, web development, web site design, web site evaluation.     #406   Garris, M. D., (1998).   Intelligent System for Reading Handwriting on Forms,   In proceedings of the Thirty-First Annual Hawaii International Conference on System Sciences, Vol. III Emerging Techologies Track, pp. 233-242, 1998. Keywords:              1997     #256  Garris, M., Omidvar, O., Blue, J., Candela, G., Dimmick, D., Geist, J., Grother, P., Janet, S., & Wilson, C.  (1997).  Design of a Handprint Recognition System.    Journal of Electronic Imaging,  6(2), 231-243.    #360   Wilson, C. L.,  A New Self-Organizing Neural Network Architecture for Parallel Multi-Map Pattern Recognition - FAUST,  published in  Progress in Neural Networks , Volume 5: Architecture, 1997.    # 319   Downey, L. L., Laskowski, S. J.,  (1997, July).  NIST Special Publication 500-237 - Symposium Transcription - Usability Engineering: Industry Government Collaboration for System Effectiveness and Efficiency,  Gaithersburg, Maryland, National Institute of Standards and Technology. Keywords: usability engineering.     #402   Laskowski, S. J., Grinstein, G., (1997).  Benchmarking the Integration of Visualization and data Mining.     In proceedings of the Knowledge Discovery in Databases Conference Workshop on Issues in the Integration of data Mining and Data Visualization, August 17, 1997. Keywords: Benchmarks, data mining, data visualization, knowledge discovery.    #391   Laskowski, S. J., Downey, L. L., (1997).   Evaluation in the Trenches: Towards Rapid Evaluation,    In proceedings of the CHI'97 Workshop on Usability Testing of World Wide Web Sites. Keywords: world wide web sites, evaluation, usability.    #255  B. Fordham,  (1997, August).   Evolving Databases: An Application to Electronic Commerce.  In proceedings of the International Database Engineering and Applications Symposium 1997, Montreal, Canada.   #254  Harman, D.  (1997).   Information Retrieval.   In  Encyclopedia of Computer Science, Fourth Edition  (pp. 858-863), Anthony ralston, Edwin D. reilly, David Hemmendinger, Eds.   #253  Hoffman, D., & Downey, L.  (1997, June).   Lessons Learned in an Informal Usability Study.   In proceedings of the 20th International ACM SIGIR Conference on Research & Development in Information Retrieval, Philadelphia, Pennsylvania, USA.   #252  Voorhees, E., & Tong, R. (1997, June).   Multiple Search Engines in Database Merging.   In proceedings of the 2nd ACM International Conference on Digital Libraries, Philadelphia, Pennsylvania, USA.   #251  Voorhees, E., & Harman, D. (1997, November).   Overview of the Sixth Text REtrieval Conference (TREC-6).   In proceedings of the Sixth Text REtrieval Conference (TREC-6), Gaithersburg, Maryland, USA.   #250  Harman, D.  (1997).   The SMART Lab Report -- The Early Cornell Years.    SIGIR Forum,  31(1), 6-12.   #249  Downey, L., Laskowski, S., Buie, E., Hefley, W. (1997).   Symposium Report - Usabilty Engineering 2: Measurement and Methods.    Special Interest Group on Computer & Human Interaction (SIGCHI) Bulletin,     #248  Voorhees, E., Garofolo, J., & Sparck Jones, K.  (1997, February).   The TREC-6 Spoken Document Retrieval Track.   In proceedings of the 1997 DARPA Speech Recognition Workshop, Chantilly, Virginia, USA.   #247  Garofolo, J., Voorhees, E., Stanford, V., & Sparck Jones, K. (1997, November).    TREC-6 1997 Spoken Document Retrieval Task Overview and Results.   In proceedings of the Sixth Text REtrieval Conference (TREC-6), Gaithersburg, Maryland, USA.   #246  Over, P. (1997, November).   TREC-6 Interactive Track Report.   In proceedings of the Sixth Text REtrieval Conference (TREC-6), Gaithersburg, Maryland, USA.    # 408   Wang, Q., Ressler, S. P., (1997).  Translating IGRIP Workcells into VRML2,    In proceedings of Deneb Robotics 1997 International Simulation Conference and NIST Interagency Report 6076.  Keywords: Deneb IGRIP/TGRIP, manufacturing simulation, translator, VRML.    #288  Martin, A., Doddington, G., Kamm, T., Ordowski, M., Przybocki, M. (1997).  The DET Curve in Assessment of Detection Task Performance.  In the proceedings of the EuroSpeech 1997, Volume #4, Pages 1985-1898.  MS Office 97 format     #289  Fiscus, J. (1997).  A Post-Processing System to Yield Reduced Word Error rates: Recognizer Output Voting Error Reduction (ROVER)  IEEE Workshop on Automatic Speech Recognition and Understanding 1997.   MS Office 97 Format         #245  Voorhees, E., & Harman, D. (Eds.) (1997).   NIST Special Publication 500-240: The Sixth Text REtrieval Conference (TREC-6).   Gaithersburg, Maryland, USA: National Institute of Standards and Technology.    #377   Wilson, C. L., Blue,J., Omidvar, O.,(1997).      Neurodynamics of Learning and Network Performance , ,  Journal of Electronic Imaging , Vol. 6, pages 379-385, 1997.    #373   Grother, P. J., Candela, G. T., Blue, J. L., (1997).  Fast Implementation of Nearest Neighbor Classifiers,   in  Pattern Recognition  Vol. 30, No. 3, pp. 459-465, 1997.    #378   Garris, M. D., Blue, J. L., Candela, G. T., Grother, P. J., Janet, S.,Wilson, C. L., 1997, January).    NIST Form-Based Handprint Recognition System (Release 2.0) ,    NIST Interagency Report 5959 , January 1997, Gaithersburg, Maryland: National Institute of Standards and Technology.     #380   Wilson, C. L., Watson, C.I. , Paek, E. G., (1997).  Combined Optical and Neural Network Fingerprint Matching.  In proceedings  Optical Pattern Recognition VIII , volume 3073, pages 373-382, SPIE, Orlando, Florida, April 1997 and    NIST Interagency Report 5955  , January 1997, Gaithersburg, Maryland: National Institute of Standards and Technology.    #381   Casasent, D., Wilson, C. L., (1997, September).     Optical Metrology for Industrialization of Optical Information Processing.   Optical Pattern Recognition IX , SPIE, 3386, pages 2-13, Orlando, April 1998 and   NIST Interagency Report 6060   , September 1997, Gaithersburg, Maryland: National Institute of Standards and Technology.    # 409   Moline, J., (1995).   NIST Interagency Report 5740 - Virtual environments for Health Care,  Gaithersburg, Maryland, National Institute of Standards and Technology.            1996     #244  Garris, M., & Dimmick, D. (1996).   Form Design for High Accuracy Optical Character Recognition.    IEEE Transactions on Pattern Analysis and Machine Intelligence,  18(6), 653-656.      338   J. Cugini, C. Piatko, S. Laskowski, (1996, November).    Interactive 3D Visualization for Document Retrieval ,   Proceedings of the Workshop on New Paradigms in Information Visualization and Manipulation, ACM Conference on Information and Knowledge Management (CIKM '96), Rockville MD, November 1996.Keywords: 3d visualization, document retrieval, information retrieval, information visualization.   #243  Voorhees, E., & Harman, D. (November, 1996).   Overview of the Fifth Text REtrieval Conference (TREC-5).   In proceedings of the Fifth Text REtrieval Conference (TREC-5), Gaithersburg, Maryland, USA.   #242  Harman, D.  (1996, August).   Panel: Building and Using Test Collections.   In proceedings of the 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Zurich, Switzerland, pp 335-337.   #241  Kantor, P., & Voorhees, E. (November, 1996).   Report on the TREC-5 Confusion Track.   In proceedings of the Fifth Text REtrieval Conference (TREC-5), Gaithersburg, Maryland, USA.   #240  Voorhees, E. (November, 1996).   The TREC-5 Database Merging Track.   In proceedings of the Fifth Text REtrieval Conference (TREC-5), Gaithersburg, Maryland, USA.   #239  Over, P.  (1996, November).   TREC-5 Interactive Track Report.   In proceedings of the Fifth Text REtrieval Conference (TREC-5), Gaithersburg, Maryland, USA.   #238  Downey, L., Laskowski, S., Buie, E., Hartson, H. (1996).  Usability Engineering: Industry-Government Collaboration for System Effectiveness and Efficiency.    Special Interest Group on Computer & Human Interaction (SIGCHI) Bulletin,  28(4), 66-67.        #237  Voorhees, E., & Harman, D. (Eds.) (1996).   NIST Special Publication 500-238: The Fifth Text REtrieval Conference (TREC-5).   Gaithersburg, Maryland, USA: National Institute of Standards and Technology.    #335   Garris, M. D., Dimmick, D. L., (1996, June).     Form Design for High Accuracy Optical Character Recognition    published in  IEEE Transaction on Pattern Analysis and Machine Intelligence , June 1996.    #336  Garris, M. D., (1996, May).       NIST Interagency Report 5843 - Component-Based Handprint Segmentation Using Adaptive Writing Style Model   , May 1996, Gaithersburg, Maryland: National Institute of Standards and Technology.    #337   Wilson, C. L., Geist, J., Garris, M. D., Chellappa, R.,  (1996, December).     NIST Interagency Report 5932 - Design, Integration, and Evaluation of Form-Based Handprint and OCR Systems,    December 1996, Gaithersburg, Maryland: National Institute of Standards and Technology.     #405   Garris, M. D., (1996, September).   NIST Internal Report 5894 - Teaching Computers to Read Handprinted Paragraphs,  , National Institute of Standards and Technology, Gaithersburg, MD. Keywords: connected components, dictionary-based correction, handprint, optical character recognition, probabilistic neural network, unconstrained paragraph           1995     #236  Harman, D.  (1995).   Lab Report Special Section: Natural Language Processing and Information Retrieval Group Information Access and User Interfaces Division, National Institute of Standards and Technology.    SIGIR Forum,  29(2), 6-10.   #234  Harman, D. (1995, November).   Overview of the Fourth Text REtrieval Conference (TREC-4).   In proceedings of the Fourth Text REtrieval Conference (TREC-4), Gaithersburg, Maryland, USA.   #233  Rogers, W., Candela, G., Harman, D.  (1995, April).   Space and Time Improvements for Indexing in Information Retrieval.   In proceedings of the Fourth Annual Symposium on Document Analysis and Information Retrieval (SDAIR '95), Las Vegas, Nevada, USA.   #232  Garris, M., Blue, J., Candela, G., Dimmick, D., Geist, J., Grother, P., Janet, S., & Wilson, C. (1995, February).     Public Domain OCR.   In proceedings of SPIE's Conference on Document Recognition Technologies, San Jose, CA, USA.   #231  Harman, D. (1995).   The TREC Conferences.   Keynote paper presented at the Hypertext-Information Retrieval Multimedia Conference, Konstanz, Germany, pp. 9-28.        #230  Harman, D. (Ed.) (1995).   NIST Special Publication 500-236: The Fourth Text REtrieval Conference (TREC-4).   Gaithersburg, Maryland, USA: National Institute of Standards and Technology.       #229  Over, P., Denenberg, R., Moen, W., & Stovel, L. (Eds.) (1995).   NIST Special Publication 500-229:  Z39.50 Implementation Experiences.   Gaithersburg, Maryland, USA: National Institute of Standards and Technology.       #228  Tebbutt, J. (1995).   NIST Special Publication 500-228:  Guidelines for the Evaluation of X.500 Directory Products.   Gaithersburg, Maryland, USA:  National Institute of Standards and Technology.       #227  Olsen, K., & Tebbutt, J. (1995).   NIST Special Publication 800-11:  The Impact of the FCC's Open Network Architecture on NS/NP Telecommunications Security.   Gaithersburg, Maryland, USA:  National Institute of Standards and Technology.    #328   Garris, M. D., Blue, J. L., Candela, G. T., Dimmick, D. L., Geist, J., Grother, P. J., Janet, S., Wilson, C. L., (1995, February).     Public Domain Optical Character Recognition     SPIE, Document Recognition II , pp. 2-14, San Jose, February 1995.    #329   Garris, M. D., Blue, J. L., Candela, G. T., Dimmick, D. L., Geist, J., Grother, P. J., Janet, S., Wilson, C. L.,(1995, October).        Off-line Handwriting Recognition from Forms  , published in  IEEE International Conference on Systems, Man, and Cybernetics , pp. 2783-2788, Vancouver, Oct. 1995.    #330   Wilson, C. L., Wilkinson, R. A., Garris, M. D., (1995).     Self-Organizing Neural Network Character Recognition Using Adaptive Filtering and Feature Extraction,  In  Progress in Neural Networks , Volume 3, pages 295-313, 1995.    #331   Garris, M. D., (1995).     Evaluating Spatial Correspondence of Zones in Document Recognition Systems   published in  IEEE International Confernce on Image Processing , Washington DC, 1995.    #333   Garris, M. D., Grother, P. J., (1995).  Generalized Form Registration Using Structure-Based Techniques,  published in  Proceedings of 5th Symposium on Document Analysis and Information Retrieval , pp. 321-334, April 1996 and       NIST Interagency Report 5726    , Nov. 1995, Gaithersburg, Maryland: National Institute of Standards and Technology.    #334   Garris, M. D., (1995).   Method and Evaluation of Character Stroke Preservation on Handprint Recognition,      NIST Interagency Report 5687  , July 13 1995 and published in  SPIE, Document Recognition III , pp. 321-332, San Jose, January 1996.    #374   Candela, G. T., Grother, P. J., Watson, C.I. , Wilkinson, R. A., Wilson, C. L., (1995).     PCASYS - A Pattern-level Classification Automation System for Fingerprints  ,   NIST Interagency Report 5647    April 1995, Gaithersburg, Maryland: National Institute of Standards and Technology.    #375   Wilson, C. L., Blue, J. L., Omidvar, O. M., 1995.   Improving Neural Network Performance for Character and Fingerprint Classification by Altering Network Dynamics,  published in  World Congress on Neural Networks Proceedings , II, pgs. 151-158, July '95, Washington, DC., and      NIST Interagency Report 5695 , July 1995, Gaithersburg, Maryland: National Institute of Standards and Technology.    #376   Wilson, C. L., Blue, J. L., Omidvar, O. M., (1995).  Training Dynamics and Neural Network Performance,  published in  Neural Networks  Vol. 10, No. 5, pp 907-923, 1997, and       NIST Interagency Report 5696   , July, 1995, Gaithersburg, Maryland: National Institute of Standards and Technology.             1994     #226  Harman, D. (1994, October).   Analysis of Data from the Second Text REtrieval Conference (TREC-2).   In proceedings of the Intelligent Multimedia Information Retrieval Systems and Management Conference (RIAO '94) at Rockefeller University, New York, New York, USA.    #225  Harman, D. (1993, September).   Document Detection Data Preparation.   In proceedings of the TIPSTER Text Program Phase I, San Mateo, California, USA.   #224  Harman, D. (1993, September).   Document Detection: Summary of Results.   In proceedings of the TIPSTER Text Program Phase I, San Mateo, California, USA.   #410  Harman, D. (1993, September).   Document Detection: Overview.   In proceedings of the TIPSTER Text Program Phase I, San Mateo, California, USA.    #385  Janet, S. A.,(1994, October)   NIST Interagency Report 5506, The Maximal Accuracy Rate Algorithm for Sequence Alignment and Comparison  , October 1994, Gaithersburg, Maryland: National Institute of Standards and Technology.   #223  Harman, D. (1994, November).   Overview of the Third Text REtrieval Conference (TREC-3).   In proceedings of the Third Text REtrieval Conference (TREC-3), Gaithersburg, Maryland, USA.   #222  Willman, N. (1994, October).   A Prototype Information Retrieval System to Perform a Best-Match Search for Names.  In proceedings of the Intelligent Multimedia Information Retrieval Systems and Management (RAIO '94) at Rockefeller University, New York, New York, USA.   #221  Harman, D. (1994, April).   The Text REtrieval Conference (TREC).   Keynote paper presented at the Third Annual Symposium on Document Analysis and Information Retrieval, Las Vegas, Nevada, USA, pp. 43-67.        #220  Harman, D. (Ed.) (1994).   NIST Special Publication 500-225: Overview of the Third Text REtrieval Conference (TREC-3).   Gaithersburg, Maryland, USA: National Institute of Standards and Technology.       #219  Garris, M., & Dimmick, D. (1994).    NIST Interagency Report 5364: Evaluating Form Designs for Optical Character Recognition.  Gaithersburg, Maryland, USA: National Institute of Standards and Technology.         #218  Garris, M., Blue, J., Candela, G., Dimmick, D., Geist, J., Grother, P., Janet, S., Wilson, C.  (1994).   NIST Interagency Report 5469: Form-Based Handprint Recognition System.   Gaithersburg, Maryland, USA: National Institute of Standards and Technology.    #321   Garris, M. D.,    Design, Collection, and Analysis of Handwriting Sample Image Databases  , Encyclopedia of Computer Science and Technology , volume 31, supplement 16, Marcel Dekker, N.Y., pp. 189-213, 1994.    #322   Garris, M. D.,(1994)  Correlated Run Length Algorithm (CURL) for Detecting Form Structure Within Digitized Documents  , published in the  Third Annual Symposium of Document Analysis and Information Retrieval (UNLV) , Las Vegas, NV, pp. 413-424, April 1994.    #323   Garris, M. D.,  Unconstrained Handprint Recognition Using a Limited Lexicon,        NIST Interagency Report 5310   ,December 1993 and published in  Document Recognition IS&T/SPIE 1994 International Symposium of Electronic Imaging:Science & Technology , San Jose, CA, February 1994.    #324  Garris, M. D., Dimmick, D. L., (1994, February).      NIST Interagency Report 5364 - Evaluating Form Designs for Optical Character Recognition,  , February 1994, Gaithersburg, Maryland: National Institute of Standards and Technology.    #325   Geist, J., Wilkinson, R. A., Janet, S., Grother, P. J., Hammond, B., Larsen, N. W., Klear, R. M., Burges, C.J. C., Creecy, R., J., Hull, J., Vogl, T. P., Wilson, C. L., (1994, June).   The Second Census Optical Character Recongition Systems Conference,       NIST Interagency Report 5452 Part 1 (pgs. 1-103)  ,     NIST Interagency Report 5452 Part 2 (pgs. 104-261)   ( NOTE: Part 2 is a tar file. Many of the pages are several megabytes in size because they are composed of multiple scanned viewgraphs. So that your printer spooling queue capacity is not exceeded, it is recommended that these pages be printed one at a time in a loop that either waits for the queue to empty or sleeps for the typical printing time for a large file, a few minutes on a SparcPrinter, but dozens of minutes on older printers like a LaserWriter II.    #342   Blue, J. L., Candela, G. T., Grother, P. J., Chellappa, R., C. L. Wilson, J. D. Blue, (1994).     Evaluation of Pattern Classifiers for Fingerprint and OCR Application    , in  Pattern Recognition , 27, pp. 485-501, 1994.    #343   Wilson, C. L., Candela, G. T., Watson,  C.I. (1994)     Neural Network Fingerprint Classification  , in  Journal for Artificial Neural Networks, 1(2) , 203-228, 1994.    #344  Watson,  C.I. , Candela, G., Grother, P., ( 1994, September).        NIST Interagency Report 5493 - Comparison of FFT Fingerprint Filtering Methods for Neural Network Classification,    , September 1994.    #348  Wilson, C. L., Barnes, C. Chellappa, R., Sirohey, (1994). ,    NIST Interagency Report 5465 - Face Recognition Technology for Law Enforcement Applications  . July 1994 and  Proceedings of the IEEE , 1994.    #370   Wilson, C. L., Omidvar, O. M., (1994).  Information Content in Neural Net Optimization.  In  Journal of Connection Science , Vol. 6, No. 1, 1994.    #372  Wilson, C. L., Grother, P. J., Barnes, C.S.,        NIST Interagency Report 5542 - Binary Decision Clustering for Neural Network Based OCR,      and Journal  Pattern Recogniton , Vol. 29, No. 3, pp. 425-437, (1996). December 1994. Keywords: OCR, neural networks, data clustering, pattern recognition, K-L transform, dynamic systems.     #403H,   Wilson, C. L., Candela, G. T., Watson, C.I.,     Neural-Network Fingerprint Classification,    published in the   Journal of Artifical Neural Networks , 1(2), pgs 203-228, (1994). Keywords: optimization, ridge-valley, K-L transform fingerprint, segmentation.             1993         #217  Harman, D. (Ed.) (1993).   NIST Special Publication 500-215: The Second Text REtrieval Conference (TREC-2).   Gaithersburg, Maryland, USA: National Institute of Standards and Technology.    #312   Wilson, C. L.,(1993)   Evaluation of Character Recognition Systems , In  Neural Networks for Signal Processing III , IEEE, New York, pp. 485-496 1993.    #313   Wilson, C. L.,(1993).  Effectiveness of Feature and Classifier Algorithms in Character Recognition Systems,  NIST Interagency Report 4995 , December 1992 and In D. P. D'Amato, editor, Volume 1906. SPIE, San Jose, CA, 1993.    #314  Wilkinson, R. A., Garris, M. D., Geist, J. C., (1993).  Machine-Assisted Human Classification of Segmented Characters for OCR Testing and Training,     NIST Interagency Report 5105 , December,1992 and In D. P. D'Amato, editor, volume 1906. SPIE, San Jose, CA, 1993.    #315   Garris, M. D., (1993, February).   NIST Interagency Report 5129 - Methods for Evaluating the Performance of Systems Intended to Recognize Characters from Image Data Scanned from Forms,   , February 1993.    #316  Garris, M. D.,(1993, April).       NIST Interagency Report 5173 - NIST Scoring Package Certification Procedures in Conjunction with NIST Special Databases 2 and 6,   April 1993.    #317   Wilkinson, R. A.,( 1993, April).      NIST Interagency Report 5180 - Dictionary Production for Census Form Conference  , April 1993.    #320   Garris, M. D., (1993, August).    NIST Interagency Report 5249 - NIST Scoring Package Cross-Reference for use  with NIST Internal Reports 4950 and 5129,  , August 1993    #364   Wilson, C. L., (1993).   Statistical Analysis of Information Content for Training Pattern Recognition Networks,    NIST Interagency Report 5149  , March 1993 and In  Proceedings, Applications of Artificial Neural Networks IV.  SPIE, Orlando, Florida, April 1993.    #365   Candela, G. T., Chellappa, R., (1993,April).     NIST Interagency Report 5163 - Comparative Performance of Classification Methods for Fingerprints,    , April 1993.    #366  Blue, J. L., (1993).      NIST Interagency Report 5168 - Computational Experience with Radial Basis Function Networks .    #367   Omidvar, O. M., Wilson, C. L., (1993).  Optimization of ART Network with Boltzmann Machine,  in Dennis W. Ruck, editor, Science of Artificial Neural Networks II , volume 1966, SPIE, Orlando, FL, 1992 and    NIST Interagency Report 5176 , April 1993.    #368   Grother, P. J., Candela, G. T., (1993, June).    NIST Interagency Report 5209 - Comparison of Handprinted Digit Classifiers,    , June 1993.              1992     #216  Harman, D. (1992).   Automatic Indexing.  In R. Fidel et. al. (Eds.),  Challenges in Indexing Electronic Text and Images  (pp. 247-264), Medford, New Jersey, USA: Learned Information Inc.   #215  Harman, D. (1992).   Evaluation Issues in Information Retrieval.    Information Processing and Management,  28(4), 439-440.   #214  Harman, D., Fox, E., Baeza-Yates, R., & Lee, W. (1992).   Inverted Files.   In W. Frakes & R. Baeza-Yates (Eds.),  Information Retrieval: Data Structures and Algorithms  (pp.28-43), Englewood Cliffs, New Jersey, USA: Prentice-Hall.   #213  Harman, D. (1992).   Ranking Algorithms.   In W. Frakes & R. Baeza-Yates (Eds.),  Information Retrieval: Data Structures and Algorithms  (pp.363-392), Englewood Cliffs, New Jersey, USA: Prentice-Hall.   #212  Harman, D. (1992, June).   Relevance Feedback Revisited.   In proceedings of the 15th International Conference on Research and Development in Information Retrieval, Copenhagen, Denmark.     #301   Grother, P. J.,    Cross Validation Comparison of NIST OCR Databases   in  SPIE,   D. P. D'Amato, editor, volume 1906,  San Jose, 1993 and  NIST Interagency Report 5123, January 1992.    #302   Garris, M. D., Wilson, C. L., Blue, J. L., Candela, G. T., P. Grother, Janet, S., Wilkinson, R. A.,    Massively Parallel Implementation of Character Recognition Systems.   In  Conference on Character Recognition and Digitizer Technologies , volume 1661, pages 269-280, SPIE, San Jose, California, February 1992., and  NIST Interagency Report 4750, January 1992.    # 303   Garris, M. D., Wilson, C. L.,(1992, March)   A Neural Approach to Concurrent Character Segmentation and Recognition    In  Southcon '92 Conference Record , pages 154-159, IEEE, Orlando, Florida, March 1992.    # 304   Garris, M. D., (1992, April).    A Platform for Evolving Genetic Automata for Text Segmentation (GNATS)  In  Science of Artificial Neural Networks , volume 1710, pages 714-724, SPIE, Orlando, Florida, April 1992.    # 305   Garris, M. D., Wilson, C. L.,   Reject Mechanisms for Massively Parallel Neural Network Character Recognition Systems.  In Su-Shing Chen, editor,  Neural and Stochastic Methods in Image and Signal Processing , volume 1766. SPIE, San Diego, California, 1992, and NIST Interagency Report 4863, June, 1992.    #306   Wilkinson, R. A., Geist, J. Janet, S., Grother, P. J., Burges, C. J. C., Creecy,R., Hammond, B., Hull, J. J., Larsen, N. J., Vogle, T. P., Wilson, C. L., (1992, August).   NIST Interagency Report 4912 - The First Optical Character Recognition Systems Conference.      #307   Wilkinson, R. A., Garris, M. D.,  Comparison of Massively Parallel Hand-Print Segmentors.   In D. P. Casasent, editor, Intelligent Robots and Computer Vision: Algorithms, Techniques, and Active Vision , volume 1825. SPIE, Boston, Massachusetts, 1992 and   NIST Interagency Report 4923  , September 1992.    #308   Wilkinson, R. A., Garris, M. D., Wilson, C. L.,  Using Self-Organizing Recognition as a Mechanism for Rejecting Segmentation Errors.    In D. P. Casasent, editor,  Intelligent Robots and Computer Vision: Algorithms, Techniques, and Active Vision , volume 1825. SPIE, Boston, Massachusetts, 1992 and    NIST Interagency Report 4938 , October 1992.    #309   Garris, M. D., Janet, S., (1992, October).    NIST Interagency Report 4950 - NIST Scoring Package User's Guide - Release 1.0,    , October 1992.( NOTE: This is a tar file containing three postscript files. )    #310   Garris, M. D.,  (1992)  Design and Collection of a Handwriting Sample Image Database  ,  Social Science Computing Journal ,volume 10, pages, 196-214, 1992.    #292  McCabe, R., Wilson, C. L., Grubb, D., (1992, July)    NIST Interagency Report 4892 - Research Considerations Regarding FBI-IAFIS Tasks & Requirements.         #311   Geist, J., Wilkinson, R. A., (1992) OCR Error Rate Versus Rejection Rate for Isolated Handprint Characters,  in D. P. D'Amato, editor, volume 1906. SPIE, San Jose, CA, 1993 and    NIST Interagency Report 4990 , December 1992.    #300   Wilson, C. L., Blue, J. L.,(1992).  Neural Network Methods Applied to Character Recognition.  In  Social Science Computer Review , volume 10, pages, 173-195, 1992.   #211  Harman, D. (1992).   Relevance Feedback and Other Query Modification Techniques.    In W. Frakes & R. Baeza-Yates (Eds.),  Information Retrieval: Data Structures and Algorithms  (pp.241-263), Englewood Cliffs, New Jersey, USA: Prentice-Hall.   #210  Harman, D. (1992).   User Friendly Systems Instead of User Friendly Front-Ends.    Journal of the American Society for Information Science,  43(2), 164-174.   #209  Harman, D. (Ed.) (1992).   NIST Special Publication 500-207: The First Text REtrieval Conference (TREC-1).   Gaithersburg, Maryland, USA: National Institute of Standards and Technology.   #208  Harman, D. (1992).   NIST Interagency Report 4873: Automatic Indexing.   Gaithersburg, Maryland, USA: National Institute of Standards and Technology.    #340   Wilson, C. L., Candela, G. T., Grother, P. J., Watson, C. I., Wilkinson, R. A., (1992, July).       NIST Interagency Report 4880 - Massively Parallel Neural Network Fingerprint Classification System.   , July 1992.    #357  Wilson, C. L., Omidvar, O. M., (1992, February).       NIST Interagency Report 4766 - Optimization of Neural Network Topology and Information Content Using Boltzmann Methods  , Feb. 1992.    #358   Blue, J. L., Grother, P. J., (1992).  Training Feed Forward Networks Using Conjugate Gradients.  In  Conference on Character Recognition and Digitizer Technologies,  volume 1661, pages 179-190, SPIE, San Jose, California, February 1992 and      NIST Interagency Report 4776   .        #358   Wilson, C. L.,(1992, March).    FAUST: A Vision Based Neural Network Multi-Map Pattern Recognition Architecture.    In  Proceedings:Applications of Artificial Neural Networks III . SPIE, Orlando, Florida, April 1992 and      NIST Interagency Report 4805  , March 1992.    #361  Grother, P. J.,  Karhunen Loève Feature Extraction for Neural Handwritten Character Recognition.   In  Proceedings: Applications of Artificial Neural Networks III . SPIE, Orlando, Florida. April 1992 and        NIST Interagency Report 4824 .    #362  Wilson, C. L., (1992, June).  Massively Parallel Neural Network Recognition.  In  Proceedings of the IJCNN, Volume III , pages 227-232, June 1992.     #363   Omidvar, O. M., Wilson, C. L.,(1992).  Topological Separation Versus Weight Sharing in Neural Network Optimization.  In Su-Shing Chen, editor,  Neural and Stochastic Methods in Image and Signal Processing , volume 1766. SPIE, San Diego, California, 1992 and    NIST Interagency Report 4893 , July 1992.           1991     #207  Harman, D. (1991).   How Effective is Suffixing?    Journal of the American Society for Information Science,  42(1), 7-15.    #297   Garris, M. D., Wilkinson, R. A., Wilson, C. L., (1991, May).   Analysis of a Biologically Motivated Neural Network for Character Recognition    In the proceedings: Analysis of Neural Network Applications, ACM Press, George Mason University, May 1991.    #298   Wilkinson, R. A.,  (1991).    Segmenting Text Images with Massively Parallel Machines  . In D. P. Casasent, editor,  Intelligent Robots and Computer Vision , volume 1607, pages 312-323. SPIE, Boston, Massachusetts, 1991.    #298   Garris, M. D., Wilkinson, R. A., Wilson, C. L., (1991, July).  Methods for Enhancing Neural Network Handwritten Character Recognition.   published in  International Joint Conference on Neural Networks , Vol. 1, IEEE, Seattle, July 1991.    #356   Omidvar, O. M., Wilson, C. L., (1991).   Massively Parallel Implementation of Neural Network Architectures,    In  Proceedings of the SPIE , volume 1452, pages 532-543, Feb. 25 - Mar. 1, 1991, San Jose, CA.           1990     #206  Harman, D., & Candela, G. (1990).   Bringing Natural Information Language Retrieval Out of the Closet.    SIGCHI Bulletin,  21(1), 42-48.    #296   Wilson, C. L., Wilkinson, R. A., Garris, M. D.,(1990).   Self-Organizing Neural Network Character Recognition on a Massively Parallel Computer  In the Proceedings of the IJCNN, Vol. II, San Diego, CA, June 1990.   #205  Harman, D., & Candela, G. (1990).   Retrieving Records from a Gigabyte of Text on a Minicomputer Using Statistical Ranking.    Journal of the American Society for Information Science,  41(8), 581-589.            1989     #204  Harman, D., & Candela, G.  (1989).   A Very Fast Prototype Retrieval System Using Statistical Ranking.    SIGIR Forum,  23(3 and 4), 100-110.      #386  Garris, M. D., Wilson, C.L.,(1989, September)   NIST Interagency Report 4177, Decoding Bar Codes from Image Data            1988     #203  Harman, D. (1988).   IRX: An Information Retrieval System for Experimentation and User Applications.   In proceedings of the RIAO '88 Conference on User-Oriented Content-Based Text and Image Handling Conference, Boston, Massachusetts, USA.   #202  Harman, D. (1988, June).   Towards Interactive Query Expansion.   In proceedings of the 11th International Conference on Research and Development in Information Retrieval, Grenoble, France, pp. 321-331.            1987     #201  Harman, D. (1987,June).   A Failure Analysis of the Limitation of Suffixing in an Online Environment.   In proceedings of the 10th Annual International ACM SIGIR Conference, New Orleans, Louisiana, USA, pp. 102-108.           1986     #200   Harman, D. (1986, September).   An Experimental Study of Factors Important in Document Ranking.   In proceedings of the ACM Conference on Research and Development, Pisa, Italy.                                                                                      Site created on September 19,2000               Last updated: Tuesday, May 30, 2001,   Contact  web894@dsys.ncsl.nist.gov  with corrections."
GX233-88-6228772	PREPARED FOR THE U.S. DEPARTMENT OF ENERGY, UNDER CONTRACT DE-AC02-76CH03073  PPPL-3904 UC-70  PPPL-3904  Destruction of Invariant Surfaces and Magnetic Coordinates for Perturbed Magnetic Fields by S.R. Hudson  November 2003  PRINCETON PLASMA PHYSICS LABORATORY PRINCETON UNIVERSITY, PRINCETON, NEW JERSEY   PPPL Reports Disclaimer This report was prepared as an account of work sponsored by a n agency of the United States Government. Neither the United States Government nor any agency thereof, nor any of their employees, makes any warranty, express or implied, or assumes any legal liability o r responsibility for the accuracy, completeness, or usefulness of any information, apparatus, product, or process disclosed, or represents that its use would not infringe privately owned rights. Reference herein to any specific commercial product, process, or service by trade name, trademark, manufacturer, or otherwise, does not necessarily constitute or imply its endorsement, recommendation, or favoring by the United States Government or any agency thereof. The views and opinions of authors expressed herein do not necessarily state or reflect those of the United States Government or any agency thereof.  Availability This report is posted on the U.S. Department of Energy's Princeton Plasma Physics Laboratory Publications and Reports web site in F iscal Year 2004. The home page for PPPL Reports and Publications is: http://www.pppl.gov/pub_report/ DOE and DOE Contractors can obtain copies of this report from: U.S. Department of Energy Office of Scientific and Technical Information DOE Technical Information Services (DTIS) P.O. Box 62 Oak Ridge, TN 37831 Telephone: (865) 576-8401 Fax: (865) 576-5728 Email: reports@adonis.osti.gov This report is available to the general public from: National Technical Information Service U.S. Department of Commerce 5285 Port Royal Road Springfield, VA 22161 Telephone: 1-800-553-6847 or (703) 605-6000 Fax: (703) 321-8547 Internet: http://www.ntis.gov/ordering.htm   Destruction of invariant surfaces and magnetic coordinates for perturbed magnetic fields S.R.Hudson Princeton Plasma Physics Laboratory, PO Box 451, Princeton NJ 08543, USA. Novemb er 17, 2003 Abstract  Straight-field-line coordinates are constructed for nearly integrable magnetic fields. The coordinates are based on the robust, noble-irrational rotational-transform surfaces, whose existence is determined by an application of Greene's residue criterion. A simple method to locate these surfaces is described. Sequences of surfaces with rotational-transform converging to low order rationals maximize the region of straight-field-line coordinates.  1   I  Introduction  As toroidal magnetic field line flow, with non-vanishing toroidal component, is a Hamiltonian system [1, 2], all the powerful and insightful tools valid for Hamiltonian systems may be applied (see Refs.[3, 4] for comprehensive discussions of dynamical systems). A particularly useful property is that, if the system is integrable, then action-angle coordinates may be constructed everywhere [5]. The analog of action-angle coordinates for magnetic field line flow is straight-field-line coordinates. Such coordinates greatly simplify the dynamics. Along a field line in an integrable system, the `action' coordinate,  (toroidal flux), is constant and the `angle' coordinate,  (poloidal angle), increases linearly with the `time' coordinate,  (toroidal angle), as ( ) = 0 +   ) , where  -( is the rotational-transform. As plasma dynamics is strongly influenced by the confining magnetic field, it is not surprising that straight-field-line coordinates are widely used in plasma physics, with the most common choice being Boozer coordinates [6]. Straight-field-line coordinates can only be constructed on flux-surfaces. For integrable fields, all the magnetic field lines lie on flux-surfaces. For a non-integrable field the situation is more complicated. A non-integrable field may be considered as an integrable field plus a non-integrable perturbation and is the realistic case for plasma confinement devices. Magnetic islands form at the rational rotational-transform surfaces and chaotic tra jectories emerge near unstable periodic orbits. In such regions, straight-field-line coordinates cannot be constructed. Nonetheless, the usefulness of straight-field-line coordinates is not completely lost. Depending on the magnitude of the perturbation, the Kolmogorov-Arnold-Moser (KAM) theorem [7, 8, 9] shows that fluxsurfaces with sufficiently irrational rotational-transform survive perturbation. These flux surfaces are commonly called KAM surfaces. If the integrability destroying perturbation is small, then 2   flux surfaces will exist almost everywhere. The construction of straight-field-line coordinates becomes increasingly treacherous as the perturbation strength increases. The existence of a given flux-surface is fragile. The irrational rotational-transform surfaces will disintegrate into invariant Cantor sets (cantori) for strong perturbation and the region occupied by chaotic field lines will increase. Considering that both the rational and irrationals are dense in the set of real numbers [10, 11], the structure of the magnetic field, at every level of detail, is a complex mixture of periodic orbits, island chains and chaotic tra jectories interspersed with invariant surfaces and cantori. In the plasma physics community there are several computational codes that consider the challenging problem of describing plasma behavior in perturbed and chaotic magnetic fields [12, 13, 14, 15]. While it is not necessary to use straight-field-line coordinates, it is helpful to be aware of the type of behavior that chaotic magnetic fields exhibit. This article will present a construction of straight-field-line coordinates for nearly integrable fields containing magnetic islands, chaotic field lines and invariant flux surfaces. The coordinates are based on a selection of flux surfaces with noble irrational rotational-transform. These surfaces are likely to be most robust to chaos. A sensitive technique is employed (Greene's residue criterion) to determine if a chosen surface persists or has been destroyed by perturbation. The construction of each surface employs a method that explicitly locates a surface with desired rotational-transform, without numerically calculating the rotational-transform by field line following. The rotational-transform  is defined as the limit  = lim  ,  3 (1)      where  is an arbitrary poloidal angle coordinate. This limit exists if, for any given positive  ,there exists a  such that for all  >  , |/ - | <  . For a chaotic tra jectory, it is not clear that this limit exists. A given chaotic field line may spend an arbitrarily long time in a certain region of space with a certain average transform, then squeeze through a partial barrier (cantori) into a different region with a different average transform. An appeal to the ergodic theorem, where the `time' average is equal to the accessible phase space average, may provide some relief, but for practical purposes in which a field line cannot be followed an `infinite' distance (about 1010 transits in [16]), and considering the complex structure of chaotic fields, any practical measurement of the rotational-transform will be unreliable in chaotic regions. Furthermore, in chaotic regions, nearby tra jectories will diverge exponentially at a rate characterized by the Lyapunov exponent [3]. It is likely that any finite approximation to the rotational-transform limit will be a non-monotonic function of position in the chaotic region. In regular regions of space occupied predominately with flux surfaces, the situation is different. On KAM surfaces, the limit will converge to arbitrary accuracy if the field line is followed a sufficient distance and efficient methods for calculating the transform exist [17, 18, 19]. Finally, for periodic field lines, which close after a finite number of transits, the rotational-transform can be determined exactly after following the field line a finite distance. The outline of this article is as follows. Sec.(II) will describe the theoretical and numerical tools that will be applied. In particular, the representation of magnetic field line flow as a mapping, the tangent mapping, stellarator symmetry, and the continued fraction representation will be briefly described. Also, Greene's criterion for determining the existence of a given irrational surface is described. Sec.(III) will present a selection of an `optimal' set of surfaces by choosing the rotational-transform. In Sec.(IV), the existence of each selected surface is determined using 4   Greene's criterion, and in Sec.(V) an explicit construction of an invariant surface is presented. Having i)selected, ii)determined the existence of, and iii)constructed a suitable set of invariant surfaces, magnetic coordinates are then obtained in Sec.(VI). By selecting the most-irrational surfaces, the magnetic coordinates thus constructed are likely to be those most robust to chaos. Finally, some comments are given in Sec.(VII).  II  Preliminaries  Straight-field-line coordinates are determined by the magnetic field. This article will consider the magnetic field to be given numerically in the form B = B (, , )e + B  (, , )e + B  (, , )e, (2)  where (, , ) form a toroidal coordinate system, e =  x, e =  x, and e = x where x is the position vector and B  is nowhere zero in the region of interest. This article will consider fields that are consistent with stellarator symmetry [20]. The magnetic field and coordinates are stellarator symmetric if B  (, , ) = -B  (, -, -), B  (, , ) = B  (, , ) = B  (, -, -), B  (, -, -). (3)  It follows that if {(),()} is a field line then so also is {(-), -(-)}. Stellarator symmetry is equivalent to time-reversal symmetry in dynamical systems. A field line is determined by numerically integrating d B = (, , ) =  ,  d B 5 (4)   B d  = (, , ) =  , d B from a given starting point. The magnetic field line flow produces a mapping       ~     =T ,      (5)  (6)  ~     where the mapping T integrates the field lines over one toroidal period from a given starting point, typically chosen on a symmetry plane.  Periodic orbits serve as a useful framework for understanding chaotic fields and are defined as orbits that satisfy ( +2q/N ) = (), ( +2q/N ) = ()+2p, for integers (p, q ) and where N is the toroidal periodicity of the device. The search for symmetric periodic orbits is greatly simplified for stellarator symmetric fields. By writing the mapping T as a product of involutions (which is possible for stellarator symmetric fields; see [21, 3, 22] for details), it is only required to search for orbits that satisfy ( + q/N ) = ()+ p (9) (7) (8)  on the  = 0 line. Such symmetric periodic orbits are located via a one-dimensional search (note that derivative information is available from the tangent map described below). This halves the computational effort and affords greater numerical accuracy.  6   The behavior of orbits near a given orbit is described by the tangent map [3]           ~     =M ,      (10)  ~     where M is defined     ,    ~ ~  ,    M =  (11)  ~ ~  ,    and is determined by field line integration    ,      dM  =  M.   d    ,    (12)  The full-period tangent mapping M q at a periodic orbit determines the stability of the orbit [3]. If the eigenvalues of M q are complex conjugates, the tangent orbits will display elliptical motion under the mapping near the periodic orbit and that periodic orbit is considered stable. If the eigenvalues are real reciprocals, the tangent motion will either exponentially grow or decay and the periodic orbit is unstable. Greene [21] suggested that the existence of a given irrational KAM surface is related to the stability of neighboring periodic orbits -- in particular the periodic orbits that `best approximate' the irrational. The term `best approximate' is made clear using the continued fraction representation of the real numbers. Any real number  may be expressed as  = a0 + a1 + a2 + 1 1 1 a3 + ... m  = [a0,a1,a2 ,a3,...],  (13)  where the integers a  are called the partial quotients. The continued fraction representation is  described in detail by Niven [11]. Some salient points are reproduced here for completeness: every 7   irrational has a unique representation as an infinite continued fraction; the sequence will terminate if  is rational and will continue infinitely if  is irrational; the rational pm /qm = [a0,a1,a2,...,am ] is called the mth convergent; the convergents form a sequence of consecutively better approximates that are the `best' in the sense that if |  - a/b| < |  - pm /qm | where a, b are integers, then b > qm ; and successive convergents bound the irrational pm /q pm /q m m  < - < pm+1 /q   m+1  (or pm+1 /q  m+1  < -<   depending on m). A real number is determined by the continued fraction representation  by  = 0 where 0 is defined inductively by the partial quotients n = an + 1 n+1 . (14)  The magnitude of the partial quotients a  m  indicates how rapidly the pm /q  m  converge to . Irra-  tionals with continued fraction representations terminating in infinitely many 1's, [a0, ... , am , 1, 1, 1, ...], are most difficult to approximate with rationals, in the sense that higher order rationals are required to achieve a given accuracy, and give meaning to the expression `most-irrational'. Such irrationals are commonly called noble irrationals. The most noble irrational is [1, 1, 1, 1, 1,...], and by setting n = n+1 in Eqn.(14), this value is determined  = (1 +  5)/2. Interestingly,  this number, called the golden mean, is the limiting ratio of the Fibonacci series. There is a close relationship between the continued fraction representation and the Farey tree [23]. The continued fraction representation for a rational is unique with the following exception p/q = [a0,a1,...,am] = [a0,a1,...,am - 1, 1]. (15)  The idea of Greene's method is that a given irrational surface will exist if the sequence of convergents are stable. Greene presented this method as a conjecture. It has nonetheless yielded impressive results and MacKay has discussed the reliability of the assumptions [24]. Greene con8   sidered a quantity called the residue, R, which characterizes the stability of a periodic orbit. It is defined by the tangent map evaluated along the full periodic orbit by R= 2 - Tr(M q ) . 4 (16)  For the purposes of this article is it sufficient to note the following. For a given periodic orbit, the residue is a function of the perturbation. If the residues of the convergents of a given irrational approach zero, then that irrational surface will exist. If, however, the residues become large, then that irrational surface has been destroyed. The critical value is R = 0.25: if the residues of the convergents approach 0.25 then the irrational surface is on the edge of destruction. By criticality it is meant that the KAM surface is on the verge of destruction. The behavior of R at criticality is interesting topic: for example, for the standard non-twist map [26], the residue converges to a 6-cycle [22]. Such matters are beyond the scope of the present application and it is sufficient to determine if the residues are large or small compared to 0.25. Considering the residue to be a function of the continued fraction representation, at fixed perturbation, Greene noted that R([a0 ,... ,ai +1,... ,aN ]) > R([a0,... ,ai ,... ,aN ]) (17)  where ai =maxj (aj ). Loosely speaking, rationals with smaller partial quotients (described in this article are more-noble) have smaller residue: the more-noble irrationals are more robust to chaos. Also, Greene noted that R   as ai  , implying that there is a band of chaos associated with every rational. That the noble irrationals are most robust to perturbation may be understood by noting that they are farthest from low-order rationals and thus least likely to be destroyed by island overlap 9   -- a mechanism for chaos suggested by Chirikov [25]. Greene's residue criterion gives a very sensitive and clear criterion for determining the existence of an irrational surface. Furthermore, it guides selection of the most suitable surfaces to be used as a coordinate framework in Sec.(III). The residue has been applied in the context of magnetic field line flow by Hanson and Cary [27, 28]. Not only does the residue criterion enable the existence of a given irrational surface to be determined, it can also be used to predict whether any invariant irrational surface exists in a given interval. In this context the residue criterion may be used to identify and locate the invariant surface which defines the boundary of a chaotic region [29]. Such a surface may be called the boundary surface. Consider the interval [p0 /q0 ,p1 /q1], where the rationals p0 /q0 , p1 /q1 are neighboring, p1 q0 - p0 q1 = 1, and the mediant p/q = (p0 + p1 )/(q0 + q1). By a criterion of Ref.[29], invariant surfaces with rotational-transform in [p0/q0 ,p1 /q1 ] ( are / are not) likely to exist if the average residue (R( p0 ,q0 )  + R(  p1 ,q1 )  )/2 is significantly ( smaller / larger ) than 0.25. By recursively  subdividing the interval using the mediant, and testing the ( lower / upper ) subinterval for the existence of invariant surfaces, an algorithm is devised that will ultimately lead to the invariant surface with the ( lowest / highest ) rotational-transform in the original interval. At each subdivision, the interval length is reduced. Furthermore, as the rationals p0 /q0 , p1 /q1 are neighboring, the successive rationals have a natural connection to the convergents of nearby irrationals. This allows the continued fraction representation of the boundary surface to be deduced [29]. Alternatively, the algorithm is terminated at some point (perhaps when the periodic orbits become too long and numerical accuracy becomes poor) and the locally most robust irrational surface [30] is given by (pn + pn+1 )/(qn + q used in Sec.(VI). 10 n+1  ). This method for constructing the boundary surface will be   The model magnetic field used for this study is given as B=  +  (, , ), (18)  where the field line Hamiltonian  is given  = 2 /2+ k 1 2  cos(2 - )+ 1 cos(3 - 2) . 3  (19)  The equations governing the field line are then  = -k [sin(2 - )+sin(3 - 2)] ,    = .  (20) (21)  This field is integrable, and (, , ) are action-angle coordinates, if the perturbation parameter k is zero. In this case, the rotational-transform profile is  = . For non-zero k , `primary' islands will form at the p1 /q1 = 1/2, p2 /q2 = 2/3 surfaces. As k is increased, `secondary' islands will form at all rationals on the Farey tree formed by p1 /q1 and p2 /q2 (see [23] for a description of the Farey tree), chaotic regions will develop and invariant surfaces will be destroyed. A generally perturbed field will have a perturbation spectrum that is initially unknown. The model field presented here has sufficient complexity to represent a generally perturbed chaotic field relevant for fusion confinement devices, with the exception of reversed shear systems for which a model non-twist map [26] is required.  The remainder of this article will use these ideas i)to select, ii)to determine the existence of, and iii)to construct an appropriate set of irrational surfaces. On these surfaces, magnetic coordinates will be constructed.  11   III  Surface selection  The selection of surfaces is somewhat arbitrary and may be adapted to suit the particular application. In the construction of magnetic coordinates to be described in Sec.(VI), it is assumed that the chaotic field lines are associated with a few low-order islands. We thus seek a selection of surfaces that will maximize the region of straight-field-line coordinates -- that is, lie as close as possible to the chaotic regions of the low-order islands -- and that the selection is based on the most robust irrational surfaces. The non-uniqueness of the continued fraction representation of a rational is convenient for constructing sequences of noble irrationals that converge from above and below any given rational p/q = [a0, a1, ... , am ]. The sequences of noble irrationals  = [a0,a1,...,am,n, 1, 1, 1,...],  = [a0,a1,...,am - 1, 1,n, 1, 1,...], will converge from above or below, depending on whether m is odd or even, to p/q as n increases. The `nobility' of the irrationals in these sequences decreases as n increases. As n increases, the location of the irrational surfaces will approach the chaotic region near the p/q unstable orbit and at some point will be engulfed by the associated chaos. These two sequences alone may not provide sufficient spatial resolution, but arbitrarily many such sequences, defined by a fixed set of integers {n1,n2 ,... ,nN } may be constructed  = [a0,... ,am ,n,n1 ,n2,...,nN , 1, 1,...],  = [a0,... ,am - 1, 1,n,n1 ,n2 ,... ,nN , 1, 1,...]. This is equivalent to appending an infinite sequence of 1's to the continued fraction representation of a rational. For fixed {n1 ,n2 ,... ,nN }, each such sequence will converge to the rational p/q = [a0, 12  (22)  (23)   a1 , ... , am ] as n increases. Using such sequences, an arbitrarily dense set of noble irrationals that converge to a given rational can be constructed. Without a-priori knowledge of the perturbation spectrum, it cannot be pre-determined exactly which irrational surfaces will survive perturbation. It cannot be guaranteed that a given selection of irrational surfaces is the most robust to perturbation, but the sequences as defined can be expected to be the most robust as they are based on the most noble irrational KAM surfaces. This selection adaptively expands to fill the region in which invariant tori exist and serves to partition regions of chaos.  IV  Surface existence  It is not necessary to pre-determine the existence of a selected surface. One may proceed to directly construct the surface and then determine a-posteriori if that surface was successfully constructed. However, the efficiency and reliability of Greene's method for determining the existence of an irrational flux surface justifies the additional computational cost. In fact, as any numerical method for constructing an invariant surface will fail if that invariant surface does not exist, pre-determining the existence of a required surface may save computational effort. To illustrate the application of Greene's residue criterion, the critical perturbation is determined for the surface with rotational-transform equal to the inverse golden mean  The convergents of  -1 -1  = [0, 1, 1, 1,...].  are 1/2, 2/3, 3/5, 5/8, 8/13, 13/21, 21/34, 34/55, 55/89, 89/144, 144/233,  233/377, 377/610,... . The behavior of the residue of the convergents for three perturbation values -- below, near and above criticality -- is shown in Fig.(1).  13   For less-than-critical perturbation, the residues approach zero. For near-critical perturbation, the residues approach 0.25; and for larger-than-critical perturbation, the residues become large. For these three cases, detailed Poincar plots Fig.(2),Fig.(3),Fig.(4) confirm that the residue criterion e predicts the breakup of the surface. In each of the Poincar plots, the location of the (89, 144) e periodic orbit is shown with , the (144, 233) with 2, the (233, 377) with , and the (377, 610)  is shown with +. If the invariant surface exists, it will be located between successive convergents and thus lie between the 's and the +'s. In Fig.(4), above criticality, this region has become chaotic.  To illustrate that the more-noble irrationals are more robust to chaos, the critical perturbation for each of the irrationals comprising the sequences given in Table (I) is determined. For all these sequences, as n increases the irrationals converge, either from above or below, to the limit rational. As n increases the `nobility' of the irrational decreases and it is expected that the larger n irrationals will be destroyed at lower perturbation. These expectations are consistent with the numerical determination of the critical perturbation parameter k for each irrational, as shown in Fig.(5). This fractal `critical-function' has been studied by several authors for the standard map (see for example Ref.[31]). For each irrational, the residue of the first 13 convergents is calculated in the application of Greene's criterion. To further illustrate the behavior, the critical perturbation parameter for the following irrationals is determined   = [0, 1, 1,n,j, 1, 1, 1, 1,...], (24)  = [0, 1, 1, 1,n,j, 1, 1, 1,...], where j is an arbitrary, fixed positive integer. These irrationals converge to 1/2 and 2/3 as n  14   increases. Sequences with j > 1 may be considered less-noble than the sequence with j = 1, and indeed the less-noble irrationals are destroyed at lower perturbation as shown in Fig.(6). For this figure, the critical perturbation parameters for irrational sequences of the type given in Eqn.(24) with j = 1, 5, 10 are compared. Note that the less-noble irrationals, that is those with j = 5, 10, are interspersed among the most-noble irrationals; thus, at certain perturbation parameters, between two irrational surfaces that are invariant, there are irrational surfaces that have been destroyed. This highlights the perils of interpolating straight-field-line coordinates between invariant surfaces.  V  Surface construction  Given that the preferred surfaces have been identified, and that their existence may be determined, a method to construct a surface of prescribed irrational rotational-transform will now be described. This construction is similar to the method proposed by Dewar and Meiss [32], in that a curve of prescribed transform is defined by minimizing the difference between the curve and its own image under the map. In this case, a Newton method is used to find the curve that exactly coincides with its image, which is thus an invariant curve. Newton methods for finding invariant surfaces have also been used by Reiman and Pomphrey [33], but they did not constrain the rotational-transform of the surface. A continuous arbitrary trial curve is parameterized with a poloidal angle  using a Fourier representation M  () = m=0  m cos(m), M  (25) (26) 15  () =  + m=1  m sin(m).   The parameter  will be identified with the straight-field-line poloidal coordinate. This curve may be considered as the intersection of an invariant surface, with `action' coordinate  and specified rotational-transform   ), with the plane  = 0. The even representation (cosine series) -( of the radial coordinate  and the odd representation (sine series) of the poloidal coordinate  (in addition to the secular term ) is consistent with the assumption of a stellarator symmetric field and coordinates. This curve, and therefore the surface defined by allowing the curve to flow with the magnetic field, is invariant if it is mapped to itself under the field line flow. A discrete set of points (i ,i) is constructed equally spaced in  { | i = 2i/N , i = 0,N - 1} and is mapped to (i , i) by T . ~~ By requiring the trial curve to be invariant under the rigid rotation map (,)  (, +2 , -) the discrete set of points (i ,i ) maps to M  (27)  i =   m=0  m cos[m(i +2 ], -) M m=1  (28) (29)   i = ( +2 + -)  m sin[m(i +2 ]. -)  ~ Constructing the `error' vector f = {i - i , i - i }, which has length 2N , to be a function of  ~ x = {m ,m}, which has length 2M + 1, a Newton correction for the invariant curve is found by solving for  x 0 = f (x)+ df   x. (30)  The derivative matrix df is provided by the tangent map m (i - i ) = m i -  i m ,  ~  ~ 16 (31)   m (i - i ) = - i m ,  ~ ~ ~ ~ m (i - i ) = -i m , ~  ~ m (i - i ) = m i -  i m , and is inverted using singular value decomposition (SVD) [34].  (32) (33) (34)  By choosing 2N to be greater than 2M + 1, the SVD method will minimize |f |, whether the surface indeed exists or not. The iterations are terminated when | x| is smaller than some prescribed tolerance. With Newton methods, it is required to provide an initial guess for the iterations. This may be provided by knowledge of the rotational-transform profile. Alternatively, the location of the convergents will approximate the irrational curve. A Fourier decomposition of the entire surface in straight-field-line coordinates is immediately possible by allowing the invariant curve to flow along the field one period. This point will be expanded in the following section. The accuracy to which the invariant surface is constructed is dependent on the resolution of the Fourier representation. A convenient way to determine if sufficiently many harmonics have been used is to confirm that the Fourier representation of the invariant surface lies between the convergent periodic orbits. If not, then though the invariant surface may indeed exist, insufficient Fourier resolution has been used. Other methods to establish the surface has been successfully constructed include confirming that |f | is close to zero, that the Fourier modes decay sufficiently rapidly compared to M , and that the measured rotational-transform of a field line on the surface agrees with the prescribed value.  17   VI  Straight field line coordinates  The necessary ingredients for a robust construction of magnetic coordinates are now prepared. In this section, it will be assumed that the chaotic tra jectories arise near a few low-order periodic orbits. Sequences of noble-irrationals of the type given in Eqn.(22) will be used to select which surfaces will be used. Straight-field-line coordinates can only be constructed on invariant surfaces. The coordinates may be interpolated, but between invariant surfaces there will, in general, be an island chain and associated band of chaos and the interpolated coordinates will not be straight-field-line coordinates. For low-order rationals, the width of the region contained by islands and chaos may be significant. For high-order rational surfaces, the chaotic region may become vanishingly small. In this case, the interpolated coordinates, to a sufficient approximation, may be deemed to be straight-field-line coordinates. A selection of noble-irrational surfaces converging to (2/3) from below as n increases  = [0, 1, 1, 1,n, 1, 1, 1, 1,...] (35)  is used to construct magnetic coordinates and these are shown in Fig.(7). For this figure, only the region near the (2/3) unstable periodic orbit is shown. In the left side of this figure, the surfaces that are deemed to exist (that have convergents with residue approaching zero) are constructed and plotted with solid lines, and for such surfaces the straight-field-line angle grid is shown. As n increases, the surfaces approach the chaotic region and for some n will be destroyed. The last surface of this sequence that is deemed to exist has rotational transform  = 0.65682049400299220 = [0, 1, 1, 1, 10, 1, 1, 1,...]. 18   The boundary surface is not required to be a member of this sequence. The algorithm for locating boundary surfaces is applied to the interval defined by the rationals p0 /q0 = 21/32 = [0, 1, 1, 1, 10] and p1 /q1 = 2/3 = [0, 1, 1, 1]. After 10 subdivisions of this interval, the invariant surface that is closest to the (2/3) chaotic region is determined to have rotational transform  = 0.65696574415284240 = [0, 1, 1, 1, 10, 1, 3, 1, 2, 3, 1, 1, 1,... ]. This is also shown with a solid line in the left side of the figure. To distinguish this surface, the angle coordinate grid is not extended to this surface. On all the invariant surfaces, a field line is followed 5000 toroidal periods and is plotted on both sides of the figure. These orbits coincide with the invariant surfaces. Also shown are some orbits in the chaotic region to illustrate the stochastic region. In this application, M = 100 Fourier modes were used to describe the surfaces and the perturbation parameter k = 0.5  10-3 . In determining the surfaces, the total number of T evaluations is approximately 5N , where N > (2M +1), and thus the method is computationally competitive with field line tracing methods. The global nature of the magnetic coordinates is displayed in Fig.(8). For this construction, sequences of noble irrationals approaching (1/2) and (2/3) as n increases were used as the coordinate framework  = [0, 2,n, 1, 1, 1, 1,...]  = [0, 1, 1,n, 1, 1, 1, 1,...] - = [0, 1, 2,n, 1, 1, 1, 1,...]   - = [0, 1, 1, 1,n, 1, 1, 1, 1,...] (36) (37) (38) (39)  Note that this selection will not resolve higher order islands lying between these two low order rationals, the largest of which is the (3/5) island. Surfaces were deemed to be destroyed if either 19   the residues of the convergents did not become small or the Fourier representation of the surface, where M = 50 Fourier modes were used, did not lie between successive convergents. On the left of this figure the coordinate grid is shown. On both sides, Poincar plots show the field lines that e have been started on the invariant surfaces and near the unstable (1/2) and (2/3) periodic orbits. Also shown is a field line starting near the unstable (3/5) periodic orbit. This orbit traces out the slightly chaotic (3/5) separatrix and indicates the width of the (3/5) island. The magnitude of the perturbation parameter k = 0.75  10-3 . A similar display of the coordinates, for k = 1.00  10-3 , is displayed in Fig.(9). For this case, in addition to the (1/2) and (2/3) islands, the (3/5) island is resolved and the separatrices of the (4/7) and (5/8) islands are shown. In principle, as many islands as desired may be resolved; ultimately leading to coordinates displaying the fractal nature displayed in Fig.(5). The extension of the coordinates from the  = 0 plane to the three-dimensional volume is achieved by defining the straight field line angle 0 along each of the field lines selected in Eqn.(27) as 0 =  + . (40)  Each surface may then be represented as a function of the (0 ,) coordinates, and interpolation between the surfaces enables a coordinate transformation  = (,0,),  = (,0,), (41) (42)  where  is a convenient flux surface label (perhaps the toroidal flux enclosed) and the magnetic field may be written B = B  e + B 0 e0 + B  e . 20  (43)   On the invariant surfaces B  = 0 and by using the straight-field-line form B=  0 +   )   -( .   B = 0, the magnetic field may be written in  (44)  Finally, if an alternative toroidal angle is desired (to simplify the representation of the magnetic field or impose a desired Jacobian)    + (,0,), then the straight-field-line poloidal angle becomes 0  0 + . -  VII  Comments  In many applications, the location of a selected irrational surface may be estimated from the location of the convergents. This estimate may be sufficient for field line following techniques to determine the magnetic surface or interest. Even if the surface located does not have exactly the selected rotational-transform, a small error may be tolerable as this region of space is likely to be filled with flux surfaces. Incorporated into an iterative procedure [12], this construction of magnetic coordinates presented has the advantage that the `same' surface, as defined by the value of the rotational-transform, will be constructed at each iteration. This may have benefits for numerical stability, particularly near where singularities (in the parallel current) exist. Also, a good initial guess for the trial curve may be provided by the previous iteration. If the [a0,a1 +1], [a0,a1] islands are large, then it is likely a significant [a0,a1, 2] island will form. It is possible to overlook this island. Techniques for determining island width, such as the method described in Ref.[28], may be employed to determine if an overlooked island has significant width. 21   The selection of surfaces assumes some knowledge of the rotational-transform profile, perhaps from the nearby integrable field (or from a previous iteration). If insufficient information about the profile is known, some field line tracing may be necessary to obtain an estimate profile. If the rotational-transform profile is not monotonic, the mapping becomes a non-twist map. If the field is not stellarator symmetric, all of the above ideas are applicable with the following modifications: (i) the search for periodic orbits becomes a two dimensional search and field lines must be followed the full periodic distance; and (ii) the Fourier representation of the trial curve must include all the sine and cosine terms. These modifications add to the computational effort, but do not represent any fundamental limitation of the procedure. The author thanks Vanessa Robins, Allan Boozer, Jim Hanson, Don Monticello, Neil Pomphrey and Bob Dewar for constructive discussions. This work was supported in part by U.S. Department of Energy Contract No. DE-AC02-76CH03073.  22   References [1] J.R. Cary and R.G. Littlejohn. Annals of Phys. 151,1 (1983) [2] W.D. D'haeseleer, W.N.G. Hitchon, J.D. Callen, and J.L. Shohet. Flux Coordinates and Magnetic Field Structure. Springer, Berlin, 1991. [3] A.J. Lichtenberg and M.A. Lieberman. Regular and Chaotic Dynamics, 2nd ed. SpringerVerlag, New York, 1992. [4] D.K. Arrowsmith and C.M. Place. An introduction to Dynamical Systems. Cambridge University Press, Cambridge, U.K., 1991. [5] H. Goldstein. Classical Mechanics 2nd ed. Addison-Wesley, Massachusetts, 1980. [6] A.H. Boozer. Phys. Fluids 24,1999 (1981) [7] A.N. Kolmogorov. Dokl. Akad. Nauk. SSR 98,469 (1954) [8] V.I. Arnold. Russ. Math. Surv. 18,9 (1963) [9] J. Moser. Nachr. Akad. Wiss. Gottingen, Math. Phys. Kl. II 1,1 Kl,1 (1962)  [10] R.G Bartle and D.R. Sherbet. Introduction to real analysis. John Wiley and Sons, 1982. [11] I. Niven. Irrational Numbers. The mathematical association of America, 1956. [12] A.H. Reiman and H.S. Greenside. Comp. Phys. Comm. 43,157 (1986) [13] K. Harafuji, T. Hayashi, and T. Sato. J.Comp. Phys. 81,169 (1989) [14] L.E. Sugiyama, W. Park, H.R. Strauss, S.R. Hudson, D. Stutman, and X-Z. Tang. Nucl. Fus. 41,739 (2001) 23   [15] C.R. Sovinec, T.A. Gianakon, E.D. Held, S.E. Kruger, and D.D. Schnack. Phys. Plasmas 10,1727 (2003) [16] J.D. Meiss. Physica D 74,254 (1994) [17] A.H. Reiman and H.S. Greenside. J.Comp. Phys. 75,423 (1988) [18] J. Laskar. Physica D 67,253 (1992) [19] J. Laskar, C. Froeschl, and A. Celleti. Physica D 56,253 (1992) e [20] R.L. Dewar and S.R. Hudson. Physica D 112,275 (1998) [21] J.M. Greene. J.Math. Phys. 20,1183 (1979) [22] del Castillo, D. Negrete, J.M. Greene, and P.J. Morrison. Physica D 91,1 (1996) [23] J.D. Meiss. Rev. Modern Physics, 64,795 (1992) [24] R.S. MacKay. Nonlinearity 5,161 (1992) [25] B. Chirikov. Physics Reports 52,263 (1979) [26] P.J. Morrison. Phys. Plasmas 7,2279 (2000) [27] J.D. Hanson and J.R. Cary. Phys. Fluids 27,767 (1984) [28] J.R. Cary and J.D. Hanson. Phys. Fluids B 3,1006 (1991) [29] J.M. Greene, R.S. MacKay, and J. Stark. Physica D 21,267 (1986) [30] R.S. MacKay and J. Stark. Nonlinearity 5,867 (1992) [31] S. Marmi and J. Stark. Nonlinearity 5,743 (1992) 24   [32] R.L. Dewar and J.D. Meiss. Physica D 57,476 (1992) [33] A.H. Reiman and N. Pomphrey. J.Comp. Phys. 94,225 (1991) [34] W.H. Press, B.P. Flannery, S.A. Teukolsky, and W.T. Vetterling. Numerical Recipes in Fortran 77 : The art of scientific computing. Cambridge University Press, Cambridge, U.K., 2nd edition, 1992.  25   Table I: Irrational Sequences : the irrationals given by the continued fraction representation converge to the limiting rational, from above or below, as n  . limit 1. 2. 3. 4. 5. 6. 7. 8. 9. 10. 11. 12. 13. 14. 15. 16. (1/2) (5/9) (5/9) (4/7) (4/7) + - + - +  continued fraction [0, 1, 1,n, 1, 1, 1, 1, 1,...] [0, 1, 1, 4,n, 1, 1, 1, 1,...] [0, 1, 1, 3, 1,n, 1, 1, 1,...] [0, 1, 1, 3,n, 1, 1, 1, 1,...] [0, 1, 1, 2, 1,n, 1, 1, 1,...] [0, 1, 1, 2, 1, 1,n, 1, 1,...] [0, 1, 1, 2, 2,n, 1, 1, 1,...] [0, 1, 1, 2,n, 1, 1, 1, 1,...] [0, 1, 1, 1, 1,n, 1, 1, 1,...] [0, 1, 1, 1, 1, 2,n, 1, 1,...] [0, 1, 1, 1, 1, 1, 1,n, 1,...] [0, 1, 1, 1, 1, 1,n, 1, 1,...] [0, 1, 1, 1, 2,n, 1, 1, 1,...] [0, 1, 1, 1, 2, 1,n, 1, 1,...] [0, 1, 1, 1, 3,n, 1, 1, 1,...] [0, 1, 1, 1,n, 1, 1, 1, 1,...]  (7/12)- (7/12)+ (3/5) (3/5) - +  (8/13)- (8/13)+ (5/8) (5/8) - +  (7/11)- (7/11)+ (2/3) -  26   CAPTIONS  1. Residue below (k = 0.0015701 ... , solid), near (k = 0.0015801 ... , dotted), and above (k = 0.0015901 ... , dashed) criticality for the  2. Poincar plot showing the  e -1 -1  convergents.  surface, and the periodic orbits corresponding to its conver-  gents, below criticality (k = 0.0015701 ... ). The horizontal (angle) scale is [3.117, 3.166] and the vertical (radial) scale is [0.63165, 0.63185]. 3. Poincar plot showing the  e -1  surface, and the periodic orbits corresponding to its conver-  gents, near criticality (k = 0.0015801 ... ). The horizontal (angle) scale is [3.117, 3.166] and the vertical (radial) scale is [0.63165, 0.63185]. 4. Poincar plot showing the  e -1  `surface', and the periodic orbits corresponding to its con-  vergents, after destruction (k = 0.0015901 ... ) : horizontal (angle) range = [3.117,3.166], vertical (radial) range = [0.63165,0.63185]. 5. Critical perturbation parameter, k , for destruction of invariant surfaces, with irrational rotational-transform given in Table (I), plotted against the rotational-transform. The vertical scale is 10-3 . 6. Critical perturbation parameter, k , for destruction of invariant surfaces, with irrational rotational-transform given in Eqn.(24), plotted against the rotational-transform. Irrationals defined by the sequence with j = 1 are shown with `+', for j = 5 with `', and for j = 10 with `-'. The vertical scale is 10-3 . 7. Magnetic coordinates (left), boundary surface and Poincar plot near unstable (2, 3) periodic e orbit : horizontal (angle) range = [0.963,1.131], vertical (radial) range = [0.662,0.665]. 27   8. Magnetic coordinates (left) and Poincar plot : horizontal (angle) range = [0,2 ], vertical e (radial) range = [0.440,0.715]. 9. Magnetic coordinates and Poincar plot : horizontal (angle) range = [0,2 ], vertical (radial) e range = [0.440,0.715].  28   Figure 1:  29   Figure 2:  30   Figure 3:  31   Figure 4:  32   Figure 5:  33   Figure 6:  34   Figure 7:  35   Figure 8:  36   Figure 9:  37   External Distribution Plasma Research Laboratory, Australian National University, Australia Professor I.R. Jones, Flinders University, Australia Professor Joo Canalle, Instituto de Fisica DEQ/IF - UERJ, Brazil Mr. Gerson O. Ludwig, Instituto Nacional de Pesquisas, Brazil Dr. P.H. Sakanaka, Instituto Fisica, Brazil The Librarian, Culham Laboratory, England Mrs. S.A. Hutchinson, JET Library, England Professor M.N. Bussac, Ecole Polytechnique, France Librarian, Max-Planck-Institut fr Plasmaphysik, Germany Jolan Moldvai, Reports Library, Hungarian Academy of Sciences, Central Research Institute for Physics, Hungary Dr. P. Kaw, Institute for Plasma Research, India Ms. P.J. Pathak, Librarian, Institute for Plasma Research, India Ms. Clelia De Palo, Associazione EURATOM-ENEA, Italy Dr. G. Grosso, Instituto di Fisica del Plasma, Italy Librarian, Naka Fusion Research Establishment, JAERI, Japan Library, Laboratory for Complex Energy Processes, Institute for Advanced Study, Kyoto University, Japan Research Information Center, National Institute for Fusion Science, Japan Dr. O. Mitarai, Kyushu Tokai University, Japan Dr. Jiangang Li, Institute of Plasma Physics, Chinese Academy of Sciences, People's Republic of China Professor Yuping Huo, School of Physical Science and Technology, People's Republic of China Library, Academia Sinica, Institute of Plasma Physics, People's Republic of China Librarian, Institute of Physics, Chinese Academy of Sciences, People's Republic of China Dr. S. Mirnov, TRINITI, Troitsk, Russian Federation, Russia Dr. V.S. Strelkov, Kurchatov Institute, Russian Federation, Russia Professor Peter Lukac, Katedra Fyziky Plazmy MFF UK, Mlynska dolina F-2, Komenskeho Univerzita, SK-842 15 Bratislava, Slovakia Dr. G.S. Lee, Korea Basic Science Institute, South Korea Institute for Plasma Research, University of Maryland, USA Librarian, Fusion Energy Division, Oak Ridge National Laboratory, USA Librarian, Institute of Fusion Studies, University of Texas, USA Librarian, Magnetic Fusion Program, Lawrence Livermore National Laboratory, USA Library, General Atomics, USA Plasma Physics Group, Fusion Energy Research Program, University of California at San Diego, USA Plasma Physics Library, Columbia University, USA Alkesh Punjabi, Center for Fusion Research and Training, Hampton University, USA Dr. W.M. Stacey, Fusion Research Center, Georgia Institute of Technology, USA Dr. John Willis, U.S. Department of Energy, Office of Fusion Energy Sciences, USA Mr. Paul H. Wright, Indianapolis, Indiana, USA  07/07/03   The Princeton Plasma Physics Laboratory is operated by Princeton University under contract with the U.S. Department of Energy.  Information Services Princeton Plasma Physics Laboratory P.O. Box 451 Princeton, NJ 08543  Phone: 609-243-2750 Fax: 609-243-2751 e-mail: pppl_info@pppl.gov Internet Address: http://www.pppl.gov
GX019-49-16537738	"Retrieval Group Published Papers                       [ by author ]       [ by application area ]                        Most of these documents are available in electronic format below.  If a paper copy is required, please send your request to the  Retrieval Group Manager .              2003      Alemayehu, N., (2003). Analysis of Performance Variation Using Query Expansion.  Journal of American Society for Information Science and Technology  54(5):379-391.      2002       Voorhees, E. (July 2002)  Whither Music IR Evaluation Infrastructure: Lessons to be Learned from TREC , to appear in Proceedings from Workshop on the Creation of Standardized Test Collections, Tasks, and Metrics for Music Information Retrieval (MIR) and Music Digital Library (MDL) Evaluation, Portland, OR, USA.   Voorhees, E. (2002)  The Philosophy of Information Retrieval Evaluation , To appear in Proceedings of the 2nd Workshop of the Cross-Language Evaluation Forum, CLEF 2001, Darmstadt, Germany.   Voorhees, E. (May, 2002)  The Evaluation of Question Answering Systems: Lessons Learned from the TREC QA Track ,  Proceedings of the LREC 2002 Workshop on Question Answering --- Strategy and Resources, Las Palmas de Gran Canaria, Spain, pp. 1--4.      2001      Voorhees, E. (Nov. 2001)  Question Answering in TREC  In Proceedings of the Tenth International Conference on Information and Knowledge Management, Atlanta, GA, USA, pp. 535-537   Voorhees, E. (Sept. 2001)  Evaluation by Highly Relevant Documents .   In Proceedings of the 24th Annual International ACM SIGIR Conference   on Research and Development in Information Retrieval, New Orleans, LA, USA,   pp.  74-82.    Voorhees, E. (2001)  The TREC Question Answering Track,  Natural Language   Engineering, volume 7, number 4, pages 361-378.     2000     Buckley, C. & Voorhees, E. (2000, July).  Evaluating Evaluation Measure Stability.  In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Athens, Greece, pp. 33-40.  Voorhees, E. & Tice, D. (2000, July).   Building a Question Answering Test Collection.  In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Athens, Greece, pp. 200-207.    Voorhees, E.  & Tice, D. (2000, May/June).   The TREC-8 Question Answering Track .  In proceedings of Language Resources and Evaluation Conference, Athens Greece.  Voorhees, E.  & Tice, D. (2000, May).   Implementing a Question Answering Evaluation .  In proceedings of ""Using Evaluation Within HLT Programs: Results and Trends"", a workshop of the Second International Conference on Language Resources and Evaluation, Athens, Greece.  Kantor, P. & Voorhees, E. (2000)  The TREC-5 Confusion Track: Comparing Retrieval Methods for Scanned Text .    Information Retrieval ,  2(2/3), 165-176.   Voorhees, E.  & Harman, D. (2000).   Overview of the Sixth Text REtrieval Conference .    Information Processing and Management , 36(1), 3-35.         1999     Voorhees, E. (1999).   Natural Language Processing and Information Retrieval .   In M. T. Pazienza, (Ed.),  Information Extraction:  Towards Scalable, Adaptable Systems  (pp.32-48).  Germany: Springer.  (Entry in lecture notes in artificial intelligence 1714)  Schmidt, C. & Over, P.  (1999, August).    Digital Video Test Collection.   In proceedings of the Twenty-Second Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Berkeley, California, USA.  Schmidt, C. & Harman, D.  (1999, August).    Information Retrieval Library (IRLIB).   In proceedings of the Twenty-Second Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Berkeley, California, USA.  Tebbutt, J.  (1999).    User Evaluation of Automatically Generated Semantic Hypertext Links in a Heavily Used Procedural Manual.    Information Processing and Management,  35(1), 1-18.  Downey, L. & Tice, D. (1999).   A Usability Case Study Using TREC and ZPRISE .   Information Processing and Management,  35(5), 589-603.          1998     Lagergren, E., & Over, P.  (1998, August).   Comparing Interactive Information Retrieval Systems Across Sites:  the TREC-6 Interactive Track Matrix Experiment.    In proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Melbourne, Australia.   Voorhees, E. (1998, August).   Variations in Relevance Judgements in the Measurement of Retrieval Effectiveness.   In proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Melbourne, Australia.  Towell, G. & Voorhees, E. (1998).   Disambiguating Highly Ambiguous Text.    Computational Linguistics,  24(1), 125-145.  Harman, D. (1998, June).   The Text REtrieval Conferences (TRECs) and the Cross-Language Track.   In proceedings of the First International Conference on Language Resources and Evaluation, Granada, Spain.  Tebbutt, J.  (1998, June).   Finding Links.  In proceedings of the Ninth ACM Conference on Hypertext and Hypermedia, Pittsburgh, Pennsylvania, USA.  Dimmick, D., O'Brien, G., Over, P., & Rogers, W.  (1998).   Guide to Z39.50/Prise 2.0: Its Installation, Use, & Modification.   Gaithersburg, Maryland, USA: unpublished resource.  Harman, D.  (1998).   The Text Retrieval Conferences.    Bulletin of the American Society for Information Retrieval,  vol#(iss#), pps.  Garofolo, J., Voorhees, E., Stanford, V., & Sparck Jones, K. (1998, February).    TREC-6 1997 Spoken Document Retrieval Task Overview and Results.   In proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, Lansdowne, Virginia, USA.  Voorhees, E. (1998).   Using WordNet for Text Retrieval.   In C. Fellbaum, (Ed.),  WordNet: An Electronic Lexical Database  (pp.285-303).  Cambridge, Massachusetts, USA: The MIT Press.          1997     Garris, M., Omidvar, O., Blue, J., Candela, G., Dimmick, D., Geist, J., Grother, P., Janet, S., & Wilson, C.  (1997).   Design of a Handprint Recognition System.    Journal of Electronic Imaging,  6(2), 231-243.  B. Fordham,  (1997, August).   Evolving Databases: An Application to Electronic Commerce.  In proceedings of the International Database Engineering and Applications Symposium 1997, Montreal, Canada.  Harman, D.  (1997).   Information Retrieval.   In  Encyclopedia of Computer Science, Fourth Edition  (pp. xx-xx). City, State, Country: Publishing Company Name.  Hoffman, D., & Downey, L.  (1997, June).   Lessons Learned in an Informal Usability Study.   In proceedings of the 20th International ACM SIGIR Conference on Research & Development in Information Retrieval, Philadelphia, Pennsylvania, USA.  Voorhees, E., & Tong, R. (1997, June).   Multiple Search Engines in Database Merging.   In proceedings of the 2nd ACM International Conference on Digital Libraries, Philadelphia, Pennsylvania, USA.  Voorhees, E., & Harman, D. (1997, November).   Overview of the Sixth Text REtrieval Conference (TREC-6).   In proceedings of the Sixth Text REtrieval Conference (TREC-6), Gaithersburg, Maryland, USA.  Harman, D.  (1997).   The SMART Lab Report -- The Early Cornell Years.    SIGIR Forum,  31(1), 6-12.  Downey, L., Laskowski, S., Buie, E., Hefley, W. (1997).   Symposium Report - Usabilty Engineering 2: Measurement and Methods.    Special Interest Group on Computer & Human Interaction (SIGCHI) Bulletin,  vol#(iss#), pps.  Voorhees, E., Garofolo, J., & Sparck Jones, K.  (1997, February).   The TREC-6 Spoken Document Retrieval Track.   In proceedings of the 1997 DARPA Speech Recognition Workshop, Chantilly, Virginia, USA.  Garofolo, J., Voorhees, E., Stanford, V., & Sparck Jones, K. (1997, November).    TREC-6 1997 Spoken Document Retrieval Task Overview and Results.   In proceedings of the Sixth Text REtrieval Conference (TREC-6), Gaithersburg, Maryland, USA.  Over, P. (1997, November).   TREC-6 Interactive Track Report.   In proceedings of the Sixth Text REtrieval Conference (TREC-6), Gaithersburg, Maryland, USA.          1996     Garris, M., & Dimmick, D. (1996).   Form Design for High Accuracy Optical Character Recognition.    IEEE Transactions on Pattern Analysis and Machine Intelligence,  18(6), 653-656.  Voorhees, E., & Harman, D. (November, 1996).   Overview of the Fifth Text REtrieval Conference (TREC-5).   In proceedings of the Fifth Text REtrieval Conference (TREC-5), Gaithersburg, Maryland, USA.  Harman, D.  (1996, August).   Panel: Building and Using Test Collections.   In proceedings of the 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Zurich, Switzerland.  Kantor, P., & Voorhees, E. (November, 1996).   Report on the TREC-5 Confusion Track.   In proceedings of the Fifth Text REtrieval Conference (TREC-5), Gaithersburg, Maryland, USA.  Voorhees, E. (November, 1996).   The TREC-5 Database Merging Track.   In proceedings of the Fifth Text REtrieval Conference (TREC-5), Gaithersburg, Maryland, USA.  Over, P.  (1996, November).   TREC-5 Interactive Track Report.   In proceedings of the Fifth Text REtrieval Conference (TREC-5), Gaithersburg, Maryland, USA.  Downey, L., Laskowski, S., Buie, E., Hartson, H. (1996).  Usability Engineering: Industry-Government Collaboration for System Effectiveness and Efficiency.    Special Interest Group on Computer & Human Interaction (SIGCHI) Bulletin,  28(4), 66-67.           1995     Harman, D.  (1995).   Lab Report Special Section: Natural Language Processing and Information Retrieval Group Information Access and User Interfaces Division, National Institute of Standards and Technology.    SIGIR Forum,  29(2), 6-10.  Garris, M., Blue, J., Candela, G., Dimmick, D., Geist, J., Grother, P., Janet, S., & Wilson, C. (1995, October).   Off-line Handwriting Recognition From Forms.   In proceedings of the 1995 IEEE International Conference on Systems, Man, and Cybernetics, Vancouver, British Columbia, Canada.  Harman, D. (1995, November).   Overview of the Fourth Text REtrieval Conference (TREC-4).   In proceedings of the Fourth Text REtrieval Conference (TREC-4), Gaithersburg, Maryland, USA.  Rogers, W., Candela, G., Harman, D.  (1995, April).   Space and Time Improvements for Indexing in Information Retrieval.   In proceedings of the Fourth Annual Symposium on Document Analysis and Information Retrieval (SDAIR '95), Las Vegas, Nevada, USA.  Garris, M., Blue, J., Candela, G., Dimmick, D., Geist, J., Grother, P., Janet, S., & Wilson, C. (1995, February).     Public Domain OCR.   In proceedings of SPIE's Conference on Document Recognition Technologies, San Jose, CA, USA.  Harman, D. (1995, month).   The TREC Conferences.   Keynote paper presented at the Hypertext-Information Retrieval Multimedia Conference, Konstanz, Germany.          1994     Harman, D. (1994, October).   Analysis of Data from the Second Text REtrieval Conference (TREC-2).   In proceedings of the Intelligent Multimedia Information Retrieval Systems and Management Conference (RIAO '94) at Rockefeller University, New York, New York, USA.   Harman, D. (1994, month).   Data Preparation.   In proceedings of the TIPSTER Text Program Phase I, San Mateo, California, USA.  Harman, D. (1994, month).   Document Detection: Summary of Results.   In proceedings of the TIPSTER Text Program Phase I, San Mateo, California, USA.  Harman, D. (1994, November).   Overview of the Third Text REtrieval Conference (TREC-3).   In proceedings of the Third Text REtrieval Conference (TREC-3), Gaithersburg, Maryland, USA.  Willman, N. (1994, October).   A Prototype Information Retrieval System to Perform a Best-Match Search for Names.  In proceedings of the Intelligent Multimedia Information Retrieval Systems and Management (RAIO '94) at Rockefeller University, New York, New York, USA.  Harman, D. (1994, April).   The Text REtrieval Conference (TREC).   Keynote paper presented at the Third Annual Symposium on Document Analysis and Information Retrieval, Las Vegas, Nevada, USA.          1993             1992     Harman, D. (1992).   Automatic Indexing.  In R. Fidel et. al. (Eds.),  Challenges in Indexing Electronic Text and Images  (pp. 247-264), Medford, New Jersey, USA: Learned Information Inc.  Harman, D. (1992).   Evaluation Issues in Information Retrieval.    Information Processing and Management,  28(4), 439-440.  Harman, D., Fox, E., Baeza-Yates, R., & Lee, W. (1992).   Inverted Files.   In W. Frakes & R. Baeza-Yates (Eds.),  Information Retrieval: Data Structures and Algorithms  (pp.28-43), Englewood Cliffs, New Jersey, USA: Prentice-Hall.  Harman, D. (1992).   Ranking Algorithms.   In W. Frakes & R. Baeza-Yates (Eds.),  Information Retrieval: Data Structures and Algorithms  (pp.363-392), Englewood Cliffs, New Jersey, USA: Prentice-Hall.  Harman, D. (1992, June).   Relevance Feedback Revisited.   In proceedings of the 15th International Conference on Research and Development in Information Retrieval, Copenhagen, Denmark.   Harman, D. (1992).   Relevance Feedback and Other Query Modification Techniques.   In W. Frakes & R. Baeza-Yates (Eds.),  Information Retrieval: Data Structures and Algorithms  (pp.241-263), Englewood Cliffs, New Jersey, USA: Prentice-Hall.  Harman, D. (1992).   User Friendly Systems Instead of User Friendly Front-Ends.    Journal of the American Society for Information Science,  43(2), 164-174.          1991     Harman, D. (1991).   How Effective is Suffixing?    Journal of the American Society for Information Science,  42(1), 7-15.          1990     Harman, D., & Candela, G. (1990).   Bringing Natural Information Language Retrieval Out of the Closet.    SIGCHI Bulletin,  21(1), 42-48.  Harman, D., & Candela, G. (1990).   Retrieving Records from a Gigabyte of Text on a Minicomputer Using Statistical Ranking.    Journal of the American Society for Information Science,  41(8), 581-589.           1989     Harman, D., & Candela, G.  (1989).   A Very Fast Prototype Retrieval System Using Statistical Ranking.    SIGIR Forum,  23(3 and 4), 100-110.          1988     Harman, D. (1988).   IRX: An Information Retrieval System for Experimentation and User Applications.   In proceedings of the RIAO '88 Conference on User-Oriented Content-Based Text and Image Handling Conference, Boston, Massachusetts, USA.  Harman, D. (1988).   Towards Interactive Query Expansion.   In proceedings of the 11th International Conference on Research and Development in Information Retrieval, Grenoble, France.           1987     Harman, D. (1987, month).   A Failure Analysis of the Limitation of Suffixing in an Online Environment.   In proceedings of the 10th Annual International ACM SIGIR Conference, New Orleans, Louisiana, USA.           1986     Harman, D. (1986, September).   An Experimental Study of Factors Important in Document Ranking.   In proceedings of the ACM Conference on Research and Development, Pisa, Italy.                    Last updated:      Friday, 27-Jun-03 13:37:43       Date created:    Monday, 31-Jul-00"
GX014-75-5160972	CWT      Coded Wire Tag (CWT) Overview     Coded Wire Tags (CWT) are small pieces (0.25 x 0.5 or 1.0 mm) of stainless  steel wire that are injected into the snouts of juvenile salmon and steelhead.  Each tag is etched with a binary code that identifies its release group. Until  recently all tagged fish also had their adipose fin removed. The adipose clip is  the external flag identifying which adult fish bear a CWT to samplers,  processors and fishers. Heads of all adipose clipped fish recovered in Delta  waters are sent to a lab where the tags are found using very sensitive metal  detectors, dissected out of the head and decoded. Release, catch/sample and  recovery data are merged and estimates of contribution of tag groups to sampled  fisheries are updated regularly. This stock identification tool is used by  researchers and managers to evaluate and estimate survival, and find out where  release groups are caught. The CWT release and recovery program is an integral  part of a large coordinated program.     Scalable Vector Graphics (SVG) Overview     SVG is a  language for describing two-dimensional graphics in XML. SVG allows for three  types of graphic objects: vector graphic shapes (e.g., paths consisting of  straight lines and curves), images and text. Graphical objects can be grouped,  styled, transformed and composited into previously rendered objects. Text can be  in any XML namespace suitable to the application, which enhances searchability  and accessibility of the SVG graphics. The feature set includes nested  transformations, clipping paths, alpha masks, filter effects, template objects  and extensibility. SVG drawings can be dynamic and interactive. The Document  Object Model (DOM) for SVG, which includes the full XML DOM, allows for  straightforward and efficient vector graphics animation via scripting. A rich  set of event handlers such as onmouseover and onclick can be assigned to any SVG  graphical object. Because of its compatibility and leveraging of other Web  standards, features like scripting can be done on SVG elements and other XML  elements from different namespaces simultaneously within the same Web page.    You  must install  ADOBE SVG Viewer    HERE!  to view maps.  It's free!     ·    SVG Map List:      1997 CWT Data      1998 CWT Data      1999 CWT Data      2000 CWT Data      2001 CWT Data      2002 CWT Data
GX019-56-16099467	NIST Publications        These publications are available by the Superintendent of Documents through the  U.S. Government Printing Office .           Tebbutt, J. (1999).   NIST Interagency Report 6321: The NIST LEIDER Prototype - Inserting Hypertext Links into the POMS Using Information Retrieval.  Installation Guide, User Guide and Software Documentation.   Gaithersburg, Maryland, USA: National Institute of Standards and Technology.            Voorhees, E., & Harman, D. (Eds.) (1997).   NIST Special Publication 500-240: The Sixth Text REtrieval Conference (TREC-6).   Gaithersburg, Maryland, USA: National Institute of Standards and Technology.            Voorhees, E., & Harman, D. (Eds.) (1996).   NIST Special Publication 500-238: The Fifth Text REtrieval Conference (TREC-5).   Gaithersburg, Maryland, USA: National Institute of Standards and Technology.            Harman, D. (Ed.) (1995).   NIST Special Publication 500-236: The Fourth Text REtrieval Conference (TREC-4).   Gaithersburg, Maryland, USA: National Institute of Standards and Technology.            Over, P., Denenberg, R., Moen, W., & Stovel, L. (Eds.) (1995).   NIST Special Publication 500-229:  Z39.50 Implementation Experiences.   Gaithersburg, Maryland, USA: National Institute of Standards and Technology.            Tebbutt, J. (1995).   NIST Special Publication 500-228:  Guidelines for the Evaluation of X.500 Directory Products.   Gaithersburg, Maryland, USA:  National Institute of Standards and Technology.            Olsen, K., & Tebbutt, J. (1995).   NIST Special Publication 800-11:  The Impact of the FCC's Open Network Architecture on NS/NP Telecommunications Security.   Gaithersburg, Maryland, USA:  National Institute of Standards and Technology.            Harman, D. (Ed.) (1994).   NIST Special Publication 500-225: Overview of the Third Text REtrieval Conference (TREC-3).   Gaithersburg, Maryland, USA: National Institute of Standards and Technology.            Garris, M., & Dimmick, D. (1994).    NIST Interagency Report 5364: Evaluating Form Designs for Optical Character Recognition.  Gaithersburg, Maryland, USA: National Institute of Standards and Technology.              Garris, M., Blue, J., Candela, G., Dimmick, D., Geist, J., Grother, P., Janet, S., Wilson, C.  (1994).   NIST Interagency Report 5469: Form-Based Handprint Recognition System.   Gaithersburg, Maryland, USA: National Institute of Standards and Technology.            Harman, D. (Ed.) (1993).   NIST Special Publication 500-215: The Second Text REtrieval Conference (TREC-2).   Gaithersburg, Maryland, USA: National Institute of Standards and Technology.            Harman, D. (Ed.) (1992).   NIST Special Publication 500-207: The First Text REtrieval Conference (TREC-1).   Gaithersburg, Maryland, USA: National Institute of Standards and Technology.            Harman, D. (1992).   NIST Interagency Report 4873: Automatic Indexing.   Gaithersburg, Maryland, USA: National Institute of Standards and Technology.             Last updated:  Friday, 08-Jun-01 15:03:37 Date created:  Monday, 31-Jul-00
GX230-04-1952130	"Fermi National Accelerator Laboratory FERMILAB-FN-658  50 TeV High Field Lattice: Observations from a Golden Cell Leo Michelotti  Fermi National Accelerator Laboratory P.O. Box 500, Batavia, Illinois 60510  September 1997  Operated by Universities Research Association Inc. under Contract No. DE-AC02-76CH03000 with the United States Department of Energy   Disclaimer This report was preparedas an account of work sponsored by an agency of the United States Government. Neither the United States Government nor any agency thereof, nor any of their employees, makes any warranty, expressed or implied, or assumes any legal liability or responsibility for the accuracy, completeness, or usefulness of any information, apparatus, product, or process disclosed, or represents that its use would not infringe privately owned rights. Reference herein to any speci c commercial product, process, or service by trade name, trademark, manufacturer, or otherwise, does not necessarily constitute or imply its endorsement, recommendation, or favoring by the United States Government or any agency thereof. The views and opinions of authors expressed herein do not necessarily state or re ect those of the United States Government or any agency thereof.  Distribution Approved for public release; further dissemination unlimited.   50 TeV high field lattice: observations from a golden cell. Leo Michelotti  This short note was written during a two-week ""Futures"" workshop held at Fermilab in July, 1997. It stresses the obvious point that the first important parameters that must be chosen in designing a high-field VLHC lattice are the phase advance and length of the standard cell. These decisions are not completely trivial and should be made only after some serious work has been done. Separated function and combined function machines are both under consideration.  God, grant me the serenity to accept the things I courage to change the things I can, and the wisdom to know -- Re  cannot change, the difference. Serenity Prayer inhold Neibuhr  The principal thing that cannot be changed is the Lorentz force law, from which comes the ubiquitous expression relating momentum to magnetic rigidity, p = eB. For singly charged particles, this is written in convenient units as follows, B [T-m] = 3.33564 .. .  p [GeV/c] . While it is somewhat less immutable, we will consider the specification of building a circular p-p collider  at s = 100 TeV using 12.5 Tesla dipoles to be firmly established. From these is obtained the circumference that must be taken up by the arcs. 2 = Carcs[km]= 20.958 .. .  p[TeV/c]/B[T]= 83.834 .. . One reads of a high-field VLHC collider having a circumference of  100 km, which means that  20% would not be in the arcs. Two scenarios are under consideration: the first envisions that the ring will be built up from FODO cells, and the other proposes that it be built using combined function magnets. At this time, neither possibility has been excluded.  1   20  16  12  8  4  0 0.0 0.2 0.4 0.6 0.8 1.0  Phase advance per cell []  Figure 1: Normalized curves for the FODO model: + /L, D+/L2 , N2 , and -/N.  1  Thin lens, FODO model  The key things that can be changed are the length, 2L, and ""phase advance,"" , of the machine's ""standard cell.""1 The thin lens FODO model has the advantage that one can derive a number of simple, analytic expressions relating first order optics parameters to the phase advance through a cell. To begin with, the maximum and minimum horizontal   which occur at the ""center "" of the F and D quads, respectively  are given by one of the two equivalent expressions,  / 2 L  / 2 f = = 1  sin(/2) sin  1  sin(/2) cos(/2) (1) (2)  + from Eq. 1 is plotted as one of the curves in Figure 1. Other scaled parameters of interest, for the same model, include the maximum and minimum dispersion, D /L2 = D /L = the natural chromaticity, 1 /N = - tan(/2) ,  1 This number is actually the imaginary part of the log of the eigenvalue of the one-turn map through the cell; that is, 2 of a standard cell. It literally applies to a machine constructed only from standard cells (and, if you will, perfectly matched straight sections).  1 1  , sin (/2) 2sin(/2) 2  (3)  2   and an approximate expression for the momentum compaction, N2    sin(/2) 2  ,  where N = Carcs/2L is the number of standard cells. Some of these are also shown in Figure 1. (Derivations can be found in Edwards and Syphers[3] or Bryant and Johnsen[1].) The minimum value of + occurs at the value of sin(/2) that satisfies the polynomial, (s + 1)(s2 + s - 1)= 0 The first factor is not physical; the second is solved by the golden ratio,  sin(/2)= rG =( 5 - 1)/2 = 0.618034 .. . That it is the golden ratio is of no practical importance, but since it is the this paper, we will make as much of it as possible. Accordingly, we call the to satisfy Eq.(4) a ""golden cell,"" with ""golden phase advance"" per cell  = importance is the fact that the curve is rather flat over a large domain, so particularly restrictive constraint. For the golden cell, 3/2 1/2  (4) only original observation in FODO model designed so as 76.345 .. . . Of much greater that minimizing + is not a  min + =(1/rG + 1/rG )  L  3.33019 .. .  L . We rewrite this in terms of the number of cells in the ring, min + [m] = (3 1/3)2  p[GeV/c] N B[T]  140, 000/N,  where, for aesthetic purposes, we have substituted (31/3)2 for 3.33564 .. .  3.33019 .. .. A rather extreme upper bound on N can be found by examining its implications for the integrated quadrupole strength. Within the model, 2 2rG B l = BN sin(/2)= BN = 0.39345 .. .  BN ,   where B l is the integrated quad gradient, B is the dipole field, and N is the number of standard cells in the lattice; that last scaling coefficient applies to the golden cell only. Still assuming that B = 12.5T, and N  500 - 1000, we would need something like  15 m quadrupoles with operating gradients of  160 - 330 T/m. By comparison, quadrupoles in Tevatron's standard cell have  78 T/m gradient and 1.7 m length. If we accept this as the upper limit on N, then +  140 - 280 m. Let's say, realistically, that we expect +  300 m from these simple considerations. Similar expressions provide the values of other parameters in terms of rG for the golden cell and the scaling parameters, L or, equivalently, N. For example,  -  = (1/rG - 1/rG )  L = 0.78615 .. .  L , (/rG )2 /N2 = 25.83896 ... /N2 , 1/2  3/2  1/2     = -(rG /)  N = - 0.25024 .. .  N . 3   10.0  8.0  Beta / L & rho D / L**2  6.0  4.0  2.0  0.0 0.0  0.2  0.4 0.6 Phase advance / cell [ pi ]  0.8  1.0 LPJM  Mon Jul 14 16:34:13 1997  Figure 2: Comparison of +/L and D+ / models.  L2  for the separated function FODO and combined function FD  The scaling coefficients are tabulated below for the golden cell and for cells with 60 and 90 phase advance. + / L - / L N2  /N B l /BN 60  6/ 3 = 3.46...  2/ 3 = 1.15... 42 = 39.47...  -1/ 3 = -0.18... 1/ = 0.31... Golden Cell 3/2 1/2 1/rG + 1/rG = 3.33... 3/2 1/2 1/rG + 1/rG = 0.78... (/rG )2 = 25.83... 1/2 -rG / = -0.25... 2rG / = 0.39... 90  2 + 2 = 3.41...  2 - 2 = 0.58... 22 = 19.73... - 1/ = -0.31... 2/ = 0.45...  As a perhaps not totally worthless exercise, consider using the curve for + /2 f , given by Eq.(2), rather than + /2L. Does one arrive at the same conclusions?  2 Combined function models Curves analagous to those produced by Eq.(1) and Eq.(3) but for a combined function FD model are shown as the dashed lines in Figure 2; the solid lines are repetitions of the FODO calculations, put here for the sake of comparison. While the scaling properties are no longer exact, there are remarkably small differences when these results are plotted in this way. However, the pure FD model has no spaces for sextupoles or drifts. If we take, instead, Mishra's lattice for the 3 TeV low field injector [6] and alter the parameters so as to represent a 50 TeV high field cell, the resultant + is plotted in Figure 3, for 1000 cells, and Figure 4, for 500 cells. In the same figures are also plotted the quadrupole gradient required in the bends, in Tesla per meter and as a fraction of the magnetic 4   600  70  Quad field [ % of bend field ]  0.06  500  Quad field [T/m]  60 50  0.05  Beta max  0.04  40 30  400  0.03  0.02  300  20 10  0.01  200 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0  0  0.00  Phase advance [pi]  Figure 3: The + curve and required quadupole fields for a combined function lattice with space for sextupole magnets. Here, N = 1000. field at 1 cm from the center. We make only a few observations here. (a) min + occurs much closer to  = 90 than  = 76 . (b) The curve is not as flat as before, so that the acceptable domain in  is smaller. (c) Most importantly, gradient fields have acceptable values; the gradient field at 1 cm will be  4% of the bend field.  3  Key question  The key question is: what should be the cell length, or, equivalently, how many standard cells should there be in the ring? The scaling laws of the FODO model  and their approximate counterparts for the combined function models  only underscore the importance of this decision. Larger cells are more economical but will lead to a decreased aperture when nonlinear and error effects are taken into account. As a starting point, and only as a starting point, Glenn Goderre [5] suggested in this workshop using the criterion: (a) L[m]   = 0.8, a rule he obtained from an SSC memo by Courant, et al. [2] Don Edwards has privately suggested a similar initial guess: (b) N = C[ft]. [4] These are, in fact related; (a) implies that N = 0.77 C[ft], which is remarkably close to (b). If we apply (b) blindly to the 50 TeV machine, we get N = 522 and L = 80 [m], which are in the right ball park. At the SSC, 100 m was chosen for the half cell length after considerable study, involving calculating dynamic and linear apertures using many independent tracking programs and checking the results for consistency. As a result of these studies, an empirical power law relating linear aperture to L-3/4 was noted and used to fix the 100 m value. (A[mm]= 180L[m]-0.76 , for coil diameter of 4 cm.) No corresponding studies were carried out for the combined function model or for either model with the levels of synchrotron radiation expected for the 50 TeV high field machine. (Synchrotron radiation damping and anti-damping  5   1500  20  Quad field [ Fraction of bend field at 1 cm ]  0.015  Quad field [ T/m ]  Beta max [ m ]  15  0.010  1000  10  0.005  5  500 0.0 0.2 0.4 0.6 0.8 1.0  0  0.000  Phase advance / cell [ pi ]  Figure 4: The + curve and required quadupole fields for a combined function lattice with space for sextupole magnets. Here, N = 500. must also be considered in making the choice between a FODO cell and a combined function cell.) While it is likely that the final answer is still in the neighborhood of 100 m, I would urge that we take this issue at least as seriously as was done at the SSC. In particular, this means not making a recommendation after a quickly organized two-week workshop but only after taking the time to understand completely the SSC calculations and to do comparable studies of our own for both models under consideration. With regard to the ""Serenity Prayer,"" one word is used badly. Rather than ""change the things I can,"" a better phrase would have been ""change the things I should."" It is worthwhile keeping that distinction in mind as we proceed to study the VLHC options.  References [1] Philip J. Bryant and Kjell Johnsen. The Principles of Circular Accelerators and Storage Rings. Cambridge University Press, 1993. [2] E. D. Courant, D. R. Douglas, A. A. Garren, and D. E. Johnson. Ssc test lattice designs. Technical report. SSC Note: SSC-19. [3] D. A. Edwards and M. J. Syphers. An Introduction to the Physics of High Energy Accelerators. John Wiley & Sons, New York, 1993. [4] Don Edwards. Private communication. [5] Glenn Goderre. These proceedings. [6] Shekhar Mishra. These proceedings.  6"
GX026-66-13642799	"Published Papers  - by Author                              Retrieval Group home   |   Papers and Presentations            A  B C  D  E F G  H  I  J  K L M N  O  P Q  R   S   T  U  V  W X Y Z                     Alemayehu, N.               Alemayehu, N. , (2003). Analysis of Performance Variation Using Query Expansion.  Journal of American Society for Information Science and Technology  54(5):379-391.                         A  B C  D  E F G  H  I  J  K L M N  O  P Q  R   S   T  U  V  W X Y Z                    Dimmick, D.               Dimmick, D. , O'Brien, G., Over, P., & Rogers, W.  (1998).    Guide to Z39.50/Prise 2.0: Its Installation, Use, & Modification.   Gaithersburg, Maryland, USA: unpublished resource.       Garris, M., Omidvar, O., Blue, J., Candela, G.,  Dimmick, D. , Geist, J., Grother, P., Janet, S., & Wilson, C.  (1997).  Design of a Handprint Recognition System.   Journal of Electronic Imaging,  6(2), 231-243.       Garris, M., &  Dimmick, D.  (1996).  Form Design for High Accuracy Optical Character Recognition.   IEEE Transactions on Pattern Analysis and Machine Intelligence,  18(6), 653-656.       Garris, M., Blue, J., Candela, G.,  Dimmick, D. , Geist, J., Grother, P., Janet, S., & Wilson, C. (1995, October).   Off-line Handwriting Recognition From Forms.   In proceedings of the 1995 IEEE International Conference on Systems, Man, and Cybernetics, Vancouver, British Columbia, Canada.       Garris, M., Blue, J., Candela, G.,  Dimmick, D. , Geist, J., Grother, P., Janet, S., & Wilson, C. (1995, Fubruary).   Public Domain OCR.   In proceedings of SPIE's Conference on Document Recognition Technologies, San Jose, CA, USA.                              A  B C  D  E F G  H  I  J  K L M N  O  P Q  R   S   T  U  V  W X Y Z                           Harman, D.               Voorhees, E. &  Harman, D.  (2000).   Overview of the Sixth Text REtrieval Conference .  Information Processing and Management , 36(1), 3-35.       Schmidt, C. &  Harman, D.   (1999, August).    Information Retrieval Library (IRLIB).   In proceedings of the Twenty-Second Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Berkeley, California, USA.       Harman, D.  (1998, June).   The Text REtrieval Conferences (TRECs) and the Cross-Language Track.   In proceedings of the First International Conference on Language Resources and Evaluation, Granada, Spain.       Harman, D.   (1998).  The Text Retrieval Conferences.   Bulletin of the American Society for Information Retrieval,  vol#(iss#), pps.       Harman, D.   (1997).  Information Retrieval.  In  Encyclopedia of Computer Science, Fourth Edition  (pp. xx-xx). City, State, Country: Publishing Company Name.       Harman, D.   (1997).  The SMART Lab Report -- The Early Cornell Years.   SIGIR Forum,  31(1), 6-12.       Voorhees, E., &  Harman, D.  (1997, November).   Overview of the Sixth Text REtrieval Conference (TREC-6).   In proceedings of the Sixth Text REtrieval Conference (TREC-6), Gaithersburg, Maryland, USA.       Harman, D.   (1996, August).   Panel: Building and Using Test Collections.   In proceedings of the 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Zurich, Switzerland.       Voorhees, E., &  Harman, D.  (1996, November).  Overview of the Fifth Text REtrieval Conference (TREC-5).   In proceedings of the Fifth Text REtrieval Conference (TREC-5), Gaithersburg, Maryland, USA.       Harman, D.   (1995).  Lab Report Special Section: Natural Language Processing and Information Retrieval Group Information Access and User Interfaces Division, National Institute of Standards and Technology.   SIGIR Forum,  29(2), 6-10.       Harman, D.  (1995, November).   Overview of the Fourth Text REtrieval Conference (TREC-4).   In proceedings of the Fourth Text REtrieval Conference (TREC-4), Gaithersburg, Maryland, USA.       Harman, D.  (1995, month).   The TREC Conferences.   Keynote paper presented at the Hypertext-Information Retrieval Multimedia Conference, Konstanz, Germany.       Rogers, W., Candela, G.,  Harman, D.   (1995, April).   Space and Time Improvements for Indexing in Information Retrieval.   In proceedings of the Fourth Annual Symposium on Document Analysis and Information Retrieval (SDAIR '95), Las Vegas, Nevada, USA.       Harman, D.  (1994, April).   The Text REtrieval Conference (TREC).   Keynote paper presented at the Third Annual Symposium on Document Analysis and Information Retrieval, Las Vegas, Nevada, USA.       Harman, D.  (1994, November).   Overview of the Third Text REtrieval Conference (TREC-3).   In proceedings of the Third Text REtrieval Conference (TREC-3), Gaithersburg, Maryland, USA.       Harman, D.  (1994, month).   Document Detection: Summary of Results.   In proceedings of the TIPSTER Text Program Phase I, San Mateo, California, USA.       Harman, D.  (1994, month).   Data Preparation.   In proceedings of the TIPSTER Text Program Phase I, San Mateo, California, USA.       Harman, D.  (1994, October).   Analysis of Data from the Second Text REtrieval Conference (TREC-2).   In proceedings of the Intelligent Multimedia Information Retrieval Systems and Management Conference (RIAO '94) at Rockefeller University, New York, New York, USA.        Harman, D. , Fox, E., Baeza-Yates, R., & Lee, W. (1992).  Inverted Files.  In W. Frakes & R. Baeza-Yates (Eds.),  Information Retrieval: Data Structures and Algorithms  (pp.28-43), Englewood Cliffs, New Jersey, USA: Prentice-Hall.       Harman, D.  (1992).  Relevance Feedback and Other Query Modification Techniques.  In W. Frakes & R. Baeza-Yates (Eds.),  Information Retrieval: Data Structures and Algorithms  (pp.241-263), Englewood Cliffs, New Jersey, USA: Prentice-Hall.       Harman, D.  (1992).  Ranking Algorithms.  In W. Frakes & R. Baeza-Yates (Eds.),  Information Retrieval: Data Structures and Algorithms  (pp.363-392), Englewood Cliffs, New Jersey, USA: Prentice-Hall.       Harman, D.  (1992).  Evaluation Issues in Information Retrieval.   Information Processing and Management,  28(4), 439-440.       Harman, D.  (1992).  User Friendly Systems Instead of User Friendly Front-Ends.   Journal of the American Society for Information Science,  43(2), 164-174.       Harman, D.  (1992, June).   Relevance Feedback Revisited.   In proceedings of the 15th International Conference on Research and Development in Information Retrieval, Copenhagen, Denmark.        Harman, D.  (1992).  Automatic Indexing. In R. Fidel et. al. (Eds.),  Challenges in Indexing Electronic Text and Images  (pp. 247-264), Medford, New Jersey, USA: Learned Information Inc.       Harman, D.  (1991).  How Effective is Suffixing?   Journal of the American Society for Information Science,  42(1), 7-15.       Harman, D. , & Candela, G. (1990).  Bringing Natural Information Language Retrieval Out of the Closet.   SIGCHI Bulletin,  21(1), 42-48.       Harman, D. , & Candela, G. (1990).  Retrieving Records from a Gigabyte of Text on a Minicomputer Using Statistical Ranking.   Journal of the American Society for Information Science,  41(8), 581-589.        Harman, D. , & Candela, G.  (1989).  A Very Fast Prototype Retrieval System Using Statistical Ranking.   SIGIR Forum,  23(3 and 4), 100-110.       Harman, D.  (1988).   Towards Interactive Query Expansion.   In proceedings of the 11th International Conference on Research and Development in Information Retrieval, Grenoble, France.        Harman, D.  (1988).   IRX: An Information Retrieval System for Experimentation and User Applications.   In proceedings of the RIAO '88 Conference on User-Oriented Content-Based Text and Image Handling Conference, Boston, Massachusetts, USA.       Harman, D.  (1987, month).   A Failure Analysis of the Limitation of Suffixing in an Online Environment.   In proceedings of the 10th Annual International ACM SIGIR Conference, New Orleans, Louisiana, USA.        Harman, D.  (1986, September).   An Experimental Study of Factors Important in Document Ranking.   In proceedings of the ACM Conference on Research and Development, Pisa, Italy.                               A  B C  D  E F G  H  I  J  K L M N  O  P Q  R   S   T  U  V  W X Y Z                           Jones, D.                                              A  B C  D  E F G  H  I  J  K L M N  O  P Q  R   S   T  U  V  W X Y Z                           Over, P.               Schmidt, C. &  Over, P.   (1999, August).    Digital Video Test Collection.   In proceedings of the Twenty-Second Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval, Berkeley, CA, USA.       Dimmick, D., O'Brien, G.,  Over, P. , & Rogers, W.  (1998).    Guide to Z39.50/Prise 2.0: Its Installation, Use, & Modification.   Gaithersburg, Maryland, USA: unpublished resource.       Lagergren, E., &  Over, P.   (1998, August).   Comparing Interactive Information Retrieval Systems Across Sites:  the TREC-6 Interactive Track Matrix Experiment.    In proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Melbourne, Australia.        Over, P.   (1997, November).   TREC-6 Interactive Track Report.   In proceedings of the Sixth Text REtrieval Conference (TREC-6), Gaithersburg, Maryland, USA.       Over, P.   (1996, November).   TREC-5 Interactive Track Report.   In proceedings of the Fifth Text REtrieval Conference (TREC-5), Gaithersburg, Maryland, USA.                              A  B C  D  E F G  H  I  J  K L M N  O  P Q  R   S   T  U  V  W X Y Z                           Rogers, W.               Dimmick, D., O'Brien, G., Over, P., &  Rogers, W.   (1998).    Guide to Z39.50/Prise 2.0: Its Installation, Use, & Modification.   Gaithersburg, Maryland, USA: unpublished resource.       Rogers, W. , Candela, G., Harman, D.  (1995, April).   Space and Time Improvements for Indexing in Information Retrieval.   In proceedings of the Fourth Annual Symposium on Document Analysis and Information Retrieval (SDAIR '95), Las Vegas, Nevada, USA.                              A  B C  D  E F G  H  I  J  K L M N  O  P Q  R   S   T  U  V  W X Y Z                           Schmidt, C.               Schmidt, C.  & Over, P.  (1999, August).    Digital Video Test Collection.   In proceedings of the Twenty-Second Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval, Berkeley, CA, USA.       Schmidt, C.  & Harman, D.  (1999, August).    Information Retrieval Library (IRLIB).   In proceedings of the Twenty-Second Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Berkeley, California, USA.                              A  B C  D  E F G  H  I  J  K L M N  O  P Q  R   S   T  U  V  W X Y Z                           Tebbutt, J.               Tebbutt, J.   (1999).    User Evaluation of Automatically Generated Semantic Hypertext Links in a Heavily Used Procedural Manual .   Information Processing and Management,  35(1), 1-18.       Tebbutt, J.   (1998, June).   Finding Links.   In proceedings of the Ninth ACM Conference on Hypertext and Hypermedia, Pittsburgh, Pennsylvania, USA.           Tice, D.                  Voorhees, E. &  Tice, D.  (2000, May/June).   The TREC-8 Question Answering Track . In proceedings of Language Resources and Evaluation Conference, Athens Greece.       Voorhees, E. &  Tice, D.  (2000, May).   Implementing a Question Answering Evaluation . In proceedings of ""Using Evaluation Within HLT Programs: Results and Trends"", a workshop of the Second International Conference on Language Resources and Evaluation, Athens, Greece.       Downey, L. &  Tice, D.  (1999).   A Usability Case Study Using TREC and ZPRISE .   Information Processing and Management,  35(5), 589-603.       Hoffman, D. , & Downey, L.  (1997, June).   Lessons Learned in an Informal Usability Study.   In proceedings of the 20th International ACM SIGIR Conference on Research & Development in Information Retrieval, Philadelphia, Pennsylvania, USA.                               A  B C  D  E F G  H  I  J  K L M N  O  P Q  R   S   T  U  V  W X Y Z                           Voorhees, E.          Voorhees, E. (July 2002)  Whither Music IR Evaluation Infrastructure: Lessons to be Learned from TREC , to appear in Proceedings from Workshop on the Creation of Standardized Test Collections, Tasks, and Metrics for Music Information Retrieval (MIR) and Music Digital Library (MDL) Evaluation, Portland, OR, USA.   Voorhees, E. (May, 2002)  The Evaluation of Question Answering Systems: Lessons Learned from the TREC QA Track ,  Proceedings of the LREC 2002 Workshop on Question Answering --- Strategy and Resources, Las Palmas de Gran Canaria, Spain, pp. 1--4.    Voorhees, E. (2002)  The Philosophy of Information Retrieval Evaluation , To appear in Proceedings of the 2nd Workshop of the Corss-Language Evaluation Forum, CLEF 2001, Darmstadt, Germany.     Voorhees, E. (Nov. 2001)  Question Answering in TREC  In Proceedings of the Tenth International Conference on Information and Knowledge Management, Atlanta, GA, USA, pp. 535-537     Voorhees, E. (Sept. 2001)  Evaluation by Highly Relevant Documents .   In Proceedings of the 24th Annual International ACM SIGIR Conference   on Research and Development in Information Retrieval, New Orleans, LA, USA,   pp.  74-82.      Voorhees, E. (2001)  The TREC Question Answering Track,  Natural Language   Engineering, volume 7, number 4, pages 361-378.          Buckley, C. &  Voorhees, E.  (2000, July).  Evaluating Evaluation Measure Stability.  In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Athens, Greece, pp. 33-40.       Voorhees, E.  & Tice, D. (2000, July).   Building a Question Answering Test Collection.   In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Athens, Greece, pp. 200-207.       Voorhees, E.  & Tice, D. (2000, May/June).   The TREC-8 Question Answering Track . In proceedings of Language Resources and Evaluation Conference, Athens Greece.       Voorhees, E.  & Tice, D. (2000, May).   Implementing a Question Answering Evaluation . In proceedings of ""Using Evaluation Within HLT Programs: Results and Trends"", a workshop of the Second International Conference on Language Resources and Evaluation, Athens, Greece.       Kantor, P. &  Voorhees, E.  (2000)  The TREC-5 Confusion Track: Comparing Retrieval Methods for Scanned Text .   Information Retrieval ,  2(2/3), 165-176.       Voorhees, E.  & Harman, D. (2000).   Overview of the Sixth Text REtrieval Conference .  Information Processing and Management , 36(1), 3-35.       Voorhees, E.  (1999).   Natural Language Processing and Information Retrieval .  In M. T. Pazienza, (Ed.),  Information Extraction:  Towards Scalable, Adaptable Systems  (pp. 32-48).  Germany: Springer.  (Entry in lecture notes in artificial intelligence 1714)       Towell, G. &  Voorhees, E.  (1998).  Disambiguating Highly Ambiguous Text.   Computational Linguistics,  24(1), 125-145.       Voorhees, E.  (1998).  Using WordNet for Text Retrieval.  In  C. Fellbaum, (Ed.),  WordNet: An Electronic Lexical Database  (pp.285-303).  Cambridge, Massachusetts, USA: The MIT Press.       Voorhees, E.  (1998, August).   Variations in Relevance Judgements in the Measurement of Retrieval Effectiveness.   In proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Melbourne, Australia.        Garofolo, J.,  Voorhees, E. , Stanford, V., & Sparck Jones, K. (1998, February).    TREC-6 1997 Spoken Document Retrieval Task Overview and Results.   In proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, Lansdowne, Virginia, USA.       Voorhees, E. , & Harman, D. (1997, November).   Overview of the Sixth Text REtrieval Conference (TREC-6).   In proceedings of the Sixth Text REtrieval Conference (TREC-6), Gaithersburg, Maryland, USA.       Garofolo, J.,  Voorhees, E. , Stanford, V., & Sparck Jones, K. (1997, November).   TREC-6 1997 Spoken Document Retrieval Task Overview and Results.   In proceedings of the Sixth Text REtrieval Conference (TREC-6), Gaithersburg, Maryland, USA.       Voorhees, E. , & Tong, R. (1997, June).   Multiple Search Engines in Database Merging.   In proceedings of the 2nd ACM International Conference on Digital Libraries, Philadelphia, Pennsylvania, USA.       Voorhees, E. , Garofolo, J., & Sparck Jones, K.  (1997, February).   The TREC-6 Spoken Document Retrieval Track.   In proceedings of the 1997 DARPA Speech Recognition Workshop, Chantilly, Virginia, USA.        Voorhees, E. , & Harman, D. (November, 1996).   Overview of the Fifth Text REtrieval Conference (TREC-5).   In proceedings of the Fifth Text REtrieval Conference (TREC-5), Gaithersburg, Maryland, USA.       Kantor, P., &  Voorhees, E.  (November, 1996).   Report on the TREC-5 Confusion Track.   In proceedings of the Fifth Text REtrieval Conference (TREC-5), Gaithersburg, Maryland, USA.       Voorhees, E.  (November, 1996).   The TREC-5 Database Merging Track.   In proceedings of the Fifth Text REtrieval Conference (TREC-5), Gaithersburg, Maryland, USA.                           A  B C  D  E F G  H  I  J  K L M N  O  P Q  R   S   T  U  V  W X Y Z                    The Retrieval Group is part of the   Information Access Division  (IAD),  Information Technology Laboratory  (ITL), at            NIST is an agency of the  U.S. Commerce Department's   Technology Administration  Retrieval Group Manager:  Donna K. Harman  Last updated: Monday, 30-Jun-03 13:46:26  Date created: Wednesday, 03-May-14   privacy statement/security notice"
GX180-69-8464900	arXiv: physics/0008010  v1   4 Aug 2000     Universal quantification for deterministic chaos in dynamical systems   A.Mary Selvam   Indian Institute of Tropical Meteorology, Pune, India   (Retired) email:  selvam@ip.eth.net   website:  http://www.geocities.com/amselvam   Applied Mathematical Modelling , 1993, Vol.17, 642-649   Abstract   A cell dynamical system model for deterministic chaos enables precise quantification of the round-off error growth,i.e., deterministic chaos in digital computer realizations of mathematical models of continuum dynamical systems. The model predicts the following: (a) The phase space trajectory (strange attractor) when resolved as a function of the computer accuracy has intrinsic logarithmic spiral curvature with the quasiperiodic Penrose tiling pattern for the internal structure. (b) The universal constant for deterministic chaos is identified as the steady-state fractional round-off error  k  for each computational step and is equal to  1 /  t    2   ( =0.382) where  t     is the golden mean.  k  being less than half accounts for the fractal(broken) Euclidean geometry of the strange attactor. (c) The Feigenbaum's universal constants  a  and  d  are functions of  k  and, further, the expression  2a 2  =  p   d  quantifies the steady-state ordered emergence of the fractal geometry of the strange attractor. (d) The power spectra of chaotic dynamical systems follow the universal and unique inverse power law form of the statistical normal distribution. The model prediction of (d) is verified for the Lorenz attractor and for the computable chaotic orbits of Bernoulli shifts, pseudorandom number generators, and cat maps.    Keywords:  deterministic chaos, strange attractor, Penrose tiling pattern, cell dynamical system, universal algorithm for chaos      1.    Introduction   Nonlinear mathematical models of dynamical systems are sensitively dependent on initial conditions, identified as deterministic chaos. Such deterministic chaos was first identified nearly a century ago by Poincare 1  in his study of the three-body problem. The advent of digital computers in the 1950s facilitated numerical solutions of model dynamical systems, and in 1963 Lorenz 2  identified sensitive dependence on initial conditions in a simple mathematical model of atmospheric flows. Deterministic chaos occurs in both continuum(differential equations) and discrete (maps) systems. Deterministic chaos is now an area of intensive research in all branches of science and other areas of human interest 3  . Ruelle and Takens 4  identified deterministic chaos as similar to turbulence in fluid flows; turbulence is as yet an unresolved problem. It is well-known that deterministic chaos is a direct result of the following inherent limitations of numerical solutions: (a) The differential equations of traditional Newtonian continuum dynamics are solved as difference equations introducing space-time discretizations with implicit assumption of subgrid scale homogeneity for the dynamical processes. (b) Approximations in the governing equations related to limitations of computer capacity. (c) Exact number representation is not possible at the data input stage itself because of the binary form for number representation in computer arithmetic 5  . (d) Computer-accuracy-related round-off errors magnify exponentially with time the above-mentioned uncertainties and give solutions that are not totally realistic 6  . The trajectory of the dynamical system in the phase space traces a strange attractor, so named because its strange convoluted shape is the final destination of all possible trajectories. Trajectories starting from two initially close points diverge exponentially with time though still within the strange attractor domain. The strange attractor has self-similar geometry. The word  fractal  first coined by Mandelbrot 7  means broken or fractured structure. The fractal dimension  D  of such non-Euclidean structure is given by the relation  D  =  d  ln  M / d  ln  R  where  M  is the mass contained within a distance  R  from a point within the fractal object. A constant value for  D  indicates uniform distribution of mass with distance on a logarithmic scale for the length scale  R . Objects in nature in general possess a multifractal dimension 8  . Self-similarity implies identical internal structure at all scales of magnification. The temporal fluctuations of dynamical systems have been investigated extensively and are found to exhibit a broad-band power spectrum 9  . The physics of deterministic chaos is not as yet identified. In this paper, a cell dynamical system model for the growth of strange attractor pattern of deterministic chaos in dynamical systems is described by analogy with the formation of large eddy structures as envelopes enclosing turbulent eddies in fluid flows 10,11  .   2.    Computer accuracy and round-off error   Round-off error is inherent to finite precision numerical computations and imposes a limit  dR  to the resolution with which two quantities  R  +  dR  can be distinguished as separate, thereby introducing an uncertainty in computation equal to  dR  in magnitude. Computer precision  dR  is therefore analogous to yardstick length used in the measurement of distance of separation between two points. Two points cannot be distinguished as separate if they are closer together than the yardstick length  dR  used for the measurement. The uncertainty in measurement of separation distance of two points is equal to the yardstick length  dR . Such round-off error is a direct consequence and the inevitable result of the necessity for discretization of space and time in traditional numerical computations and real-world measurements. In computer simulations of model dynamical systems, the negligibly small model input uncertainties are magnified by the round-off error and propagate into the mainstream computation with successive iterations because of the computational feedback logic inherent in such models, e.g.,  X n+1  =  F ( X n ) where the value   X n+1  of the variable at the ( n  +  1 )th interval is a function  F  of   X n  . The uncertainty or its analogue, the yardstick length,  dR , increases exponentially with each stage of computation and results in deterministic chaos as explained earlier. In the following discussions, computer precision is treated in terms of the equivalent yardstick length for length measurement. The computational domain is defined as equal to the product  WR  of the number of units of computation  W  of yardstick length  R .   3.    Cell dynamical system model for deterministic chaos   The round-off error structure growth, namely, the strange attractor pattern in computed model dynamical systems , is visualized as corresponding to the coherent structures such as large eddies (or helical vortex roll circulations) that form as the envelopes of enclosed turbulent eddies in planetary atmospheric boundary layer flows. Though turbulent eddy fluctuations are considered to be chaotic(random) and dissipative, they are an integral part of all organized coherent weather patterns such as cloud rows/streets and the hurricane spiral cloud pattern. Townsend 12  has shown that large eddy circulations form as a chance configuration of turbulent eddy fluctuations in turbulent shear flows. Just as the small-scale fluctuations contribute to the organized growth of large-eddy fluctuation domains, so also the microscale round-off error structure domains contribute to form the total uncertainty domain in the phase space of the model dynamical system.       Computational error is initiated with input data at the first step of numerical computation, i.e., one unit of computation generates one unit of uncertainty equal to the yardstick length  dR  in all directions as illustrated in  Figure 1  by the circle  OR 2 R 1 'R 2   of radius  dR .                                  Figure 1.    The growth of round-off error structures in the phase space. The domain of the round-off error  dR  is represented by the circle  OR 2 R 1 'R 2  on the left. The macroscale uncertainty domain of length scale  R  is the sum of successive stages of such microscale round-off error domains resulting from finite computer precision and shown by the close packing of circles of radii  dR  on the right.       The uncertainty domain represented by the circle  OR 2 R 1 'R 2  corresponding to the measurement  OR 1  is interpreted as follows. One unit of measurement of yardstick length  OR 1  ( =  dR  ) implies two approximations: (a) a minimum measurable distance  OR 1  and (b) round-off of all lengths less than  OR 1 ' ( = 2dR  ) as equal to  OR 1  ( =  dR  ). The domain of these two errors in the phase space is represented by a circle with center  R 1  and radius  OR 1   =  dR  because the projection of  OR 2  on  OR 1  for angles  OR 1 R 2  less than or greater than 90 degrees respectively will be measured as equal to  OR 1  . The circle  OR 2 R 1 'R 2  therefore represents the total uncertainty domain for one unit of measurement of yardstick length  OR 1   =  dR  . The precision decreases or the yardstick length  R  increases for with successive stages of computation. The increased imprecision represented by increased yardstick length  R  is composed of the microscale round-off error domain  OR 2 R 1 'R 2  as shown in  Figure 1 . Such microscopic error domain structures may be compared to turbulent eddy circulations, which contribute to form large eddy circulation patterns in fluid flows.  w *  units of computation of yardstick length  dR  is equivalent to  W  units of computation of a more imprecise larger yardstick length  R  and is quantified by analogy with the formation of large eddy circulation structures as the spatial average of the turbulent eddy fluctuation domain 10-12  . The mean square round-off error circulation  C 2  at any instant around a circular path of length scale  R  is equal to the spatial integration of the microscopic domain error structures (  OR 2 R 1 'R 2  ) over the computational domain of length scale  R  and is given as        The mean square value of  W  is then obtained as        (1)   The above equation enables one to compute, for any interval, the number of units  dW  of computation of decreased precision  R  resulting from the spatial integration of  w *  units of inherent microscale round-off error structures (  Figure 1  ) of yardstick length  dR . The computational error structure (strange attractor) growth from microscopic round-off error domains may be visualized as follows. The strange attractor domain is defined by the overall envelope of the microscopic scale round-off error domains, and incremental growth of strange attractor occurs in discrete length steps equal to the yardstick length  dR . Such a concept of strange attractor growth from microscopic round-off error domains envisions strange attractor growth in discrete length step increments  dR  and is therefore analogous to  cellular automata  computational technique where cell dynamical system growth occurs in discrete length step intervals 13 .       Equation (1) is directly applicable to digital computations of nonlinear mathematical models where  W  units of imprecise computation of yardstick length  R  are expressed in terms of  w *  units of a more precise (higher resolution) yardstick of length  dR .       Each stage of numerical computation goes to form the higher precision earlier step for the next computational step. The magnitude of the number of units  w *  of higher precision earlier stage computation that forms the internal structure of the total computed domain is obtained from equation (1) as        (2)   Equation (2) is used to derive the progrssively increasing magnitude  w *  units of higher precision computation for successive steps of computation as follows. Denoting  W n  and  W n+1  as the number of units of computation for the  n th and  (n+1 )th intervals of computation equation (2) can be written as       or      (3)   where  r n  is the uncertainty of yardstick length equal to ( dR ) n  . The magnitude of the higher precision yardstick length  r n  increases with the computation. The incremental growth ( dR ) n  in the yardstick length  R n  is generated by  W n  units of computation at the  n th step and therefore  W n  = ( dR ) n  , i.e., one unit of computation generates one unit of uncertainty. The round-off error growth for successive stages of iteration is shown in  Figure 2 .           Figure 2.    Visualization of round-off error growth in successive iterations        The uncertainty  r 1  in the computation is equal to the number of units of computation  W 1 , i.e.,  r 1  = 1 and is represented by  A 1 A 2  in  Figure 2 . The computation length  OA 1  can be any radius of the sphere or circle in three or two dimensions respectively, with center  O  and radius  OA 1  . The computational domain  W 1 R 1  is any rectangular cross-section  OA 1 B 1 O '  of the cylinder with radius of base equal to  OA 1  and height  A 1 B 1  ( Figure 2 ). At the end of the first step of computation  W 1  = 1,  R 1  = 1 and  r 1  = 1. Therefore  W 2  = 1.254 from equation (3). The first step of computation generates the length domain  R 2  =  R 1  +  r 1  = 2 ( OA 2  =  R 2 ) associated with  W 2  = 1.254 units of computation ( A 2 B 2  =  W 2 ) and corresponding uncertainty,  r 2  =  W 2  = 1.254 ( A 2 A 3  = r 2 ). Substitution in equation (3) gives  W 3  = 1.985. Similarly the values of  W n   and  R n  for the  n  successive iteration steps are computed from equation (3). The yardstick length  R n  is equal to the cumulative sum of the yardstick lengths for the previous  n  intervals of computation, i.e.,  . The values of  R n ,  W n ,  dR ,  W n+1 , and  d  q     computed as equal to  R n  / R n+1  and   are tabulated in  Table 1 .   Table 1.     The computed spatial growth of the strange attractor design traced by dynamical systems as shown in  Figure 1 .            R        W n        dR        d q        W n+1        q            1   2   3.254   5.239   8.425   13.546   21.780   35.019   56.305   90.530        1   1.254   1.985   3.186   5.121   8.234   13.239   21.286   34.225   55.029        1   1.254   1.985   3.186   5.121   8.234   13.239   21.286   34.225   55.029        1   0.627   0.610   0.608   0.608   0.608   0.608   0.608   0.608   0.608        1.254   1.985   3.186   5.121   8.234   13.239   21.286   34.225   55.029   88.479        1   1.627   2.237   2.845   3.453   4.061   4.669   5.277   5.885   6.493          It is seen that the yardstick length  R  and the corresponding number of units of computation  W  follow the Fibonacci mathematical number series. The progressive increase in imprecision represented by the increasing magnitude for the yardstick length can be plotted in polar coordinates as shown in  Figure 3  where  OR 0  is the initial yardstick length.                                            Figure 3.     The quasiperiodic Penrose tiling pattern of the round-off error structure growth in the strange attractor. The phase space trajectory is represented by the product  WR  of the number of units of computation  W  of yardstick length  R .  R  represents the round-off error in the computation. The successive values of  W  and  R  follow the Fibonacci mathematical number series, and the strange attractor pattern represented in this manner consists of the quasiperiodic Penrose tiling pattern. The overall envelope  R 0 R 1 R 2 R 3 R 4 R 5  of the strange attractor follows the logarithmic spiral  R  =  re  b  q  shown on the right where  r  =  OR 0  and  b  = tan  a   where  a   is the crossing angle.       The successively larger values of the yardstick lengths are then plotted as the radii  OR 1 ,  OR 2  ,OR 3  , OR 4  , and  OR 5  on either side of  OR 0  such that the angle between successive radii are  p  / 5 so that the ratio of the successive yardstick lengths equals the golden mean  t  . The radii can be further subdivided into the golden mean ratio so that the internal structure of the polar diagram displays the quasiperiodic Penrose tiling pattern 14 . The larger yardstick length is therefore shown to consist of microscale round-off error domains  OR 0 R 1  where  OR 0  =  R 0 R 1  =  dR .  dR  is the imprecision inherent to the computational system consisting of the model uncertainties and the round-off error of the digital computer.       The computed result  WR  is represented by a rectangle of sides  W  and  R , and therefore the phase space trajectory can also be resolved into the quasiperiodic Penrose tiling pattern. The spatial domain of the yardstick length  OR 0  is the solid of revolution generated by the rotation of the triangle  OR 1 R 0  about the axis  OR 0  . It is seen from  Table 1  and  Figure 3  that starting from either side of the initial computational step  OR 0  the computation  W  proceeds in logarithmic spiral curves  R 0 R 1 R 2 R 3 R 4 R 5  such that one complete cycle is executed by the numerical computation after five length steps of computation on either side of  OR 0  , i.e., clockwise and counterclockwise rotation. Denoting the yardstick length scale ratio  R/dR  by  Z , dominant periodicities or cycles occur in the   W  units of computation for  Z  values in multiples of  t 5n  where  n  ranges from positive to negative integer values. The internal structure of the phase space trajectory , i.e., the strange attractor, therefore consists of the quasiperiodic Penrose tiling pattern. The overall envelope of the computation  W  follows the logarithmic spiral pattern. The incremental units of computation  dW  of yardstick length  R  at any stage of computation is non-Euclidean because of internal structure generated by succesive stages of round-off error growth as shown in the triangle  OR 0 R 1  ( Figure 3 ). The incremental units of computation  dW  of yardstick length  R  at any stage of computation have intrinsic internal structure consisting of discrete spatial domains of total size  w * dR  generated by  w *  units of discrete yardstick length  dR  , which represents the uncertainty in initial conditions, i.e., the error generated by assuming that the minimum separation distance between two arbitrarily close points is equal to  dR  . At each stage of computation, the computed spatial domain  RdW  contains smaller domains of total size  w * dR  representing the uncertainty in input conditions, i.e., the error domains relating to the finite size for yardstick length. The steady-state fractional round-off error  k  in the computed model at each stage of computation is therefore given by        (4)    k  also represents the steady-state measure of the departure from Euclidean shape of the computed model, namely, the strange attractor. The successive computational steps generate angular turning  d q    of the  W  units of computation where  d q   = dR/R , which is a constant equal to  t   , the golden mean ( Figure 3  ). Further, the successive values of the  W  units of computation of yardstick length  R  follow Fibonacci mathematical number series.  k  represents the steady-state fractional error due to uncertainty in initial conditions coupled with finite precision in the computed model.  k  also gives quantitatively the fractional departure from Euclidean geometrical shape of the computed strange attractor .  k  is derived from equation (4) as     k  =  1/ t 2  = 0.382    (5)    A steady-state fractional round-off error of 0.382 and the associated quasiperiodic Penrose tiling pattern for the strange attractor are intrinsic to digital computations of nonlinear mathematical models of dynamical systems even in the absence of uncertainty in input conditions for the model. Because the steady-state fractional departure from Euclidean shape of the strange attractor design traced in the phase space by  W  units of computation is equal to 0.382, i.e., less than half, the overall Euclidean geometrical shape of the strange attractor is retained. Beck and Roepstroff 15  also find the universal constant 0.382 for the scaling relation between length of periodic orbits and computer precision in numerical computations.  k  , which is a function of the golden mean  t  , is hereby identified as the universal constant for deterministic chaos in computer realizations of mathematical models of dynamical systems.  k  is independent of the magnitude of the precision of the digital computer and, also, the spatial and temporal length steps used in model computations. In Section 4 it is shown that the Feigenbaum's universal constants 16  are functions of  k  . Dominant coherent structures in numerical computation  W  evolve for yardstick length scale ratio  Z  equal to  t 5n  ( n  ranging from negative to positive integer values) as mentioned earlier and are characterized by round-off error-generated quasiperiodic Penrose tiling pattern for the internal structure. Numerical experiments have identified the golden mean  t  to be associated with deterministic chaos in dynamical systems 17,18 . Also, recent numerical investigations indicate that the strange attractor can be defined completely as quasiperiodicities with fine structure 19 , i.e., a continuum.       Traditional computational techniques are digital in concept, i.e., they require a unit or yardstick for the computation and thereby lead inevitably to approximations, i.e., round-off errors. Because the computed quantity structure can be infinitesimally small in the limit, there exists no practical lower limit for the yardstick length. Therefore, numerical computations in the long run give results that scale with computer precision and also give quasiperiodic structures. Numerical experiments show,that, due to round-off errors, digital computer simulations of orbits of chaotic atractors will always eventually become periodic 5 . The expected period in the case of fractal chaotic attractors scales with round-off 20 . The universal quantification of the round-off error structure growth described in this paper is independent of the magnitude of the roud-off error, the time and space increments, and the details of the nonlinear differential equations and, therefore, is universally applicable for all computed model dynamical systems.       The incremental growth  dW  units of numerical computation of yardstick length  R  can be expressed in terms of  w *  units of more precise yardstick length  dR  as follows from equation(4):        (6)    Equation (6) can be integrated to obtain the  W  units of total computation starting with  w *  units of  yardstick length  r  ,where, as mentioned earlier,  dR  represents the uncertainty in initial conditions of the computational system at the beginning of the computation.        (7)    The  W  units of  computation and therefore  R  follow a logarithmic spiral with  Z  being the yardstick length scale ratio, i.e.,  Z  =  R/dR  . The logarithmic spiral  R 0 R 1 R 2 R 3 R 4 R 5  ( Figure 3 ) is given quantitatively in terms of the yardstick length  R  as        (8)    where  b  =  tan  a    with  a   ,  the crossing angle equal to  dR/R  .  a    is therefore equal to 1/ t   as shown earlier and, because  b  is equal to   a    in the limit for small increments  dW  in computation,        (9)    The yardstick length  R , which represents uncertainty in initial conditions, therefore grows exponentially with progress in computation. The separation distance  r  of two arbitrarily close points at the beginning of the computation grows to  R  at the end of the computation with the angular turning of the trajectories being equal to  p   /  5  . The exponential divergence of two arbitrarily close points is given quantitatively by the exponent 1/ t   equal to 0.618 and is identified as the Lyapunov exponent conventionally used to measure such divergence in computed dynamical systems 17 . For each length  of computation with unit angular turning (equal to  p   /  5  ) the initial yardstick length  r  increases to 1.855 r  (from equation (9)) at the end of the computation, i.e., the yardstick length (or round-off error) approximately doubles for each iteration when the phase space trajectory is expressed as the product  WR  where  W  units of computation of yardstick length  R  follow the Fibonacci mathematical number series as a natural consequence of the cumulative addition of round-off error domains. Hammel  et al. 21  mention that it is not unusual that the distance between two close points roughly doubles on every iterate of numerical computation. The Lyapunov exponent equal to  1/ t  (=  0.618) is intrinsic to numerically computed systems even in the absence of uncertainty in initial conditions for the numerical model. When uncertainty in input conditions exists for the model dynamical system, the initial yardstick length  r  effectively becomes larger and, therefore, larger divergence of initially close trjectories occurs for a shorter length step of computation as seen from equation (9). The generation of strange attractor in computer realizations of nonlinear mathematical models is a direct consequence of computer-precision-related round-off errors. The geometrical structure of the strange attractor is quantified by the recursion relation of equation (2). Equation (2) is hereby identified as the universal algorithm for the generation of the strange attractor pattern with underlying universality quantified by the Feigenbaum's universal constants 16 a  and  d  in computer realizations of nonlinear mathematical models of dynamical systems. In the following section it is shown quantitatively that equation (2) gives directly the universal characteristics such as the Feigenbaum's constants identifying deterministic chaos of diverse nonlinear mathematical models.   4.    Universal algorithm for deterministic chaos incorporating Feigenbaum's universal constants   The basic example with the potential to display the main features of the erratic behavior characterizing deterministic chaos is the Julia model given below     X n+1  =  F ( X n ) =  LX n (1 -  X n )    (10)    The above nonlinear model represents the population values of the parameter  X  at different time periods  n  , and  L  parameterizes the rate of growth of  X  for small  X  . Feigenbaum's research 16   showed that the two universal constants  a  and  d  are independent of the details of the nonlinear equation for the period doubling sequences        (11)    a = -2.5029      (12)    d = 4.6692    In the above equation   denotes the  X  spacing between adjoining period doublings ( n +2) and ( n +1), i.e.,   and similarly   .   is the  L  spacing between period doublings ( n +2) and ( n +1) . The universal recursion relation quantifying deterministic chaos in nonlinear mathematical models, namely, equation (2) is analogous to the Julia model, because the macroscale computation structure  W  is determined by the microscopic yardstick length  dR  . The Feigenbaum's constants  a  and  d  for the universal period doubling route to chaos may be derived directly from the universal recursion relation (equation (2)) as shown in the following. The universal  relation (equation (2)) is used for computing quantitatively the successive length step increments in the magnitude of the number of units of computation  w *  of yardstick length  dR  incorporated in the computation.       Feigenbaum's constant  a  is given by the successive spacing ratios  W  for adjoining period doublings.  W  and  R  are respective successive spacing ratios , because by  definition  W  and  R  are computed as incremental growth steps  dW  and  dR  for each stage of computation.       Feigenbaum's constant  a   is obtained as the successive spacing ratios of  W  , i.e.,        (13)    The total computational domain  WR  at any stage of computation may be considered to result from spatial integration of round-off error domain  W 1 R 1  or  W 2 R 2  where  R 1  and  R 2  refer to the precision. From equation (2)  W 1 2 R 1  = W 2 2 R 2  = constant. Therefore        From equations (4) and (5)     a = 1/k =  t    2    The Feigenbaum's constant  a  therefore denotes the relative increase in the computed domain with respect to the yardstick length (round-off error) domain and is equal to       t    2   (=2.618) and is inherently negative because the round-off error has a negative sign by convention.       Further,   a  2  = variance of a =  t    4    2a  2  = total variance of the fractional geometrical evolution of computed domain for both clockwise and counterclockwise phase space trajectories.       The Feigenbaum's constant  d  is the successive spacing ratios of  R  for the universal recursion relation(equation (2)) for the numerical computation.   Because  W 1 2 R 1  = W 2 2 R 2  as explained above        The Feigenbaum's constant  d  is therefore obtained as        (14)    W  4  represents the fourth moment about the reference level for the instantaneous trajectory in the representative volume  R  3  of the phase space.  d  is, therefore, equal to the relative volume intermittency of occurrence of Euclidean structure in the phase space during each computational step, i.e.,  p  / 5  radian angular rotation as shown earlier in  Table 1  for the quasiperiodic Penrose tiling pattern traced by the strange attractor. For one complete cycle(period) of computation, five length steps of simultaneous clockwise and counterclockwise, i.e., counter-rotating, computations are performed. Therefore, for one complete cycle of computation the relative volume intermittency of occurrence of Euclidean structure in the computed phase space domain is  p  d  . The universal recursion relation for deterministic chaos(2) can be written as        (15)    The reformulated universal algorithm for numerical computation at equation (15) can now be written in terms of the universal Feigenbaum's constants (equations (13) and (14)) as     2a 2  =  p  d    (16)    The above equation states that the relative volume intermittency of occurrence of Euclidean structure for one dominant cycle of computation contributes to the total variance of the fractional Euclidean structure of the strange atractor in the phase space of the computed domain. Numerical computations by Delbourgo 22  give the relation  2a 2  = 3d , which is almost identical to the model-predicted equation (16).       Feigenbaum's universal constants  a = 2.503  and  d = 4.6692  (equations (11) and (12)) have been determined by numerical computations at period doublings  n ,  n+1  and  n+2  where  n  is large. At large  n , computational difficulties in resolution of adjacent period doublings impose a limit to the accuracy with which  a  and  d  can be estimated.       The Feigenbaum's constants  a  and  d  computed from the universal algorithm  2a 2  =  p  d  refer to an infinitesimally small value for the computer round-off-error(yardstick length), i.e., an infinitely large number of period doublings. The model-predicted and computed  a  and  d  are therefore not identical.   5.    Universal quantification for the power spectra of chaotic dynamical systems   The temporal fluctuations of chatic dynamical systems are found to exhibit broad-band power spectra 9 . A complete description of the temporal variability is therefore possible in terms of the component periodicities and their phases 19 . Such a description in terms of cycles or periodicities of nonlinear fluctuations of real world dynamical systems has been reported 23 . In the following, it is shown that the power spectra of the nonlinear fluctuations of chaotic dynamical systems can be quantified in terms of the universal characteristics of the statistical normal distribution. Because the successive number of units of computation  W  is obtained as the R.M.S. value of inherent round-off error domains as given in equation (1), the mean square variance  W  2  for the continuum of computed  W  will follow normal distribution characteristics. The model predictions are in agreement with continuous periodogram analysis 24  of the Lorenz attractor 25  and the computable chaotic orbits of (a) Bernoulli shifts, (b)cat maps, and (c) pseudorandom number generators 5  . The details of the mathematical equations of the computable chaotic dynamical systems 5  used is given in the following:   1.    Bernoulli shifts       x  ----> 3 x  mod 1,( x 0 ,  x 1 ,......,  x n ,...)   with  x 0  = 0.1   2.    Cat map     F ( x ,  y ) = ( x  +  y  mod 1,  x  + 2 y  mod 1)     for all 0<=  x ,  y  < 1       with initial points(0.1,0.0)   3.    Pseudorandom number generator: minimal standard Lehmer generator       X n+1  = 16807  X n  mod 2147483647;  X 0  = 0.1                   Figure 4.     The results of continuous periodogram analysis of the Lorenz attractor and the computable chaotic orbits of the Bernoulli shifts, pseudorandom number generators and cat maps. The data series are samples of different sections of each chaotic orbit, and averages of up to 50 successive values were used for the study. Details of the data series corresponding to the spectra numbered 1-10 in the figure are listed in the following: (1) Lorenz attractor, 100 successive 25-point averages of  Y  from fifth point. (2) Lorenz attractor, 100 successive 25-point averages of  X   from fifth point.  (3) Lorenz attractor, 100 successive 25-point averages of  Y  from 4001th point. (4) Bernouille shifts, 100 successive three-point averages starting from 301th value. (5)  Bernouille shifts, 100 successive two-point averages starting from 501th value. (6) Bernouille shifts, 100 successive three-point averages starting from 1001th value. (7) Pseudorandom numbers, 100 values from 301th value. (8) Pseudorandom numbers, 100 values from 101th value. (9) Cat map, 100 values of  X  from 101th value. (10) Cat map, 50 values of  Y   from 101th value.                 The power spectra of the above chaotic dynamical systems( Figure 4 ) are found to be the same as the normal probability density distribution with the normalized variance representing the eddy probability density corresponding to the normalized standard deviation  t  equal to [(log  P /log  P 50 ) - 1] where  P  is the period and  P 50  , the period up to which the cumulative percentage contribution to total variance is equal to 50. The above relation for the normalized standard deviation  t  in terms of the periodicities follows directly from equation (7) because by definition  W  and  W  2  represent respectively the standard deviation and variance as a direct consequence of  W  being computed as the instantaneous average round-off error domains for each stage of computation. Therefore, for a constant value of  w *  , the number of units of computation of precision  dR  , the ratio of the R.M.S. units of computation  W 1  and  W 2  of respective yardstick lengths  R 1  and   R 2  will give the ratios of the standard deviations of the unit  W  of computation. From equation (7)        (17)    Starting with reference level standard deviation  s    equal to  W 1 , the successive steps of computation have standard deviations  W 2  equal to  s   , 2 s   , 3 s   , .....from equation (17) where  Z 2  = Z 1 n  and  n  = 1, 2, 3, ....for successive period doubling growth sequences.       The important result of the present study is the quantification of the round-off error structure, namely, the strange attractor in model dynamical systems in terms of the universal and unique characteristics of the statistical normal distribution. The power spectra of the Lorenz attractor and the computable chaotic orbits of the Bernouille shifts, pseudorandom number generators, and cat map exhibit ( Figure 4 ) the universal inverse power law form of the statistical normal distribution. The inverse power law form for the power spectra of the temporal fluctuations is ubiquitous to real-world dynamical systems and is recently identified as the temporal signature of self-organized criticality 26  and indicates long-range temporal correlations or non-local connections. Sensitive dependence on initial conditions, i.e., deterministic chaos, is therefore a manifestation of self-organized criticality in model dynamical systems and is a natural consequence of the spatial integration of microscopic domain round-off error structures as postulated by the cell dynamical system model described in Section 3. The universal quantification for deterministic chaos, or self-organized criticality in terms of the unique inverse power law form of the statistical normal distribution identifies the universality underlying numerical computations of chaotic dynamical systems. The total pattern of fluctuations of chaotic dynamical systems is predictable, because self-organization of the nonlinear fluctuations of all scales contributes to form the unique pattern of the normal distribution ( Figure 4 ).   6.    Conclusion   In summary, the cell dynamical system model for round-off error growth in computer realizations of nonlinear dynamical systems visualizes the computer precision (round-off error) as analogous to yardstick length in length measurement. The computed domain consists of the cumulative integrated mean of enclosed round-off error domains. The computed domain, namely, the phase space trajecory, is the product  WR  of the number of units of computation  W  of precision (yardstick length)  R  . The phase space trajectory thus defined traces an overall logarithmic spiral pattern with the golden mean winding number and quasiperiodic Penrose tiling pattern for the internal structure, implying long-range temporal correlations, namely, sensitive dependence on initial conditions. The universality underlying deterministic chaos is quantified in terms of the following universal constants, which are functions of the golden mean  t   : (1) The constant  k   for deterministic chaos equal to  1 /  t    2  represents the steady-state fractional round-off error for each computational step.  k  also represents the fractional departure from Euclidean geometry of the strange attractor. (2) The Lyapunov exponent is equal to  1 /  t  . (3) The Feigenbaum's constants  a  and  d  define the algorithm for deterministic chaos as  2 a  2  =  p  d  , which states that the relative volume intermittency of occurrence equal to  p  d   of fractional Euclidean structure contributes to the total variance equal to  2 a  2  of the strange attractor.  a  is equal to  t    2    and represents the fractional Euclidean structure of the strange attractor. (4) The power spectra of computed chaotic dynamical systems follow the universal inverse power law form of the statistical normal distribution. Continuous periodogram power spectral analysis of Lorenz attractor, Bernouille shifts, pseudorandom number generators, and cat maps are in agreement with model predictions.   Acknowledgements   The author is grateful to Dr.A.S.R.Murty for his keen interest and encouragement during the course of this study. Thanks are due to Shri R.Vijayakumar for assistance with computer graphics and to Shri M.I.R.Tinmaker for typing the manuscript.   References   1    Poincare, H.  Les meyhodes nouvelle de la mecanique celeste . Gautheir-Villars,     Paris, 1892   2    Lorenz, E. N.  J .  Atmos.Sci . (1963),  20 , 130   3    Gleick, J.  Chaos: Making a New Science.   Viking, New York, 1987   4    Ruelle, D. and Takens, F.  Commun. Math. Phys . 1971,  20 , 167   5    Palmore, J. and  Herring, C.  Physica D  1990,  42 , 99   6    Stewart, I.  Nature  1992,  355 , 16   7    Mandelbrot, B. B. PAGEOPH 1989,  131  (1/2), 5   8    Stanley, H. E. and Meakin, P.  Nature  1988,  335 , 405   9    Procaccia, I.  Nature  1988,  333 , 618   10  Selvam, A. Mary,  Can. J. Phys . 1990,  68 , 831   11  Selvam, A. Mary, Pethkar, J. S. and Kulkarni, M. K.  Int. J. Climatol . 1992,  12 , 137   12  Townsend, A. A.  The Structure of Turbulent Shear Flow . Cambridge University Press, Cambridge, 1956   13   Oona, Y. and Puri, S.  Phys. Rev. A . 1988,  38 , 434   14    Janssen, T.  Phys. Rep.  1988,  168 , 1   15    Beck, C. and Roepstorff, G.  Physica D  1987,  25 , 173   16    Feigenbaum, M. J.  Los Alamos Sci.  1980,  1 , 4   17    McCauley, J. L.  Physica Scripta  1988,  T20 , 1   18    Stewart, I.  New Scientist  1992,  135 , 14   19    Cvitanovic, P.  Phys. Rev. Lett.  1988,  61 , 2729   20    Grebogi, C., Ott, E. and Yorke, J. A.  Phys. Rev. A  1988,  38 , 3688   21    Hammel, S. M., Yorke, J. A. and Grebogi, C.  Amer. Math. Soc.  1988,  19 (2), 465   22    Delbourgo, R.  Asia-Pacific Physics News  1986,  1 , 7   23    Olsen, L. F. and Schaffer, W. M.  Science  1990,  249 , 499   24    Jenkinson, A. F. A powerful elementary method of spectral analysis for use with monthly, seasonal or annual meteorological time series. U. K. Meteorol. Office, Met. O 13 Branch Memorandum No. 57, 1977, p.23   25    Blackadar, A.  Weatherwise  1990  43 (4),210   26    Bak, P., Tang, C. and Wiesenfeld, K.  Phys. Rev.A  1988,  38 , 364
GX180-21-0195876	"Yang CM, July, 2003  The bi-pyramidal nature, the Lucas series in the genetic code and their relation to aminoacyl-tRNA synthetases Chi Ming Yang, Ph.D. Physical Organic Chemistry and Chemical Biology, Nankai University, Tian Jin 30007, China. E-mail: yangchm@nankai.edu.cn Fax: 011 86 22 2350 3863  (July, 2003)  Abstract It has been unclear what principle governs the selection of the 20 canonical amino acids in the genetic code. Based on a previous study of the 28-gonal and rotational symmetric arrangement of the 20 amino acids in the genetic code, new analyses of the organization of the genetic code system together with their relation to the two classes of aminoacyl-tRNA synthetases are reported in this work. A closer inspection revealed how the enzymes and the 20 amino acids of the genetic code are intertwined on a polyhedron model. Complimentarily and cooperative symmetry between class I and class II aminoacyl-tRNA synthetases displayed by a 28-gon model are discussed, and we found that the two previously suggested evolutionary axes of the genetic code overlap the three two-fold symmetry axes within the two classes of aminoacyl-tRNA synthetases. Moreover, it is identified that the amino-acid side-chain carbon-atom numbers (1, 3, 4 and 7) in the overwhelming majority of the amino acids recognized by each of the two classes of aminoacyl-tRNA synthetases meet a mathematical relationship, the Lucas series. Key words: Aminoacyl-tRNA synthetase; Atomic number balance; Evolutionary axis; Lucas series; Icosikaioctagon; Polyhedron symmetry; Amino-acid side-chain carbon-atom; 28-gon  1.  Introduction Since Weber and Miller (1981) described reasons for the occurrence of the twenty coded protein  amino acids, the fundamental properties of the 20 canonical amino acids have since inspired tremendous amount of excellent research and speculation (Davydov, 1998; Dufton, 1997; Rakocevi and Jokic, 1996; Rakocevi, 1998; Shcherbak, 1989). The genetic code was more recently shown to be determined by Golden mean through the unity of the binary-code tree and the Farey tree in Rakocevi's work (1998), and it was identified that atom number balance in amino acids are directed by Golden mean route, also directed by the double-triple system of amino acids, as well as by two classes of aminoacyl-t-RNA synthetases (AARS's). As is well known, the first two bases in the tri-nucleotide codons are recognized to be the most specific fragments of the codons. Therefore, they are instrumental for analysis aimed at elucidation of the coding principle underlying the genetic code system. The 64 codons can be divided into 16 groups of genetic code doublets, such as UUN, UCN, UGN, UAN...., N = U, C, G and A. We recently  1   Yang CM, July, 2003  explored a structural regularity within the four RNA nucleobases U, C, G and A by invoking the nature of covalent bonding, the sp2-hybrid nitrogen-atom numbers in the nucleobase molecules, to investigate the symmetry inherent in the genetic code (Yang, 2003a). By advancing and modifying some early graphic and geometric approaches to understanding the genetic code (Karasev and Stefanov, 2001; Yang, 2003b), solid geometric analysis of the 16 groups of genetic code doublets revealed that the rotational symmetries inherent in the distribution of both the number of the amino acids and their side-chain carbon-atom contents in the genetic code follow a quasi-28-gon (icosikaioctago) model with two presumed symmetry axes (Yang, 2003b; 2003c). The newly identified polyhedral symmetric nature as a 28-gonal pyramid, or a double pyramidal nature (Yang, 2003b), of the genetic code, echoes the similar, but different structure of the simplest viruses, i.e., spherical viruses, which is of icosahedral symmetry (Casper and Klug, 1962; Racaniello, 1996). The overall symmetric feature of the code described by our spherical and polyhedral model together with two presumed evolutionary axes is in an excellent agreement with the recent Trifonov's proposal that Ala could be the first codonic amino acid (Trifonov and Bettecken, 1997; Trifonov, 2000). From the polyhedron symmetric model, we can also identify that the symmetric distribution of the 20 amino acids around the 28-gon excellently fits the Rumer 's regularity (Shcherbak, 1989). Both the amino acids in the Group IV (degeneracy 4) and the rest of the amino acids in the Quasi-group III-II-I are each occupying half of the surface on the 28-gonal model (Yang, 2003), see Figure 1. Qu asi -group /o Y /Q C/W/ o N/K H L R P Gro up IV V G A (b) S/R E/D M/ I  17 car bon ato ms ( on the amin o-aci d si de-ch ains) 7 17 9 1 4/7 4 4 3 0 3 1 (c) /3 4 1/9/ 0 2/4 1/ 4 3/2 2 17 4/ 3  4 5 6 5 6 5 6 4 (a) 6 5 5 6 5 6 F/L  5  5  S  T  9  Figure 1. The icosikaioctagonal or 28-gonal relation within the 20 amino acids in the genetic code. (a) An icosikaioctagon or a 28-gon. The numbers at vertices indicate the edges at the vertices. (b) The distribution of the 20 amino acids is symmetric following a 28-gon model. (c) The side-chain carbon-atom numbers (SCCAN) within the amino acid molecules at each of the 16 genetic code doublets. Two presumed symmetry axes are indicated by arrows, """" and """". Block lines (both red and blue) are edges on the polyhedron; red lines (both block and dotted) are for neighboring code-doublet connection. The establishment of the genetic code essentially requires 20 enzymes aminoacyl-tRNA synthetases (AARS's), which catalyze aminoacylation of tRNAs by joining an amino acid to its cognate tRNA to establish the specificity of protein synthesis (de Pouplana and Schimmel, 2001; Hartman, 1995; Hartlein and Cusack, 1995; Cavalcanti et al., 2000; Di Giulio, 1992; Woese et al., 2000). In the present work we extend the study to the cooperative symmetry within the AARS's in response to the polyhedral and rotational symmetry of the genetic code system, and seek an explanation for the possibly intertwined symmetry between the genetic code and AARS's. Finally, we reach a  2   Yang CM, July, 2003  conclusion by identifying the Lucas series, from the carbon-atom numbers in amino acid side-chains, and propose that the mathematical principle as the Lucas series may have been underlying the natural selection of the amino-acid contents in the genetic code. 2 A polyhedral symmetric model displaying how the genetic code may be intertwined with AARS's AARS's possess very high specificity in selecting their cognate amino acid and tRNA substrates. Presently, AARS's have been classified into two distinct while structurally unrelated classes (I and II), each class encompassing 10 amino acids (Woese, 2000), see Table 1. A well-known assumption is that the tRNA-charging function of the AARS's evolved at least twice. Perhaps the two classes reflect a dichotomous origin of protein translation processes via two different primitive processes (Woese, 2000; Nagel and Doolittle, 1991). Therefore, 20 AARS's, which are responsible for establishing the genetic code, are the potential markers of genetic code development (de Pouplana and Schimmel, 2001). Consequently, it has been suggested that the evolution of the genetic code and evolution of the AARS enzymes are intertwined (Woese, 2000), and accordingly, the evolution of the AARS's are instrumental in understanding the selective pressures maintaining the genetic code. The coexistence of the two distinct classes of AARS's is one of the most striking features of the AARS's (Arnez and Moras, 1997; Burbaum and Schimmel, 1991; Cusack, 1997; Eriani et al., 1990). However, despite a clear existence of correlation between the genetic code and the evolutionary patterns of the AARS's, the biological importance of this fact is not known. According to this reasoning, an investigation into the internal relation of AARS's with respect to the symmetric relation within the amino acids is likely to provide insight into the conservation and evolution of the genetic code. Table 1. Two classes of aminoacyl-tRNA synthetases corresponding to a  two classes of amino acids. AARS Class Presumed Evolutionary Stage Stage 3  Class I  Class II  Stage 2  TyrRS TrpRS CysRS GlnRS LysRS-I LeuRS ArgRS MetRS IleRS GluRS ValRS  PheRS AsnRS HisRS LysRS-II AspRS SerRS  Stage 1  ProRS ThrRS GlyRS AlaRS  a  Here, ArgRS represents arginyl-tRNA synthetase, and so forth.  Two types of lysyl-tRNA synthetases (class I and class II) are labeled accordingly.  3   Yang CM, July, 2003  Given the distribution of the 20 amino acids is symmetric following a 28-gon model (Yang, 2003b), we now start from examining how the polyhedral symmetric relation of the 20 amino acids within the genetic code and the AARS's are intertwined on the polyhedral symmetric model (Figure 2). Two calsses of aminoacyl-tRNA synthetases (7)/o Y (7)F/(4) L (4) /Q(3) H C(1)/ W( 9)/ o N(2) /K(4) (7) F/(4 )L (7)/ o Y (4) /Q(3) H  Variant codons C(1 )/W(9 ) N(2)/ K(4)  (1)S  (4)L (4)R V(3 ) (0)G A (1)  S(1) /R( 4) M(3) /I(4 ) E(3)/ D(2 ) (1 )S (4 )L (4)R T(2 ) V(3) (0)G A (1)  S(1) /o M(3) /I(4) E(3 )/D (2)  (3 )P  (3) P  T(2)  Universal Genetic Codes Red brown: Calss I aminoacyl-tRNA synthetase Blue: Calss II aminoacyl-tRNA synthetase  Mitochondrial Genetic Codes  Differences b etween t he Un iversal and Mitocho ndrial Genetic Codes Codon UGA AGA AGG AUA Univ ers al cod e Stop Ar g Ar g Ile Human mitocho ndrial code Trp Sto p Sto p M et  Figure 2. Amino acids, recognized by two classes of aminoacyl-tRNA synthetases, corresponding to the universal genetic code and mitochondria genetic code on an icosikaioctagon or a 28-gon model. The numbers at vertices indicate the edges at the vertices. Shown on the side of amino-acid letters are the side-chain carbon-atom numbers (SCCAN) within the amino acid molecules. Two presumed symmetry axes are indicated by arrows, """" and """". Block lines (both red and blue) are edges on the polyhedron; red lines (both block and dotted) are for neighboring code-doublet connection. In Figure 2, the intertwining of the genetic code with AARS's can be clearly displayed on the 28-gon. In an effort to explore the internal symmetric relation of two classes of AARS's and the 20 amino acids, we divide the whole system into three presumed stages for the genetic code being intertwined with AARS's on the polyhedral symmetric mode. As is evident from Figure 3, there are three two-fold symmetry axes within the two classes of AARS's, i.e., one two-fold symmetry axis within every presumed evolutionary stage. The display helps understand that the close evolutionary relation between the (class I, II) AARS's is mirrored in the obvious coding relationship between the corresponding amino acids and the relationship between their codons. Although the degeneracies of all the 20 amino acids vary, exactly half of the 16 genetic code doublets positions are recognized by one of the two classes of AARS's, Figure 3.  4   Yang CM, July, 2003  It has been pointed out that the central role played by the AARSs in translation may suggest that their evolution and that of the genetic code are somehow intertwined. Hence, the finding in the present work may provide a clue to answer the question of whether the AARS's in their evolution have contributed to the code's present structure, or are the codon assignments simply reflections of AARS's evolutionary wanderings (Woese, 2000)?  Stage 1  Stage 2  Stage 3  Amino acids recognized by class I aminoacyl-tRNA synthetases Amino acids recognized by class II aminoacyl-tRNA synthetases Figure 3. Distribution of the two classes of amino acids are also symmetric with a two-fold  symmetric axis at each of the three presumed stages for the genetic code, which is intertwined with aminoacyl-tRNA synthetases on the polyhedral symmetric model. (, Amino acids recognized by class I aminoacyl-tRNA synthetases; , Amino acids recognized by class II aminoacyl-tRNA synthetases. The special case is lysine, which in some organisms is charged by a class I enzyme.)  We now analyze amino acid side-chain carbon-atom number balance based on the polyhedral symmetry of the genetic code with respect to the two classes of AARS's. The results are shown in Figure 4.  5   Yang CM, July, 2003  17 carbon atoms  17 carbon atoms 17 carbon atoms  17 carbon atoms 17 side-chain carbon atoms on amino acids along the green line  9 carbon atoms  9 carbon atoms on amino-acid side-chains  17 carbon atoms 17 side-chain carbon atoms on amino acids along the yellow line  Figure 4.  Amino-acid side-chain carbon-atom number balance summarized on the polyhedral (, Amino acids recognized by class I  symmetric model and their relation to the two classes of aminoacyl-tRNA synthetases. We can see the unique number ""17"" in several groups of amino acids. aminoacyl-tRNA synthetases; , Amino acids recognized by class II aminoacyl-tRNA synthetases. The special case is lysine, which in some organisms is charged by a class I enzyme.) Depicted in Figure 3 and 4, after we deploy the 20 amino acids on the polyhedral model in response to class I and II AARS's, the two presumed evolutionary axes within the genetic code are more evident. First, the 20 amino acids can also be grouped into 3 evolutionary stages, i.e., 1) A, P, G, V and T; 2) S, L, R, E, D, M and I; 3) F, L, H, Q, N, K, C, W and Y. Second, at each stage on the polyhedral model, there is one C-2 symmetric plane with respect to the two classes of AARS's, see Figure 3. Also of note is the fact that the total numbers of amino-acid side-chain carbon atoms within each stage are 9, 9+17, 17+17+7, respectively, Figure 4. From this step-by-step graphic examination of the AARS's and amino-acid contents within the genetic code, it is clear that the both the symmetric and possible evolutionary relation between the genetic code and AARS's are closely coiled with each other. In addition to the amino-acid side-chain carbon atom number balance at ""17"" indicated at several positions at stages 2 and 3 in Figure 4, amino-acid side-chain carbon-atom number balance at ""17"" also occurs at F(7); N(2)/K(4); H(4)  Q(3); C(1)/W(9); L(4) On the left of the `equation' are the amino acids recognized by class II AARS's, on the right are the amino acids byclass I. We will discuss this stranger but interesting number ""17"" in Section 3.  6   Yang CM, July, 2003  17 side-chain carbon atoms on amino-acids  17 side-chain carbon atoms on amino acids along the green line  17 side-chain carbon atoms on amino acids along the yellow line  Figure 5. The bi-pyramidal nature of the genetic code, displayed by the amino-acid side-chain carbon-atom number balance on the polyhedral (icosikaioctagon or 28-gon) model when the 20 amino acids in the genetic code are viewed as a whole. Provided that the 20 canonical amino acids in the genetic code can be deployed on the surface of a 28-gon model, the above atomic content analysis also revealed a bi-pyramidal nature of the genetic code, shown in Figure 5a. Based on the relative atomic compositions in amino acids, see Figure 5b, vertically, from APSF/LY, the total number of the amino acid side-chain carbon-atoms equals to 23; from ATM/IN/KY, the total number of the amino acid side-chain carbon-atoms equals to 23; by comparison, horizontally, from SRE/DM/I, the total number of the amino acid side-chain carbon-atoms equals to 17; from SLS/RM/I, the total number of the amino acid side-chain carbon-atoms equals to 17. A ratio of 15 [(P + S + F/L) or (T + M/I + N/K)] to 9 [(R + E/D) or (L + S/R)] equals to 1.6666, which is close the Golden mean 1.6180339... This finding helps explain why the genetic code is one of the most highly conserved characters in living organisms. 3. A Fibonacci-Lucas relationship within the canonical amino acids We now explore the possible existence of a mathematical relationship within the canonical amino acids. 3.1 The Fibonacci series Certain mathematical concepts of similarity and proportion hold one of the keys to understanding processes of growth in the natural world. For example, the Fibonacci series: 1, 1, 2, 3, 5, 8, ..., is well known to lie at the heart of plant growth and living organisms (Kappraff and Marzec, 1983). The Fibonacci sequence is generated by adding the previous two numbers in the list together to form the next and so on and so on... 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, . . . The basic Fibonacci relationship is in eq. 1: F i+2  =F  i+1  + Fi  (1 )  7   Yang CM, July, 2003  In 1202, Leonardo Fibonacci (1170-1240) in Italy discovered this simple numerical series that is the foundation for a remarkable mathematical relationship behind Phi or . Starting with 0 and 1, each new number in the series is simply the sum of the two before it. Divide any number in the Fibonacci sequence by the one before it, for example 55/34, or 21/13, and the answer quickly converge on the Golden mean Phi or , 1.618033988749895 . . .. After the 40th number in the series, the ratio is accurate to 15 decimal places.  3.2 The Lucas series A French mathematician, Edouard Lucas (1842-1891), who gave the above series of numbers 0, 1, 1, 2, 3, 5, 8, 13, .. the name the Fibonacci Numbers, found a similar series occurs often when investigating Fibonacci number patterns: 2, 1, 3, 4, 7, 11, 18, 29, 47, 76, 123, ... Here, the Fibonacci rule of adding the latest two to get the next is kept, but it starts from 2 and 1 (in this order) instead of 0 and 1 for the ordinary Fibonacci numbers. This series, called the Lucas Numbers, is defined as follows: where we write its members as Li, for Lucas: Li = Li-1 + Li-2 for i >1 L0 = 2 L1 = 1 L2 = 3 There are three formulae relating the Fibonacci and Lucas numbers, Li = Fi-1 + Fi+1 for all integers i and 5 Fi = Li-1 + Li+1 for all integers i The third one is called Lucas factors of Fibonacci numbers, F2i = Fi x L i  (2 )  (3) (4) (5)  Therefore, the Lucas numbers are very closely related to the properties of Fibonacci numbers. 3.3 The Lucas relationship within the canonical amino acids Previous studies show that it is possible to subdivide the set of 20 amino acids in many contrasting and overlapping ways. Organic synthetically, 20 canonical amino acids structurally and chemical compositionally vary from one to another by their different side-chain groups, which carry different number of carbon atoms. In the previous section, we discussed that, on the 28-gonal symmetric model of the genetic code system, a ratio of 15 [(P + S + F/L) or (T + M/I + N/K)] to 9 [(R + E/D) or (L + S/R)] equals to 1.6666, which is close to the Golden mean 1.6180339..., see Figure 5b. In view of the structural chemistry of a biomolecule, the number of carbon atoms within an organic molecule or within a particular functional group of the molecule can often provide a clue to its biochemical origin. From a bio-organic synthesis point of view, 20 amino acids vary from one to another by their distinctive side-chain groups which carry a series of carbon atoms. The numbers of side-chain carbon atoms in canonical amino acids are composed of very simple numbers 0, 1, 2, 3, 4, 7 and 9, i.e., ranging from 0 for glycine (G) to 9 for tryptophan (W) (Figure 6). A very interesting phenomenon is that any bigger number of the side-chain carbon-atoms (>2) in a bigger amino acid can 8   Yang CM, July, 2003  be the sum of the two smaller numbers of the side-chain carbon-atoms in two smaller amino acids from anscester codons (eqn. 6). These two properties of the canonical amino acids are reminiscent of the Fibonacci-Lucas relationship in mathematics, see eqn's. 1, 2, and 6. aai + aai+1 = aai+ 2  (6)  3 1 2 2 2 1 3 3 3 3  4 4 4 4 7 4 7 9  7 9 4 2 1 3  0  1  Figure 6.  Spiral rectangular relation of the side-chain carbon numbers within the 20 amino acid.  Table 2. A comparison of some Fibonacci numbers and Lucas numbers with side-chain carbon-atom numbers of the canonical amino acids.a i 0 1 2 Fibonacci 0 1 1 numbers or Fi Lucas numbers 2 3 1 or Li SCCAN 0 2 1 Type I C(1) amino acids Type II G(0) S(1) T(2); D(2) amino acids A(1) N(2) Multiplicity 1 3 3 a  3 2 4 3 V(3); M(3) E(3); Q(3) P(3) 5  4 3 7 4 L(4); R(4) I(4); K*(4) H(4); K*(4) 5  5 5 11 7 Y(7) F(7) 2  6 8 18 9 W(9)  1  Type I amino acids are amino acids recognized by class I aminoacyl-tRNA synthetases; Type II amino Amino acids in bold are produced from Miller's prebiotic  acids are recognized by class II aminoacyl-tRNA synthetases; The special case is lysine, which in some organisms is charged by a class I enzyme. simulation experiment (Miller, 1987). Three observations can be obtained from a comparison of the Fibonacci-Lucas numbers with the side-chain carbon-atom numbers within canonical amino acids in Table 2. First, except Glycine (G), which possesses no side-chain carbon atom, and tryptophan (W), which carries 9 carbon atoms on its side-chain, 18 other canonical amino acids carry 1, 2, 3, 4, to 7 side-chain carbon-atoms, respectively, which exactly meet the initial 5 consecutive Lucas numbers, 2, 1, 3, 4 and 7. Second, within the type I amino acids recognized by class I AARS's, the numbers of side-chain carbon atoms include 1, 3, 4 and 7, except W, following the Lucas rule, independent of the type II amino acids. Within the type II amino acids recognized by class II AARS's, except G, the numbers of side-chain carbon atoms are 1, 2, 3, 4 and 7, being also consist with the Lucas sequence, independent of the type I amino acids. Third, 9   Yang CM, July, 2003  the above two observations merge to a suggestion that the majority of the 20 canonical amino acids selected in the genetic code are naturally selected by a double-Golden mean. Moreover, the sum of 1, 2, 3, 4 and 7 equals to 17, somehow in conformity with the number ""17"", which has been repetitively found in the amino-acid side-chain carbon-atom balance on the 28-gon model, as is pointed out in Section 2.2. Seemingly, the above correlation between the amino-acid side-chain carbon-atom numbers (SCCAN) and Fibonacci-Lucas series prompts enquiry as to whether this quantitative property of the canonical amino acids appears to be a coincidence, especially considered together with the Miller's prebiotic simulation experiment (Miller, 1987), in which numerous small bio-molecules other than the standard amino acids are produced. However, in addition to the symmetric feature manifested by amino-acid side-chain carbon-atom number balance which is directed by the two classes of AARS's, very interestingly, the numbers ""5"" and ""6"", which are not in the Lucas series, are not selected into the amino-acid SCCAN category either. Within the last decade, significant strides have been made toward understanding the molecular basis of the genetic code. One very important advance in this area was the definition of an amino acid property called the polar requirement (Woese, 1966). Recently, much progress has also been made in elucidating how specific interactions between amino acids and nucleic acids may have played an important role in the origin of the genetic code (Yarus, 1998; Szathmary, 1999). Whereas the evolutionary dynamic that shaped the code remains an enigma, and what property or properties of the amino acids the code actually reflects has long remained a mystery. In the present work, a pronounced similarity exists between the Lucas series (a numerical basis of the Golden mean) and amino-acid side-chain carbon-atom numbers, presumably suggesting that the amino acids selected into the genetic code system seems to follow the natural Golden mean by the Lucas series, when the 20 amino acids in the genetic code system are viewed as a whole. 4 Conclusion Based on a detailed analysis of amino-acid side-chain carbon-atom numbers, it can be identified that the 28-gonal and rotational symmetric features within the internal relation of the 20 amino acids in the genetic code is stepwise intertwined with two classes of aminoacyl-tRNA synthetases. The two previously suggested evolutionary axes within the genetic code cooperatively overlap the three two-fold symmetry axes within two classes of aminoacyl-tRNA synthetases on the 28-gon model. Importantly, it has been revealed from this work that the natural selection of the side-chain carbon-atom numbers (1, 3, 4 and 7) in amino-acids is in an excellent agreement with a mathematical relationship, the Lucas series. These findings may provide new insight into biological understanding what principle governs the selection of the 20 canonical amino acids in the genetic code, and allow new opportunities in further theoretically exploring the information logic in the genetic code system. Abbreviations aa, amino acid; 20 amino acids are represented by A(Ala), P(Pro), V(Val), G(Gly), T(Thr), S(Ser), L(Leu), R(Arg), D(Asp), E(Glu), M(Met), I(Ile), F(Phe), C(Cys), W(Trp), H(His), Q(Gln), N(Asn), K(Lys) and Y(Tyr). ""o"" denotes ""stop"" codons. SCCAN, Side-chain carbon-atom number; AARS, aminoacyl-tRNA synthetase.  10   Yang CM, July, 2003  REFERENCES Arnez, J. G. and Moras, D. (1997). Trends Biochem. Sci. 22, 211-216. Burbaum, J. J. and Schimmel, P. (1991). J. Biol. Chem. 266, 16965-16968. Casper, D. L. D. and Klug, A. (1962). Cold Spring Harb. Symp. Quant. Biol. 27, 1-24. Cavalcanti, A. R., Neto, B. D. and Ferreira, R. (2000). J. Theor. Biol. 204, 15-20. Cusack, S. (1997). Curr. Opin. Struct. Biol. 7, 881-889. Davydov, O. V. (1998). J. Theor. Biol. 193, 679-690. Di Giulio, M. (1992). Orig. Life Evol. Biosph. 22, 309-19. Dufton, M. J. (1997). J. Theor. Biol. 187, 165-173. Eriani, G., Delarue, M., Poch, O., Gangloff, J. and Moras, D. (1990). Nature 347, 203-206. Hartlein, M. and Cusack, S. (1995). J. Mol. Evol. 40, 519-30. Hartman, H. (1995). Orig. Life Evol. Biosph. 25, 265-9. Jimnez-Montao, M. A., de la Mora-Basaez, R. and Pschel, T. (1996). BioSystems 39, 117-125. Kappraff, J. M. and Marzec, C. (1983). J. Theor. Biol. 103, 201-226. Karasev, V. A. and Stefanov, V. E. (2001). J. Theor. Biol. 209, 303-17. Miller, S. L. (1987). Cold Spring Harb. Symp. Quant. Biol. 52, 17-27. Nagel, G. M. and Doolittle, R. F. (1991). Proc. Natl. Acad. Sci. USA 88, 8121-8125. Racaniello, V. R. (1996). Proc. Natl. Acad. Sci. U S A. 21, 11378-81. Rakocevi, M. M. (1998). BioSystem 46, 283-291. Ribas de Pouplana, L. and Schimmel, P. (2001). Trends Biochem. Sci. 26, 591-6. Schimmel, P. and Ribas de Pouplana, L. (2001). Cold Spring Harb. Symp. Quant. Biol. 66, 161-6. Shcherbak, V. I. (1989). J. Theor. Biol. 139, 271-6. Szathmary, E. (1999). Trends Genet. 15, 223-229. Trifonov, E. N. and Bettecken, T. (1997). Gene 205, 1-6. Trifonov, E. N. (2000). Gene 261, 139-51. Woese, C. R., Olsen, G. J., Ibba, M. and Sll, D. (2000). Microbiol. Mol. Biol. Rev. 64, 202236. Weber, A. L. and Lacey, J. C. Jr. (1978). J. Mol. Evol. 11, 199-210. Weber, A. L. and Miller, S. L. (1981). J. Mol. Evol. 17, 273-284. Yang, C.M. (2003a). in press. Yang, C. M. (2003b). in press. Yang, C. M. (2003c). in press. Yarus, M. (1998). J. Mol. Evol. 47, 109-117.  (CMY, 2003/7/18)  11"
GX170-59-14968337	"arXiv: chao-dyn/9810017  v1   14 Oct 1998                                   Self-Organized Criticality in Daily Incidence of Acute Myocardial Infarction                     A.M. SELVAM 1 , D. SEN 2  and S.M.S. MODY 3   1. Indian Institute of Tropical Meteorology, Pune 411 008, India   2. Bombay Hospital, Bombay, 400020, India   3. Wadia Institute of Cardiology, Pune 411 001, India                                 Corresponding author:                   Dr.(Mrs.)A.M.Selvam   Indian Institute of Tropical Meteorology,   Dr. Homi Bhabha Road, Pashan, Pune, 411 008, India   Email: selvam@tropmet.ernet.in   Website: http://www.geocities.com/CapeCanaveral/Lab/5833   Telephone: 091-0212-330846   Fax: : 091-0212-347825                           Abstract                           Continuous periodogram power spectral analysis of daily incidence of acute myocardial infarction ( AMI ) reported at a leading hospital for cardiology in Pune, India for the two-year period June 1992 to May 1994 show that the power spectra follow the universal and unique inverse power law form of the statistical normal distribution. Inverse power law form for power spectra of space-time fluctuations are ubiquitous to dynamical systems in nature and have been identified as signatures of self-organized criticality. The unique quantification for self-organized criticality presented in this paper is shown to be intrinsic to quantumlike mechanics governing fractal space-time fluctuation patterns in dynamical systems. The results are consistent with El Naschie's concept of cantorian fractal spacetime characteristics for quantum systems.                         1. INTRODUCTION                           The daily incidence of acute myocardial infarction ( AMI ) during the two-year period June 1992 to May 1994 was obtained from admission records of a premier Institute of cardiology at Pune, India. Continuous periodogram power spectral analysis of the data show a broadband spectrum with embedded dominant wavebands, the bandwidth increasing with period length. Broadband spectra for fluctuations are ubiquitous to dynamical systems in nature   [1], such as atmospheric flows, stock market price fluctuations, population growth, spread of infectious diseases, etc. The broadband power spectra exhibit inverse power law  f  -B   where  f  is the frequency and  B  the exponent. Inverse power law form for power spectra imply long-range (space-time) correlations. Long-range spatiotemporal correlations are ubiquitous to dynamical systems in nature and are identified as signatures of self-organized criticality   [2]. The physics of self-organized criticality is not yet identified. Atmospheric flows exhibit self-organized criticality manifested as the selfsimilar fractal geometry to the spatial pattern concomitant with inverse power law form for spectra of temporal fluctuations, documented and discussed in detail by Lovejoy and his group [3]. A recently developed cell dynamical system model for atmospheric flows predicts the observed self-organized criticality as intrinsic to quantumlike mechanics governing flow dynamics [4-9]. The model predicts the universal inverse power law form of the statistical normal distribution for the power spectrum of fluctuations thereby providing universal quantification for self-organized criticality. The model is based on the concept that cumulative summation (integration) of small scale fluctuations give rise to large scale perturbations generating a hierarchical network, the generation mechanism being dependent only on the intensity of fluctuations and independent of the detailed mechanisms governing the fluctuations. The model is therefore a general systems theory   [10-11] applicable to all dynamical systems in nature. The model concepts are applied to show that daily incidence of  AMI , probably triggered by stress-free and stressful activity cycle corresponding respectively to sleep-wake diurnal (night to day) activity rhythm self-organizes to form a broadband spectrum for temporal fluctuations, with universal inverse power law form of the statistical normal distribution. Daily incidence of  AMI  exhibits self-organized criticality with model predicted unique quantification in terms of the statistical normal distribution. Quantumlike mechanical laws may therefore govern fluctuation pattern of  AMI  incidence. The results are consistent with El Naschie's concept [12] of cantorian fractal space-time fluctuations for of quantum systems   The data used in this study was collected in connection with the dissertation entitled ' A Study of Circadian Rhythm and Meteorological Factors Influencing Acute Myocardial Infarction  ' submitted to the university of Pune, India, by Dr. D. Sen, M.B.B.S., in 1995, for the M.D.(Doctor of Medicine) Degree (General Medicine) Branch 1 [13].                       2. MODEL CONCEPTS                           In summary   [4-9] the model is based on Townsend’s   [14] concept originally proposed for growth of large eddy structures visualized as envelopes enclosing internal small scale eddy circulations in atmospheric flows. A hierarchical eddy continuum is generated by successive cumulative integration of internal small scale fluctuations, the eddy growth process being dependent only on the intensity and length/time scale of fluctuations and independent of details of mechanisms generating the fluctuations. Large scale fluctuations of intensity  W 2  and length scale  R  result from integration of enclosed small scale fluctuations of intensity  w *   2   and length scale  r  given by the relation                                                                                     (1)   The fluctuations self-organize to form a hierarchical eddy continuum. Since the intensity   W 2  at any large scale is the cumulative integration of enclosed small scale eddies of intensities  w *   2 , the eddy energy spectrum follows the statistical normal distribution. The square of the eddy amplitude represent the probability. Such a result that additive amplitudes of eddies, when squared, represent probabilities is observed in subatomic dynamics of quantum systems. The growth of fluctuation pattern therefore follows quantumlike mechanical laws. The above visualization for growth of large scale structures from small scale fluctuations result in the following model predictions.     1) The successive values of amplitude  W  and length scale  R  follow the Fibonacci mathematical series.   2) The growth of fluctuation pattern follows an overall logarithmic spiral trajectory  OR 1 R 2 R 3 R 4 R 5  with the quasiperiodic  Penrose  tiling pattern for the internal structure (Fig. 1). The amplitudes of fluctuations for successive growth stages follow the logarithmic relationship.                                                                           (2)      where  Z  is the scale ratio equal to  R/r  and  k  is the steady state fractional volume dilution of large eddy by turbulent eddy fluctuations.   3.) The logarithmic spiral can be resolved as an eddy continuum with embedded dominant wavebands  R o OR 1 ,R 1 OR 2 ,R 2 OR 3 , etc. ,the peak periodicities  P n  being given by      P n  =  t   n (2+ t  )T                                                              (3)      where  t  is the  golden mean  equal to  (1+ Ö  5)/2 ( »  1.618)  and  T  is the diurnal trigger such as the sleep - wake/night - day cycle associated with stress-free and stressful activity rhythms.   4.) The angular turning  d q   for successive stages in growth of the logarithmic spiral trajectory is given from Equation(1) as      d q   =                                                                           (4)      The phase angle  q  at any stage of growth is therefore proportional to the variance from Equation(1)      q   µ  W 2                                                                      (5)      The phase spectrum will therefore represent the variance spectrum.   The successive growth stages of the logarithmic spiral trajectory may therefore be visualized, particularly in traditional power spectrum analysis, as a continuum of eddies with progressive increase in phase.   The association between phase angle, variance and length scale as obtained above at Equations 4 and 5 are intrinsic to the microscopic dynamic of quantum systems and has been identified as  Berry’s phase  [15-16].   5) The root mean square (r.m.s.) amplitude of fluctuations  W  and  w *   (2) represent the standard deviation and also the mean, since each level represents the mean for next stage of eddy growth. The standard deviation of the fluctuations is therefore represented by  logZ  where  Z  is the scale ratio representing the ratio of frequencies( or periods or wavelengths).   6) The conventional power spectrum plotted as cumulative percentage contribution to total variance versus the frequency (or period or wavelength )on log-log scale will now represent the cumulative percentage probability on log scale versus the standard deviation on linear scale since earlier (1) it was shown that variance, i.e.  W 2  distribution corresponding to  logZ  represents probability densities and also that  logZ  represents the standard deviation of the fluctuations (2).   Following traditional concepts in statistics, a normalized standard deviation   t  for  l ogZ   distribution can be defined as      t =                                                            (5)      where   L  is the period in years and  T 50  the period up to which the cumulative percentage contribution to total variance is equal to  50 .  LogT 50  will correspond to the mean value for the variance  W 2  distribution which was shown to follow normal distribution (1).   The power spectrum when plotted as cumulative percentage contribution to total variance versus  logZ  expressed in terms of the normalized standard deviation  t  (5) will represent the statistical normal distribution.     The above model concepts are dependent on the amplitude  W  of fluctuations of length (or time) scale  R  alone and totally independent of the detailed mechanisms underlying the fluctuations. The model predicts that temporal (or spatial) fluctuations of dynamical systems in general self -organize to form the universal inverse power law form of the statistical normal distribution (1 - 6).                   3. DATA AND ANALYSIS                           The daily incidence of acute myocardial infarction (AMI) for the two year period June 1992 to May 1994, was obtained from admission records of a premier Institute for Cardiology at Pune, India. The power spectrum of  AMI  incidence (daily) was computed by an elementary but very powerful method of analysis developed by Jenkinson   [17] which provides a quasi-continuous form of the classical periodogram allowing systematic allocation of the total variance and degrees of freedom of the data series to logarithmically spaced elements of the frequency ranges ( 0.5 ,  0 ). The peridogram was constructed for a fixed set of  10000(m)  periodicities which increase geometrically as   L m  =  2 exp  ( Cm )  where  C  = .001  and  m  = 0, 1, 2,......m . The data series  Y t  for the  N  data points were used. The periodogram estimates the set of  A m cos (2 p   n   m t -  f   m )  where  A m ,  n   m  and   f   m  denote respectively the amplitude, frequency and phase angle for the  m th  periodicity. The cumulative percentage contribution to total variance was computed starting from the high frequency side of the spectrum. The period  T 50  at which  50%  contribution to total variance occurs is taken as reference and the normalized standard deviation  t m  values are computed as (6).            t m  = (log L m  / log t 50 ) - 1                                                              (7)    The corresponding phase spectrum was computed as the cumulative percentage contribution to total rotation, i.e. normalized with respect to total rotation. The variance spectrum, phase spectrum and the statistical normal distribution plotted respectively as cumulative percentage contribution to total variance, cumulative percentage contribution to total rotation and cumulative percentage probability are shown in Fig. 2.   It is seen that variance and phase spectra follow each other closely and also the statistical normal distribution. The ""goodness of fit"" of variance spectrum and phase spectrum to statistical normal distribution is within 95% level of significance as determined by the standard statistical chi-square test   [18].   The dominant wavebands identified as those for which normalized variance is greater than or equal to  1.0  are shown in Fig.3a plotted in the conventional manner, i.e. normalized variance versus logarithm of period in days. Fig.3b shows the cumulative percentage contribution to total variance and cumulative normalized phase for each dominant wave band. The peak periodicities corresponding to each dominant waveband is listed in Table 1.                   4. DISCUSSION AND CONCLUSIONS                         A general systems theory for self-organization of fluctuations gives following model predictions. Space-time integration of small-scale fluctuations give rise to an overall logarithmic spiral trajectory with the quasiperiodic  Penrose  tiling pattern for the internal structure. The logarithmic spiral trajectory can also be resolved as an hierarchical eddy continuum with progressive increase in phase. The eddy continuum has embedded in it dominant wavebands, the bandwidth increasing with period length. The dominant peak periodicities are functions of the golden mean and the primary triggering cycle of stress - free and stressful activity cycle associated with sleep - wake (night to day) rhythm. Since cumulative integration of enclosed small scale fluctuations results in large scale fluctuations, the eddy energy spectrum follows inverse power law form of statistical normal distribution according to  Central Limit Theorem . The square of the eddy amplitude or, the variance represents the probabilities. Such a result that additive amplitudes of eddies, when squared, represent probability densities is observed in the subatomic dynamics of quantum systems such as the electron or photon. The dynamics of spacetime fractal fluctuation pattern formation therefore follows quantumlike mechanical laws, consistent with El Naschie's concept of cantorian fractal characteristics for quantum systems [12].   Continuous periodogram power spectral analysis of daily incidence of acute myocardial infarction for the two-year period June 1992 to May 1994, reported at an Institute for Cardiology in Pune, India show that the following dynamical characteristics of  AMI  variability are consistent with model predictions summarized above.     (1) The spectrum is broadband with embedded dominant wavebands, the bandwidth increasing with period length (Fig. 3).   (2) The dominant peak periodicities (Table 1) closely correspond to model predicted values (3)  2.24 ,  3.62 ,  5.85 ,  9.47 ,  15.33 ,  24.80 ,  40.13 ,  64.92  corresponding respectively to  n  values ranging from  -1 to 6 .   (3) The spectrum follows the universal inverse power law form of statistical normal distribution (Fig.2) which signifies (a) quantumlike mechanics for the dynamics of  AMI  incidence (b) long-range temporal correlations, or fractal structure to temporal fluctuations, namely self-organized criticality [5] .   (4) The phase spectrum closely follows the variance spectrum, for the total spectrum and also within each dominant waveband (Figs. 2 - 3). The close association between phase, variance and period length is a feature intrinsic to quantum systems and identified as "" Berry’s phase ""   [15-16].     Self-organized criticality, namely long-range spatiotemporal correlations exhibited by dynamical systems in nature, is a signature of quantumlike mechanics governing the dynamics of fractal space-time pattern evolution. Universal spectrum of AMI day to day variability implies prediction of total pattern of fluctuations. The results are consistent with El Naschie's concept of cantorian fractals characteristics for quantum systems.               ACKNOWLEDGMENTS         The authors are grateful to Dr. A. S. R. Murty for his keen interest and encouragement during the course of the study. The authors are indebted to Professor M. S. El Naschie for inspiration and guidance in this new field of research. Thanks are due to Mr. R. D. Nair for typing the paper.                     REFERENCES       1. Schroeder, M. , Fractals , Chaos and Powerlaws , W.H.Freeman and Co., N.Y., 1991.   2. Bak, P.C., Tang, C. Wiesenfeld, K, Self-organized criticality.  Phys. Rev. A . 1988,  38 , 364 - 374.   3. Tessier, Y., Lovejoy, S., Hubert, P., Schertzer, D., Pecknold, S., Multifractal analysis and modelling of rainfall and river flows and scaling, causal transfer function.  J. Geophys. Res ., 1996,   101(D21) , 26427-26440.   4. Mary Selvam, A. Deterministic chaos, fractals and quantumlike mechanics in atmospheric flows.  Can. J. Phys . 1990,  68 , 831-841.   5. Mary Selvam,A., Universal quantification for deterministic chaos in dynamical systems.  Applied Math. Modelling  1993,  17 , 642-649.   6. Selvam A. M. and Suvarna Fadnavis, Signatures of a universal spectrum for atmospheric interannual variability in some disparate climatic regimes . Meteorology and Atmospheric Physics  , 1998,  66 , 87-112. ( http://aps.arXiv.org/abs/chao-dyn/9805028 )   7. Selvam A. M. and Suvarna Fadnavis, Superstrings, cantorian-fractal space-time and quantum-like chaos in atmospheric flows.  Chaos, Solitons and Fractals , 1998 (in Press)   8. Selvam A. M. and Suvarna Fadnavis, Cantorian fractal spacetime, quantum-like chaos and scale relativity in atmospheric flows.  Chaos, Solitons and Fractals,  1998. (in Press)   9. Mary Selvam, A., Quasicrystalline pattern formation in fluid substrates and phyllotaxis, In: Jean, R. V. and Barabe, D.,(eds.), Symmetry in Plants . World Scientific Series No 4. in Mathematical Biology and Medicine, World Scientific, Singapore 1998,795-805.   10. Peacocke, A. R.,  The Physical Chemistry of Biological Organization , Clarendon Press, Oxford, U.K, 1989.   11. Klir, G. J., Systems science : a guided tour,  J. Biological Systems   1992,  1 , 27-58.   12. El Naschie, M. S., Penrose tiling, semi-conduction and Cantorian 1/f a  spectra in four and five dimensions,  Chaos, Solitons and Fractals  ,1993,  3(4) , 489-491.   13. Selvam, A. M., Sen, D. and Mody, S. M. S.,  A Study   of Circadian Rhythm and Meteorological Factors Influencing Acute Myocardial Infarction , Research Report number RR-073, April 1997, Indian Institute of Tropical Meteorology, Pune 411008, India.   14. Townsend, A. A.,  The Structure of Turbulent Shear Flow . Cambridge University Press, London, U.K., 1956.   15. Berry, M. V., The geometric phase.  Sci. Amer . 1988,  Dec ., 26-32.   16. Maddox, J., Can quantum theory be understood ?  Nature  1993,  361 , 493.   17. Jenkinson, A. F.,  A powerful elementary method of spectral analysis for use with monthly, seasonal or annual meteorological time series.  (U.K. Meteorol. Office) Met. O 13 Branch Memorandum No. 57, 1-23, 1977.   18. Spiegel, M. R.,  Statistics . McGraw-Hill Book Co., N.Y., 359 pp. 1961.                             Table 1 Dominant peak periodicities (days)            2-3        3-4        4-6        6-12        12-20        20-30        30-50        50-80        120-200        200-300            2.006,2.014,2.022,   2.030,2.042,2.055,   2.067,2.082, 2.090 ,   2.122,2.128,2.136,   2.151,2.158,2.186,   2.199,2.217,2.228,   2.246,2.280,2.301,   2.317, 2.326 ,2.352,   2.373,2.411,2.440,   2.448,2.460,2.477,   2.517,2.527,2.548,   2.565,2.578,2.623,   2.636,2.649,2.660,   2.673,2.689,2.721,   2.735,2.757,2.785,   2.810,2.824,2.838,   2.901,2.933,2.963        3.035,3.050,   3.065,3.081,   3.146, 3.181,   3.197,3.213,   3.248,3.274,   3.307,3.341,   3.357,3.418,   3.439,3.484,   3.544,3.576,   3.604,3.688,   3.714,3.759,   3.785,3.866,   3.916,3.964,   3.991        4.101,4.134,   4.175,4.209,   4.268,4.394,   4.514,4.550,   4.596,4.670,   4.880,4.944 ,   5.044,5.079,   5.151,5.208,   5.388 , 5.535 ,   5.636,5.687,   5.750 ,5.872,   5.972        6.099,6.342,   6.450,6.574,   6.694,7.023,   7.251,7.442,   7.669, 8.441 ,   8.569, 8.707,   8.856,8.990,   9.135,9.357,   9.499,9.652,   9.916 ,10.383,   10.763,11.637,   11.895        12.124   13.412   14.256   15.598   16.186   16.763   17.657   18.712   19.436        23.598   27.144   29.969        32.367   36.276   39.931        64.531   73.563        162.73        245.70               Periodicities significant at or less than 5% level are given in  bold  letters.                     Legend        Fig. 1. The quasiperiodic  Penrose  tiling pattern   Fig. 2. Variance and phase spectra.The statistical normal distribution is also shown in the Figure.   Fig. 3. The periodogram is plotted in two sections, on logarithmic scale for the periods(days) on the x-axis, the upper half for periods upto 10-days and the lower half for periods 10 - 100 days. Each section contains the following two displays:       (a). The power spectrum plotted as normalized variance versus period (days) for dominant wavebands(normalized variance >= 1.0).       (b) cumulative percentage contribution to total variance versus cumulative normalized phase for each dominant waveband, demonstrating  Berry’s phase                                        Fig. 1                                     Fig. 2.                                       Fig. 3."
GX001-48-0025362	"Text Only                     Home          >   Scientific        Research   >          Research        Projects   >          Artificial Intelligence                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Artificial Intelligence - the ability of a machine to perform those                activities that are normally thought to require intelligence. Here                at NASA's Marshall Space Flight Center in Huntsville, Ala., some                of the advanced concepts that are being applied to control spacecraft                propulsion subsystems is the fuzzy logic controller. Fuzzy logic                is a methodology that unlike the traditional, hard computing is                aimed at exploiting the uncertainty in decision making while providing                for a robust, simplistic controller.               A                fuzzy controller involves the following concepts: an observable                variable, a controlling parameter, a set of fuzzy rules that encode                knowledge about the system, and a set of desired functional parameters.                The aim is to incorporate expert human knowledge in the control                algorithm that makeup the rules of the fuzzy algorithm. The rules                are a set of if/then statements with each rule having the following                pattern: IF X THEN Y, where X and Y are linguistic variables. The                linguistic variables, also called fuzzy variables, are variables                whose arguments are fuzzy numbers or membership functions. Many                if/then rules with connectives (e.g., ELSE) form the basic structure                of the fuzzy algorithm.               The most common controller used in many system processes is the                proportional-integral derivative (PID) controller due to its simplicity                and robustness. PID is a powerful composite mode of control that                is effective for any linear process condition. The derivative mode                tends to minimize oscillation of the system and prevent overshoot,                while integral control addresses permanent offset. Offset is sometimes                caused by derivative mode or when the system is slow to return to                desired reference levels. Unfortunately, the performance of nuclear                power regulation using the PID controller depends on initial parameter                setup, which often requires operator's expert knowledge on the system.                When the controller operates over a wide power range, it is non-trivial                to adjust the parameters of the PID controller. In cases where the                process is nonlinear, PID fails to achieve proper system stability.                On the other hand, fuzzy models are well suited for modeling non-linear                systems and lend themselves to adaptive techniques.               The research being conducted now at the Marshall Center is for                design and implementation of a fuzzy logic process control system                to regulate dynamic variables related to nuclear system power and                navigational propulsion. The entire system is expected to be self-adaptive,                i.e., capable of responding to long-range changes in the space environment.                Particular attention in the development of the fuzzy logic algorithm                shall ensure that the system process remains at setpoint, virtually                eliminating overshoot on start-up and during in-process disturbances.                The controller design will withstand harsh environments and applications                where it might come in contact with water, corrosive chemicals,                radiation fields, etc.               Fuzzy logic is just one of the many soft computing methodologies                being explored by the Marshall Center's Nuclear Propulsion Group                to apply ""the intelligence"" in space.                                                                                                                                                                                                                                                                                     Related Information                                                                                                                                                                 Fuzzy Logic                                (pdf)"
GX068-45-0047209	Home     Manual     Packages     Global Index     Keywords     Quick Reference          /*  * plato.i -- $Id$  *  * Copyright (c) 1998.  See accompanying LEGAL file for details.  */    local  plato  ; /* DOCUMENT plato.i      Contains routines to generate points related to Platonic      solids and other pleasing or simple 3D geometrical objects.         pt_tet        All these return points of the solid imbedded in       pt_cube   a pleasing way inside the cube with corners +-1.       pt_oct   With a non-zero parameter, the points are instead       pt_dodec   normalized to  unit  length.       pt_ico          bucky         return points or faces of geodesic dome-like                   solids and their solid angles  */    func  pt_tet  (norm) {   /* half of corners of a cube, no particular order */   p= [[1,1,1],[-1,-1,1],[1,-1,-1],[-1,1,-1]];   /* use other choice of corners if norm<0 */    if (norm) p*=  sqrt (1./3.)* sign (norm);   return p; }    func  pt_cube  (norm) {   /* points at corners of cube, hence 2x2x2 organization     *  where  indices are x, y, z directions */   p=  array (-1,3,2,2,2);   p(1,2,,)= p(2,,2,)= p(3,,,2)= 1;   if (norm) p*=  sqrt (1./3.);   return p; }    func  pt_oct  (norm) {   /* one point on each face of a cube, hence 2x3 organization     *  where  1st index selects + or - face of cube    *       2nd index selects xyz face of cube */   p=  array (0,3,2,3);   p(1,,1)= p(2,,2)= p(3,,3)= [-1,1];   return p; }    func  pt_ico  (norm) {   /* two points on each face of a cube, hence 2x2x3 organization,     *  where  1st index selects + or - point on face of cube,    *       2nd index selects + or - face of cube    *       3rd index selects xyz face of cube */    g= 0.5*( sqrt (5.)-1.);  /* reciprocal golden ratio */   p= [[[0,g,1],[0,-g,1]],[[0,g,-1],[0,-g,-1]]];    p= [p,  roll (p,[1,0,0]),  roll (p,[2,0,0])];    if (norm) p/=  abs (p(1,..),p(2,..),p(3,..))(-,..);   return p; }    func  pt_dodec  (norm) {   /* two points on each face of a cube, followed by corners of cube */    g= 0.5*( sqrt (5.)-1.);  /* reciprocal golden ratio */   g2= 1.-g;  /* equals g*g */   p= [[[0,g2,1],[0,-g2,1]],[[0,g2,-1],[0,-g2,-1]]];    p= [p,  roll (p,[1,0,0]),  roll (p,[2,0,0])];    p=  grow (p(,*),g* pt_cube ()(,*));    if (norm) p/=  abs (p(1,..),p(2,..),p(3,..))(-,..);   return p; }    func  bucky  (n,faces,&domega) {   /* two rings of five plus two points at poles     * rings have  cos (theta) = +- g (reciprocal golden ratio),     *             sin (theta) =  sqrt (g)    *    * The points are organized into five spiral strips running    * southeast from the north pole to the south pole.  Each strip     * consists of a 3x2  array  of points which bound a strip of four    * triangular faces; the diagonals run from (1,1) to (2,2) and    * from (2,1) to (3,2).  The point (1,2) is always the north pole,    * and the point (3,1) is always the south pole.  The points    * (2,2) and (3,2) on the eastern edge of one strip are the same    * as the points (1,1) and (3,2), respectively, on the western    * edge of the strip immediately to the east.     * Hence, the dimensionality of the returned  array  of points    * is 3 by 3x2x5.  (30 - 4 duplications of north pole    * - 4 duplications of south pole - 5*2 other duplicates = 12)    *     * When you specify n (default 0),  bucky  will halve each of the    * initial 3x2 strips n times, to produce a (2^(n+1)+1)x(2^n+1)     *  array  of equally spaced points in place of the 3x2 points of    * the n=0 pattern.    *   n=0 -->   12 points   20 faces    *   n=1 -->   42 points   80 faces    *   n=2 -->  162 points  320 faces    *   n=4 -->  642 points 1280 faces    *   n=5 --> 2562 points 5120 faces    *     * When radius==1, icosahedron apothem== sqrt ((2+g)/(2-g)/3),     * suggesting a worst case  area  ratio of (2+g)/(2-g)/3 = 0.631476.    * However, by renormalizing the points after each doubling, this    * ratio is considerably improved; the worst case in the limit of    * many doublings is a little under 0.769.    */    g= 0.5*( sqrt (5.)-1.);  /* reciprocal golden ratio */   p=  array (0., 3,3,2,5);   p(,1,2,)= [0,0,1];   /* north pole */   p(,3,1,)= [0,0,-1];  /* south pole */   c36= 0.5*(1.+g);  /* equals 0.5/g */   s36= 0.5* sqrt (2.-g);   c72= 0.5*g;   s72= 0.5* sqrt (3.+g);    g= 1./(1.+2.*g);  /*  cos  of ring angle */    s= 2.*g;          /*  sin  is twice  cos  for this magic angle */   ringn= s*[[1,0],[c72,s72],[-c36,s36],[-c36,-s36],[c72,-s72]];   rings= s*[[c36,s36],[-c72,s72],[-1,0],[-c72,-s72],[c36,-s36]];   p(3,1,1,)= p(3,2,2,)= g;   p(1:2,1,1,)= ringn;   p(1:2,2,2,)=  roll (ringn,[0,-1]);   p(3,2,1,)= p(3,3,2,)= -g;   p(1:2,2,1,)= rings;   p(1:2,3,2,)=  roll (rings,[0,-1]);   if (!n) n= 0;   while (n--) {     dims=  dimsof (p);     dims(3:4)= 2*dims(3:4)-1;     q=  array (0., dims);     q(,1:0:2,1:0:2,)= p;     q(,2:-1:2,1:0:2,)= p(,zcen,,);     q(,1:0:2,2:-1:2,)= p(,,zcen,);     q(,2:-1:2,2:-1:2,)= p(,2:0,2:0,)+p(,1:-1,1:-1,); /* *0.5 */      p= q/ abs (q(1,..),q(2,..),q(3,..))(-,..);     q= x= [];   }   if (faces) {     dims=  dimsof (p);      q=  array (0.,3,2,dims(3)-1,dims(4)-1,5);     llur= (pb=p(,1:-1,1:-1,)) + (pc=p(,2:0,2:0,));     q(,1,,,)= llur + (pa=p(,1:-1,2:0,));  /* a-b-c */     q(,2,,,)= llur + (pd=p(,2:0,1:-1,));  /* d-c-b */     domega= q(1,..);      p= q/ abs (domega,q(2,..),q(3,..))(-,..);      domega(1,..)=  pt_solid2 (pa,pb,pc);      domega(2,..)=  pt_solid2 (pd,pc,pb);   }   return p; }    func  pt_cross  (a,b) {   c= 0.*a;   c(1,..)= a(2,..)*b(3,..)-a(3,..)*b(2,..);   c(2,..)= a(3,..)*b(1,..)-a(1,..)*b(3,..);   c(3,..)= a(1,..)*b(2,..)-a(2,..)*b(1,..);   return c; }    func  pt_solid  (a,b,c) {   vab=  pt_cross (a,b);    sab=  sqrt ((vab*vab)( sum ,..));   vbc=  pt_cross (b,c);    sbc=  sqrt ((vbc*vbc)( sum ,..));   vca=  pt_cross (c,a);    sca=  sqrt ((vca*vca)( sum ,..));   cosa= -(vab*vca)( sum ,..)/(sab*sca);   cosb= -(vbc*vab)( sum ,..)/(sbc*sab);   cosc= -(vca*vbc)( sum ,..)/(sca*sbc);   /* this formula is simple, but inaccurate for small triangles */    return  acos (cosa)+ acos (cosb)+ acos (cosc)- pi ; }    func  pt_solid2  (a,b,c) {   vab=  pt_cross (a,b);    sab=  sqrt ((vab*vab)( sum ,..));   vbc=  pt_cross (b,c);    sbc=  sqrt ((vbc*vbc)( sum ,..));   vca=  pt_cross (c,a);    sca=  sqrt ((vca*vca)( sum ,..));   da= 1./(sab*sca);   db= 1./(sbc*sab);   dc= 1./(sca*sbc);   cosa= -(vab*vca)( sum ,..)*da;   cosb= -(vbc*vab)( sum ,..)*db;   cosc= -(vca*vbc)( sum ,..)*dc;   sina=  pt_cross (vab,vca);    sina=  sqrt ((sina*sina)( sum ,..))*da;   sinb=  pt_cross (vab,vca);    sinb=  sqrt ((sinb*sinb)( sum ,..))*db;   sinc=  pt_cross (vab,vca);    sinc=  sqrt ((sinc*sinc)( sum ,..))*dc;    sabc=  abs (sina*(cosb*cosc-sinb*sinc)+cosa*(sinb*cosc+cosb*sinc));   cabc= 1. - cosa*(cosb*cosc-sinb*sinc) + sina*(sinb*cosc+cosb*sinc);   /* this formula is good for small triangles,     * bad for triangles with  area  near 2* pi  */    return 4.* asin (0.5*sabc/ sqrt (cabc*(1.+ sqrt (0.5*cabc)))); }
GX031-89-12977741	Using Hierarchical Shape Models to Spot Keywords in Cursive Handwriting Data         Abstract   Different instances of a handwritten word consist of the same basic features (humps, cusps, crossings, etc.) arranged in a deformable spatial pattern. Thus, keywords in cursive text can be detected by looking for the appropriate features in the ``correct'' spatial configuration. A keyword can be modeled hierarchically as a  set of word fragments, each of which consists of lower-level features. To allow flexibility, the spatial configuration of keypoints within a fragment is modeled using a Dryden-Mardia (DM) probability density over the shape of the configuration. In a writer-dependent test on a  transcription of the Declaration of Independence ($\sim$1300 words, $\sim$7500 characters), the method detected all eleven instances of the keyword ``government'' with only four false positives.      Keywords:   hierarchical models, cursive handwriting, keyword spotting, arrangement, spatial configuration, probabilistic shape models.
GX031-90-1929051	Recognition of Visual Object Classes         Abstract   Humans can look at a scene or a photograph and easily recognize objects. Outside my window I can see cars, people walking a dog on a brick pathway, trees, buildings, etc. This perception is so effortless that it belies the difficulty of the task. Visual  perception begins with light that is reflected from the scene into the eye. The light impinges upon the retina and is transduced by a two-dimensional array of photoreceptors into noisy electrical signals. The brain must then accomplish the difficult task of transforming from this low-level representation to a higher-level understanding of the scene in terms of regions, surfaces, textures, and objects.   For computer vision the problem is the same, but the hardware is different. A camera approximates the function of the eye and retina; that is, the camera produces a two-dimensional array of numbers (pixel values) representing the intensity of light reflected from the scene. The fundamental question addressed in this thesis is the following:  what mathematical processing should be applied to the pixel values in order for a computer to recognize objects?  The methods we propose are not intended as a model of human brain function, although they may provide some insight.  We are simply trying to solve the same visual recognition problems as the brain without concern for whether (or how) our algorithms could be realized in neuronal ``hardware.''   We have developed a new framework for recognizing visual object classes in which the class members consist of characteristic parts in a deformable spatial configuration. Human faces are an object class of  this type, since faces consist of eyes, nose, and mouth arranged in a configuration that varies depending on expression and pose and also from one person to another. A second object class is cursive handwriting, which consists of loops, cusps, crossings, etc.\ arranged in a deformable pattern. In our approach, the allowed object deformations are represented through shape statistics, which are learned from examples. Instances of an object in an image are detected by finding the appropriate features in the correct spatial configuration.  Our algorithm is robust with respect to partial occlusion, detector false alarms, and missed features.   Potential applications include intelligent tools for finding objects in image databases, human-machine interfaces, user authentication, intelligent data gathering and compression, signature verification, and keyword spotting.  Experimental results will be presented for two problems: (1) locating quasi-frontal views of human faces in cluttered scenes and with occlusions and (2) spotting keywords in on-line cursive handwriting data.        Keywords:   object recognition, matched filtering, principal components analysis, deformable, shape statistics, configurations, geometric invariants, face localization, occlusion, local photometry, handwriting, cursive, keyword spotting.
GX253-09-2189886	"M ISC ELLANEO US PU B LICAT10 N 236 1967 EDITION  NBS STANDARD FREQUENCY AND TIME SERVICES Radio Stations WWV WWVH WWVB WWVL  U.S. DEPARTMENT OF COMMERCE National Bureau of Standards   NBS STANDARD FREQUENCY  TIME SERVICES Radio Stations WWV WWVH WWVB WWVL  AND  U.S. DEPARTMENT OF COMMERCE Alexander B. Trowbridge, Acting Secretary NATIONAL BUREAU OF STANDARDS A. V. Astin, Director  MISCELLANEOUS PUBLICATION 236 - 1967 EDITION Issued 1967 For sale by the Superlntendent of Documents, US. Government Prlntlng Office Washington, D.C., 20402 -Price 15 Cents   Services Provided by NBS Standard Frequency Stations WWV, WWVH, WWVB, and WWVL Detailed descriptions are given of eight technical services provided by the National Bureau of Standards radio stations WWV, WWVH, WWVB, and WWVL. These services are: 1. Standard radio frequencies; 2. Standard audio frequencies ; 3. Standard musical pitch; 4. Standard time intervals; 5. Time signals; 6. UT2 corrections; 7. Radio propagation forecasts; and 8. Geophysical alerts. In order to provide users with the best possible services, occasional changes in the broadcasting schedules are required. This publication shows the schedules in effect on June 1, 1967. Annual revisions will be made. Advance notices of changes occurring between revisions will be sent to regular users of these services upon request.1 Current data relating to standard frequencies and time signals are also available monthly in the Time and Frequency Services Bulletin.2 Key Words : Broadcast of standard frequencies, high frequency, low frequency, standard frequencies, time signals, very low frequency.  1. Technical Services and Related Information The standard frequency stations of the National Bureau of Standards provide these services: (I)  1.1. Standard Radio Frequencies (a) Program  Station  0)  (I)  FI .-  L  &~~ 4 Ld  .*  .*  $ n WWV WWVH WWVB WWVL 192: 194t 196: 196:  Es 4 4 4  a  The NBS radio stations are located as follows: ""WWV Fort Collins, Colorado 80521 Leo Honea, Engineer-in-Charge Telephone-303-484-3164 (40 ""40'49""N, 105""02'27""W) WWVH Box 578, Puunene, Maui, Hawaii 96784 Sadami Katahara, Engineer-inCharge Telephone-79-41 11 (20 46'02""N, 156"" 27'42"" W) WWVB Box 83-E, Route 2, Fort Collins, Colorado 80521 Richard Carle, Engineer-in-Charge Telephone-303-484-2372 ( 4Oo40'28.3""N, 105""02'39.5""W) WWVL Box 83-E, Route 2, Fort Collins, Colorado 80521 Richard Carle, Engineer-in-Charge Telephone-303-484-2372 (40 40'51.3""N, 105003'00.0''W) O O  Station WWV broadcasts on nominal radio frequencies of 2.5, 5, 10, 15, 20, and 25 MHz. The broadcasts are continuous, night and day, except WWV is interrupted for 4 min each hour. The silent period commences at 45 min 15 see after each hour. (fig. 1.) Station WWVH broadcasts on nominal radio frequencies of 2.5, 5, 10, and 15 MHz. The broadcast is interrupted for approximately 4 min each hour. The silent period commences at 15 min (plus 0 to 15 see) after each hour. Station WWVB broadcasts on the standard radio frequency of 60 kHz. The service is continuous. Station WWVL broadcasts on the nominal radio frequency of 20 kHz. The service is continuous. (b) Accuracy  Since December 1, 1957, the standard radio transmissions from stations WWV and WWVH have been held as nearly constant as possible with 1-espect to the atomic frequency standards maintained and operated by the Radio Standards Laboratory of the National Bureau of Standards. Carefully made atomic standards have been shown to realize the ideal Cs resonance frequency, fCs, to a few parts in The present standard realizes this resonance to 5 parts in 10.l2 The number fcs = 9,192,631,770 Hz, originally measured with an uncertainty of 2 parts 1 Inquiries concerning the broadcast services may be addressed to the Engineer-in-Charge at a particular station or to Mr. David H. Andrews, Frequency-Time Broadcast Services, National Bureau of Standards, Boulder, Colo. 80302. Tel: 303-447-1000. ZInquiries concerning this Bulletin may be addressed to the Editor, Radio Physics Division, NBS, Boulder, Colo. 80302. Markowitz, Hall, Essen. and Parry-Frequency of cesium in terms of ephemeris time-Phys. Rev. Letters 1. 105 (1958).  `Note: On 1 December 1966, WWV was relocated to Fort Collins, Colorado, from its former site at Greenbelt, Maryland.  1   in log, is now defined as the exact value assigned to the atomic frequency standard to be used temporarily for the physical measure of time. This was officially announced by the International Committee of Weights and Measures at the XIIth General Conference in October 1964. On January 1, 1960 the NBS standard was brought into agreement with fc8 as quoted above by arbitrarily increasing its assigned value by 74.5 parts in 10 lo. Frequencies measured in terms of the NBS standard between December 1, 1957 and January 1, 1960 may be referred to the above value of fo8 and to the Ephemeris second by means of this relative correction 4. The frequencies transmitted by WWV are held stable to 2 parts in 10 l1 at all times. Deviations at WWV are normally less than 1 part in 10 l1 from day to day. Incremental frequency adjustments not exceeding 1 part in lo1' are made at WWV as necessary. Frequency adjustments made at WWVH do not exceed 5 parts in lolo. Changes in the propagation medium (causing Doppler effect, diurnal shifts, etc.) result at times in fluctuations in the carrier frequencies as received which may be very much greater than the uncertainties described above. WWVB and WWVL frequencies are normally stable to 2 parts in lot1. Deviations from day to day are within 1 part in 10 ll. The effects of the propagating medium on the received frequencies are much less at LF and VLF. The full transmitted accuracy may be obtained using appropriate receiving techniques. (c) Corrections  May 1958 and included data from December 1, 1957.5 The carrier frequency at WWVL (20 kHz) is also offset from standard frequency by the same amount noted above. While WWVB (60 kHz) initially transmitted with the offset frequency, sin& January 1, 1965 the frequency transmitted has been without offset. Thus, one of the NBS transmissions makes available to the users the standard of frequency so that absolute frequency comparisons may be made directly. This frequency will not be subject to annual offset change as are the other stations' frequencies.  1.2. Standard Audio Frequencies Standard audio frequencies of 440 Hz and 600 Hz are broadcast on each radio carrier frequency at WWV and WWVH. The audio frequencies are transmitted aIternately at 5-min intervals starting with 600 Hz on the hour (fig. 1). The first tone period at WWV (600 Hz) is of 3-min duration. The remaining periods are of 2-min duration. At WWVH all tone periods are of 3-min duration. WWVB and WWVL do not transmit standard audio frequencies. The accuracy of the audio frequencies, as transmitted, is the same as that of the carrier. The frequency offset mentioned under 1.1. (c) applies. Changes in the propagation medium will sometimes result in fluctuations in the audio frequencies as received. While 1000 Hz is not considered one of the standard audio frequencies, the time code which is transmitted 10 times an hour from WWV does contain this frequency and may be used as a standard with the same accuracy as the audio frequencies. The audio tones used for code information prior to the voice announcements are not standard frequencies. (b) Accuracy (a) Program  All carrier and modulation frequencies at WWV are derived from cesium controlled oscillators and at WWVH are derived from precision quartz oscillators. These frequencies are intentionally offset from standard frequency by a small but precisely known amount to reduce departure between the time signals as broadcast and astronomical time, UT2. The offset for 1960 was -150 parts in 10 lo; in 1962 and 1963 -130 parts in 10 lo; in 1964 and 1965 -150 parts in 10 lo; and in 1966 and 1967 -300 parts in 10 lo. Although UT2 is subject to unpredictable changes readily noted at this level of precision, a particular off set from standard frequency will remain in effect for the entire calendar year. Corrections to the transmitted frequency are continuously determined with respect to the NBS standard and are published monthly in the Proceedings of the IEEE. These commenced in `National standards of time and frequency in the United States, Proc. IRE 48, 105 (1960).  1.3. Standard Musical Pitch The frequency 440 Hz for the note A, above middle C, is the standard in the music industry in many countries and has been in the United States since 1925. The radio broadcast of this standard was commenced by the National Bureau of Standards in 1937. The periods of transmission of 440 Hz from WWV and WWVH are shown in figure 1. With this broadcast the standard pitch is maintained, and musical instruments are manufactured and adjusted in terms of this practical standard. The majority of musical instruments manufactured can be tuned to this frequency. Music listeners are thus benefited by the improvement in tuning accuracy. 6 W. D. George, WWV standard frequency transmissions. Proe, IRE 46, 910 (1958) and subsequent issues.  2  ~   THE HOURLY BROADCAST SCHEDULES OF WWV, WWVH, WWVB AND WWVL  SECONDS PULSES  -  WWV, WWVH WWVB WWVL  -  CONTINUOUS EXCEPT FOR 59@ SECOND OF EACH MINUTE AND DURING SILENT PERIODS SPECIAL TIME CODE NONE 100 PPS loo0 H t MODULATION WWV TIMING CODE  w-MORSE  STATION ANNOUNCEMENT  m-  m-  CODE- CALL LETTERS, UNIVERSAL TIME, PROPAGATION FORECAST VOICE - GREENWICH MEAN TIME MORSE CODE - FREOUENCY OFFSET (ON THE HOUR ONLY1 MORSE CODE- CALL LETTERS, UNIVERSAL TIME, VOICE - GREENWICH MEAN TIME MORSE CODE FREOUENCY OFFSET (ON THE HOUR ONLY I MORSE CODE- CALL LETTERS, FREOUENCY OFFSET  0 TONE  TONE MODULATION 600 Hz MODULATION 440 Ht  GEOALERTS lDENTIFICATION w A  -  s~ H ~l ~ ~  UT-2 TIME CORRECTION  6  SPECIAL TIME CODE  FIGURE The hourly broadcast schedules of WWV, WWVH, WWVB, and WWVL. 1.  3   WWV  AND  W m SECONDS PULSES  THE SPECTRA ARE COMPOSED OF DISCRETE FREQUENCY CMRMlENTS AT IMTERVALS OF I.0CPS. THE COMPONENTS AT THE SECTPAL MAXIMA HAVE AMPLITUDES OF 0.005 VOLT FOR A WLSE AMPLITUDE OF 1.0 U T . THE WWV PUSE CONSISTS OF FNE CYCLES OF loo0 CPS. THE WWVH WLSE CONSISTS OF SIX CYCLES OF 12OOCPS.  0.025 SEC.  TONE  FIGURE Sample characteristics of time pulses broadcast from NBS stations WWV & WWVH. 2.  1.4. Standard Time Intervals Seconds pulses at precise intervals are derived from the same oscillator that controls the radio carrier frequencies, e.g., they commence at intervals of 5,000,000 cycles of the 5 MHz carrier. They are given by means of double-sideband amplitude-modulation on each radio carrier frequency. Intervals of 1 min are marked by the omission of the pulse at the beginning of the last second of every minute and by commencing each minute with two pulses spaced by 0.1 second. The first pulse marks the beginning of the minute. The 2-min, 3-min, and 5-min intervals are synchronized with the seconds pulses and are marked by the beginning or ending of the periods when the audio frequencies are not transmitted. The pulse duration is 5 milliseconds. The pulse waveform is shown in figure 2. At WWV each pulse contains 5 cycles of 1000 Hz frequency. At WWVH the pulse consists of 6 cycles of 1200 Hz frequency. The pulse spectrum is composed of discrete frequency components at intervals of 1 Hz. The components have maximum amplitudes at approximately 995 Hz for WWV and 1194 Hz for WWVH pulses. The tone is interrupted 40 milliseconds for each seconds pulse. The pulse starts 10 milliseconds after commencement of the interruption. WWVB transmits seconds pulses continuously using a special time code described in section 1.10. WWVL does not transmit seconds markers, however, accurate time intervals may be obtained directly from the carrier using appropriate techniques. 1.5. Time Signals (a)  2 min before each hour at WWVH. They are resumed on the hour at WWV and WWVH, and at 5- and 10-minute intervals throughout the hour as indicated in figure 1. Universal Time (referenced to the zero meridian at Greenwich, England) is announced in International Morse Code each 5 min from WWV and WWVH. This provides a ready reference to correct time where a timepiece may be in error by a few minutes. The 0 to 24 hour system is used starting with 0000 at midnighi at longitude zero. The first two figures give the hour, and the last two figures give the number of minutes past the hour when the tone returns. For example, at 1655 UT, the four figures 1-6-5-5 are broadcast in code. The time announcement refers to the end of an announcement interval, Le., to the time when the audio frequencies are resumed. At station WWV a voice announcement of Greenwich Mean Time is given during the last half of every fifth minute during the hour. At 10:35 a.m., GMT, for instance, the voice announcement given in English is: ""National Bureau of Standards, WWV, Fort Collins, Colorado; next tone begins at ten hours, thirtyfive minutes Greenwich Mean Time."" At WWVH a similar voice announcement of Greenwich Mean Time occurs during the first half of every fifth minute during the hour. Time-of-day information is given from WWVB using the time code described in section 1.10. WWVL does not transmit time-of-day information. (b) Corrections  Program  The audio frequencies are interrupted at precisely 3 min before each hour at WWV and 4  Time signals broadcast from WWV and WWVH are kept in close agreement with UT2 (astronomical time) by making step adjustments of 100 milliseconds as necessary. These adjustments are made at 0000 UT on the first day of a month. Advance notice of such ad-   by the Bureau International de 1'Heure in Paris that an adjustment is to be made. Decision to adjust the time signals is based upon observations by a network of internationaS observatories and is made by an international committee. Corrections to the time signals are published periodically by the U.S. Naval Observatory. Seconds pulses broadcast from WWVB will depart from UT2 at a different rate due to the fact that WWVB broadcasts 60 kHz with no offset (see l.l(c)). Step time adjustments of 200 milliseconds will be made at 0000 UT on the first day of a month with appropriate advance notice. The Bureau International de l'Heure advises when such adjustments are to be made in order to maintain the secoads pulses within about 100 milliseconds of UT2.  justrrlents is given to the public upon advice  1.6. UT2 Corrections Since a majority of time users do not require UT2 information to better than 100 milliseconds the systems described in 1.5. (b) are quite satisfactory. An additional service is provided in cooperation with the US. Naval Observatory which makes available the best values of UT2 on a daily basis. Corrections to be applied to the time signals as broadcast are given in International Morse Code during the last half of the 19th min of each hour from WWV and during the last half of the 49th min of each hour from WWVH. Similar information is incorporated in the WWVB Time Code. The symbols which are broadcast are as follows: ""UT2"" then ""AD"" or ""SU"" followed by a three digit number. This number is the correction ih milliseconds. To obtain UT2, add the correction to the time indicated by the Time Signal pulse if ""AD"" is broadcast. Subtract if ""SU"" is broadcast. Thus, a clock keeping step with the time signals being broadcast will be fast with respect to UT2 if ""SU"" is the symbol used. The corrections necessary to obtain UT2 are derived from extrapolated data furnished by the U.S. Naval Observatory which indicate the variation in UT2 with respect to broadcast time. The probable error is -+ 3 milliseconds. Final data, with a probable error of -+ 1 millisecond, are published in the Time Service Bulletins of the Naval Observatory. These corrections will be revised daily, the new value appearing for the first time during the hour after OOOOUT, and will remain unchanged for the following 24 hour period. 1.7. Propagation Forecasts A forecast of radio propagation conditions is  broadcast in International Morse Code during the last half of every fifth minute of each hour on each of the standard frequencies from WWV. Propagation notices were first broadcast from WWV in 1946. The five-minute announcement was commenced on November 15, 1963. The present type of propagation forecast has been broadcast from WWV since July 1952 and was broadcast from WWVH from January 1954 until November 1964. The forecast announcement tells users the condition of the ionosphere at the regular time of issue and the radio quality to be expected during the next six hours. The NBS forecasts are based on data obtained from a worldwide network of geophysical and solar observatories. These data include radio soundings of the upper atmosphere, short wave reception data, observations of the geomagnetic field, solar activity and similar information. Trained forecasters evaluate the information and formulate the forecasts using known sun-earth relationships. The forecast announcements from WWV refer to propagation along paths in the North Atlantic Area, such as Washington, D.C. to London or New York City to Berlin. The announcements are the short term forecasts prepared by the ESSA, Telecommunications Space Disturbance Service Center, Box 178, Fort Belvoir, Virginia 22060. The regular times of issue of the forecasts are 0500, 1200 (1100 in summer), 1700 and 2300 UT. The forecast announcement is broadcast as a letter and a number. The letter portion identifies the radio quality at the time the forecast is made. The letters denoting quality are ""N,"" ""U,"" and ""W"" signifying, respectively, that radio propagation conditions are either normal, unsettled or disturbed. The number portion of the forecast announcement from WWV is the forecast of radio propagation quality on a typical North Atlantic path during the six hours after the forecast is issued. Radio quality is based on the ITSA 1 to 9 scale which is defined as follows: Disturbed Unsettled Normal grades (W) grade (U) grades (N) 1. useless 5. fair 6. fair-to-good 2. verypoor 7. good 3. poor 8. verygood 4. poor-to-fair 9. excellent If, for example, propagation conditions are normal at the time the forecast is issued but are expected to become ""poor-to-fair"" during the next six hours, the forecast announcement would be broadcast as N4 in International Morse Code. 1.8. Geophysical Alerts A letter symbol indicating the current geo5   z  100 PPS CODE  lLiPANDL0 TIME  SCALE1  I  38 BINARY DIGIT, 100 PPS CODE (1000 CPS CARRIER) IWlT""0""T  oc  VDI  8*@EI  UARKllSI  FIGURE Chart of time code transmissions from NBS radio station WWV. 3.  physical alert (Geoalert) as declared by the World Warning Agency of the International Ursigram and World Days Service (IUWDS) is broadcast in very slow International Morse Code from WWV and WWVH on each of the standard radio carrier frequencies. These broadcasts are made from WWV during the first half of the 19th min of each hour and from WWVH during the first half of the 49th min of each hour. Such notices have been broadcast since the International Geophysical Year, 1957-58, and have continued by international agreement. The symbol indicates to experimenters and researchers in radio, geophysical and solar sciences the content of the IUWDS Geoalert message which is issued daily at 0400 UT to identify days on which outstanding solar or geophysical events are expected or have occurred in the preceding 24-hour period. There are five types of Geoalerts which may be used and thus there are five letter symbols used to identify them plus a sixth symbol to signify that there is no Geoalert. The various letter symbols used and the type of Geoalert to which each refers are as follows: C-Cosmic event M-Magnetic storm S-Soflare proton flare W-Stratwarm N-Soflare flares E-No geoalert The Geoalert broadcast is identified by the letters GEO in International Morse Code preceding one of the above letter symbols. The letter symbol is repeated 5 times to insure proper identification. If, for example, soflare proton flares are expected or have been ob6  served, the Geoalert broadcast would be GEO SSSSS to signify this fact. If a significant magnetic storm is expected or exists the broadcast would be GEO MMMMM. The no alert symbol, sent as GEO EEEEE signifies that any preceding Geoalert may be considered finished and that there is no alert in progress. Since it is possible that two types of Geoalerts could be in effect at the same time, the symbols will be broadcast in the following priority order : C, M, S, W, N or E. 1.9. WWV Time Code  On January 1,1961 WWV commenced broadcasting the time code shown in figure 3 for one minute out of each five, ten times an hour, as shown in figure 1. This time code provides a standardized timing base for use when scientific observations are made simultaneously at widely separated locations. It may be used, for instance, where signals telemetered from a satellite are recorded along with the time code; subsequent analysis of the data is then aided by having unambiguous time markers accurate to a thousandth of a second. Astronomical observations may also benefit by the increased timing potential provided by the pulse-coded signals. The code format being broadcast is generally known as the NASA 36-Bit Time Code. The code is produced at a 100 pps rate and is carried on 1000 Hz modulation. The code contains time-of-year information (Universal Time) in seconds, minutes, hours and day of year. The code is synchronous with the frequency and time signals.   The binary coded decimal (BCD) system is used. Each second contains 9 BCD groups in this order: 2 groups for seconds. 2 groups for minutes, 2 groups for hours, and 3 groups for day of year. The code digit weighting is 1-24-8 for each BCD group multiplied by 1, 10, or 100 as the case may be. A complete time frame is 1 second. The binary groups follow the 1 second reference marker. ""On time"" occurs at the leading edge of all pulses. The code contains 100/second clocking rate, 10/second index markers, and a l/second reference marker. The 1000 Hz is synchronous with the code pulses so that millisecond resolution is obtained readily. The 10/second index markers consist of ""binary one"" pulses preceding each code group except at the beginning of the second where a ""binary zero"" pulse is used. The l/second reference marker consists of five ""binary one"" pulses followed by a ""binary zero"" pulse. The second begins at the leading edge of the ""binary zero"" pulse. The code is a spaced code format, that is, a binary group follows each of the 10/second index markers. The last index marker is followed by an unused 4-bit group of ""binary zero"" pulses just preceding the l/second reference marker. A ""binary zero"" pulse consists of 2 cycles of 1000 Hz amplitude modulation, and the ""binary one"" pulse consists of 6 cycles of 1000 Hz amplitude modulation. The leading edges of the time code pulses coincide with positivegoing zero-axis-crossings of the 1000 Hz modulating frequency.  from left to right.) Each marker is generated by reducing the power of the carrier by 10 dB at the beginning of the corresponding second and restoring it 0.2 second later for an uncoded marker or binary ""zero"", 0.5 second later for a binary ""one"", and 0.8 second later for a `10-second position marker or for a minute reference marker. Several examples of binary ""ones"" are indicated by I in figure 4. (c) Marker Order and Groups  curring during each second. (Time progresses  60 markers each minute, with one marker oc-  The 10-second position markers, labeled PO to P5 on the diagram, occur respectively in the 60th, loth, 20th, 30th, 40th, and 50th seconds of each minute. The minute reference marker occurs in the 1st second of the minute. Uncoded markers occur periodically in the 5th, 15th, 25th, 35th, 45th, and 55th seconds of each minute, and also in the llth, 12th, 21st, 22nd, 36th, 56th, 57th, 58th, and 59th seconds. Thus, every minute contains twelve groups of five markers, each group ending either with a position marker or an uncoded marker. The signal pulses lasting for 0.2 seconds after a position marker are shown blackened in figure 4; the signal pulses lasting for 0.8 second after a periodically uncoded marker are shaded ; other signal pulses following uncoded markers are labelled with a U. Save for the uncoded and reference markers specifically excepted in the foregoing, the remaining markers in each of the groups are utilized to convey additional information. (d) Information Sets  1.10. WWVB Time Code (a) Code and Carrier  On July 1, 1965, Radio Station WWVB, Fort Collins, Colorado, began broadcasting time information using a level-shift carrier time code. The code, which is binary coded decimal (BCD), is broadcast continuously and is synchronized with the 60 kHz carrier signal. The new system replaces the method whereby seconds pulses of uniform width obtained by level-shift carrier keying were broadcast. The carrier is no longer interrupted for keyed station identification, since the characteristic phase advance by 45"" at 10 minutes after every hour followed by a similar phase retardation 5 minutes later continues to serve to identify the station. (b) Marker Generation  As shown in figure 4, the signal consists of  Each minute the code presents time-of-year information in minutes, hours, and day of the year and the actual milliseconds difference between the time as broadcast and the best known estimate of UT2. A set of groups, containing the first two BCD groups in the minute, specifies the minute of the hour; the third and fourth BCD groups make up a set which specifies the hour of the day; the fifth, sixth, and seventh groups form a set which specifies the day of the year; a set, made up of the ninth, tenth and eleventh BCD groups, specifies the number of milliseconds to be added to or subtracted from the code time as broadcast in order to obtain UT2. The relationship of the UT2 scale to the time as coded is indicated in the eighth group. If UT2 is ""slow"" with respect to the code time, a binary ""one"", labeled SUB (subtract) in figure 4, will be broadcast in the eighth group during the 38th second of the minute. If UT2 is ""fast"" with respect to the code time, binary ""ones"", labeled ADD, will be broadcast  7   TIME-  TIME FRAME I MINUTE (INDEX COUNT I SECOND)  -l  /---REFERENCE  TIME  REFERENCE MARKER FOR MINUTES  ik  0.2 SECOND -BINARY ""ZERO"" (TYPICAL)  I 40 20 10 8  PI  I 2010  I 8 4 2 1  P2  4  2  I  ~MINUTES SET 1  -TIME  -  HOURS SET --  -l  P2  I 200100  I  I P3 I 8  80 4 0 20 10  DAYS SET TIME AT THIS POINT EQUALS 258 DAYS, 18 HOURS, -. 42 MINUTES, 35 SECONDS. TO OBTAIN THE CORRESPONDING UT2 SCALE READING, SUBTRACT 41 MILLISECONDS. 7  /// 4  u u lmUU m I 2 1  P4 ADD~~~.ADD ,  r  UT2 RELATIONSHIP  -TIME50 I 1 I l i I l l I 1  40  I  1  1  1  I  60 1 1 1 I  p4  400 100 800 200  U U l.llHA tlUULll U U I P5 80402010 UT2 SET -DIFFERENCE OF UT2 FROM CODED TIME IN MILLISECONDS 8 4  it  0.8 SECOND POSITION IDENTIFIER (TY PlCAL )  WAJuUuUuU'I I 2  PO  I GROUP  2  MARKER  i  POSITION MARKER AND PULSE  PERIODIC UNCODED MARKER AND PULSE  UNCODED MARKER AND PULSE  BINARY ""ONE""  L  BINARY ""ZERO""  FIGURE Chart of time code transmissions from NBS station WWVB. 4.  8   in the eighth group during the 37th and 39th seconds of the minute. The twelfth group is not used to convey information. (e)  1.12. Station Identification  Digital Information  When used to convey numerical information, the four coded markers used as digits in a BCD group are indexed 8-4-2-1 in that order. Sometimes only the last two or three of the coded markers in a group are needed, as in the first groups in the minutes, hours, and days sets. In these cases the markers are indexed 2-1, or 4-2-1, accordingly. The indices of the first group in each set which contains two groups are multiplied by 10, those of the second group of such a set are multiplied by 1. T'he indices of the first group in each set which contains three groups are multiplied by 100, those of the second group by 10, and those of the third group by 1. Example  WWVL identifies by International Morse Code durinq the lst, 21st, and 41st min of each hour.* WWVB identifies by its unique Time Code (see section 1.10) and by advancing the carrier phase 45"" at 10 min after each hour and returning to normal phase at 15 min after each hour. 1.13. Radiated Power, Antennas and Modulation (a) Radiated Power -  minutes.  WWV and WWVH identify by International Morse Code and voice (in English) every five  A specific example is inaicated in figure 4. The occurrence of two binary ""ones"" in the ""minutes set"" indicates that the minute contemplated is the 40 + 2 = 42nd minute. Similarly, the two binary ""ones"" in the ""hours set"" indicate the 10 + 8 = 18th hour of the day, while the four binary ""ones"" in the ""days set"" indicate the 200 40 10 8 = 258th day of the year. It is seen from the ""UT2 Relationship"" group and the ""UT2 set"" that one should subtract, from any second in this minute, 40 + 1 = 41 milliseconds to get the best estimate of UT2. For example, the 35th UT2 interval would end 41 milliseconds later than the end of the 35th second; or, in other words, the TJT2 scale reading for the end of the 35th second would be 18 42 ""' 34. 959 since 35.000-0.041 = 34.959.  Freauencv. "", MHz 0.020 0.060 2.5 5 10 15 20 25  I  Radiated Dower. kw WWV WWVH -  2.5 10 10 10 2.5 2.5  -  -  1 2 2 2 -  +  ++  (b) Transmitting Antennas  The broadcasts on 2.5 and 5 MHz from WWVH are from vertical quarter-wave antennas. The broadcasts on all other frequencies from WWV and WWVH are from vertical halfwave dipoles. The antennas are omnidirectional. The antennas used by WWVB and WWVL are 400-foot high vertical antennas with capacity toploading. (0)  Modulation  1.11. Offset Frequencies  WWV, WWVH, and WWVL transmit reminders of the fact that all transmitted frequencies are offset from nominal by a fixed amount. International Morse Code symbols for M300 are transmitted from WWV and WWVH immediately following the ""on-the-hour"" voice announcement. WWVL transmits International Morse Code for Minus 300 following the station call sign repeated three times. This is transmitted during the lst, 21st, and 41st min of each hour.* Since WWVB is transmitting standard frequency no offset reminder is given.  At WWV and WWVH all modulation is double sideband amplitude, with 75 percent on the steady tones and 100 percent peak for seconds pulses and voice. WWVB employs 10 dB carrier-level reduction for transmitting time information (see section 1.10). WWVL uses no amplitude modulation. Various experimental techniques are being studied in an attempt to develop a good timing system at Very Low Frequencies. -A WWVL frequently alternates between 20.0 kHz and 19.9 or 20.5 kHz, the change being made every 10 seconds. During these experiments the code transmissions are not given.  9   c'  10   2. How NBS Controls the Transmitted Frequencies In figure 5 a simplified diagram of the NBS frequency control system is shown. The entire system depends upon the basic frequency reference shown in this diagram as the Cesium (Cs) Beam. This standard is used to calibrate the oscillators, dividers and clocks which generate the controlled frequency and the NBS time scales. Information from this reference is provided to receivers which monitor the WWVB transmissions and compare the received phase with the standard phase. If an error exists between the reference and received phases a signal is then transmitted by a 50 MHz transmitter to the transmitting site at Fort Collins which in turn operates automatic phase correction equipment to correct the transmitted  WWVH is performed manually at present based upon signals from WWVB and WWVL which are received by LF and VLF phase-lock receivers. The oscillator controlling the trans-  The control of the signals transmitted from  ously compared with the LF and VLF signals. Adjustments are then made to the controlling oscillator manually which compensate for the characteristic drift of crystal oscillators. To assure that systematic errors do not enter into the system the NBS time scale is compared with the transmitting station clocks by the use of a very precise portable clock. With these of a second can be attained.  mitted frequencies and time signals is continu-  phase.  clocks time synchronization to a few millionths  THE NATIONAL BUREAU OF STANDARDS The National Bureau of Standards1 provides measurement and technical information services essential to the efficiency and effectiveness of the work of the Nation's scientists and engineers. The Bureau serves also as a focal point in the Federal Government for assuring maximum application of the physical and engineering sciences to the advancement of technology in industry and commerce. To accomplish this mission, the Bureau is organized into three institutes covering broad program areas of research and services :  THE INSTITUTE FOR MATERIALS RESEARCH . . . conducts materials research and provides  . . . provides the central basis within the United States for a complete and consistent system of physical measurements, coordinates that system with the measurement systems of other nations, and furnishes essential services leading to accurate and uniform physical measurements throughout the Nation's scientific community, industry, and commerce. This Institute comprises a series of divisions, each serving a classical subject matter area: Mathematics-Electricity-Metrology-Mechanics-Heat-Atomic Physics-Physical -Applied Chemistry-Radiation Physics-Laboratory Astrophysics2-Radio Standards Laboratory,' which includes Radio Standards Physics and Radio Standards Engineering-Office of Standard Reference Data. THE INSTITUTE FOR BASIC STANDARDS  THE INSTITUTE FOR APPLIED TECHNOLOGY . . . provides technical services to promote the use of available technology and to facilitate technological innovation in industry and government. The principal elements of this Institute are: -Building Research-Electronic Instrumentation-Technical Analysis-Center for Computer Sciences and Technology-Textile and Apparel Technology Center-Office of Weights and Measures -Office of Engineering Standards Services-Office of Invention and Innovation-Office of Vehicle Systems Research-Clearinghouse for Federal Scientific and Technical Information3-Materials Evaluation Laboratory-NBS/GSA Testing Laboratory. 1  associated materials services including mainly reference materials and data on the properties of materials. Beyond its direct interest to the Nation's scientists and engineers, this Institute yields services which are essential to the advancement of technology in industry and commerce. This Institute is organized primarily by technical fields : -Analytical Chemistry-Metallurgy-Reactor Radiations-Polymers-Inorganic Materials-Cryogenics2-Office of Standard Reference Materials.  Headquarters and Laboratories at Gaithersburg, Maryland, unless otherwise noted; mailing address Washington, D. C.,  20234.  2 3  Located at Boulder, Colorado, 80302. Located at 5285 Port Royal Road, Springfield, Virginia 22151.  11 GPO : 1967 0  -  265-984   US. DEPARTMENT OF COMMERCE WASHINGTON, D.C.  POSTAGE AND FEES PAID  20230  U.S.  DEPARTMENT OF COMMERCE  OFFICIAL BUSINESS  NBS Fort Collins facility in upper photo, showing the WWVB and WWVL transmitter building in the center, new 470-foot standby antenna mast in center, and 400-foot main masts on each side which are part of the two, four-mast antenna systems, WWVL to the left and WWVB to the right. At lower left are WWV transmitter building and antennas at Ft. Collins, Colorado. At lower right are antennas, transmitter building, and administrative buildings for WWVH, Maui, Hawaii."
GX012-35-4699390	"Linux & AI/Alife  mini-HOWTO     Version 2.0   This document is maintained by  John A. Eikenberry    The master page for this document is   http://www.ai.uga.edu/~jae/ai.html     Last modified: Tue Jan 13 17:23:21 EST 1998             Table of Contents     What's New   Introduction     Purpose   Finding the Software   Updates and Comments     Programming Languages   Traditional Artificial Intelligence     AI class/code libraries   AI software kits, applications, etc.     Connectionism  (neural nets)    Connectionist Class/Code Libraries   Connectionist Applications     Evolutionary Computing (EC)  (genetic algorithms/programming)    EC Class/Code Libraries   EC Applications     Artificial Life     Alife class/code libraries   Alife software kits, applications, etc.     Autonomous Agents and Bots   AI & Alife related newsgroups   AI & Alife resource links           What's New   Note: For a list of new additions to this mini-howto, please visit the   master page  listed above.   (1.11.98)Thanks to  Dirk Heise  for his submission of Gas. I've also removed a couple more of the non-AI oriented scheme implementations. I just felt having 10 scheme implementations listed was a bit silly, especially  since some of them weren't really meant for AI stuff. If you want a listing of more scheme's look  here  or  here .   (12.29.97) Added a couple more entries and web links, including a patched  lil-gp.    (10.1.97) Ok. I struck the motherload of Baysean Networks, see the  Connectionism section for more. Plus I cleaned off the 'New' flags  from anything before August.    (9.26.97) The reformat is done! :) If you are reading this somewhere other than the master page, please    take a look  and  let me know what you  think . Now I can get back to work on my thesis...         Purpose   The Linux OS has evolved from its origins in hackerdom to a full blown UNIX, capable of rivaling any commercial UNIX.  It now provides an inexpensive base to build a great workstation.  It has shed its hardware dependencies, having been ported to DEC Alphas, Sparcs,  PowerPCs, with others on the way.  This potential speed boost along with its networking support will make it great for workstation clusters.  As a workstation it allows for all sorts of research and development, including artificial intelligence and artificial life.   The purpose of this Mini-Howto is to provide a source to find out about various software packages, code libraries, and anything else that will help someone get started working with (and find resources for) artificial intelligence and artificial life.  All done with Linux specifically in mind.       Where to find this software   All this software should be available via the net (ftp || http).  The links to where to find it will be provided in the description of each package.  There will also be plenty of software not covered on these pages (which is usually platform independent) located on one of the  resources listed under the  Web Links  section.      Updates and comments    If you find any mistakes, know of updates to one of the items below, or have problems compiling and of the applications, please mail me at:  jae@ai.uga.edu  and I'll see what I can do.      If you know of any AI/Alife applications, class libraries, etc.  Please    email me  about them. Include your name, ftp and/or http sites where they can be found, plus a brief overview/commentary on the software (this info would make things a lot easier on me... but don't feel obligated ;).     I know that keeping this list up to date and expanding it will take quite a bit of work. So please be patient (I do have other projects). I hope you  will find this document helpful.                  Programming languages           While any programming language can be used for artificial     intelligence/life research, these are programming languages which     are used extensively for, if not specifically made for, artificial     intelligence programming.                               ECoLisp  [New]   Web site:  www.di.unipi.it/~attardi/software.html                    ECoLisp (Embeddable Common Lisp) is an implementation of  Common Lisp designed for being embeddable into C based  applications. ECL uses standard C calling conventions for Lisp  compiled functions, which allows C programs to easily call  Lisp functions and viceversa. No foreign function interface is  required: data can be exchanged between C and Lisp with no  need for conversion. ECL is based on a Common Runtime Support  (CRS) which provides basic facilities for memory managment,  dynamic loading and dumping of binary images, support for  multiple threads of execution. The CRS is built into a library  that can be linked with the code of the application. ECL is  modular: main modules are the program development tools (top  level, debugger, trace, stepper), the compiler, and CLOS. A  native implementation of CLOS is available in ECL: one can  configure ECL with or without CLOS. A runtime version of ECL  can be built with just the modules which are required by the  application. The ECL compiler compiles from Lisp to C, and  then invokes the GCC compiler to produce binaries.                         Gödel   Web page:  www.cs.bris.ac.uk/~bowers/goedel.html                        Gödel is a declarative, general-purpose programming language  in the family of logic programming languages.  It is a strongly typed  language, the type system being based on many-sorted logic with  parametric polymorphism.  It has a module system.  Gödel supports  infinite precision integers, infinite precision rationals, and also  floating-point numbers.  It can solve constraints over finite domains  of integers and also linear rational constraints. It supports  processing of finite sets.  It also has a flexible computation rule  and a pruning operator which generalizes the commit of the concurrent  logic programming languages.  Considerable emphasis is placed on  Gödel's meta- logical facilities which provide significant  support for meta-programs that do analysis, transformation,  compilation, verification, debugging, and so on.                             LIFE        Web page:  www.isg.sfu.ca/life                  LIFE (Logic, Inheritance, Functions, and Equations) is an       experimental programming language proposing to integrate three       orthogonal programming paradigms proven useful for symbolic       computation.  From the programmer's standpoint, it may be perceived as       a language taking after logic programming, functional programming, and       object-oriented programming.  From a formal perspective, it may be       seen as an instance (or rather, a composition of three instances) of a       Constraint Logic Programming scheme due to Hoehfeld and Smolka       refining that of Jaffar and Lassez.                         CLisp ( Lisp )        FTP site:  sunsite.unc.edu/pub/Linux/devel/lang/lisp/                    CLISP is a Common Lisp implementation by Bruno Haible and Michael       Stoll.  It mostly supports the Lisp described by           'Common LISP: The Language (2nd edition)'  and the ANSI Common Lisp       standard.  CLISP includes an interpreter, a byte-compiler, a large       subset of CLOS (Object-Oriented Lisp) , a foreign language interface       and, for some machines, a screen editor.               The user interface language (English, German, French) is chosen at       run time.  Major packages that run in CLISP include CLX & Garnet.       CLISP needs only 2 MB of memory.                         CMU Common  Lisp        Web page:  www.mv.com/users/pw/lisp/index.html        FTP site:  sunsite.unc.edu/pub/Linux/devel/lang/lisp/                     CMU Common Lisp is a public domain ""industrial strength"" Common       Lisp programming environment. Many of the X3j13 changes have been       incorporated into CMU CL. Wherever possible, this has been done so as       to transparently allow the use of either CLtL1 or proposed ANSI       CL. Probably the new features most interesting to users are SETF       functions, LOOP and the WITH-COMPILATION-UNIT macro.                         GCL ( Lisp )        FTP site:  sunsite.unc.edu/pub/Linux/devel/lang/lisp/                     GNU Common Lisp (GCL) has a compiler and interpreter for Common       Lisp.  It used to be known as Kyoto Common Lisp.  It is very portable       and extremely efficient on a wide class of applications.  It compares       favorably in performance with commercial Lisps on several large       theorem-prover and symbolic algebra systems. It supports the CLtL1       specification but is moving towards the proposed ANSI definition.  GCL       compiles to C and then uses the native optimizing C compilers (e.g.,       GCC).  A function with a fixed number of args and one value turns into       a C function of the same number of args, returning one value, so GCL       is maximally efficient on such calls.  It has a conservative garbage       collector which allows great freedom for the C compiler to put Lisp       values in arbitrary registers.               It has a source level Lisp debugger for interpreted code, with display       of source code in an Emacs window.  Its profiling tools (based on the       C profiling tools) count function calls and the time spent in each       function.                           Mercury        Web page:  www.cs.mu.oz.au/research/mercury/                     Mercury is a new, purely declarative logic programming language.       Like Prolog and other existing logic programming languages, it is a       very high-level language that allows programmers to concentrate on the       problem rather than the low-level details such as memory management.       Unlike Prolog, which is oriented towards exploratory programming,       Mercury is designed for the construction of large, reliable, efficient       software systems by teams of programmers. As a consequence,       programming in Mercury has a different flavor than programming in       Prolog.                         DFKI  OZ        Web page:  www.ps.uni-sb.de/oz/        FTP site:  ps-ftp.dfki.uni-sb.de/pub/oz2/                     Oz is a high-level programming language designed for concurrent       symbolic computation.  It is based on a new computation model       providing a uniform and simple foundation for several programming       paradigms, including higher-order functional, constraint logic, and       concurrent object-oriented programming.  Oz is designed as a successor       to languages such as Lisp, Prolog and Smalltalk, which fail to support       applications that require concurrency, reactivity, and real-time       control.               DFKI Oz is an interactive implementation of Oz featuring a programming       interface based on GNU Emacs, a concurrent browser, an object-oriented       interface to Tcl/Tk, powerful interoperability features (sockets, C,       C++), an incremental compiler, a garbage collector, and support for       stand-alone applications.  Performance is competitive with commercial       Prolog and Lisp systems.                         BinProlog        Web site(documentation):  clement.info.umoncton.ca/BinProlog/UNCOMPRESSED/doc/html/art.html        FTP site(source):  clement.info.umoncton.ca/BinProlog        FTP site(binary):  clement.info.umoncton.ca/BinProlog/UNCOMPRESSED/bin                     BinProlog is a fast and compact Prolog compiler, based on the       transformation of Prolog to binary clauses. The compilation technique       is similar to the Continuation Passing Style transformation used in       some ML implementations. BinProlog 5.00 is also probably the first       Prolog system featuring dynamic recompilation of asserted predicates       (a technique similar to the one used in some object oriented languages       like SELF 4.0), and a very efficient segment preserving copying heap       garbage collector.               Although it (used to) incorporate some last minute research       experiments, which might look adventurous at the first sight,       BinProlog is a fairly robust and complete Prolog implementation       featuring both C-emulated execution and generation of stand-alone       applications by compilation to C.                         SWI  Prolog         Web page:    swi.psy.uva.nl/projects/xpce/SWI-Prolog.html         FTP site:    swi.psy.uva.nl/pub/SWI-Prolog/                     SWI is a free version of prolog in the Edinburgh Prolog family       (thus making it very similar to Quintus and many other versions).       With: a large library of built in predicates, a module system, garbage       collection, a two-way interface with the C language, plus many other       features. It is meant as a educational language, so it's compiled code       isn't the fastest. Although it similarity to Quintus allows for easy       porting.               XPCE is freely available in binary form for the Linux version of SWI-prolog.       XPCE is an object oriented X-windows GUI development package/environment.                         Kali Scheme        Web site:  www.neci.nj.nec.com/PLS/Kali.html                     Kali Scheme is a distributed implementation of Scheme that       permits efficient transmission of higher-order objects such as       closures and continuations. The integration of distributed       communication facilities within a higher-order programming       language engenders a number of new abstractions and paradigms       for distributed computing. Among these are user-specified       load-balancing and migration policies for threads,       incrementally-linked distributed computations, agents, and       parameterized client-server applications. Kali Scheme supports       concurrency and communication using first-class procedures and       continuations. It integrates procedures and continuations into a       message-based distributed framework that allows any Scheme       object (including code vectors) to be sent and received in a       message.                   RScheme        Web site: www.rosette.com/~donovan/rs/rscheme.html        FTP site:  ftp.rosette.com/pub/rscheme                     RScheme is an object-oriented, extended version of the Scheme       dialect of Lisp. RScheme is freely redistributable, and offers       reasonable performance despite being extraordinarily portable.       RScheme can be compiled to C, and the C can then compiled with a       normal C compiler to generate machine code. By default, however,       RScheme compiles to bytecodes which are interpreted by a       (runtime) virtual machine. This ensures that compilation is fast       and keeps code size down. In general, we recommend using the       (default) bytecode code generation system, and only compiling       your time-critical code to machine code. This allows a nice       adjustment of space/time tradeoffs.  (see web site for details)                         Scheme 48         Web site:  www-swiss.ai.mit.edu/~jar/s48.html  (download from ftp site)        FTP site:  swiss-ftp.ai.mit.edu:/archive/s48/                     Scheme 48 is a Scheme implementation based on a virtual machine       architecture. Scheme 48 is designed to be straightforward, flexible,       reliable, and fast. It should be easily portable to 32-bit       byte-addressed machines that have POSIX and ANSI C support.  In       addition to the usual Scheme built-in procedures and a development       environment, library software includes support for hygienic macros (as       described in the Revised^4 Scheme report), multitasking, records,       exception handling, hash tables, arrays, weak pointers, and FORMAT.       Scheme 48 implements and exploits an experimental module system       loosely derived from Standard ML and Scheme Xerox.  The development       environment supports interactive changes to modules and interfaces.                         SCM ( Scheme )         Web site:  www-swiss.ai.mit.edu/~jaffer/SCM.html        FTP site:  swiss-ftp.ai.mit.edu:/archive/scm/                     SCM conforms to the Revised^4 Report on the Algorithmic Language       Scheme and the IEEE P1178 specification. Scm is written in C. It uses       the following utilities (all available at the ftp site).            SLIB (Standard Scheme Library) is a portable Scheme    library which is intended to provide compatibility and utility    functions for all standard Scheme implementations, including    SCM, Chez, Elk, Gambit, MacScheme, MITScheme, scheme->C,    Scheme48, T3.1, and VSCM, and is available as the file    slib2c0.tar.gz. Written by Aubrey Jaffer.     JACAL is a symbolic math system written in Scheme, and is    available as the file jacal1a7.tar.gz.   Interfaces to standard libraries including REGEX string regular    expression matching and the CURSES screen management package.   Available add-on packages including an interactive debugger, database,    X-window graphics, BGI graphics, Motif, and Open-Windows packages.   A compiler (HOBBIT, available separately) and dynamic linking of    compiled modules.                                 Shift  [New]        Web site:  www.path.berkeley.edu/shift/                         Shift is a programming language for describing dynamic       networks of hybrid automata.  Such systems consist of       components which can be created, interconnected and destroyed       as the system evolves. Components exhibit hybrid behavior,       consisting of continuous-time phases separated by       discrete-event transitions. Components may evolve       independently, or they may interact through their inputs,       outputs and exported events. The interaction network itself       may evolve.                                             Traditional Artificial Intelligence   Traditional AI is based around the ideas of logic, rule  systems, linguistics, and the concept of rationality.  At its  roots are programming languages such as Lisp and Prolog.  Expert systems are the largest successful example of this  paradigm.  An expert system consists of a detailed knowledge  base and a complex rule system to utilize it.  Such systems  have been used for such things as medical diagnosis support  and credit checking systems.             AI class/code libraries       These are libraries of code or classes for use in programming within     the artificial intelligence field.  They are not meant as stand alone     applications, but rather as tools for building your own applications.                             AI Search   FTP site:  ftp.icce.rug.nl/pub/peter/   Submitted by:  Peter M. Bouthoorn                   Basically, the library offers the programmer a set of search  algorithms that may be used to solve all kind of different  problems. The idea is that when developing problem solving software  the programmer should be able to concentrate on the representation of  the problem to be solved and should not need to bother with the  implementation of the search algorithm that will be used to actually  conduct the search. This idea has been realized by the implementation  of a set of search classes that may be incorporated in other software  through  C++ 's features of derivation and inheritance.  The  following search algorithms have been implemented:     - depth-first tree and graph search.   - breadth-first tree and graph search.   - uniform-cost tree and graph search.   - best-first search.   - bidirectional depth-first tree and graph search.   - bidirectional breadth-first tree and graph search.   - AND/OR depth tree search.   - AND/OR breadth tree search.      Peter plans to release a new version of the library soon, which will  also be featured in a book about C++ and AI to appear this year.                          Chess In Lisp (CIL)        FTP site:  chess.onenet.net/pub/chess/uploads/projects/                         The CIL (Chess In Lisp) foundation is a Common Lisp       implementaion of all the core functions needed for development       of chess applications.  The main purpose of the CIL project is       to get AI researchers interested in using Lisp to work in the       chess domain.                        DAI  [New]        Web site:  starship.skyport.net/crew/gandalf/DNET/AI                     A library for the Python programming language that provides an       object oriented interface to the CLIPS expert system tool. It        includes an interface to COOL (CLIPS Object Oriented Language)       that allows:           Investigate COOL classes    Create and manipulate with COOL instances    Manipulate with COOL message-handler's    Manipulate with Modules                                   Nyquist  [New]   Web site:  www.cs.cmu.edu/afs/cs.cmu.edu/project/music/web/music.html                         The Computer Music Project at CMU is developing computer music       and interactive performance technology to enhance human musical       experience and creativity. This interdisciplinary effort draws       on Music Theory, Cognitive Science, Artificial Intelligence and       Machine Learning, Human Computer Interaction, Real-Time Systems,       Computer Graphics and Animation, Multimedia, Programming       Languages, and Signal Processing. A paradigmatic example of       these interdisciplinary efforts is the creation of interactive       performances that couple human musical improvisation with       intelligent computer agents in real-time.                   Screamer        Web site:  www.cis.upenn.edu/~screamer-tools/home.html                     Screamer is an extension of Common Lisp that adds support for       nondeterministic programming. Screamer consists of two       levels. The basic nondeterministic level adds support for       backtracking and undoable side effects.  On top of this       nondeterministic substrate, Screamer provides a comprehensive       constraint programming language in which one can formulate and       solve mixed systems of numeric and symbolic       constraints. Together, these two levels augment Common Lisp with       practically all of the functionality of both Prolog and       constraint logic programming languages such as CHiP and CLP(R).       Furthermore, Screamer is fully integrated with Common       Lisp. Screamer programs can coexist and interoperate with other       extensions to Common Lisp such as CLOS, CLIM and Iterate.                                         AI software kits, applications, etc.           These are various applications, software kits, etc. meant for research     in the field of artificial intelligence. Their ease of use will vary,     as they were designed to meet some particular research interest more     than as an easy to use commercial package.                           ASA - Adaptive Simulated Annealing   Web site:  www.ingber.com/#ASA-CODE   FTP site:  ftp.ingber.com/                    ASA (Adaptive Simulated Annealing) is a powerful global  optimization C-code algorithm especially useful for nonlinear and/or  stochastic systems.     ASA is developed to statistically find the best global fit of a  nonlinear non-convex cost-function over a D-dimensional space. This  algorithm permits an annealing schedule for 'temperature' T decreasing  exponentially in annealing-time k, T = T_0 exp(-c k^1/D). The  introduction of re-annealing also permits adaptation to changing  sensitivities in the multi-dimensional parameter-space. This annealing  schedule is faster than fast Cauchy annealing, where T = T_0/k, and  much faster than Boltzmann annealing, where T = T_0/ln k.                           Babylon        FTP site:  ftp.gmd.de/gmd/ai-research/Software/Babylon/                     BABYLON is a modular, configurable, hybrid environment for       developing expert systems. Its features include objects, rules with       forward and backward chaining, logic (Prolog) and constraints. BABYLON       is implemented and embedded in Common Lisp.                         CLEARS        Web site:  www.coli.uni-sb.de/~clears/                     The CLEARS system is an interactive graphical environment for       computational semantics. The tool allows exploration and       comparison of different semantic formalisms, and their       interaction with syntax. This enables the user to get an idea of       the range of possibilities of semantic construction, and also       where there is real convergence between theories.                         CLIPS        Web site:  www.jsc.nasa.gov/~clips/CLIPS.html          FTP site:  cs.cmu.edu/afs/cs.cmu.edu/project/ai-repository/ai/areas/expert/systems/clips                     CLIPS is a productive development and delivery expert system tool       which provides a complete environment for the construction of rule       and/or object based expert systems.                  CLIPS provides a cohesive tool for handling a wide variety of       knowledge with support for three different programming paradigms:       rule-based, object-oriented and procedural.  Rule-based programming       allows knowledge to be represented as heuristics, or ""rules of thumb,""       which specify a set of actions to be performed for a given       situation. Object-oriented programming allows complex systems to be       modeled as modular components (which can be easily reused to model       other systems or to create new components).  The procedural       programming capabilities provided by CLIPS are similar to capabilities       found in languages such as C, Pascal, Ada, and LISP.                          EMA-XPS  - A Hybrid Graphic Expert System Shell        Web site:  wmwap1.math.uni-wuppertal.de:80/EMA-XPS/                     EMA-XPS is a hybrid graphic expert system shell based on the       ASCII-oriented shell Babylon 2.3 of the German National Research       Center for Computer Sciences (GMD). In addition to Babylon's AI-power       (object oriented data representation, forward and backward chained       rules - collectible into sets, horn clauses, and constraint networks)       a graphic interface based on the X11 Window System and the OSF/Motif       Widget Library has been provided.                         FOOL & FOX        FTP site:  ntia.its.bldrdoc.gov/pub/fuzzy/prog/                     FOOL stands for the Fuzzy Organizer OLdenburg. It is a result from       a project at the University of Oldenburg. FOOL is a graphical user       interface to develop fuzzy rulebases.  FOOL will help you to invent       and maintain a database that specifies the behavior of a       fuzzy-controller or something like that.               FOX is a small but powerful fuzzy engine which reads this database,       reads some input values and calculates the new control value.                         FUF and SURGE         Web site:  www.dfki.de/lt/registry/generation/fuf.html        FTP site:  ftp.cs.columbia.edu/pub/fuf/                    FUF is an extended implementation of the formalism of functional       unification grammars (FUGs) introduced by Martin Kay specialized to       the task of natural language generation. It adds the following       features to the base formalism:           Types and inheritance.    Extended control facilities (goal freezing, intelligent backtracking).    Modular syntax.               These extensions allow the development of large grammars which can be       processed efficiently and can be maintained and understood more       easily.  SURGE is a large syntactic realization grammar of English       written in FUF. SURGE is developed to serve as a black box syntactic       generation component in a larger generation system that encapsulates a       rich knowledge of English syntax. SURGE can also be used as a platform       for exploration of grammar writing with a generation perspective.                         The Grammar Workbench        Web site:  www.cs.kun.nl/agfl/GWB.html                     The Grammar Workbench, or GWB for short, is an environment for the       comfortable development of Affix Grammars in the AGFL-formalism. Its       purposes are:            to allow the user to input, inspect and modify a grammar;    to perform consistency checks on the grammar;    to compute grammar properties;    to generate example sentences;    to assist in performing grammar transformations.                                  GSM Suite  [New]        Web site:  www.slip.net/~andrewm/gsm/                     The GSM Suite is a set of programs for using Finite State       Machines in a graphical fashion. The suite consists of programs       that edit, compile, and print state machines. Included in the       suite is an editor program, gsmedit, a compiler, gsm2cc, that       produces a C++ implementation of a state machine, a PostScript       generator, gsm2ps, and two other minor programs. GSM is licensed       under the GNU Public License and so is free for your use under       the terms of that license.                   Illuminator        Web site:  documents.cfar.umd.edu/resources/source/illuminator.html                           Illuminator is a toolset for developing OCR and Image       Understanding applications.  Illuminator has two major parts: a       library for representing, storing and retrieving OCR       information, heretofore called dafslib, and an X-Windows ""DAFS""       file viewer, called illum. Illuminator and DAFS lib were       designed to supplant existing OCR formats and become a standard       in the industry. They particularly are extensible to handle more       than just English.              The features of this release:            5 magnification levels for images    flagged characters and words    unicode support -- American, British, French, German, Greek, Italian, MICR, Norwegian, Russian, Spanish, Swedish, keyboards     reads DAFS, TIFF's, PDA's (image only)    save to DAFS, ASCII/UTF or Unicode    Entity Viewer - shows properties, character choices, bounding boxes image fragment for a selected entity, change type, change content, hierarchy mode                            Jess , the Java Expert System Shell        Web site:  herzberg.ca.sandia.gov/jess/                     Jess is a clone of the popular CLIPS expert system shell written       entirely in Java. With Jess, you can conveniently give your       applets the ability to 'reason'. Jess is compatible with all       versions of Java starting with version 1.0.2. Jess implements       the following constructs from CLIPS: defrules, deffunctions,       defglobals, deffacts, and deftemplates.                     learn        FTP site:  sunsite.unc.edu/pub/Linux/apps/cai/                           Learn is a vocable learning program with memory model.                    Otter: An Automated Deduction System        Web site:  www.mcs.anl.gov/home/mccune/ar/otter                     Our current automated deduction system  Otter is designed to prove       theorems stated in first-order logic with equality.  Otter's       inference rules are based on resolution and paramodulation, and it       includes facilities for term rewriting, term orderings, Knuth-Bendix       completion, weighting, and strategies for directing and restricting       searches for proofs.   Otter can also be used as a symbolic       calculator and has an embedded equational programming system.                               RIPPER        Web site:  www.research.att.com/~wcohen/ripperd.html                     Ripper is a system for fast effective rule induction. Given a set       of data, Ripper will learn a set of rules that will predict the        patterns in the data. Ripper is written in ASCI C and comes with       documentation and some sample problems.                   SNePS        Web site:  www.cs.buffalo.edu/pub/sneps/WWW/        FTP site:  ftp.cs.buffalo.edu/pub/sneps/                    The long-term goal of The SNePS Research Group is the design and       construction of a natural-language-using computerized cognitive       agent, and carrying out the research in artificial intelligence,       computational linguistics, and cognitive science necessary for       that endeavor. The three-part focus of the group is on knowledge       representation, reasoning, and natural-language understanding       and generation. The group is widely known for its development of       the SNePS knowledge representation/reasoning system, and Cassie,       its computerized cognitive agent.                           Soar        Web site:  www.isi.edu/soar/soar.html        FTP site:  cs.cmu.edu/afs/cs/project/soar/public/Soar6/                     Soar has been developed to be a general cognitive architecture.       We intend ultimately to enable the Soar architecture to:           work on the full range of tasks expected of an    intelligent agent, from highly routine to extremely difficult,    open-ended problems   represent and use appropriate forms of knowledge, such as    procedural, declarative, episodic, and possibly iconic   employ the full range of problem solving methods    interact with the outside world and    learn about all aspects of the tasks and its performance on them.                In other words, our intention is for Soar to support all the       capabilities required of a general intelligent agent.                   TCM  (Toolkit for Conceptual Modeling)        Web site:  www.cs.vu.nl/~tcm/        FTP site:  ftp.cs.vu.nl/pub/tcm/                     TCM (Toolkit for Conceptual Modeling) is our suite of graphical       editors. TCM contains graphical editors for Entity-Relationship       diagrams, Class-Relationship diagrams, Data and Event Flow       diagrams, State Transition diagrams, Jackson Process Structure       diagrams and System Network diagrams, Function Refinement trees       and various table editors, such as a Function-Entity table       editor and a Function Decomposition table editor.  TCM is easy       to use and performs numerous consistency checks, some of them       immediately, some of them upon request.                   WEKA  [New]        Web site:  lucy.cs.waikato.ac.nz/~ml/                         WEKA (Waikato Environment for Knowledge Analysis) is an       state-of-the-art facility for applying machine learning       techniques to practical problems. It is a comprehensive software       ""workbench"" that allows people to analyse real-world data. It       integrates different machine learning tools within a common       framework and a uniform user interface. It is designed to       support a ""simplicity-first"" methodology, which allows users to       experiment interactively with simple machine learning tools       before looking for more complex solutions.                                     Connectionism   Connectionism is a technical term for a group of related  techniques. These techniques include areas such as Artificial  Neural Networks, Semantic Networks and a few other similar  ideas. My present focus is on neural networks (though I am  looking for resources on the other techniques). Neural  networks are programs designed to simulate the workings of the  brain. They consist of a network of small mathematical-based  nodes, which work together to form patterns of information.  They have tremendous potential and currently seem to be having  a great deal of success with image processing and robot  control.             Connectionist class/code libraries       These are libraries of code or classes for use in programming within     the Connectionist field.  They are not meant as stand alone     applications, but rather as tools for building your own applications.                    ANSI-C Neural Networks        Web site:  www.geocities.com/CapeCanaveral/1624/                     This site contains ANSC-C source code for 8 types of neural       nets, including:           Adaline Network     Backpropagation    Hopfield Model    (BAM) Bidirectional Associative Memory    Boltzmann Machine    Counterpropagation    (SOM) Self-Organizing Map    (ART1) Adaptive Resonance Theory                 They were designed to help turn the theory of a particular       network model into the design for a simulator implementation ,       and to help with embeding an actual application into a       particular network model.                       BELIEF  [New]   Web site:  www.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/reasonng/probabl/belief/0.html                           BELIEF is a Common Lisp implementation of the Dempster and Kong       fusion and propagation algorithm for Graphical Belief Function       Models and the Lauritzen and Spiegelhalter algorithm for       Graphical Probabilistic Models. It includes code for       manipulating graphical belief models such as Bayes Nets and       Relevance Diagrams (a subset of Influence Diagrams) using both       belief functions and probabilities as basic representations of       uncertainty. It uses the Shenoy and Shafer version of the       algorithm, so one of its unique features is that it supports       both probability distributions and belief functions.  It also       has limited support for second order models (probability       distributions on parameters).                     IDEAL  [New]   Web site:  www.rpal.rockwell.com/ideal.html                          IDEAL is a test bed for work in influence diagrams and  Bayesian networks. It contains various inference algorithms  for belief networks and evaluation algorithms for influence  diagrams. It contains facilities for creating and editing  influence diagrams and belief networks.          IDEAL is written in pure Common Lisp and so it will run in  Common Lisp on any platform. The emphasis in writing IDEAL has  been on code clarity and providing high level programming  abstractions. It thus is very suitable for experimental  implementations which need or extend belief network  technology.          At the highest level, IDEAL can be used as a subroutine  library which provides belief network inference and influence  diagram evaluation as a package. The code is documented in a  detailed manual and so it is also possible to work at a lower  level on extensions of belief network methods.           IDEAL comes with an optional graphic interface written in  CLIM. If your Common Lisp also has CLIM, you can run the  graphic interface.                     Matrix Class   FTP site:  pink.cs.ucla.edu/pub/                          A simple, fast, efficient C++ Matrix class designed for  scientists and engineers. The Matrix class is well suited for  applications with complex math algorithms. As an demonstration  of the Matrix class, it was used to implement the backward error  propagation algorithm for a multi-layer feed-forward artificial  neural network.                       Pulcinella   Web site:  iridia.ulb.ac.be/pulcinella/Welcome.html                       Pulcinella is written in CommonLisp, and appears as a library of       Lisp functions for creating, modifying and evaluating valuation       systems. Alternatively, the user can choose to interact with       Pulcinella via a graphical interface (only available in Allegro       CL). Pulcinella provides primitives to build and evaluate       uncertainty models according to several uncertainty calculi,       including probability theory, possibility theory, and       Dempster-Shafer's theory of belief functions; and the       possibility theory by Zadeh, Dubois and Prade's. A User's Manual       is available on request.                   S-ElimBel  [New]        Web site:  www.spaces.uci.edu/thiery/elimbel/                S-ElimBel is an algorithm that computes the belief in a  Bayesian network, implemented in MIT-Scheme. This algorithm has  the particularity of being rather easy to understand. Moreover,  one can apply it to any kind of Bayesian network - it being  singly connected or muliply connected. It is, however, less  powerful than the standard algorithm of belief propagation.  Indeed, the computation has to be reconducted entirely for each  new evidence added to the network. Also, one needs to run the  algorithm as many times as one has nodes for which the belief is  wanted.                         Software for Flexible Bayesian Modeling  [New]        Web site:  www.cs.utoronto.ca/~radford/fbm.software.html                         This software implements flexible Bayesian models for regression       and classification applications that are based on multilayer       perceptron neural networks or on Gaussian processes.  The       implementation uses Markov chain Monte Carlo methods.  Software       modules that support Markov chain sampling are included in the       distribution, and may be useful in other applications.                    Spiderweb2  [New]   Web site:                      A C++ artificial neual net library.  Spiderweb2 is a complete       rewrite of the original Spiderweb library, it has grown into a       much more flexible and object-oriented system. The biggest       change is that each neuron object is responsible for its own       activations and updates, with the network providing only the       scheduling aspect. This is a very powerful change, and it allows       easy modification and experimentation with various network       architectures and neuron types.               There is still a lack of documentation and support for file I/O,       but that will eventually get done.                      Symbolic Probabilistic Inference (SPI)   FTP site:  ftp.engr.orst.edu/pub/dambrosi/spi/          Paper (ijar-94.ps):  ftp.engr.orst.edu/pub/dambrosi/                       Contains Common Lisp function libraries to implement SPI type baysean nets.        Documentation is very limited.        Features:             Probabilities, Local Expression Language Utilities, Explanation,     Dynamic Models, and a TCL/TK based GUI.                             TresBel   FTP site:  iridia.ulb.ac.be/pub/hongxu/software/                       Libraries containing (Allegro) Common Lisp code for Belief Functions        (aka. Dempster-Shafer evidential reasoning) as a representation        of uncertainty. Very little documentation. Has a limited GUI.                   Various (C++) Neural Networks        Web site:  www.mcs.com/~drt/svbp.html        Submitted by:  Don Tveter                       Example neural net codes from the book,   The Basis of Artificial  Intelligence .  These are simple example codes of these various       neural nets. They work well as a good starting point for simple       experimentation and for learning what the code is like behind the       simulators. The types of networks available on this site are:        (implemented in C++)                   The Backprop Package   The Nearest Neighbor Algorithms   The Interactive Activation Algorithm   The Hopfield and Boltzman machine Algorithms   The Linear Pattern Classifier   ART I   Bi-Directional Associative Memory   The Feedforward Counter-Propagation Network                                      Connectionist software kits/applications           These are various applications, software kits, etc. meant for research     in the field of Connectionism. Their ease of use will vary, as they     were designed to meet some particular research interest more than as     an easy to use commercial package.                       Aspirin/MIGRAINES  (am6.tar.Z on ftp site)   FTP site:  sunsite.unc.edu/pub/academic/computer-science/neural-networks/programs/Aspirin/                    The software that we are releasing now is for creating,   and evaluating, feed-forward networks such as those used with the   backpropagation learning algorithm. The software is aimed both at   the expert programmer/neural network researcher who may wish to tailor  significant portions of the system to his/her precise needs, as well  as at casual users who will wish to use the system with an absolute  minimum of effort.                       JavaBayes  [New]   Web site:  www.cs.cmu.edu/People/javabayes/index.html/                    The JavaBayes system is a set of tools, containing a  graphical editor, a core inference engine and a parser.  JavaBayes can produce:         the marginal distribution for any variable in a network.      the expectations for univariate functions (for example,       expected value for variables).      configurations with maximum a posteriori probability.      configurations with maximum a posteriori expectation for       univariate functions.                        Neureka ANS (nn/xnn)   Web site:  www.bgif.no/neureka/      FTP site:  ftp.ii.uib.no/pub/neureka/linux/                     nn is a high-level neural network specification language. The       current version is best suited for feed-forward nets, but       recurrent models can and have been implemented, e.g. Hopfield       nets, Jordan/Elman nets, etc.  In nn, it is easy to change       network dynamics. The nn compiler can generate C code or       executable programs (so there must be a C compiler available),       with a powerful command line interface (but everything may also       be controlled via the graphical interface, xnn). It is possible       for the user to write C routines that can be called from inside       the nn specification, and to use the nn specification as a       function that is called from a C program. Please note that no       programming is necessary in order to use the network models that       come with the system (`netpack').                  xnn is a graphical front end to networks generated by the nn       compiler, and to the compiler itself. The xnn graphical       interface is intuitive and easy to use for beginners, yet       powerful, with many possibilities for visualizing network data.                NOTE: You have to run the install program that comes with this       to get the license key installed. It gets put (by default) in       /usr/lib. If you (like myself) want to install the package       somewhere other than in the /usr directory structure (the       install program gives you this option) you will have to set up       some environmental variables (NNLIBDIR & NNINCLUDEDIR are       required). You can read about these (and a few other optional       variables) in appendix A of the documentation (pg 113).                     PDP++        Web site:  www.cnbc.cmu.edu/PDP++/        FTP site (US):  cnbc.cmu.edu/pub/pdp++/        FTP site (Europe):   unix.hensa.ac.uk/mirrors/pdp++/                      As the field of Connectionist modeling has grown, so has the need       for a comprehensive simulation environment for the development and       testing of Connectionist models. Our goal in developing PDP++ has been       to integrate several powerful software development and user interface       tools into a general purpose simulation environment that is both user       friendly and user extensible. The simulator is built in the C++       programming language, and incorporates a state of the art script       interpreter with the full expressive power of C++. The graphical user       interface is built with the Interviews toolkit, and allows full access       to the data structures and processing modules out of which the       simulator is built. We have constructed several useful graphical       modules for easy interaction with the structure and the contents of       neural networks, and we've made it possible to change and adapt many       things. At the programming level, we have set things up in such a way       as to make user extensions as painless as possible. The programmer       creates new C++ objects, which might be new kinds of units or new       kinds of processes; once compiled and linked into the simulator, these       new objects can then be accessed and used like any other.                   Simple Neural Net  (in Python)   Web site:  starship.skyport.net/crew/amk/unmaintained/                     Simple neural network code, which implements a class for 3-level       networks (input, hidden, and output layers). The only learning       rule implemented is simple backpropagation. No documentation (or       even comments) at all, because this is simply code that I use to       experiment with. Includes modules containing sample datasets       from Carl G. Looney's NN book. Requires the Numeric       extensions.                    SCNN        Web site:  apx00.physik.uni-frankfurt.de/e_ag_rt/cnn/SCNN/homepage.html                     SCNN is an universal simulating system for Cellular Neural       Networks (CNN).  CNN are analog processing neural networks       with regular and local interconnections, governed by a set of       nonlinear ordinary differential equations. Due to their local       connectivity, CNN are realized as VLSI chips, which operates       at very high speed.                   Semantic Networks in Python        Web site:  www-acs.ucsd.edu/~jstrout/python/ai/                         The semnet.py module defines several simple classes for       building and using semantic networks.  A semantic network is a  way of representing knowledge, and it enables the program to       do simple reasoning with very little effort on the part of the       programmer.               The following classes are defined:               Entity : This class represents a noun; it is    something which can be related to other things, and about    which you can store facts.   Relation : A Relation is a type of relationship    which may exist between two entities.  One special relation,    ""IS_A"", is predefined because it has special meaning (a sort    of logical inheritance).   Fact : A Fact is an assertion that a relationship     exists between two entities.                       With these three object types, you can very quickly define knowledge        about a set of objects, and query them for logical conclusions.                   SNNS        Web site:            www.informatik.uni-stuttgart.de/ipvr/bv/projekte/snns/snns.html        FTP site:    ftp.informatik.uni-stuttgart.de/pub/SNNS/                     Stuttgart Neural Net Simulator (version 4.1).  An awesome neural       net simulator. Better than any commercial simulator I've seen. The       simulator kernel is written in C (it's fast!). It supports over 20       different network architectures, has 2D and 3D X-based graphical       representations, the 2D GUI has an integrated network editor, and can       generate a separate NN program in C.  SNNS is very powerful, though       a bit difficult to learn at first. To help with this it comes with       example networks and tutorials for many of the architectures.        ENZO, a supplementary system allows you to evolve your networks with       genetic algorithms.               The Readme.linux file that comes with this package must be old.        It's instructions for building the package are wrong. I've edited       it to reflect what I had to do to get the package to compile.        Please download  SNNS.Readme.linux         and use it instead of the Readme.linux file that comes with       the distribution.                   SPRLIB/ANNLIB        Web site:  www.ph.tn.tudelft.nl/~sprlib/                         SPRLIB (Statistical Pattern Recognition Library) was developed       to support the easy construction and simulation of pattern       classifiers. It consist of a library of functions (written in C)       that can be called from your own program. Most of the well-known       classifiers are present (k-nn, Fisher, Parzen, ....), as well as       error estimation and dataset generation routines.               ANNLIB (Artificial Neural Networks Library) is a neural network       simulation library based on the data architecture laid down by       SPRLIB. The library contains numerous functions for creating,       training and testing feed-forward networks.  Training algorithms       include back-propagation, pseudo-Newton, Levenberg-Marquardt,       conjugate gradient descent, BFGS.... Furthermore, it is possible       - due to the datastructures' general applicability - to build       Kohonen maps and other more exotic network architectures using       the same data types.                                    Evolutionary Computing   Evolutionary computing is actually a broad term for a vast  array of programming techniques, including genetic algorithms,  complex adaptive systems, evolutionary programming, etc.  The main thrust of all these techniques is the idea of  evolution. The idea that a program can be written that will   evolve  toward a certain goal.  This goal can be  anything from solving some engineering problem to winning a  game.             EC class/code libraries           These are libraries of code or classes for use in programming within     the evolutionary computation field.  They are not meant as stand alone     applications, but rather as tools for building your own applications.                        FORTRAN GA   Web site:  www.staff.uiuc.edu/~carroll/ga.html                          This program is a FORTRAN version of a genetic algorithm driver.  This code initializes a random sample of individuals with  different parameters to be optimized using the genetic algorithm  approach, i.e.  evolution via survival of the fittest.  The  selection scheme used is tournament selection with a shuffling  technique for choosing random pairs for mating.  The routine  includes binary coding for the individuals, jump mutation, creep  mutation, and the option for single-point or uniform crossover.  Niching (sharing) and an option for the number of children per  pair of parents has been added.  More recently, an option for  the use of a micro-GA has been added.                             GAGS   Web site:  kal-el.ugr.es/gags.html   FTP site:  kal-el.ugr.es/GAGS/                    Genetic Algorithm  application generator and class library  written mainly in C++.   As a class library, and among other thing, GAGS includes:               A  chromosome hierarchy  with variable length      chromosomes.   Genetic operators : 2-point crossover,      uniform crossover, bit-flip mutation, transposition (gene      interchange between 2 parts of the chromosome), and      variable-length operators: duplication, elimination, and      random addition.     Population level operators  include steady state, roulette      wheel and tournament selection.     Gnuplot wrapper : turns gnuplot into a       iostreams -like class.     Easy sample file loading and configuration file parsing.            As an application generator (written in  PERL ),  you only need to supply it with an ANSI-C or C++ fitness  function, and it creates a C++ program that uses the above  library to 90% capacity, compiles it, and runs it, saving  results and presenting fitness thru  gnuplot .                     GAlib: Matthew's Genetic Algorithms Library         Web Site:  lancet.mit.edu/ga/         FTP site:  lancet.mit.edu/pub/ga/        Register GAlib at:    http://lancet.mit.edu/ga/Register.html                     GAlib contains a set of C++ genetic algorithm objects.  The       library includes tools for using genetic algorithms to do       optimization in any C++ program using any representation and genetic       operators.  The documentation includes an extensive overview of how       to implement a genetic algorithm as well as examples illustrating       customizations to the GAlib classes.                   GALOPPS        Web site:  GARAGe.cps.msu.edu/software/software-index.html        FTP site:  garage.cps.msu.edu/pub/GA/galopps/                     GALOPPS is a flexible, generic GA, in 'C'.  It was based upon       Goldberg's Simple Genetic Algorithm (SGA) architecture, in order to       make it easier for users to learn to use and extend.               GALOPPS extends the SGA capabilities several fold:            (optional) A new Graphical User Interface, based on TCL/TK, for    Unix users, allowing easy running of GALOPPS 3.2 (single or multiple    subpopulations) on one or more processors.  GUI writes/reads    ""standard"" GALOPPS input and master files, and displays graphical    output (during or after run) of user-selected variables.    5 selection methods: roulette wheel, stochastic remainder    sampling, tournament selection, stochastic universal sampling,    linear-ranking-then-SUS.    Random or superuniform initialization of ""ordinary""    (non-permutation) binary or non-binary chromosomes; random    initialization of permutation-based chromosomes; or user-supplied    initialization of arbitrary types of chromosomes.    Binary or non-binary alphabetic fields on value-based    chromosomes, including different user-definable field sizes.    3 crossovers for value-based representations: 1-pt, 2-pt, and    uniform, all of which operate at field boundaries if a non-binary    alphabet is used.    4 crossovers for order-based reps: PMX, order-based, uniform    order-based, and cycle.    4 mutations: fast bitwise, multiple-field, swap and random    sublist scramble.    Fitness scaling: linear scaling, Boltzmann scaling, sigma    truncation, window scaling, ranking.   Plus  a whole lot more....                            GAS  [New]        Web site:  starship.skyport.net/crew/gandalf        FTP site:  ftp.coe.uga.edu/users/jae/ai                     GAS means ""Genetic Algorithms Stuff"".       GAS is freeware.        Purpose of GAS is to explore and exploit artificial evolutions.       Primary implementation language of GAS is Python.  The GAS       software package is meant to be a Python framework for applying       genetic algorithms. It contains an example application where it       is tried to breed Python program strings.  This special problem       falls into the category of Genetic Programming (GP), and/or       Automatic Programming.  Nevertheless, GAS tries to be useful for       other applications of Genetic Algorithms as well.                    GECO        FTP site:  ftp://ftp.aic.nrl.navy.mil/pub/galist/src/                     GECO (Genetic Evolution through Combination of Objects), an       extendible object-oriented tool-box for constructing genetic algorithms       (in Lisp).  It provides a set of extensible classes and methods       designed for generality. Some simple examples are also provided to       illustrate the intended use.             GPdata        FTP site:  ftp.cs.bham.ac.uk/pub/authors/W.B.Langdon/gp-code/          Documentation (GPdata-icga-95.ps):  cs.ucl.ac.uk/genetic/papers/                     GPdata-3.0.tar.gz (C++) contains a version of Andy Singleton's        GP-Quick version 2.1 which has been extensively altered to support:           Indexed memory operation (cf. teller)   multi tree programs          Adfs          parameter changes without recompilation          populations partitioned into demes          (A version of) pareto fitness               This ftp site also contains a small C++ program (ntrees.cc) to        calculate the number of different there are of a given length and        given function and terminal set.                   gpjpp  Genetic Programming in Java        Web site:  www.turbopower.com/~kimk/gpjpp.asp                     gpjpp is a Java package I wrote for doing research in genetic       programming. It is a port of the gpc++ kernel written by Adam       Fraser and Thomas Weinbrenner. Included in the package are       four of Koza's standard examples: the artificial ant, the       hopping lawnmower, symbolic regression, and the boolean       multiplexer. Here is a partial list of its features:           graphic output of expression trees   efficient diversity checking    Koza's greedy over-selection option for large populations   extensible GPRun class that encapsulates most details of a    genetic programming test   more robust and efficient streaming code, with automatic checkpoint    and restart built into the GPRun class   an explicit complexity limit that can be set on each GP   additional configuration variables to allow more testing without    recompilation   support for automatically defined functions (ADFs)   tournament and fitness proportionate selection   demetic grouping   optional steady state population   subtree crossover   swap and shrink mutation                           GP Kernel        Web site:  www.emk.e-technik.th-darmstadt.de/~thomasw/gp.html                     The GP kernel is a C++ class library that can be used to apply       genetic programming techniques to all kinds of problems. The       library defines a class hierarchy. An integral component is the       ability to produce automatically defined functions as found in       Koza's ""Genetic Programming II"".Technical documentation       (postscript format) is included. There is also a short       introduction into genetic programming.               Functionality includes; Automatically defined functions (ADFs),       tournament and fitness proportionate selection, demetic grouping,       optional steady state genetic programming kernel, subtree crossover,       swap and shrink mutation, a way of changing every parameter of the       system without recompilation, capacity for multiple populations,       loading and saving of populations and genetic programs, standard       random number generator, internal parameter checks.                   lil-gp        Web site:  isl.msu.edu/GA/software/lil-gp/index.html        FTP site:  isl.cps.msu.edu/pub/GA/lilgp/                      patched lil-gp *  [New]   Web site:  www.cs.umd.edu/users/seanl/patched-gp                       lil-gp is a generic 'C' genetic programming tool. It was written       with a number of goals in mind: speed, ease of use and support for a       number of options including:            Generic 'C' program that runs on UNIX workstations    Support for multiple population experiments, using arbitrary and    user settable topologies for exchange, for a single processor (i.e.,    you can do multiple population gp experiments on your PC).    lil-gp manipulates trees of function pointers which are allocated    in single, large memory blocks for speed and to avoid swapping.               * The patched lil-gp kernel is strongly-typed, with modifications on        multithreading, fixes to some *serious* bugs in lilgp, coevolution,        and other tweaks and features.                   PGAPack  Parallel Genetic Algorithm Library        Web site:  www.mcs.anl.gov/home/levine/PGAPACK/index.html        FTP site:  ftp.mcs.anl.gov/pub/pgapack/                     PGAPack is a general-purpose, data-structure-neutral, parallel       genetic algorithm library. It is intended to provide most capabilities       desired in a genetic algorithm library, in an integrated, seamless,       and portable manner. Key features are in PGAPack V1.0 include:           Callable from Fortran or C.    Runs on uniprocessors, parallel computers, and workstation networks.   Binary-, integer-, real-, and character-valued native data types.    Full extensibility to support custom operators and new data types.    Easy-to-use interface for novice and application users.    Multiple levels of access for expert users.    Parameterized population replacement.    Multiple crossover, mutation, and selection operators.    Easy integration of hill-climbing heuristics.    Extensive debugging facilities.    Large set of example problems.    Detailed users guide.                           Sugal        Web site:  www.trajan-software.demon.co.uk/sugal.htm                     Sugal [soo-gall] is the SUnderland Genetic ALgorithm system. The aim of       Sugal is to support research and implementation in Genetic Algorithms on a       common software platform. As such, Sugal supports a large number of variants       of Genetic Algorithms, and has extensive features to support customization       and extension.                        EC software kits/applications           These are various applications, software kits, etc. meant for research     in the field of evolutionary computing. Their ease of use will vary, as they     were designed to meet some particular research interest more than as     an easy to use commercial package.                        ADATE (Automatic Design of Algorithms Through Evolution)   Web site:  www-ia.hiof.no/~rolando/adate_intro.html                    ADATE is a system for automatic programming i.e., inductive  inference of algorithms, which may be the best way to develop  artificial and general intelligence.     The ADATE system can automatically generate non-trivial and novel  algorithms. Algorithms are generated through large scale combinatorial  search that employs sophisticated program transformations and  heuristics. The ADATE system is particularly good at synthesizing  symbolic, functional programs and has several unique qualities.                     esep & xesep  [New]        Web site(esep):  www.iit.edu/~linjinl/esep.html        Web site(xesep):  www.iit.edu/~linjinl/xesep.html                     This is a new scheduler, called Evolution Scheduler, based on       Genetic Algorithms and Evolutionary Programming. It lives with       original Linux priority scheduler.This means you don't have to       reboot to change the scheduling policy. You may simply use the       manager program esep to switch between them at any time, and       esep itself is an all-in-one for scheduling status, commands,       and administration. We didn't intend to remove the original       priority scheduler; instead, at least, esep provides you with       another choice to use a more intelligent scheduler, which       carries out natural competition in an easy and effective way.               Xesep is a graphical user interface to the esep (Evolution       Scheduling and Evolving Processes). It's intended to show users       how to start, play, and feel the Evolution Scheduling and       Evolving Processes, including sub-programs to display system       status, evolving process status, queue status, and evolution       scheduling status periodically in as small as one mini-second.                    FSM-Evolver  [New]        Web site:  pages.prodigy.net/czarneckid                     A Java (jdk-v1.0.2+) code library that is used to evolve finite       state machines. The problem included in the package is the       Artificial Ant problem. You should be able to compile the .java       files and then run: java ArtificialAnt.                        GPsys  [New]        Web site:  www.cs.ucl.ac.uk/staff/A.Qureshi/gpsys.html                         GPsys (pronounced gipsys) is a Java (requires Java 1.1 or       later) based Genetic Programming system developed by Adil       Qureshi.  The software includes documentation, source and       executables.               Feature Summary:           Steady State engine    ADF support    Strongly Typed         supports generic functions and terminals    has many built-in primitives    includes indexed memory         Save/Load feature         can save/load current generation to/from a file    data stored in GZIP compression format to minimise disk requirements    uses serialisable objects for efficiency         Fully Documented         Example Problems         Lawnmower (including GUI viewer)    Symbolic Regression         Totally Parameterised         Fully Object Oriented and Extensible         High Performance         Memory Efficient                                            Alife   Alife takes yet another approach to exploring the mysteries of  intelligence.  It has many aspects similar to EC and  Connectionism, but takes these ideas and gives them a  meta-level twist. Alife emphasizes the development of  intelligence through  emergent  behavior of  complex  adaptive systems .  Alife stresses the social or group  based aspects of intelligence. It seeks to understand life and  survival. By studying the behaviors of groups of 'beings' Alife  seeks to discover the way intelligence or higher order  activity emerges from seemingly simple individuals. Cellular  Automata and Conway's Game of Life are probably the most  commonly known applications of this field.             Alife class/code libraries        These are libraries of code or classes for use in programming within     the artificial life field.  They are not meant as stand alone     applications, but rather as tools for building your own applications.                              John von Neumann Universal Constructor   Web site:  alife.santafe.edu/alife/software/jvn.html   FTP site:  alife.santafe.edu/pub/SOFTWARE/jvn/                    The universal constructor of John von Neumann is an extension of  the logical concept of universal computing machine.  In the cellular  environment proposed by von Neumann both computing and constructive  universality can be achieved.  Von Neumann proved that in his cellular  lattice both a Turing machine and a machine capable of producing any  other cell assembly, when fed with a suitable program, can be  embedded. He called the latter machine a ""universal  constructor"" and showed that, when provided with a program  containing its own description, this is capable of self-reproducing.                     Swarm        Web site:          www.santafe.edu/projects/swarm        FTP site:   ftp.santafe.edu/pub/swarm                     The swarm Alife simulation kit. Swarm is a simulation environment       which facilitates development and experimentation with simulations       involving a large number of agents behaving and interacting within a       dynamic environment.  It consists of a collection of classes and       libraries written in Objective-C and allows great flexibility in       creating simulations and analyzing their results.  It comes with three       demos and good documentation.                Swarm 1.0 is out. It requires  libtclobjc  and  BLT 2.1        (both available at the swarm site).                              Alife software kits, applications, etc.       These are various applications, software kits, etc. meant for research     in the field of artificial life. Their ease of use will vary, as they     were designed to meet some particular research interest more than as     an easy to use commercial package.                        BugsX   FTP site:  ftp.Germany.EU.net/pub/research/softcomp/Alife/packages/bugsx/                     Display and evolve biomorphs. It is a program which draws the  biomorphs based on parametric plots of Fourier sine and cosine series  and let's you play with them using the genetic algorithm.                     The Cellular Automata Simulation System        Web site:  www.cs.runet.edu/~dana/ca/cellular.html                     The system consists of a compiler for the Cellang cellular       automata programming language, along with the corresponding       documentation, viewer, and various tools. Cellang has been       undergoing refinement for the last several years (1991-1995),       with corresponding upgrades to the compiler.  Postscript       versions of the tutorial and language reference manual are       available for those wanting more detailed information. The most       important distinguishing features of Cellang, include support       for:           any number of dimensions;    compile time specification of each dimensions size;     cell neighborhoods of any size (though bounded at compile time) and    shape;    positional and time dependent neighborhoods;    associating multiple values (fields), including arrays,    with each cell;   associating a potentially unbounded number of mobile    agents [ Agents are mobile entities based on a mechanism of    the same name in the Creatures system, developed by Ian    Stephenson (ian@ohm.york.ac.uk).] with each cell; and   local interactions only, since it is impossible to    construct automata that contain any global control or    references to global variables.                           dblife & dblifelib        FTP site:  ftp.cc.gatech.edu/ac121/linux/games/amusements/life/                      dblife:  Sources for a fancy Game of Life program for X11       (and curses).  It is not meant to be incredibly fast (use xlife for       that:-).  But it IS meant to allow the easy editing and viewing of       Life objects and has some powerful features.  The related dblifelib       package is a library of Life objects to use with the program.                dblifelib:  This is a library of interesting Life objects,       including oscillators, spaceships, puffers, and other weird things.       The related dblife package contains a Life program which can read the       objects in the Library.                   Drone        Web site:  pscs.physics.lsa.umich.edu/Software/Drone/                     Drone is a tool for automatically running batch jobs of a simulation       program. It allows sweeps over arbitrary sets of parameters, as well       as multiple runs for each parameter set, with a separate random seed       for each run. The runs may be executed either on a single computer or       over the Internet on a set of remote hosts. Drone is written in Expect       (an extension to the Tcl scripting language) and runs under Unix. It       was originally designed for use with the Swarm agent-based simulation       framework, but Drone can be used with any simulation program that       reads parameters from the command line or from an input file.                   EcoLab  [New]        Web site:  parallel.acsu.unsw.edu.au/rks/ecolab.html                           EcoLab is a system that implements an abstract ecology model. It       is written as a set of Tcl/Tk commands so that the model       parameters can easily be changed on the fly by means of editing       a script. The model itself is written in C++.                   Game Of Life (GOL)  [New]        Web site:  www.arrakeen.demon.co.uk/downloads.html        FTP site:  sunsite.unc.edu/pub/Linux/science/ai/life                     GOL is a simulator for conway's game of life (a simple cellular       automata), and other simple rule sets. The emphasis here is on       speed and scale, in other words you can setup large and fast       simulations.                    LEE        Web site:  www-cse.ucsd.edu/users/fil/lee/lee.html        FTP site:  cs.ucsd.edu/pub/LEE/                      LEE (Latent Energy Environments) is both an Alife model and a       software tool to be used for simulations within the framework of that       model. We hope that LEE will help understand a broad range of issues       in theoretical, behavioral, and evolutionary biology. The LEE tool       described here consists of approximately 7,000 lines of C code and       runs in both Unix and Macintosh platforms.                   Net-Life & ZooLife        Web site: www.geocities.com/SiliconValley/Heights/1051 **        FTP site:  ftp.coe.uga.edu/users/jae/alife/        *(netlife-2.0.tar.gz contains both Net-Life and ZooLife)                      Net-Life  is a simulation of artificial-life, with neural ""brains""       generated via slightly random techniques. Net-Life uses artificial       neural nets and evolutionary algorithms to breed artificial organisms       that are similar to single cell organisms.  Net-life uses asexual       reproduction of its fittest individuals with a chance of mutation       after each round to eventually evolve successful life-forms.                ZooLife  is a simulation of artificial-life. ZooLife uses       probabilistic methods and evolutionary algorithms to breed       artificial organisms that are similar to plant/animal zoo       organisms.  ZooLife uses asexual reproduction with a chance of       mutation.                   Primordial Soup        Web site:  alife.santafe.edu/alife/software/psoup.html                         Primordial Soup is an artificial life program. Organisms in the       form of computer software loops live in a shared memory space       (the ""soup"") and self-reproduce. The organisms mutate and       evolve, behaving in accordance with the principles of Darwinian       evolution.                  The program may be started with one or more organisms seeding       the soup. Alternatively, the system may be started ""sterile"",       with no organisms present. Spontaneous generation of       self-reproducing organisms has been observed after runs as short       as 15 minutes.                   Tierra        Web site:  www.hip.atr.co.jp/~ray/tierra/tierra.html          FTP site:  alife.santafe.edu/pub/SOFTWARE/Tierra/        Alternate FTP site:  ftp.cc.gatech.edu/ac121/linux/science/biology/                 Tierra's written in the C programming language. This source code       creates a virtual computer and its operating system, whose       architecture has been designed in such a way that the executable       machine codes are evolvable. This means that the machine code can be       mutated (by flipping bits at random) or recombined (by swapping       segments of code between algorithms), and the resulting code remains       functional enough of the time for natural (or presumably artificial)       selection to be able to improve the code over time.                    TIN        FTP site:  ftp.coe.uga.edu/users/jae/alife/                     This program simulates primitive life-forms, equipped with some       basic instincts and abilities, in a 2D environment consisting of       cells.  By mutation new generations can prove their success, and thus       passing on ""good family values"".               The brain of a TIN can be seen as a collection of processes, each       representing drives or impulses to behave a certain way, depending on       the state/perception of the environment ( e.g. presence of food,       walls, neighbors, scent traces) These behavior process currently are       : eating, moving, mating, relaxing, tracing others, gathering food and       killing. The process with the highest impulse value takes control, or       in other words: the tin will act according to its most urgent need.                   XLIFE        FTP site:  ftp.cc.gatech.edu/ac121/linux/games/amusements/life/                     This program will evolve patterns for John Horton Conway's game       of Life.  It will also handle general cellular automata with the       orthogonal neighborhood and up to 8 states (it's possible to recompile       for more states, but very expensive in memory).  Transition rules and       sample patterns are provided for the 8-state automaton of E. F. Codd,       the Wireworld automaton, and a whole class of `Prisoner's Dilemma'       games.                                    Autonomous Agents    Also known as intelligent software agents or just agents, this  area of AI research deals with simple applications of small  programs that aid the user in his/her work. They can be mobile  (able to stop their execution on one machine and resume it on  another) or static (live in one machine). They are usually  specific to the task (and therefore fairly simple) and meant  to help the user much as an assistant would. The most popular  (ie. widely known) use of this type of application to date are  the web robots that many of the indexing engines  (eg. webcrawler) use.                               AgentK   FTP site:  ftp.csd.abdn.ac.uk/pub/wdavies/agentk                    This package synthesizes two well-known agent paradigms:  Agent-Oriented Programming, Shoham (1990), and the Knowledge Query  & Manipulation Language, Finin (1993). The initial implementation  of AOP, Agent-0, is a simple language for specifying agent  behaviour. KQML provides a standard language for inter-agent  communication. Our integration (which we have called Agent-K)  demonstrates that Agent-0 and KQML are highly compatible. Agent-K  provides the possibility of inter-operable (or open) software agents,  that can communicate via KQML and which are programmed using the AOP  approach.                     Agent , the Perl5 Module         FTP site:  ftp.hawk.igs.net/pub/users/jduncan/modules/Agent/                     The Agent is a prototype for an Information Agent system. It is       both platform and language independent, as it stores contained       information in simple packed strings. It can be packed and shipped       across any network with any format, as it freezes itself in its       current state.                   AGENT TCL        Web site:  www.cs.dartmouth.edu/~agent/        FTP site:  ftp.cs.dartmouth.edu/pub/agents/                     A transportable agent is a program that can migrate from machine       to machine in a heterogeneous network.  The program chooses when and       where to migrate.  It can suspend its execution at an arbitrary point,       transport to another machine and resume execution on the new machine.       For example, an agent carrying a mail message migrates first to a       router and then to the recipient's mailbox.  The agent can perform       arbitrarily complex processing at each machine in order to ensure that       the message reaches the intended recipient.                   Aglets Workbench        Web site:  www.trl.ibm.co.jp/aglets/                     An aglet is a Java object that can move from one host on the       Internet to another.  That is, an aglet that executes on one host can       suddenly halt execution, dispatch to a remote host, and resume       execution there. When the aglet moves, it takes along its program code       as well as its state (data). A built-in security mechanism makes it       safe for a computer to host untrusted aglets. The Java Aglet API       (J-AAPI) is a proposed public standard for interfacing aglets and       their environment. J-AAPI contains methods for initializing an aglet,       message handling, and dispatching, retracting,       deactivating/activating, cloning, and disposing of the aglet. J-AAPI       is simple, flexible, and stable. Application developers can write       platform-independent aglets and expect them to run on any host that       supports J-AAPI.                   Ara        Web site:  www.uni-kl.de/AG-Nehmer/Ara/                     Ara is a platform for the portable and secure execution of       mobile agents in heterogeneous networks. Mobile agents in this       sense are programs with the ability to change their host machine       during execution while preserving their internal state. This       enables them to handle interactions locally which otherwise had       to be performed remotely. Ara's specific aim in comparison to       similar platforms is to provide full mobile agent functionality       while retaining as much as possible of established programming       models and languages.                    JAFMAS  [New]        Web site:  www.ececs.uc.edu/~abaker/JAFMAS                     JAFMAS provides a framework to guide the coherent development of       multiagent systems along with a set of classes for agent       deployment in Java. The framework is intended to help beginning       and expert developers structure their ideas into concrete agent       applications. It directs development from a speech-act       perspective and supports multicast and directed communication,       KQML or other speech-act performatives and analysis of       multiagent system coherency and consistency.               Only four of the provided Java classes must be extended for any       application. Provided examples of the N-Queens and Supply Chain       Integration use only 567 and 1276 lines of additional code       respectively for implementation.                    JATLite        Web site:  java.stanford.edu/java_agent/html/                     JATLite is providing a set of java packages which makes easy to       build multi-agent systems using Java. JATLite provides only       light-weight, small set of packages so that the developers can       handle all the packages with little efforts. For flexibility       JATLite provides four different layers from abstract to Router       implementation. A user can access any layer we are       providing. Each layer has a different set of assumptions. The       user can choose an appropriate layer according to the       assumptions on the layer and user's application. The       introduction page contains JATLite features and the set of       assumptions for each layer.                   Java(tm) Agent Template        Web site:  cdr.stanford.edu/ABE/JavaAgent.html                     The JAT provides a fully functional template, written entirely in       the Java language, for constructing software agents which communicate       peer-to-peer with a community of other agents distributed over the       Internet. Although portions of the code which define each agent are       portable, JAT agents are not migratory but rather have a static       existence on a single host. This behavior is in contrast to many other       ""agent"" technologies. (However, using the Java RMI, JAT agents could       dynamically migrate to a foreign host via an agent resident on that       host).  Currently, all agent messages use KQML as a top-level protocol       or message wrapper. The JAT includes functionality for dynamically       exchanging ""Resources"", which can include Java classes (e.g. new       languages and interpreters, remote services, etc.), data files and       information inlined into the KQML messages.                   Java-To-Go        Web site:  ptolemy.eecs.berkeley.edu/dgm/javatools/java-to-go/                     Java-To-Go is an experimental infrastructure that assists in the       development and experimentation of mobile agents and agent-based       applications for itinerative computing (itinerative computing:       the set of applications that requires site-to-site       computations. The main emphasis here is on a easy-to-setup       environment that promotes quick experimentation on mobile       agents.                   Kafka  [New]        Web site:  www.fujitsu.co.jp/hypertext/free/kafka/                         Kafka is yet another agent library designed for constructing       multi-agent based distributed applications. Kafka is a       flexible, extendable, and easy-to-use java class library for       programmers who are familiar with distributed programming. It       is based on Java's RMI and has the following added features:           Runtime Reflection:   Agents can modify their behaviour (program codes) at    runtime. The behaviour of the agent is represented by an    abstract class Action. It is useful for remote maintenance or    installation services.   Remote Evaluation:   Agents can receive and evaluate program codes (classes)    with or without the serialized object. Remote evaluation is a    fundamental function of a mobile agent and is thought to be a    push model of service delivery.   Distributed Name Service:   Agents have any number of logical names that don't contain the host    name. These names can be managed by the distributed directories.   Customizable security policy   a very flexible, customizable, 3-layered security model is    implemented in Kafka.   100% Java and RMI compatible:   Kafka is written completely in Java. Agent is a Java RMI    server object itself. So, agents can directly communicate with    other RMI objects.                           Khepera Simulator        Web site:  diwww.epfl.ch/lami/team/michel/khep-sim/                     Khepera Simulator is a public domain software package written by        Olivier MICHEL        during the preparation of his Ph.D. thesis, at the Laboratoire I3S,       URA 1376 of CNRS and University of Nice-Sophia Antipolis, France. It       allows to write your own controller for the mobile robot Khepera using       C or C++ languages, to test them in a simulated environment and       features a nice colorful X11 graphical interface. Moreover, if you own       a Khepera robot, it can drive the real robot using the same control       algorithm. It is mainly oriented toward to researchers studying autonomous       agents.                   Mole        Web site:  www.informatik.uni-stuttgart.de/ipvr/vs/projekte/mole.html                     Mole is an agent system supporting mobile agents programmed in       Java.  Mole's agents consist of a cluster of objects, which have       no references to the outside, and as a whole work on tasks given       by the user or another agent. They have the ability to roam a       network of ""locations"" autonomously. These ""locations"" are an       abstraction of real, existing nodes in the underlying       network. They can use location-specific resources by       communicating with dedicated agents representing these       services. Agents are able to use services provided by other       agents and to provide services as well.                   Odyssey        Web site:  www.genmagic.com/agents/                     Odyssey is General Magic's initial implementation of mobile       agents in 100% pure Java. The Odyssey class libraries enable you       to develop your own mobile agent applications. Use mobile agents       to access data, make decisions and notify users. Your       agent-enabled applications may also take full advantage of the       Java platform and use other third party libraries, for example,       to access remote CORBA objects or to access relational databases       using JDBC. To see how it's done, take a look at the sample       applications included as part of the Odyssey download.                   Penguin!         FTP site:  www.perl.org/CPAN/modules/by-category/23_Miscellaneous_Modules/Penguin/FSG/                     Penguin is a Perl 5 module. It provides you with a set of functions which       allow you to:           send encrypted, digitally signed Perl code to a remote machine to be    executed.   receive code and, depending on who signed it, execute it in an    arbitrarily secure, limited compartment.               The combination of these functions enable direct Perl coding of       algorithms to handle safe internet commerce, mobile       information-gathering agents, ""live content"" web browser helper       apps, distributed load-balanced computation, remote software       update, distance machine administration, content-based       information propagation, Internet-wide shared-data applications,       network application builders, and so on.                   SimRobot        Web site:  www.informatik.uni-bremen.de/~simrobot/          FTP site:  ftp.uni-bremen.de/pub/ZKW/INFORM/simrobot/                     SimRobot is a program for simulation of sensor based robots in a       3D environment. It is written in C++, runs under UNIX and X11 and       needs the graphics toolkit XView.           Simulation of robot kinematics   Hierarchically built scene definition via a simple definition    language   Various sensors built in: camera, facette eye, distance     measurement, light sensor, etc.   Objects defined as polyeders   Emitter abstractly defined; can be interpreted e.g. as    light or sound   Camera images computed according to the raytracing or    Z-buffer algorithms known from computer graphics   Specific sensor/motor software interface for communicating with the    simulation   Texture mapping onto the object surfaces: bitmaps in various formats   Comprehensive visualization of the scene: wire frame w/o    hidden lines, sensor and actor values   Interactive as well as batch driven control of the agents    and operation in the environment   Collision detection   Extendability with user defined object types   Possible socket communication to e.g. the Khoros image processing    software                           TclRobots   FTP site:  ftp.neosoft.com/pub/tcl/sorted/games/tclrobots-2.0/          Redhat Patch:  ftp.coe.uga.edu/users/jae/ai/tclrobots-redhat.patch          RPMs:  ftp://ftp.redhat.com/contrib/                      TclRobots is a programming game, similar to 'Core War'.  To play       TclRobots, you must write a Tcl program that controls a robot.  The       robot's mission is to survive a battle with other robots.  Two, three,       or four robots compete during a battle, each running different       programs (or possibly the same program in different robots.)  Each       robot is equipped with a scanner, cannon, drive mechanism.  A single       match continues until one robot is left running.  Robots may compete       individually, or combine in a team oriented battle.  A tournament       can be run with any number of robot programs, each robot playing every       other in a round-robin fashion, one-on-one.  A battle simulator is       available to help debug robot programs.               The TclRobots program provides a physical environment, imposing       certain game parameters to which all robots must adhere.  TclRobots       also provides a view on a battle, and a controlling user interface.       TclRobots requirements: a wish interpreter built from Tcl 7.4 and Tk       4.0.                   The Tocoma Project        Web site:  www.cs.uit.no/DOS/Tacoma/index.html                     An agent is a process that may migrate through a computer network       in order to satisfy requests made by clients. Agents are an attractive       way to describe network-wide computations.               The TACOMA project focuses on operating system support for agents and       how agents can be used to solve problems traditionally addressed by       operating systems. We have implemented a series of prototype systems       to support agents.               TACOMA Version 1.2 is based on UNIX and TCP. The system supports       agents written in C, Tcl/Tk, Perl, Python, and Scheme (Elk). It is       implemented in C. This TACOMA version has been in public domain since       April 1996.               We are currently focusing on heterogeneity, fault-tolerance, security       and management issues. Also, several TACOMA applications are under       construction. We implemented StormCast 4.0, a wide-area network       weather monitoring system accessible over the internet, using TACOMA       and Java. We are now in the process of evaluating this application,       and plan to build a new StormCast version to be completed by June       1997.                    Virtual Secretary Project (ViSe)  (Tcl/Tk) [New]        Web site:  www.cs.uit.no/DOS/Virt_Sec                     The motivation of the Virtual Secretary project is to construct       user-model-based intelligent software agents, which could in       most cases replace human for secretarial tasks, based on modern       mobile computing and computer network. The project includes two       different phases: the first phase (ViSe1) focuses on information       filtering and process migration, its goal is to create a secure       environment for software agents using the concept of user       models; the second phase (ViSe2) concentrates on agents'       intelligent and efficient cooperation in a distributed       environment, its goal is to construct cooperative agents for       achieving high intelligence. (Implemented in Tcl/TclX/Tix/Tk)                    VWORLD        Web site:  www.ai.uga.edu/~jae/projects.html#vworld                     Vworld is a simulated environment for research with autonomous       agents written in prolog. It is currently in something of an       beta stage. It works well with SWI-prolog, but should work with       Quitnus-prolog with only a few changes.  It is being designed to       serve as an educational tool for class projects dealing with       prolog and autonomous agents. It comes with three demo worlds or       environments, along with sample agents for them.  There are       two versions now. One written for SWI-prolog and one written for       LPA-prolog. Documentation is roughly done (with a       student/professor framework in mind), and a graphical interface       is planned.                     WebMate   Web site:  www.cs.cmu.edu/~softagents/webmate/                       WebMate is a personal agent for World-Wide Web browsing and       searching. It accompanies you when you travel on the internet       and provides you what you want.                Features include:           Searching enhancement, including parallel search, searching keywords refinement using our relevant keywords extraction technology, relevant feedback, etc.   Browsing assistant, including learning your current interesting, recommending you new URLs according to your profile and selected resources, monitoring bookmarks of Netscape or IE, sending the current browsing page to your friends, etc.   Offline browsing, including downloading the following pages from the current page for offline browsing.   Filtering HTTP header, including recording http header and all the transactions between your browser and WWW servers, etc.   Checking the HTML page to find the errors or dead links,  etc.   Programming in Java, independent of operating system, runing in multi-thread.                                             AI & Alife related newsgroups       These newsgroups are not Linux specific. But they are good     resources for anyone working in artificial intelligence or     artificial life. If you can't access these newsgroups, many of     their FAQs are available at:         http://www.cis.ohio-state.edu/hypertext/faq/bngusenet/comp/ai/top.html                comp.ai         comp.ai.edu         comp.ai.genetic         comp.ai.neural-nets         comp.ai.vision         comp.ai.fuzzy         comp.ai.games         comp.ai.jair.announce         comp.ai.jair.papers         comp.ai.nat-lang         comp.ai.nlang-know-rep         comp.ai.philosophy         comp.ai.shells         comp.ai.alife         comp.ai.doc-analysis.misc         comp.ai.doc-analysis.ocr         comp.lang.prolog         comp.lang.lisp         alt.irc.bots                    AI & Alife resource links       These are a few of the many AI and Alife sites out there. These sites     are good places to start hunting for more information or for finding     software.  I'll be adding more links to this list soon, as well as     organizing it better. These links are not Linux specific, but I wanted     to include them to provide a jump off point to the huge amount of info     related to these topics located on the web.             AI/Alife Archives, Research, Bibliographies, etc.              All Catagories              SAL.KachinaTech.COM/Z/3/ -Scientific Applications for Linux's AI page        www.cs.cmu.edu/Web/Groups/AI/html/repository.html -CMU Artificial Intelligence Repository        liinwww.ira.uka.de/bibliography/Ai/index.html  -Bibliographies on Artificial Intelligence  [New]             Traditional AI              www.mcs.net/~jorn/html/ai.html -Outsider's Guide to AI        www.cs.cmu.edu/afs/cs.cmu.edu/project/ai-repository/ai/html/cltl/clm/clm.html -Common Lisp Book        www.elwoodcorp.com/alu/table/contents.htm -The Association of Lisp Users        www.cs.indiana.edu/scheme-repository/home.html -Scheme repository        www.aic.nrl.navy.mil/ -Navy Center for Applied Research in Artificial Intelligence        intranet.ca/~sshah/waste/waste.html -WASTE (AI Contest)        www.cs.washington.edu/research/jair/home.html -Journal of Artificial Intelligence Research        www.cs.ucl.ac.uk/misc/ai/ -University of London's AI Resource Page        www.cris.com/~swoodcoc/ai.html  -Artificial Intelligence in Games        strips.lboro.ac.uk/bib/  -An Online Bibliography on Planning and Scheduling           Connectionism              www.neuronet.ph.kcl.ac.uk/neuronet/software/software.html -NEuroNet - ANN software           Evolutionary Computing              ftp://ftp.mad-scientist.com/pub/genetic-programming/ -John Koza's Genetic Programming archive.        www.cpsc.ucalgary.ca/~jacob/Evolvica/  -Tutorial: Evolutionary Algorithms in Action        alife.santafe.edu/~joke/encore/ -ENCORE-The Hitch-Hikers Guide to Evolutionary Computation        gal4.ge.uiuc.edu/illigal.home.html -IlliGAL Home Page (GA's)        isl.msu.edu/GA/ -MSU GARAGe Home Page (GA's)        www.aracnet.com/~wwir/NovaGenetica/ -Nova Genetica (GA's)           Artifical Life              alife.santafe.edu/~joke/zooland/ -ZooLand Artificial Life Resources        alife.santafe.edu -Santafe's Alife page        www.krl.caltech.edu/~brown/alife/ -Alife FAQ        www.cogs.susx.ac.uk/users/ezequiel/alife-page/alife.html -ALife Bibliography        complex.csu.edu.au/complex/ -Complex Systems Information Network        www.geneticprogramming.com/ -The Genetic Programming Notebook        gracco.irmkant.rm.cnr.it/luigi/lupa_algames.html -The Artificial Life Games Homepage        www.krl.caltech.edu/~charles/alife-game/ -Project: Von Neumann           Agents & Bots              agents.www.media.mit.edu/groups/agents/ -MIT Media Lab, Autonomous Agents Group        www.cs.umbc.edu/agents/agentnews/ -AgentNews Webletter        www.cs.umbc.edu/agents -UMBC AgentWeb (includes KQML info) [New]        www.agent.org -Agent Society Home Page        www.botspot.com/main.html -The BotSpot (a software agent resource page) [New]        www.cselt.it/fipa/ -FIPA Foundation for Intelligent Physical Agents        www.robotmag.com/ -Robot Magazine        www.geocities.com/SiliconValley/3086/robots/index2.htm -Intro to Indexing Bots (aka spiders) [New]                    luz.cs.nmt.edu/~rtlinux -Real time linux (for robotics, etc)"
